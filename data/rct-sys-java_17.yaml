- en: Chapter 13\. Observing Reactive and Event-Driven Architectures
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章 观察反应式和事件驱动架构
- en: 'So far, we’ve focused on how to develop reactive systems. What we haven’t discussed
    is how to ensure that all the components of our reactive system are functioning
    as we expect them to. This is the focus of the chapter: how we monitor and observe
    our reactive and event-driven architecture.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们专注于如何开发反应式系统。但我们还没有讨论的是如何确保我们的反应式系统的所有组件都按照我们期望的方式运行。这是本章的重点：我们如何监视和观察我们的反应式和事件驱动架构。
- en: Why Is Observability Important?
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么可观测性很重要？
- en: When an application is a single deployment, or *monolith*, we have a relatively
    easy time observing how the application is performing. Everything we need to observe
    is in one place. Whether it’s checking logs for errors, monitoring the utilization
    of CPU and memory, or any other aspect, it’s all accessible.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序是单一部署或*单体应用*时，我们很容易观察应用程序的运行情况。我们需要观察的一切都在一个地方。无论是检查错误日志，监控CPU和内存的利用率，还是其他任何方面，都可以轻松访问。
- en: With a reactive and event-driven architecture, instead of one deployment, it’s
    often several, dozens, or even hundreds. We’re no longer dealing with a single
    place to view the information we need to monitor and observe, but many places!
    Observability tooling provides a means for us to gather this information and provide
    a single place to view it again.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 具有反应式和事件驱动架构时，通常不再只有一个部署，而是几个、数十个，甚至数百个。我们不再处理单一地查看需要监控和观察的信息的地方，而是有许多地方！可观测性工具提供了一种方法，使我们能够收集这些信息，并提供一个单一的查看位置。
- en: 'However, we need to gather the necessary information, or telemetry, from the
    components in the event-driven architecture to enable a singular view. *Telemetry*
    consists of any information we gather from processes for the purpose of observing
    a system. The most common types of telemetry are as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们需要从事件驱动架构的组件中收集必要的信息或遥测数据，以实现单一视图。*遥测*包括我们从过程中收集的任何信息，用于观察系统。最常见的遥测类型如下：
- en: Logs
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 日志
- en: Textual messages often written to console output, logfiles, or exported to specific
    log-processing systems. We can also provide more structured logging in a JSON
    format to facilitate more accurate data extraction.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 文本消息通常写入控制台输出、日志文件或导出到特定的日志处理系统。我们还可以提供以JSON格式呈现的更结构化的日志记录，以便更精确地提取数据。
- en: Metrics
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 指标
- en: 'A single metric measures a specific piece of information, such as HTTP server
    requests. Various types of metrics are available: counter, gauge, timer, and histogram,
    to name a few.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 单个度量衡量特定的信息片段，例如HTTP服务器请求。有各种类型的度量指标可用：计数器、仪表、计时器和直方图等。
- en: Traces
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪
- en: Represents a single request through a system, broken into specific operations.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 代表系统中的单个请求，分解为特定操作。
- en: When running distributed systems utilizing reactive or event-driven architecture,
    we need solid telemetry produced from the components to support sufficient reasoning
    about the system. Without being able to reason about the system based on what
    we can observe from the outside, our reactive system is not truly observable.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行利用反应式或事件驱动架构的分布式系统时，我们需要从组件产生可靠的遥测数据，以支持对系统的足够推理。如果无法根据从外部观察到的信息对系统进行推理，那么我们的反应式系统就无法真正地被观察到。
- en: Let’s clarify some terms. *Monitoring* and *observability* can be conflated
    to mean the same thing. Though there are overlaps, they do mean different things.
    Monitoring focuses on specific metrics and measuring them against specific goals,
    service-level objectives (SLOs), and alerting operations when those goals are
    not met. Monitoring is also called *known unknowns*, as we know what data, or
    metrics, to measure to see a problem, but we don’t know what might cause a specific
    problem. *Unknown unknowns* refers to observability, because we don’t know what
    will cause a problem, and when one occurs, it requires observation of a system
    from its outputs to determine the cause.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们澄清一些术语。*监控*和*可观测性*有时可以混为一谈，但它们实际上有不同的含义。监控侧重于特定的度量指标，并根据特定目标、服务级别目标（SLOs）进行测量，并在未达到这些目标时发出警报。监控也被称为*已知未知*，因为我们知道要测量什么数据或指标来发现问题，但我们不知道可能导致特定问题的原因。*未知未知*指的是可观测性，因为我们不知道什么会导致问题，当问题发生时，需要观察系统的输出以确定原因。
- en: Kubernetes is a great place to run reactive systems, as it provides the mechanism
    to monitor, scale, and repair a system gracefully. However, we need to provide
    information for Kubernetes to do that properly, such as with health checks. Health
    checks can serve many purposes; for our needs, the readiness and liveness probes
    in Kubernetes can utilize them. *Readiness probes* let Kubernetes know a container
    is ready to begin accepting requests, and *liveness probes* let Kubernetes know
    if a container needs to be restarted because of unrecoverable failures when communicating
    with Kafka.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是运行反应式系统的好地方，因为它提供了监视、扩展和优雅修复系统的机制。然而，我们需要为 Kubernetes 提供适当的信息，例如健康检查。健康检查可以有多种用途；对于我们的需求，Kubernetes
    中的准备探针和存活探针可以利用它们。*准备探针*让 Kubernetes 知道容器已准备好开始接受请求，而*存活探针*让 Kubernetes 知道是否需要重启容器，因为与
    Kafka 通信时出现不可恢复的故障。
- en: Throughout the rest of the chapter, we explain how to effectively monitor and
    observe reactive systems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将解释如何有效地监控和观察反应式系统。
- en: Health with Messaging
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用消息进行健康检查
- en: Kubernetes utilizes health checks to determine the state of a container. If
    containers don’t provide health checks, Kubernetes is unable to determine the
    state of a container. This may result in users experiencing errors caused by deadlocked
    containers that cannot be stopped or by containers that are not ready to process
    requests.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 利用健康检查来确定容器的状态。如果容器未提供健康检查，Kubernetes 将无法确定容器的状态。这可能导致用户因死锁容器或未准备好处理请求的容器而遇到错误。
- en: 'We can implement three types of health checks for our containers:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以为容器实现三种类型的健康检查：
- en: Liveness Probe
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 存活探针
- en: This probe lets Kubernetes know a container should be restarted. If we can write
    a meaningful health check, it’s a good way to catch situations of application
    deadlock or connection issues with external systems. We can possibly resolve intermittent
    issues by allowing a clean slate by restarting the container. The probe is periodically
    run based on the frequency we define. We want to ensure that the frequency is
    not too large, so we prevent containers being stuck for long periods of time,
    but not too small either, as that would increase resource consumption.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此探针让 Kubernetes 知道应重启容器。如果我们能编写一个有意义的健康检查，这是捕获应用程序死锁或与外部系统连接问题的好方法。我们可能通过重新启动容器来解决间歇性问题。该探针基于我们定义的频率定期运行。我们希望确保频率不要太大，以避免容器长时间被卡住，但也不要太小，以避免增加资源消耗。
- en: Readiness Probe
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 准备探针
- en: This probe informs Kubernetes when a container is ready to begin receiving traffic
    from a service. We can use this type of health check to provide enough time for
    HTTP servers and connections to external systems to be available before we begin
    accepting requests. This prevents users from experiencing errors because the container
    was not ready to process a request. This probe executes only once during the life
    of a container. The readiness probe is necessary to effectively allow scaling
    up without causing undue user errors.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个探针告知 Kubernetes，容器已准备好接收来自服务的流量。我们可以利用这种健康检查来为 HTTP 服务器和与外部系统的连接提供足够的时间，在开始接受请求之前等待它们可用。这可以防止用户因为容器未准备好处理请求而遇到错误。这个探针在容器生命周期中只执行一次。准备探针对于有效地允许扩展而不引起不必要的用户错误是必要的。
- en: Startup Probe
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 启动探针
- en: A recent health check addition, this probe has a similar purpose as the liveness
    probe. However, this probe allows us to set a different wait period before declaring
    the container unhealthy. This is especially beneficial in situations where a container
    could take a very long time to be alive, possibly due to connecting with legacy
    systems. We’re able to set a shorter time-out for a *Liveness Probe*, while allowing
    a much longer time-out for a *Startup Probe*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最近添加的健康检查，此探针与存活探针具有相似的目的。然而，此探针允许我们在声明容器不健康之前设置不同的等待时间。这在容器可能需要很长时间才能启动，可能是由于与遗留系统连接时尤为有益。我们可以为*存活探针*设置较短的超时，同时允许*启动探针*设置更长的超时。
- en: Each of these probes supports HTTP, TCP, or commands run inside the container
    itself. Nothing prevents other protocols from being used for probes, but they’re
    currently not implemented in Kubernetes. Which probe we use for an application
    will depend on whether there are HTTP endpoints we can utilize for the probes,
    or whether we need custom commands within the container. Quarkus has an extension
    for [SmallRye Health](https://github.com/smallrye/smallrye-health) to develop
    health checks available over HTTP.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 每个探针都支持 HTTP、TCP 或在容器内运行的命令。目前 Kubernetes 尚未实现其他协议用于探针。我们选择应用程序使用哪种探针将取决于是否有可用于探测的
    HTTP 端点，或者是否需要在容器内部使用自定义命令。Quarkus 有一个扩展支持 [SmallRye 健康检查](https://github.com/smallrye/smallrye-health)，可通过
    HTTP 开发健康检查。
- en: 'How do these probes relate to a reactive application? Readiness indicates that
    a Reactive Messaging connector, such as Kafka, has successfully connected to the
    broker, or backend, there were no failures, and optionally the topic we intend
    to use exists in the broker. In this state, the connector is ready to begin sending
    or receiving messages. Verifying the presence of any topics is disabled by default
    because it’s a lengthy operation requiring use of the admin client. Enabling topic
    verification is done by setting `health-readiness-topic-verification: true`.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '这些探针与响应式应用程序有什么关系？就绪性表示响应式消息连接器（如 Kafka）已成功连接到代理或后端，并且没有失败，并且可选地我们打算使用的主题在代理中存在。在此状态下，连接器准备好开始发送或接收消息。默认情况下，验证任何主题的存在是禁用的，因为这是一个耗时的操作，需要使用管理员客户端。启用主题验证通过设置
    `health-readiness-topic-verification: true` 来完成。'
- en: Liveness should fail when the Reactive Messaging connector has experienced an
    unrecoverable failure or a disconnection from the broker. These types of transient
    failures can disappear after a restart of the container. For example, the application
    may connect to another broker.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当响应式消息连接器遇到不可恢复的故障或与代理断开连接时，存活性应该失败。这些类型的瞬态故障可以在容器重新启动后消失。例如，应用程序可能连接到另一个代理。
- en: As we covered in [“Apache Kafka”](ch11.html#_kafka_101), Kafka has built-in
    resilience. The last committed offset is not updated until a consumer has successfully
    processed the record, ensuring that records are not forgotten if a consumer fails
    while processing it. Also, Kafka is able to rebalance consumers, within the same
    consumer group, if any of them fail. Any consumer(s) that might crash while processing
    records from a partition will be replaced with other consumers from the same group.
    When using Kubernetes health checks, the consumers will be rebalanced when containers
    stop, and re-balanced again when Kubernetes has started new instances of the containers.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在 [“Apache Kafka”](ch11.html#_kafka_101) 中介绍的，Kafka 具有内置的韧性。只有在消费者成功处理记录后，最后提交的偏移量才会更新，确保记录不会因消费者在处理过程中失败而被遗忘。此外，如果任何消费者失败，Kafka
    能够重新平衡消费者组内的消费者。当使用 Kubernetes 健康检查时，当容器停止时，消费者将重新平衡，并在 Kubernetes 启动新容器实例时再次重新平衡。
- en: It is now time to see how it all works with an example. We will take the example
    from [Chapter 11](ch11.html#event-bus) and extend it. We want to customize the
    consumer to highlight the behaviors of health checks. We will have a specific
    process service, `processor-health`. You can find the complete code in the *chapter-13*
    directory.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候通过一个示例看看它是如何工作的了。我们将使用 [第11章](ch11.html#event-bus) 中的示例并进行扩展。我们想要自定义消费者以突出健康检查的行为。我们将有一个特定的进程服务，名为
    `processor-health`。你可以在 *chapter-13* 目录下找到完整的代码。
- en: First we need to add the extension for SmallRye Health to the *pom.xml* of each
    service, as shown in [Example 13-1](#dependency-health-support).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在每个服务的 *pom.xml* 中添加 SmallRye 健康扩展，如 [示例 13-1](#dependency-health-support)
    所示。
- en: Example 13-1\. Dependency for the health support (*chapter-13/processor-health/pom.xml*)
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 13-1\. 健康支持的依赖项 (*chapter-13/processor-health/pom.xml*)
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To generate the necessary Kubernetes deployment YAML, including the liveness
    and readiness probes, we need the Kubernetes extension. In this case, though,
    we use the minikube extension as we’re deploying to it; see [Example 13-2](#dependency-minikube-deployment-feature).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成必要的 Kubernetes 部署 YAML 文件，包括存活检测和就绪检测，我们需要 Kubernetes 扩展。在这种情况下，我们使用 minikube
    扩展进行部署；请参阅 [示例 13-2](#dependency-minikube-deployment-feature)。
- en: Example 13-2\. Dependency for the minikube deployment feature (*chapter-13/processor-health/pom.xml*)
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 13-2\. minikube 部署功能的依赖项 (*chapter-13/processor-health/pom.xml*)
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Run `mvn clean package` in the */chapter-13* directory to generate the deployment
    YAML. Take a look in the */target/kubernetes* directory of one of the modules
    and view the generated YAML. We see the desired liveness and readiness probes
    added to the deployment specification.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在 */chapter-13* 目录中运行 `mvn clean package` 以生成部署 YAML。在任何一个模块的 */target/kubernetes*
    目录中查看生成的 YAML。我们看到所需的活跃探测和准备探测已添加到部署规范中。
- en: By default, the period between each liveness probe request is 30 seconds. Let’s
    reduce it to 10 seconds to enable Kubernetes to restart our consumer, `processor-health`,
    sooner if there are problems by modifying `application.properties` ([Example 13-3](#configure-liveness-probe)).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，每个活跃探测请求之间的间隔为30秒。让我们将其减少到10秒，以便在问题发生时使 Kubernetes 更早地重新启动我们的消费者`processor-health`，方法是修改`application.properties`（参见[示例 13-3](#configure-liveness-probe)）。
- en: Example 13-3\. Configure the liveness probe (*chapter-13/processor-health/src/main/resources/application.properties*)
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 13-3\. 配置活跃探测 (*chapter-13/processor-health/src/main/resources/application.properties*)
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Example 13-4](#observability:nack) shows how to modify `Processor` to simulate
    failures.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 13-4](#observability:nack) 展示了如何修改 `Processor` 以模拟故障。'
- en: Example 13-4\. Processor to nack every eighth message received (*chapter-13/processor-health/src/main/java/org/acme/Processor.java*)
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 13-4\. 处理器每接收八条消息否定一次（*chapter-13/processor-health/src/main/java/org/acme/Processor.java*）
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](assets/1.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-1)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-1)'
- en: Use manual acknowledgment, so we can nack messages explicitly.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用手动确认，这样我们可以显式地否定消息。
- en: '[![2](assets/2.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-2)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-2)'
- en: We need to change the method signature to use `Message` instead of `Long` to
    use manual acknowledgment.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 需要将方法签名更改为使用`Message`而不是`Long`以使用手动确认。
- en: '[![3](assets/3.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-3)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-3)'
- en: Every eighth message should be nacked, and we return a `null` instead.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每接收八条消息应否定一次，并返回`null`。
- en: '[![4](assets/4.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-4)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-4)'
- en: Explicitly ack a message.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 显式确认一条消息。
- en: 'As the default `failure-strategy` is `fail`, when we nack a message, the processing
    of messages fails. This message failure will cause the health check of the consumer
    to also fail, triggering a container restart once the next liveness probe runs.
    Refer to [“Kafka on Kubernetes”](ch11.html#bus::install-kafka), or the *README*
    of */chapter-13*, to start minikube and deploy Kafka. Then, run `m⁠v⁠n⁠ v⁠e⁠r⁠i⁠f⁠y⁠ ‑D⁠q⁠u⁠a⁠r⁠k⁠u⁠s⁠.​k⁠u⁠b⁠e⁠r⁠n⁠e⁠t⁠e⁠s⁠.d⁠e⁠p⁠l⁠o⁠y⁠=t⁠r⁠u⁠e`
    for each of the three services: ticker, viewer, processor. Verify that all three
    services are running with `kubectl get pods`.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于默认的`failure-strategy`是`fail`，当我们否定一个消息时，消息处理失败。此消息失败将导致消费者的健康检查也失败，一旦下一个活跃探测运行，容器将重新启动。请参考[“Kubernetes
    上的 Kafka”](ch11.html#bus::install-kafka)，或 */chapter-13* 的 *README*，启动 minikube
    并部署 Kafka。然后，对三个服务（ticker、viewer、processor）分别运行 `mvn verify -Dquarkus.kubernetes.deploy=true`。使用
    `kubectl get pods` 验证所有三个服务正在运行。
- en: With the services deployed, we can see the overall health check status by accessing
    `/q/health` of a service. We get the response shown in [Example 13-5](#observability:health-output)
    for the `processor-health` service.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 部署服务后，可以通过访问服务的 `/q/health` 查看整体健康检查状态。对于 `processor-health` 服务，我们得到如 [示例 13-5](#observability:health-output)
    所示的响应。
- en: Example 13-5\. Reactive application health check with no errors
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 13-5\. 反应式应用程序健康检查，没有错误
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO2-1)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO2-1)'
- en: The data within the check shows the channels we’re connected to and their respective
    status.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 检查中的数据显示我们连接的通道及其各自的状态。
- en: We saw, when viewing the generated deployment YAML, that there are also `/q/health/live`
    and `/q/health/ready` endpoints. These represent the liveness and readiness probes,
    respectively. Access them in a browser, or via `curl`, to see the specific checks
    of each probe.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看生成的部署 YAML 时，我们看到还有 `/q/health/live` 和 `/q/health/ready` 端点。它们分别代表存活探针和就绪探针。在浏览器中或通过
    `curl` 访问它们，以查看每个探针的具体检查。
- en: Open up the *VIEWER_URL*, from the terminal, in a browser. Based on the producer
    we’ve defined, we will see seven messages with the same processor pod name, before
    it hit the message that we nacked. There will be a pause while Kubernetes restarts
    the container; then we will see another seven messages, and this sequence repeats.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从终端中在浏览器中打开 *VIEWER_URL*。基于我们定义的生产者，我们会看到带有相同处理器 pod 名称的七条消息，然后再命中我们 nacked
    的消息。在 Kubernetes 重新启动容器时会有一个暂停，然后我们会看到另外七条消息，这个序列会重复。
- en: If we take a look at the pods in Kubernetes, we can see that the container for
    the processor service has been restarted, as shown in [Example 13-6](#use-kubectl-to-list-pods).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看 Kubernetes 中的 pods，我们可以看到处理器服务的容器已经重启，如 [示例 13-6](#use-kubectl-to-list-pods)
    所示。
- en: Example 13-6\. Use `kubectl` to list pods
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 13-6\. 使用 `kubectl` 列出 pods
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: After several restarts in a short amount of time, the pod will be in the state
    of `CrashLoopBackoff`, which will slowly increase the delay between pod restarts.
    As we don’t have a “happy” container for at least 10 minutes, we end up in a state
    where the pod will not restart for a while. That’s not a problem for these examples,
    but is worth noting.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在短时间内多次重启后，pod 将处于 `CrashLoopBackoff` 状态，这将逐渐增加 pod 重启之间的延迟。由于至少 10 分钟没有“happy”容器，我们最终处于
    pod 将暂时不会重启的状态。对于这些示例来说，这并不是问题，但值得注意。
- en: When viewing the health checks at `/q/health`, it can be difficult to “catch”
    the failed health check before the container restarts. To make it easier, we can
    modify the `quarkus.kubernetes.liveness-probe.period` of the processor service
    to a large period of time, like `100s`. With a longer period, we give ourselves
    a chance to view the failed health check before the container restarts, as shown
    in [Example 13-7](#observability:check-fail).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 `/q/health` 查看健康检查时，在容器重新启动之前“捕捉”到失败的健康检查可能会有些困难。为了更容易些，我们可以将处理器服务的 `quarkus.kubernetes.liveness-probe.period`
    修改为较长的时间，例如 `100s`。通过延长周期，我们有机会在容器重新启动之前查看到失败的健康检查，如 [示例 13-7](#observability:check-fail)
    所示。
- en: Example 13-7\. Reactive application health check with errors
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 13-7\. 带有错误的响应式应用健康检查
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-1)'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-1)'
- en: Liveness check is `DOWN`, causing the entire health check to be `DOWN`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 存活检查是 `DOWN`，导致整个健康检查为 `DOWN`。
- en: '[![2](assets/2.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-2)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-2)'
- en: The `ticks` channel is not OK, showing the failure from the exception sent in
    `nack`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`ticks` 通道出现问题，显示了从 `nack` 发送的异常的失败。'
- en: '[![3](assets/3.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-3)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-3)'
- en: There is no subscription, because the `process` method has failed and is no
    longer subscribing. The `ticks` channel is still OK; it’s just waiting for a subscription.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 没有订阅，因为 `process` 方法失败且不再订阅。`ticks` 通道仍然正常，只是等待订阅。
- en: We can now check the health of our applications and utilize them in the container
    orchestration of Kubernetes. Next, we see how our reactive applications can generate
    metrics for monitoring and utilize those metrics for autoscaling.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以检查我们应用的健康，并在 Kubernetes 的容器编排中利用它们。接下来，我们看看我们的响应式应用如何生成用于监控的度量，并利用这些度量进行自动扩展。
- en: Metrics with Messaging
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带有消息的度量
- en: 'Metrics are a critical part of our applications, even more so with reactive
    applications. Metrics can feed monitoring systems for alerting operations and
    SREs of problems in applications. Before delving into how we can do that, let’s
    explain some monitoring-related terms:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 度量是我们应用的关键部分，尤其是在响应式应用中更为重要。度量可以向监控系统提供信息，用于警报操作和 SRE 发现应用中的问题。在深入讨论如何做到这一点之前，让我们解释一些与监控相关的术语：
- en: SLA (service-level agreement)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: SLA（服务级别协议）
- en: A contract between a service provider and its customers as to the availability,
    performance, etc. of the service.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 服务提供商与其客户之间关于服务可用性、性能等的合同。
- en: SLO (service-level objective)
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: SLO（服务水平目标）
- en: A goal for a service provider to reach. SLOs are internal goals used to help
    prevent a service provider from breaking an SLA with its customers. Developers
    define rules, or thresholds, for the SLOs of a service to alert Operations or
    SREs when we’re at risk of breaking SLAs.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 服务提供商的目标是达到的。SLO是内部目标，用于帮助防止服务提供商违反与客户的SLA。开发人员为服务的SLO定义规则或阈值，以便在有风险违反SLA时向运维或SRE发出警报。
- en: SLI (service-level indicator)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: SLI（服务水平指标）
- en: A specific measurement used in measuring an SLO. These are the metrics we generate
    from a reactive application.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 用于衡量SLO的特定测量。这些是从响应式应用生成的指标。
- en: If an organization doesn’t define SLAs, SLOs, and SLIs, that’s OK. It’s still
    beneficial to gather metrics from a reactive application to at least define the
    thresholds indicating when everything is OK and when it is not. A “good” metric
    for a particular reactive application can differ depending on the specific use
    case.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果组织没有定义SLA、SLO和SLI，那也没关系。从响应式应用中收集指标仍然有益，至少可以定义指示一切正常和不正常时的阈值。对于特定用例，一个“好”的指标可能会有所不同。
- en: 'However, all reactive systems should be gathering and monitoring certain metrics:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，所有的响应式系统都应该收集和监控某些指标：
- en: Queue length
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 队列长度
- en: If the queue of messages waiting to be processed is too large, it impacts the
    speed at which messages flow through the system. If messages aren’t flowing fast
    enough, a time-sensitive reactive application, such as stock trading, will see
    delays and problems as a result. High queue length is an indication we need to
    increase the number of consumers within a consumer group. It may also indicate
    that we need to increase the number of partitions for a topic if we’re already
    at the maximum number of consumers.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果等待处理的消息队列过大，将影响消息在系统中流动的速度。如果消息流动不够快，例如股票交易这样对时间敏感的响应式应用将会因此而出现延迟和问题。高队列长度表明我们需要增加消费者组中的消费者数量。如果我们已经达到最大消费者数量，这也可能表明我们需要增加主题的分区数量。
- en: Processing time
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 处理时间
- en: When a consumer takes too long to process a message, it will likely cause an
    increase in queue length. Long processing times can also indicate other issues
    with a reactive application, dependent on what work a consumer does. We could
    see network latency issues because of another service we’re interacting with,
    database contention, or any other number of possible problems.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当消费者处理消息花费太长时间时，很可能会导致队列长度增加。长时间的处理也可能表明响应式应用存在其他问题，这取决于消费者的工作内容。由于与另一个服务的交互，数据库争用或其他可能的问题，我们可能会看到网络延迟问题。
- en: Messages processed in a time window
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在时间窗口内处理的消息
- en: This metric provides an overall understanding of the throughput of a reactive
    application. Knowing the actual number of messages processed is likely less important
    than monitoring variations. A significant drop could indicate a problem in messages
    not being received, or large numbers of customers leaving the application too
    early.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此指标提供对响应式应用吞吐量的整体理解。了解实际处理的消息数量可能不如监控变化重要。显著下降可能表明存在消息未被接收的问题，或者大量客户提前离开应用。
- en: Ack-to-nack ratio
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Ack-to-nack 比率
- en: We want this metric to be as high as possible, as it means we’re not seeing
    many failures in the messages we process. If too many failures occur, we need
    to investigate whether it’s due to upstream systems providing invalid data, or
    failures in the processor to handle different data types properly.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这个指标尽可能高，因为这意味着我们在处理的消息中看不到太多故障。如果发生太多故障，我们需要调查是由于上游系统提供了无效数据，还是处理器未能正确处理不同数据类型引起的故障。
- en: All of these metrics we’ve discussed are great for detecting possible bottlenecks
    in a reactive application. We may see several of these metrics go in a bad direction
    at the same time—definitely a sign we have a problem in processing messages! We
    can also define basic rules for detecting bottlenecks. When using HTTP, or request/reply,
    we should check the response time and success rate. High response times or low
    success rates would indicate a problem needing investigation. For messaging applications,
    the number of *in-flight*, not yet processed, messages is a key measurement to
    track.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的所有这些度量都非常适合检测反应式应用程序中可能存在的瓶颈。如果同时看到几个这些度量朝着不利方向发展，这无疑是我们在处理消息中遇到问题的迹象！我们还可以定义检测瓶颈的基本规则。在使用
    HTTP 或请求/响应时，应检查响应时间和成功率。高响应时间或低成功率可能表明需要调查的问题。对于消息传递应用程序，尚未处理的 *in-flight* 消息数量是一个关键的跟踪指标。
- en: We’ve covered a lot of theory, but what do we need to do to capture these metrics?
    The key change is to add the dependency for Micrometer,^([1](ch13.html#idm45358813004512))
    and in this case we want the metrics available in the Prometheus format.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了许多理论知识，但是我们需要做什么来捕获这些度量标准呢？关键的变化是为 Micrometer 添加依赖项^([1](ch13.html#idm45358813004512))，在本例中我们希望以
    Prometheus 格式提供度量标准。
- en: '*Micrometer* is the preferred metrics solution for Quarkus because it offers
    key benefits for developers:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*Micrometer* 是 Quarkus 的首选度量解决方案，因为它为开发者提供了关键的优势：'
- en: Ability to switch the monitoring backend from Prometheus, to Datadog, to Splunk,
    to New Relic, and many others, without needing to modify existing code-creating
    metrics. All that’s required is a dependency change to use a different registry!
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够在不需要修改现有代码创建度量的情况下，从 Prometheus 切换监控后端到 Datadog、Splunk、New Relic 等等。只需更改依赖项以使用不同的注册表！
- en: Provides `MeterBinder` implementations for many of the frameworks used in Quarkus.
    These provide metrics for frameworks such as JAX-RS, Vert.x, and Hibernate, without
    developers needing to specifically code metrics themselves.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供了许多 Quarkus 中使用的框架的 `MeterBinder` 实现。这些为诸如 JAX-RS、Vert.x 和 Hibernate 等框架提供度量标准，而无需开发者专门编写度量代码。
- en: To expose the metrics in the Prometheus format, add the dependency in [Example 13-8](#dependency-micrometer-prometheus-support)
    to your application.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要以 Prometheus 格式公开度量标准，请向您的应用程序添加 [示例 13-8](#dependency-micrometer-prometheus-support)
    中的依赖项。
- en: Example 13-8\. Dependency for the Micrometer Prometheus support (*chapter-13/viewer/pom.xml*)
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 13-8\. Micrometer Prometheus 支持的依赖项 (*chapter-13/viewer/pom.xml*)
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With this dependency, we have an endpoint showing all the metrics of an application
    at `/q/metrics`. When using Prometheus in Kubernetes, we then need only a `ServiceMonitor`
    to inform Prometheus of this endpoint for it to scrape the metrics. For this example,
    we won’t be utilizing Prometheus and Grafana, two common tools for monitoring
    metrics. Plenty of documentation online explains how to set them up in Kubernetes
    for readers to view the metrics in these tools.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个依赖项，我们有一个显示应用程序所有度量的端点位于 `/q/metrics`。在 Kubernetes 中使用 Prometheus 时，我们只需要一个
    `ServiceMonitor` 告知 Prometheus 这个端点以便它抓取度量。对于本示例，我们不会使用 Prometheus 和 Grafana，这两个常见的监控工具。有很多在线文档可以解释如何在
    Kubernetes 中设置它们，以便读者查看这些工具中的度量。
- en: If minikube is not still running from the earlier health example, follow the
    instructions in the *README* of */chapter-13* to start it, deploy Kafka, and build
    and deploy the three services. Verify that they’re running with `kubectl get pods`,
    and then open the URL of the viewer. Once you’ve seen messages appear, open up
    the metrics endpoint for the processor. The URL can be found with `minikube service
    --url observability-processor` and then add `/q/metrics` to the end.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 minikube 尚未从早期健康示例中运行，请按照 */chapter-13* 的 *README* 中的说明启动它，部署 Kafka，并构建和部署三个服务。使用
    `kubectl get pods` 验证它们是否正在运行，然后打开查看器的 URL。一旦看到消息出现，请打开处理器的指标端点。可以使用 `minikube
    service --url observability-processor` 找到该 URL，然后在末尾添加 `/q/metrics`。
- en: You will see metrics such those shown in [Example 13-9](#observability:metrics).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到类似于 [示例 13-9](#observability:metrics) 中显示的度量标准。
- en: Example 13-9\. Sample of metrics for a reactive application
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 13-9\. 反应式应用程序度量的示例
- en: '[PRE8]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Example 13-9](#observability:metrics) shows a condensed version of the metrics
    generated from the processor service, as the complete version would require much
    more space!'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 13-9](#observability:metrics) 显示了从处理器服务生成的度量标准的简化版本，完整版本将需要更多空间！'
- en: Metrics would also enable us to develop a Kubernetes operator to autoscale the
    consumers of a reactive system. The operator can use the Kafka admin API to measure
    the number of messages that have not been consumed. If there are fewer consumers
    than the number of partitions, the operator can scale up the number of replicas
    for a consumer to process more messages in the same time. When the number of un-consumed
    messages drops below a threshold, the operator can then scale back consumers from
    within the consumer group.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 指标还将使我们能够开发一个 Kubernetes 运算符，以自动缩放反应式系统的消费者。该运算符可以使用 Kafka 管理 API 来测量未被消费的消息数量。如果消费者少于分区数，则运算符可以增加一个消费者的副本数量，以在相同的时间内处理更多的消息。当未被消费的消息数量降低到阈值以下时，运算符可以从消费者组内缩减消费者数量。
- en: Distributed Tracing with Messaging
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用消息传递的分布式跟踪
- en: '*Distributed tracing* is an extremely important part of observability for reactive
    systems. When we have a single application deployment, all the interactions usually
    occur within the same process. Additionally, reactive systems have the complexity
    of one service not knowing where, or often when, another service will consume
    the message they’ve created. With nonreactive systems, we’re usually able to infer
    the connections by reading the code to see where outgoing HTTP calls are made.
    That is not possible with a reactive system built around messaging.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '*分布式跟踪* 是反应式系统可观测性的一个极其重要的部分。当我们有一个单一的应用部署时，所有的交互通常发生在同一个进程中。此外，反应式系统具有一个服务不知道另一个服务将在何时何地消耗它们创建的消息的复杂性。在非反应式系统中，我们通常可以通过阅读代码来推断连接，以查看出站
    HTTP 调用的位置。在围绕消息构建的反应式系统中，这是不可能的。'
- en: This is where distributed tracing shines, connecting the many dots—services—in
    a system across space and time to provide an overall perspective on the message
    flows. For the example, we will be using the OpenTelemetry extension, with an
    exporter to send the captured traces to Jaeger.^([2](ch13.html#idm45358812819136))
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是分布式跟踪的优点所在，它能够连接系统中许多点（服务），跨越空间和时间，从而提供消息流的整体视角。在本示例中，我们将使用 OpenTelemetry
    扩展，配合一个导出器将捕获的跟踪发送到 Jaeger。^([2](ch13.html#idm45358812819136))
- en: 'First, though, let’s cover some terminology:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们来了解一些术语：
- en: Span
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Span
- en: A single operation within a trace (defined next). Many spans can be created
    within a single service, depending on the level of detail you want to collect.
    A span can have parent or child spans associated with it, representing a chain
    of execution.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在追踪中的单个操作（在下面定义）。根据您想要收集的详细程度，可以在单个服务中创建许多 span。一个 span 可以有与之关联的父 span 或子 span，代表一系列执行。
- en: Trace
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Trace
- en: A collection of operations, or spans, representing a single request processed
    by an application and its components.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一组操作或 span，代表一个应用及其组件处理的单个请求。
- en: When using Reactive Messaging in Quarkus for Kafka or AMQP, spans are automatically
    created when messages are consumed and when they’re produced. This is done by
    the extension propagating the existing trace and span into the headers of any
    produced message, which is extracted when consuming it. This process allows OpenTelemetry
    to chain together the spans across multiple services in different processes to
    provide a singular view of the flow with Jaeger.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当在 Quarkus 中使用 Kafka 或 AMQP 的反应式消息传递时，当消息被消费和产生时，会自动创建 span。这是通过扩展将现有的跟踪和 span
    传播到任何生成的消息的标头中完成的，在消费时进行提取。此过程允许 OpenTelemetry 跨多个进程中的多个服务链接 span，以提供 Jaeger 中流程的单一视图。
- en: Let’s update the example for distributed tracing! We add the Quarkus extension
    for OpenTelemetry to each service in *pom.xml*, as shown in [Example 13-10](#jaeger-exporter-open-telemetry-dependency).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新分布式跟踪的示例！我们在 *pom.xml* 中为每个服务添加 Quarkus 对 OpenTelemetry 的扩展，如 [示例 13-10](#jaeger-exporter-open-telemetry-dependency)
    中所示。
- en: Example 13-10\. Jaeger exporter for OpenTelemetry dependency
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 13-10\. OpenTelemetry 依赖的 Jaeger 导出器
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For each service to be able to send the gathered spans to Jaeger, we also need
    to update `application.properties` for each service with the URL of the collector
    ([Example 13-11](#jaeger-collector-endpoint)).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使每个服务能够将收集到的 spans 发送到 Jaeger，我们还需要更新每个服务的 `application.properties`，其中包含收集器的
    URL（参见 [示例 13-11](#jaeger-collector-endpoint)）。
- en: Example 13-11\. Jaeger collector endpoint
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 13-11\. Jaeger 收集器端点
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To simplify the deployment of Jaeger, we will deploy the *all-in-one* image,
    as shown in [Example 13-12](#install-jaeger-all-in-one).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化 Jaeger 的部署，我们将部署 *all-in-one* 映像，如 [示例 13-12](#install-jaeger-all-in-one)
    中所示。
- en: Example 13-12\. Install Jaeger all-in-one
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 13-12\. 安装 Jaeger 全功能版
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Details of the Kubernetes deployment and service for Jaeger can be examined
    by reviewing */deploy/jaeger/jaeger-simplest.yaml*. The key point to note is the
    service exposing port 14250 for collecting spans, which is the port we set in
    [Example 13-11](#jaeger-collector-endpoint).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过查看 */deploy/jaeger/jaeger-simplest.yaml* 来检查 Jaeger 的 Kubernetes 部署和服务的详细信息。需要注意的关键点是，服务公开了端口
    14250 用于收集 span，这是我们在 [示例 13-11](#jaeger-collector-endpoint) 中设置的端口。
- en: Retrieve the URL for the Jaeger UI, `minikube service --url jaeger-ui -n jaeger`,
    and open it in a browser. We see the initial page to search for traces, but without
    any services in the drop-down to search for, as nothing is running yet.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 检索 Jaeger UI 的 URL，`minikube service --url jaeger-ui -n jaeger`，在浏览器中打开它。我们看到了用于搜索跟踪的初始页面，但是在下拉菜单中找不到任何服务，因为尚未运行任何服务。
- en: 'Follow the *README* for */chapter-13* to rebuild and redeploy the three services:
    ticker, processor, viewer. Once they’re deployed, open up the viewer URL, `minikube
    service --url observability-viewer`, in a browser to begin receiving messages.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 按照 */chapter-13* 的 *README* 重建和重新部署三个服务：ticker、processor、viewer。部署完成后，在浏览器中打开
    viewer 的 URL，`minikube service --url observability-viewer`，开始接收消息。
- en: Once messages are appearing, go back to the Jaeger UI and refresh the page.
    There will now be four services to select from; choose `observability-ticker`,
    the first service in the reactive stream. Click the Find Traces button to retrieve
    the traces for the service. Select one of the traces from the list to open a view
    containing all the details of the spans ([Figure 13-1](#image:jaeger-ui-trace)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦消息开始出现，返回到 Jaeger UI 并刷新页面。现在将会有四个服务可供选择；选择 `observability-ticker`，这是反应式流中的第一个服务。点击“查找跟踪”按钮以检索该服务的跟踪。从列表中选择一个跟踪以打开包含所有
    spans 详细信息的视图（[图 13-1](#image:jaeger-ui-trace)）。
- en: '![Jaeger UI showing reactive system trace](assets/rsij_1301.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![Jaeger UI showing reactive system trace](assets/rsij_1301.png)'
- en: Figure 13-1\. Jaeger UI showing reactive system trace
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-1\. Jaeger UI showing reactive system trace
- en: In this example, we have four spans within a single trace. There is a span for
    each step in the reactive stream that first produces a message from the ticker
    services, then consumes and produces a message in the processor service, and finally
    consumes the message in the viewer service. In the Jaeger UI, explore the data
    captured within the spans for each step. In [Figure 13-1](#image:jaeger-ui-trace),
    we see the details of the `ticks send` span, including the type of span, producer,
    and the details of where the message was sent.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们在单个跟踪中有四个 spans。每个步骤都有一个 span，首先从 ticker 服务生成一个消息，然后在 processor 服务中消费并生成一个消息，最后在
    viewer 服务中消费该消息。在 Jaeger UI 中，探索每个步骤中 spans 中捕获的数据。在 [图 13-1](#image:jaeger-ui-trace)
    中，我们看到了 `ticks send` span 的详细信息，包括 span 的类型、生产者以及消息发送的详细信息。
- en: Note
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Though *ticks send* is grammatically incorrect for past tense, the name of the
    span is dictated by the semantic conventions of OpenTelemetry in *blob/main/specification/trace/semantic_conventions/messaging.md#span-name.*
    The span name is a combination of the destination, *ticks*, and the operation
    type, *send*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 *ticks send* 在语法上是过去时不正确的，但 span 的名称由 OpenTelemetry 的语义约定所决定，详见 *blob/main/specification/trace/semantic_conventions/messaging.md#span-name.*
    Span 名称是目的地 *ticks* 和操作类型 *send* 的组合。
- en: So far, you’ve seen how to utilize traces when message flows are almost instantaneous
    between services. However, a benefit of reactive systems is being able to decouple
    components within the system according to time. In other words, a message flow
    can take hours, days, weeks, months, or even years to be completed, with messages
    waiting in a topic, for instance, for a lengthy amount of time before being consumed.
    It’s also possible for the same message to be processed by a different consumer,
    such as an audit process, some time after it was originally consumed. Let’s simulate
    a delayed scenario and see how the traces work.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，您已经看到了如何在消息流几乎瞬间传输的情况下利用跟踪。然而，反应式系统的一个好处是能够根据时间解耦系统内的组件。换句话说，消息流可能需要数小时、数天、数周、数月甚至数年才能完成，在主题中等待，例如，在消费之前可能需要很长时间。同一条消息也可能在最初消费后的一段时间内由不同的消费者（如审计过程）处理。让我们模拟一个延迟的场景，看看跟踪是如何工作的。
- en: To start, let’s clear out the existing services with `kubectl delete all --all
    -n default`. To ensure that we’re starting with a clean slate, we should also
    delete and re-create the existing Kafka topics, as shown in [Example 13-13](#update-application-deployments).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用 `kubectl delete all --all -n default` 清除现有的服务。为了确保我们从一个干净的状态开始，我们还应该删除并重新创建现有的
    Kafka 主题，如 [示例 13-13](#update-application-deployments) 所示。
- en: Example 13-13\. Update the application deployments
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 13-13\. 更新应用部署
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: To simulate delayed processing, let’s deploy the ticker service and then remove
    it again after 20–30 seconds to have a reasonable number of messages produced
    ([Example 13-14](#deploy-remove-ticker-application)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟延迟处理，让我们部署 ticker 服务，然后在 20 到 30 秒后再将其删除，以生成合理数量的消息（[示例 13-14](#deploy-remove-ticker-application)）。
- en: Example 13-14\. Deploy and remove the ticker application
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 13-14\. 部署和移除 ticker 应用程序
- en: '[PRE13]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Search again for traces of the `observability-ticker` service and you’ll see
    traces with only a single span. The only span in every trace is the one from the
    ticker service. For the processor to receive messages from before it was running,
    we need to update `application.properties` to indicate we want the earliest messages;
    see [Example 13-15](#configure-first-offset-new-consumer-groups).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 再次搜索 `observability-ticker` 服务的跟踪，您将看到仅有一个跨度的跟踪。每个跟踪中仅有一个跨度，即来自 ticker 服务的跨度。为了处理在它运行之前的消息，我们需要更新
    `application.properties` 以指示我们需要最早的消息；参见 [示例 13-15](#configure-first-offset-new-consumer-groups)。
- en: Example 13-15\. Configure the first offset to read for new consumer groups
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 13-15\. 配置新消费者组读取的第一个偏移量
- en: '[PRE14]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With the change made, deploy the viewer and processor services, and open the
    viewer URL in a browser to receive the messages. Once the messages have been received
    by the viewer, go back to the Jaeger UI and search for the traces again. We see
    the traces that previously had only a single span now have all four spans! We
    successfully processed messages after some time, and Jaeger was able to associate
    the spans with the right trace.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 更改后，部署观察者和处理器服务，并在浏览器中打开观察者 URL 以接收消息。一旦观察者接收到消息，返回 Jaeger UI 并再次搜索跟踪。我们看到以前仅有一个跨度的跟踪现在有了全部四个跨度！我们成功地在一段时间后处理了消息，并且
    Jaeger 能够将这些跨度与正确的跟踪关联起来。
- en: In a real production system, whether the preceding process works would depend
    on the retention of tracing data. If we retain tracing data for a year but want
    to process messages older than that, Jaeger will consider them as traces with
    only the spans from today. Any spans from the same trace will no longer be present
    for Jaeger to properly link them for visualization.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际生产系统中，前述过程是否奏效将取决于跟踪数据的保留。如果我们保留一年的跟踪数据，但希望处理比那更早的消息，Jaeger 将把它们视为仅包含今天跨度的跟踪。对于
    Jaeger 来说，来自同一跟踪的任何跨度都将不再存在，以便正确地将它们关联到可视化。
- en: Summary
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter detailed the importance of observability for reactive systems in
    Kubernetes. Observability is the key to ensuring the resiliency and elasticity
    of reactive systems. Health checks help systems to be resilient, by triggering
    the restart of services that are not healthy. Specific metrics of a reactive system
    can be used to provide elasticity by scaling up and down consumers as needed,
    dependent on message queue size, for instance.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本章详细介绍了 Kubernetes 中反应式系统的可观察性的重要性。可观察性是确保反应式系统弹性和弹性的关键。健康检查通过触发重启不健康的服务来帮助系统具有弹性。反应式系统的特定指标可用于提供弹性，例如根据消息队列大小缩放消费者的数量。
- en: We covered observability in Kubernetes with health checks, metrics, and distributed
    tracing. What we’ve covered only scratches the surface of observability for reactive
    systems, but provides sufficient detail for developers to delve deeper themselves.
    Though we can provide general guidelines for observability of reactive systems,
    specifics of what is desired will depend heavily on the use cases of the system.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了 Kubernetes 中的可观察性，包括健康检查、指标和分布式跟踪。我们所讨论的只是可观察性在反应式系统中的冰山一角，但为开发人员提供了足够的细节，让他们自己深入探究。虽然我们可以提供反应式系统可观察性的一般准则，但具体的期望将严重依赖于系统的用例。
- en: We’ve reached the end of [Part IV](part04.html#patterns-part), where we covered
    patterns of Reactive Messaging and its support of event buses, connecting messages
    to/from HTTP endpoints, and observing reactive systems.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经到达了[第四部分](part04.html#patterns-part)的末尾，在这里我们讨论了反应式消息传递的模式以及其支持事件总线、连接消息到/从
    HTTP 端点和观察反应式系统。
- en: ^([1](ch13.html#idm45358813004512-marker)) [Micrometer](https://micrometer.io)
    provides a facade over the instrumentation clients of popular monitoring systems,
    such as Prometheus.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch13.html#idm45358813004512-marker)) [微米计](https://micrometer.io) 提供了对流行监控系统（如Prometheus）的仪表客户端的外观封装。
- en: ^([2](ch13.html#idm45358812819136-marker)) [OpenTelemetry](https://opentelemetry.io)
    is a CNCF project combining OpenCensus and OpenTracing into a single project for
    the collection of telemetry signals. [Jaeger](https://www.jaegertracing.io) is
    a CNCF project for collecting and visualizing traces.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch13.html#idm45358812819136-marker)) [OpenTelemetry](https://opentelemetry.io)
    是一个CNCF项目，将OpenCensus和OpenTracing结合为一个项目，用于收集遥测信号。[Jaeger](https://www.jaegertracing.io)
    是一个CNCF项目，用于收集和可视化跟踪数据。
