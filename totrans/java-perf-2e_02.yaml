- en: Chapter 2\. An Approach to Performance Testing
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章。性能测试方法
- en: 'This chapter discusses four principles of getting results from performance
    testing: test real applications; understand throughput, batching, and response
    time; understand variability; and test early and often. These principles form
    the basis of the advice given in later chapters. The science of performance engineering
    is covered by these principles. Executing performance tests on applications is
    fine, but without scientific analysis behind those tests, they can too often lead
    to incorrect or incomplete analysis. This chapter covers how to make sure that
    testing produces valid analysis.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了从性能测试中获取结果的四个原则：测试真实应用；理解吞吐量、批处理和响应时间；理解变异性；以及早期和频繁地测试。这些原则构成了后续章节建议的基础。性能工程的科学就是通过这些原则来覆盖的。在应用程序上执行性能测试是可以的，但如果没有这些测试背后的科学分析，它们往往会导致不正确或不完整的分析。本章介绍了如何确保测试产生有效的分析。
- en: Many of the examples given in later chapters use a common application that emulates
    a system of stock prices; that application is also outlined in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 后续章节中的许多示例使用了一个模拟股票价格系统的常见应用程序；该应用程序也在本章中进行了概述。
- en: Test a Real Application
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试一个真实应用
- en: 'The first principle is that testing should occur on the actual product in the
    way the product will be used. Roughly speaking, three categories of code can be
    used for performance testing: microbenchmarks, macrobenchmarks, and mesobenchmarks.
    Each has its own advantages and disadvantages. The category that includes the
    actual application will provide the best results.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个原则是测试应该在实际产品上以产品将被使用的方式进行。粗略地说，可以使用三类代码进行性能测试：微基准测试、宏基准测试和中基准测试。每种都有其自身的优缺点。包含实际应用程序的类别将提供最佳结果。
- en: Microbenchmarks
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微基准测试
- en: 'A *microbenchmark* is a test designed to measure a small unit of performance
    in order to decide which of multiple alternate implementations is preferable:
    the overhead in creating a thread versus using a thread pool, the time to execute
    one arithmetic algorithm versus an alternate implementation, and so on.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*微基准测试* 是一种设计用来测量小单位性能的测试，以便决定哪个多个备选实现是首选：创建线程的开销与使用线程池的开销，执行一个算法与替代实现的时间等等。'
- en: Microbenchmarks may seem like a good idea, but the features of Java that make
    it attractive to developers—namely, just-in-time compilation and garbage collection—make
    it difficult to write microbenchmarks correctly.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 微基准测试可能看起来是一个不错的主意，但是 Java 的特性使其对开发人员很有吸引力 —— 即时编译和垃圾回收 —— 这使得编写正确的微基准测试变得困难。
- en: Microbenchmarks must use their results
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微基准测试必须使用其结果
- en: Microbenchmarks differ from regular programs in various ways. First, because
    Java code is interpreted the first few times it is executed, it gets faster the
    longer it is executed. For this reason, all benchmarks (not just microbenchmarks)
    typically include a warm-up period during which the JVM is allowed to compile
    the code into its optimal state.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 微基准测试与常规程序在各种方面有所不同。首先，因为 Java 代码在首次执行时是解释执行的，随着执行时间的增加，它会变得更快。因此，所有基准测试（不仅仅是微基准测试）通常包括一个预热期，期间
    JVM 可以将代码编译成其最佳状态。
- en: 'That optimal state can include a lot of optimizations. For example, here’s
    a seemingly simple loop to calculate an implementation of a method that calculates
    the 50th Fibonacci number:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳状态可能包括许多优化。例如，这里有一个看似简单的循环来计算一个计算第50个斐波那契数的方法的实现：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This code wants to measure the time to execute the `fibImpl1()` method, so it
    warms up the compiler first and then measures the now-compiled method. But likely,
    that time will be 0 (or more likely, the time to run the `for` loop without a
    body). Since the value of `l` is not read anywhere, the compiler is free to skip
    its calculation altogether. That depends on what else happens in the `fibImpl1()`
    method, but if it’s just a simple arithmetic operation, it can all be skipped.
    It’s also possible that only parts of the method will be executed, perhaps even
    producing the incorrect value for `l`; since that value is never read, no one
    will know. (Details of how the loop is eliminated are given in [Chapter 4](ch04.html#JustInTimeCompilation).)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码想要测量执行`fibImpl1()`方法的时间，因此它首先热身编译器，然后测量现在已编译的方法。但很可能那个时间是0（或者更可能是运行没有主体的`for`循环的时间）。由于`l`的值没有在任何地方读取，编译器可以自由地跳过其计算。这取决于`fibImpl1()`方法中还发生了什么，但如果只是一个简单的算术操作，就可以全部跳过。还可能只有方法的部分会被执行，甚至可能产生错误的`l`值；由于该值从未被读取，因此没有人会知道。（有关如何消除循环的详细信息，请参阅[第4章](ch04.html#JustInTimeCompilation)。）
- en: 'There is a way around that particular issue: ensure that each result is read,
    not simply written. In practice, changing the definition of `l` from a local variable
    to an instance variable (declared with the `volatile` keyword) will allow the
    performance of the method to be measured. (The reason the `l` instance variable
    must be declared as `volatile` can be found in [Chapter 9](ch09.html#ThreadPerformance).)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种解决这个特定问题的方法：确保每个结果都被读取，而不仅仅是写入。实际上，将`l`的定义从局部变量更改为实例变量（用`volatile`关键字声明）将允许测量方法的性能。（`l`实例变量必须声明为`volatile`的原因可以在[第9章](ch09.html#ThreadPerformance)中找到。）
- en: Microbenchmarks must test a range of input
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微基准测试必须测试一系列输入。
- en: 'Even then, potential pitfalls exist. This code performs only one operation:
    calculating the 50th Fibonacci number. A smart compiler can figure that out and
    execute the loop only once—or at least discard some iterations of the loop since
    those operations are redundant.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 即便如此，仍存在潜在的陷阱。这段代码只执行一项操作：计算第50个斐波那契数。聪明的编译器可以发现这一点，并且仅执行一次循环，或者至少丢弃循环的一些迭代，因为这些操作是多余的。
- en: Additionally, the performance of `fibImpl1(1000)` is likely to be very different
    from the performance of `fibImpl1(1)`; if the goal is to compare the performance
    of different implementations, a range of input values must be considered.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`fibImpl1(1000)`的性能很可能与`fibImpl1(1)`的性能大不相同；如果目标是比较不同实现的性能，则必须考虑一系列输入值。
- en: 'A range of inputs could be random, like this:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的范围可以是随机的，像这样：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: That likely isn’t what we want. The time to calculate the random numbers is
    included in the time to execute the loop, so the test now measures the time to
    calculate a Fibonacci sequence `nLoops` times, plus the time to generate `nLoops`
    random integers.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能不是我们想要的。计算随机数的时间包括在执行循环的时间中，所以测试现在测量了计算斐波那契数列`nLoops`次所需的时间，加上生成`nLoops`个随机整数的时间。
- en: 'It is better to precalculate the input values:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最好预先计算输入值：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Microbenchmarks must measure the correct input
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微基准测试必须测量正确的输入。
- en: 'You probably noticed that now the test has to check for an exception when calling
    the `fibImpl1()` method: the input range includes negative numbers (which have
    no Fibonacci number) and numbers greater than 1,476 (which yield a result that
    cannot be represented as a `double`).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到现在测试必须检查调用`fibImpl1()`方法时是否会出现异常：输入范围包括负数（没有斐波那契数）和大于1,476的数字（其结果不能表示为`double`）。
- en: 'When that code is used in production, are those likely input values? In this
    example, probably not; in your own benchmarks, your mileage may vary. But consider
    the effect here: let’s say that you are testing two implementations of this operation.
    The first is able to calculate a Fibonacci number fairly quickly but doesn’t bother
    to check its input parameter range. The second immediately throws an exception
    if the input parameter is out of range, but then executes a slow, recursive operation
    to calculate the Fibonacci number, like this:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当该代码用于生产时，这些可能是常见的输入值吗？在这个示例中，可能不是；在你自己的基准测试中，结果可能会有所不同。但要考虑这里的影响：假设你正在测试这个操作的两种实现。第一种能够相当快地计算斐波那契数，但不检查其输入参数范围。第二种如果输入参数超出范围就会立即抛出异常，然后执行一个缓慢的递归操作来计算斐波那契数，像这样：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Comparing this implementation to the original implementation over a wide range
    of input values will show this new implementation is much faster than the original
    one—simply because of the range checks at the beginning of the method.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 将此实现与原始实现在广泛的输入值范围内进行比较将表明，这种新实现比原始实现快得多——仅仅因为方法开始时的范围检查。
- en: If, in the real world, users are always going to pass values less than 100 to
    the method, that comparison will give us the wrong answer. In the common case,
    the `fibImpl1()` method will be faster, and as [Chapter 1](ch01.html#Introduction)
    explained, we should optimize for the common case. (This is obviously a contrived
    example, and simply adding a bounds test to the original implementation makes
    it a better implementation anyway. In the general case, that may not be possible.)
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在现实世界中，用户总是将小于100的值传递给该方法，那么比较将给出错误的答案。通常情况下，`fibImpl1()` 方法会更快，并且正如[第1章](ch01.html#Introduction)所解释的，我们应该为常见情况进行优化。（这显然是一个假设的例子，而原始实现中简单添加边界测试会使其成为更好的实现。在一般情况下，这可能是不可能的。）
- en: Microbenchmark code may behave differently in production
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微基准测试代码在生产环境中可能会表现不同。
- en: So far, the issues we’ve looked at can be overcome by carefully writing our
    microbenchmark. Other things will affect the end result of the code after it is
    incorporated into a larger program. The compiler uses profile feedback of code
    to determine the best optimizations to employ when compiling a method. The profile
    feedback is based on which methods are frequently called, the stack depth when
    they are called, the actual type (including subclasses) of their arguments, and
    so on—it is dependent on the environment in which the code actually runs.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们看过的问题可以通过仔细编写我们的微基准测试来克服。其他因素将影响代码最终在纳入更大程序后的结果。编译器使用代码的配置反馈来确定在编译方法时使用的最佳优化方法。配置反馈基于哪些方法频繁调用、它们被调用时的堆栈深度、实际类型（包括子类）的参数等等——它依赖于代码实际运行的环境。
- en: Hence, the compiler will frequently optimize code differently in a microbenchmark
    than it optimizes that same code when used in a larger application.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在微基准测试中，编译器通常会以不同的方式优化代码，而不是在较大的应用程序中使用相同的代码时优化。
- en: 'Microbenchmarks may also exhibit very different behavior in terms of garbage
    collection. Consider two microbenchmark implementations: the first one produces
    fast results but also produces many short-lived objects. The second is slightly
    slower but produces fewer short-lived objects.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 微基准测试也可能在垃圾收集方面表现出非常不同的行为。考虑两种微基准测试的实现：第一个产生快速结果，但也产生许多短寿对象。第二个稍慢一些，但产生的短寿对象较少。
- en: 'When we run a small program to test these, the first will likely be faster.
    Even though it will trigger more garbage collections, they will be quick to discard
    the short-lived objects in collections of the young generation, and the faster
    overall time will favor that implementation. When we run this code in a server
    with multiple threads executing simultaneously, the GC profile will look different:
    the multiple threads will fill up the young generation faster. Hence, many of
    the short-lived objects that were quickly discarded in the case of the microbenchmark
    may end up getting promoted into the old generation when used in the multithreaded
    server environment. This, in turn, will lead to frequent (and expensive) full
    GCs. In that case, the long times spent in the full GCs will make the first implementation
    perform worse than the second, “slower” implementation that produces less garbage.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行一个小程序来测试这些内容时，第一个可能会更快。即使它会触发更多的垃圾收集，它们会迅速丢弃年轻代集合中的短寿对象，总体更快的时间会偏向于这种实现。当我们在具有多个线程同时执行的服务器上运行此代码时，GC（垃圾收集）的配置文件将会有所不同：多个线程将更快地填满年轻代。因此，在微基准测试情况下迅速丢弃的许多短寿对象，在多线程服务器环境中使用时可能会被提升到老年代。这反过来会导致频繁（且昂贵）的全面GC。在这种情况下，长时间在全面GC中花费会使第一个实现比产生较少垃圾的第二个“较慢”实现表现更差。
- en: Finally, there is the issue of what the microbenchmark actually means. The overall
    time difference in a benchmark such as the one discussed here may be measured
    in seconds for many loops, but the per iteration difference is often measured
    in nanoseconds. Yes, nanoseconds add up, and “death by 1,000 cuts” is a frequent
    performance issue. But particularly in regression testing, consider whether tracking
    something at the nanosecond level makes sense. It may be important to save a few
    nanoseconds on each access to a collection that will be accessed millions of times
    (for example, see [Chapter 12](ch12.html#Misc)). For an operation that occurs
    less frequently—for example, maybe once per request for a REST call—fixing a nanosecond
    regression found by a microbenchmark will take away time that could be more profitably
    spent on optimizing other operations.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，还有一个问题，那就是微基准实际上意味着什么。在像这里讨论的基准测试中，整体时间差可能以秒为单位测量许多循环，但每次迭代的差异通常以纳秒为单位测量。是的，纳秒是可以累加的，“千刀万剐”的问题经常成为性能问题。但特别是在回归测试中，请考虑跟踪纳秒级别的事务是否有意义。对于那些会被访问数百万次的集合来说，在每次访问时节省几个纳秒可能是重要的（例如，参见[第12章](ch12.html#Misc)）。对于发生频率较低的操作，比如每个REST调用请求可能只会发生一次的操作，通过修复由微基准测试发现的纳秒回归可能会耗费时间，而这些时间本应该更有利地用于优化其他操作。
- en: 'Still, for all their pitfalls, microbenchmarks are popular enough that the
    OpenJDK has a core framework to develop microbenchmarks: the Java Microbenchmark
    Harness (`jmh`). `jmh` is used by the JDK developers to build regression tests
    for the JDK itself, as well as providing a framework for the development of general
    benchmarks. We’ll discuss `jmh` in more detail in the next section.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管微基准测试存在许多缺陷，但它们足够受欢迎，以至于OpenJDK有一个核心框架用于开发微基准测试：Java微基准测试工具（`jmh`）。`jmh`被JDK开发人员用于构建JDK本身的回归测试，并为一般基准测试的开发提供框架。我们将在下一节更详细地讨论`jmh`。
- en: Macrobenchmarks
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 宏基准测试
- en: The best thing to use to measure performance of an application is the application
    itself, in conjunction with any external resources it uses. This is a *macrobenchmark*.
    If the application normally checks the credentials of a user by making calls to
    a directory service (e.g., via Lightweight Directory Access Protocol, or LDAP),
    it should be tested in that mode. Stubbing out the LDAP calls may make sense for
    module-level testing, but the application must be tested in its full configuration.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 评估应用程序性能的最佳方法是使用应用程序本身，结合其使用的任何外部资源。这就是*宏基准测试*。例如，如果应用程序通常通过调用目录服务（例如轻量级目录访问协议，或LDAP）检查用户的凭据，应该以该模式进行测试。对LDAP调用进行存根化可能对模块级测试有意义，但必须以其完整配置测试应用程序。
- en: As applications grow, this maxim becomes both more important to fulfill and
    more difficult to achieve. Complex systems are more than the sum of their parts;
    they will behave quite differently when those parts are assembled. Mocking out
    database calls, for example, may mean that you no longer have to worry about the
    database performance—and hey, you’re a Java person; why should you have to deal
    with the DBA’s performance problem? But database connections consume lots of heap
    space for their buffers; networks become saturated when more data is sent over
    them; code is optimized differently when it calls a simpler set of methods (as
    opposed to the complex code in a JDBC driver); CPUs pipeline and cache shorter
    code paths more efficiently than longer code paths; and so on.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 随着应用程序的增长，这一准则变得更加重要并且更难实现。复杂系统不仅仅是其各个部分的总和；当这些部分组装在一起时，它们的行为将会有所不同。例如，模拟数据库调用可能意味着您不再需要担心数据库的性能问题——嘿，您是Java开发人员；为什么还要处理DBA的性能问题呢？但数据库连接为其缓冲区消耗大量堆空间；当通过网络发送更多数据时，网络会饱和；与JDBC驱动程序中复杂代码相比，调用更简单方法集的代码将被优化得不同；CPU更有效地在较短的代码路径上进行流水线处理和缓存等等。
- en: The other reason to test the full application is one of resource allocation.
    In a perfect world, there would be enough time to optimize every line of code
    in the application. In the real world, deadlines loom, and optimizing only one
    part of a complex environment may not yield immediate benefits.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个测试整个应用程序的原因是资源分配的问题。在理想的世界中，将有足够的时间优化应用程序中的每一行代码。然而在现实世界中，截止日期逼近，仅优化复杂环境中的一部分可能不会立即产生效益。
- en: Consider the data flow shown in [Figure 2-1](#FigureBenchmark). Data comes in
    from a user, a proprietary business calculation is made, data based on that is
    loaded from the database, more proprietary calculations are made, changed data
    is stored back to the database, and an answer is sent back to the user. The number
    in each box is the number of requests per second (RPS) that the module can process
    when tested in isolation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: From a business perspective, the proprietary calculations are the most important
    thing; they are the reason the program exists, and the reason we are paid. Yet
    making them 100% faster will yield absolutely no benefit in this example. Any
    application (including a single, standalone JVM) can be modeled as a series of
    steps like this, where data flows out of a box (module, subsystem, etc.) at a
    rate determined by the efficiency of that box. Data flows into a subsystem at
    a rate determined by the output rate of the previous box.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![jp2e 0201](assets/jp2e_0201.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. Typical program flow
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Assume that an algorithmic improvement is made to the business calculation
    so that it can process 200 RPS; the load injected into the system is correspondingly
    increased. The LDAP system can handle the increased load: so far, so good, and
    200 RPS will flow into the calculation module, which will output 200 RPS.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: But the data loading can still process only 100 RPS. Even though 200 RPS flow
    into the database, only 100 RPS flow out of it and into the other modules. The
    total throughput of the system is still only 100 RPS, even though the efficiency
    of the business logic has doubled. Further attempts to improve the business logic
    will prove futile until time is spent improving other aspects of the environment.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'The time spent optimizing the calculations in this example isn’t entirely wasted:
    once effort is put into the bottlenecks elsewhere in the system, the performance
    benefit will finally be apparent. Rather, it is a matter of priorities: without
    testing the entire application, it is impossible to tell where spending time on
    performance work will pay off.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Mesobenchmarks
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Mesobenchmarks* are tests that occupy a middle ground between a microbenchmark
    and a full application. I work with developers on the performance of both Java
    SE and large Java applications, and each group has a set of tests they characterize
    as microbenchmarks. To a Java SE engineer, that term connotes an example even
    smaller than that in the first section: the measurement of something quite small.
    Application developers tend to use that term to apply to something else: benchmarks
    that measure one aspect of performance but that still execute a lot of code.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of an application microbenchmark might be something that measures
    how quickly the response from a simple REST call can be returned from a server.
    The code for such a request is substantial compared to a traditional microbenchmark:
    there is a lot of socket-management code, code to read the request, code to write
    the answer, and so on. From a traditional standpoint, this is not microbenchmarking.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 应用微基准的一个示例可能是测量从服务器返回简单 REST 调用响应的速度。与传统的微基准相比，这种请求的代码要复杂得多：包括大量的套接字管理代码、读取请求的代码、写入答案的代码等等。从传统的角度来看，这不是微基准。
- en: 'This kind of test is not a macrobenchmark either: there is no security (e.g.,
    the user does not log in to the application), no session management, and no use
    of a host of other application features. Because it is only a subset of an actual
    application, it falls somewhere in the middle—it is a mesobenchmark. That is the
    term I use for benchmarks that do some real work but are not full-fledged applications.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这种测试也不是宏基准：没有安全性（例如，用户不登录应用程序）、没有会话管理，也没有使用其他应用程序功能。因为它只是实际应用程序的一个子集，它位于中间位置——这是我用来描述做一些实际工作但不是完整应用程序的基准测试的术语，称为
    Mesobenchmark。
- en: 'Mesobenchmarks have fewer pitfalls than microbenchmarks and are easier to work
    with than macrobenchmarks. Mesobenchmarks likely won’t contain a large amount
    of dead code that can be optimized away by the compiler (unless that dead code
    exists in the application, in which case optimizing it away is a good thing).
    Mesobenchmarks are more easily threaded: they are still more likely to encounter
    more synchronization bottlenecks than the code will encounter when run in a full
    application, but those bottlenecks are something the real application will eventually
    encounter on larger hardware systems under larger load.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Mesobenchmark 比微基准有更少的陷阱，比宏基准更容易处理。Mesobenchmark 可能不会包含大量可以由编译器优化去掉的死代码（除非这些死代码存在于应用程序中，在这种情况下，优化掉它们是件好事）。Mesobenchmark
    更容易进行线程化：它们仍然更有可能遇到更多同步瓶颈，但这些瓶颈是真实应用程序在更大的硬件系统和更大的负载下最终会遇到的问题。
- en: Still, mesobenchmarks are not perfect. A developer who uses a benchmark like
    this to compare the performance of two application servers may be easily led astray.
    Consider the hypothetical response times of two REST servers shown in [Table 2-1](#TableMesoBenchmark).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Mesobenchmark 并不完美。使用这样的基准来比较两个应用服务器性能的开发人员可能很容易误入歧途。考虑 [表 2-1](#TableMesoBenchmark)
    中展示的两个 REST 服务器的假设响应时间。
- en: Table 2-1\. Hypothetical response times for two REST servers
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-1\. 两个 REST 服务器的假设响应时间
- en: '| Test | Server 1 | Server 2 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 服务器 1 | 服务器 2 |'
- en: '| --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Simple REST call | 19 ± 2.1 ms | 50 ± 2.3 ms |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 简单的 REST 调用 | 19 ± 2.1 毫秒 | 50 ± 2.3 毫秒 |'
- en: '| REST call with authorization | 75 ± 3.4 ms | 50 ± 3.1 ms |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 带授权的 REST 调用 | 75 ± 3.4 毫秒 | 50 ± 3.1 毫秒 |'
- en: The developer who uses only a simple REST call to compare the performance of
    the two servers might not realize that server 2 is automatically performing authorization
    for each request. They may then conclude that server 1 will provide the fastest
    performance. Yet if their application always needs authorization (which is typical),
    they will have made the incorrect choice, since it takes server 1 much longer
    to perform that authorization.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 只使用简单的 REST 调用来比较两台服务器性能的开发人员可能没有意识到，服务器 2 自动为每个请求执行授权。他们可能会得出服务器 1 提供最快性能的结论。然而，如果他们的应用程序总是需要授权（这是典型的情况），他们就做出了错误的选择，因为服务器
    1 执行授权的时间要长得多。
- en: 'Even so, mesobenchmarks offer a reasonable alternative to testing a full-scale
    application; their performance characteristics are much more closely aligned to
    an actual application than are the performance characteristics of microbenchmarks.
    And there is, of course, a continuum here. A later section in this chapter presents
    the outline of a common application used for many of the examples in subsequent
    chapters. That application has a server mode (for both REST and Jakarta Enterprise
    Edition servers), but those modes don’t use server facilities like authentication,
    and though it can access an enterprise resource (i.e., a database), in most examples
    it just makes up random data in place of database calls. In batch mode, it mimics
    some actual (but quick) calculations: for example, no GUI or user interaction
    occurs.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 即便如此，中基准测试提供了一种合理的替代方案，而不是测试完整应用程序；它们的性能特征与实际应用程序更加接近，而不是微基准测试的性能特征。当然，这里存在一个连续性。本章后面的一个部分将介绍一个常见应用程序的概要，该应用程序在后续章节的许多示例中使用。该应用程序有服务器模式（适用于REST和Jakarta企业版服务器），但这些模式不使用像身份验证这样的服务器功能，虽然它可以访问企业资源（即数据库），但在大多数示例中，它只是使用随机数据来替代数据库调用。在批处理模式下，它模拟了一些实际（但快速）的计算：例如，没有GUI或用户交互。
- en: Mesobenchmarks are also good for automated testing, particularly at the module
    level.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 中基准测试也非常适合自动化测试，特别是在模块级别。
- en: Quick Summary
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速总结
- en: Good microbenchmarks are hard to write without an appropriate framework.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要编写好的微基准测试，需要一个适当的框架。
- en: Testing an entire application is the only way to know how code will actually
    run.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试整个应用程序是了解代码实际运行方式的唯一途径。
- en: Isolating performance at a modular or operational level—via a mesobenchmark—offers
    a reasonable approach but is no substitute for testing the full application.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过中基准测试（mesobenchmark）在模块化或操作级别上分离性能提供了一个合理的方法，但不能替代对整个应用程序的测试。
- en: Understand Throughput, Batching, and Response Time
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解吞吐量、批处理和响应时间
- en: The second principle is to understand and select the appropriate test metric
    for the application. Performance can be measured as throughput (RPS), elapsed
    time (batch time), or response time, and these three metrics are interrelated.
    Understanding those relationships allows you to focus on the correct metric, depending
    on the goal of the application.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原则是理解并选择适合应用程序的适当测试指标。性能可以通过吞吐量（RPS）、经过时间（批处理时间）或响应时间来衡量，这三个指标相互关联。了解这些关系可以根据应用程序的目标选择正确的指标。
- en: Elapsed Time (Batch) Measurements
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经过时间（批处理）测量
- en: The simplest way to measure performance is to see how long it takes to accomplish
    a certain task. We might, for example, want to retrieve the history of 10,000
    stocks for a 25-year period and calculate the standard deviation of those prices,
    produce a report of the payroll benefits for the 50,000 employees of a corporation,
    or execute a loop 1,000,000 times.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量性能的最简单方法是看完成某项任务需要多长时间。例如，我们可能想要检索过去25年间1万只股票的历史，并计算这些价格的标准偏差，为某公司5万名员工的工资福利制作报告，或执行100万次循环。
- en: 'In statically compiled languages, this testing is straightforward: the application
    is written, and the time of its execution is measured. The Java world adds a wrinkle
    to this: just-in-time compilation. That process is described in [Chapter 4](ch04.html#JustInTimeCompilation);
    essentially it means that it takes anywhere from a few seconds to a few minutes
    (or longer) for the code to be fully optimized and operate at peak performance.
    For that (and other) reasons, performance studies of Java are concerned about
    warm-up periods: performance is most often measured after the code in question
    has been executed long enough for it to have been compiled and optimized.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在静态编译语言中，这种测试非常直接：编写应用程序，然后测量其执行时间。Java世界为此增加了一些复杂性：即时编译。该过程在[第4章](ch04.html#JustInTimeCompilation)中有描述；基本上意味着代码需要几秒到几分钟（或更长时间）才能完全优化并在最高性能下运行。由于这个（以及其他）原因，Java的性能研究关注热身期：通常在执行了足够长时间的代码后进行性能测量，以确保已编译并优化。
- en: On the other hand, in many cases the performance of the application from start
    to finish is what matters. A report generator that processes ten thousand data
    elements will complete in a certain amount of time; to the end user, it doesn’t
    matter if the first five thousand elements are processed 50% more slowly than
    the last five thousand elements. And even in something like a REST server—where
    the server’s performance will certainly improve over time—the initial performance
    matters. It will take some time for a server to reach peak performance; to the
    users accessing the application during that time, the performance during the warm-up
    period does matter.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在许多情况下，应用程序从开始到结束的性能才是重要的。一个处理一万个数据元素的报告生成器将在一定时间内完成；对于最终用户来说，如果前五千个元素的处理速度比后五千个元素慢50%，那并不重要。即使在像REST服务器这样的场景中——服务器的性能肯定会随着时间的推移而提高——初始性能也很重要。服务器要达到最佳性能需要一些时间；在此期间访问应用程序的用户，确实在乎热身期间的性能。
- en: For those reasons, many examples in this book are batch-oriented (even if that
    is a little uncommon).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，本书中的许多示例都是批处理型的（尽管这有点不太常见）。
- en: Throughput Measurements
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 吞吐量测量
- en: 'A *throughput measurement* is based on the amount of work that can be accomplished
    in a certain period of time. Although the most common examples of throughput measurements
    involve a server processing data fed by a client, that is not strictly necessary:
    a single, standalone application can measure throughput just as easily as it measures
    elapsed time.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*吞吐量测量*基于在一定时间内可以完成的工作量。虽然吞吐量测量的最常见例子涉及服务器处理客户端提供的数据，但这并非绝对必要：一个单独的独立应用程序可以像测量经过的时间一样容易地测量吞吐量。'
- en: In a client/server test, a throughput measurement means that clients have no
    think time. If there is a single client, that client sends a request to the server.
    When the client receives a response, it immediately sends a new request. That
    process continues; at the end of the test, the client reports the total number
    of operations it achieved. Typically, the client has multiple threads doing the
    same thing, and the throughput is the aggregate measure of the number of operations
    all clients achieved. Usually, this number is reported as the number of operations
    per second, rather than the total number of operations over the measurement period.
    This measurement is frequently referred to as *transactions per second* (TPS),
    *requests per second* (RPS), or *operations per second* (OPS).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端/服务器测试中，吞吐量测量意味着客户端没有思考时间。如果只有一个客户端，那么该客户端将向服务器发送一个请求。当客户端收到响应后，它立即发送一个新请求。这个过程持续进行；在测试结束时，客户端报告它实现的总操作数。通常，客户端有多个线程执行相同的操作，吞吐量是所有客户端实现的操作数量的综合测量。通常，这个数字报告为每秒操作数，而不是测量期间的总操作数。这种测量通常称为*每秒事务数*（TPS）、*每秒请求数*（RPS）或*每秒操作数*（OPS）。
- en: The configuration of the client in client/server tests is important; you need
    to ensure that the client can send data quickly enough to the server. This may
    not occur because there aren’t enough CPU cycles on the client machine to run
    the desired number of client thread, or because the client has to spend a lot
    of time processing the request before it can send a new request. In those cases,
    the test is effectively measuring the client performance rather than the server
    performance, which is usually not the goal.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端/服务器测试中客户端的配置非常重要；您需要确保客户端能够快速地向服务器发送数据。这可能不会发生，因为客户端机器上没有足够的CPU周期来运行所需数量的客户端线程，或者因为客户端必须花费大量时间处理请求，然后才能发送新请求。在这些情况下，测试实际上是在测量客户端的性能，而不是服务器的性能，这通常不是目标。
- en: This risk depends on the amount of work that each client thread performs (and
    the size and configuration of the client machine). A zero-think-time (throughput-oriented)
    test is more likely to encounter this situation, since each client thread is performing
    more requests. Hence, throughput tests are typically executed with fewer client
    threads (less load) than a corresponding test that measures response time.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此风险取决于每个客户端线程执行的工作量（以及客户端机器的大小和配置）。零思考时间（以吞吐量为导向）的测试更有可能遇到这种情况，因为每个客户端线程正在执行更多的请求。因此，吞吐量测试通常使用较少的客户端线程（较少的负载）执行，而不是测量响应时间的相应测试。
- en: Tests that measure throughput also commonly report the average response time
    of requests. That is an interesting piece of information, but changes in that
    number don’t indicate a performance problem unless the reported throughput is
    the same. A server that can sustain 500 OPS with a 0.5-second response time is
    performing better than a server that reports a 0.3-second response time but only
    400 OPS.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 测量吞吐量的测试通常也报告请求的平均响应时间。这是一个有趣的信息，但是该数字的变化并不表明性能问题，除非报告的吞吐量相同。一个服务器如果可以以0.5秒的响应时间维持500
    OPS，那么它的性能比报告0.3秒响应时间但只有400 OPS的服务器要好。
- en: Throughput measurements are almost always taken after a suitable warm-up period,
    particularly because what is being measured is not a fixed set of work.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎总是在适当的预热期之后进行吞吐量测量，特别是因为被测量的内容不是固定的一组工作。
- en: Response-Time Tests
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 响应时间测试
- en: 'The last common test is one that measures *response time*: the amount of time
    that elapses between the sending of a request from a client and the receipt of
    the response.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个常见的测试是测量*响应时间*：即客户端发送请求和接收响应之间经过的时间。
- en: 'The difference between a response-time test and a throughput test (assuming
    the latter is client/server based) is that client threads in a response-time test
    sleep for a period of time between operations. This is referred to as *think time*.
    A response-time test is designed to mimic more closely what a user does: the user
    enters a URL in a browser, spends time reading the page that comes back, clicks
    a link in the page, spends time reading that page, and so on.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 响应时间测试和吞吐量测试（假设后者是基于客户端/服务器的）之间的区别在于响应时间测试中的客户端线程在操作之间会睡眠一段时间。这被称为*思考时间*。响应时间测试旨在更贴近用户的实际操作：用户在浏览器中输入URL，花时间阅读返回的页面，点击页面中的链接，花时间阅读该页面，依此类推。
- en: 'When think time is introduced into a test, throughput becomes fixed: a given
    number of clients executing requests with a given think time will always yield
    the same TPS (with slight variance; see the following sidebar). At that point,
    the important measurement is the response time for the request: the effectiveness
    of the server is based on how quickly it responds to that fixed load.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当测试引入思考时间后，吞吐量就变成了固定的：给定数量的客户端使用给定的思考时间执行请求，将始终产生相同的TPS（稍有变化；详见下面的侧边栏）。此时，重要的测量值是请求的响应时间：服务器响应该固定负载的速度决定了其效率。
- en: 'We can measure response time in two ways. Response time can be reported as
    an average: the individual times are added together and divided by the number
    of requests. Response time can also be reported as a *percentile request*; for
    example, the 90th% response time. If 90% of responses are less than 1.5 seconds
    and 10% of responses are greater than 1.5 seconds, then 1.5 seconds is the 90th%
    response time.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用两种方式测量响应时间。响应时间可以报告为平均值：各个时间相加后除以请求总数。响应时间也可以报告为*百分位请求*；例如，90%的响应时间。如果90%的响应时间小于1.5秒，而10%的响应时间大于1.5秒，则1.5秒是90%的响应时间。
- en: 'One difference between average response time and a percentile response time
    is in the way outliers affect the calculation of the average: since they are included
    as part of the average, large outliers will have a large effect on the average
    response time.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 平均响应时间和百分位响应时间之间的一个区别在于异常值对平均值计算的影响方式：由于它们被包括在平均值中，大的异常值会对平均响应时间产生较大的影响。
- en: '[Figure 2-2](#FigureRT) shows a graph of 20 requests with a somewhat typical
    range of response times. The response times range from 1 to 5 seconds. The average
    response time (represented by the lower heavy line along the x-axis) is 2.35 seconds,
    and 90% of the responses occur in 4 seconds or less (represented by the upper
    heavy line along the x-axis).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 2-2](#FigureRT) 显示了20个请求的响应时间图表，响应时间的范围相对典型，从1到5秒不等。平均响应时间（由x轴上的较低粗线表示）为2.35秒，90%的响应在4秒或更短的时间内完成（由x轴上的较高粗线表示）。'
- en: This is the usual scenario for a well-behaved test. Outliers can skew that analysis,
    as the data in [Figure 2-3](#FigureRTOutlier) shows.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个表现良好的测试的典型场景。异常值可能会扭曲分析结果，正如[图 2-3](#FigureRTOutlier) 中的数据所示。
- en: 'This data set includes a huge outlier: one request took one hundred seconds.
    As a result, the positions of the 90th% and average response times are reversed.
    The average response time is a whopping 5.95 seconds, but the 90th% response time
    is 1.0 second. Focus in this case should be given to reducing the effect of the
    outlier (which will drive down the average response time).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集包含一个巨大的异常值：一个请求耗时100秒。因此，第90%和平均响应时间的位置被颠倒了。平均响应时间高达5.95秒，但第90%的响应时间是1.0秒。在这种情况下，应重点关注减少异常值的影响（这将降低平均响应时间）。
- en: '![Graph of Typical Response Times](assets/jp2e_0202.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![典型响应时间图](assets/jp2e_0202.png)'
- en: Figure 2-2\. Typical set of response times
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2\. 典型响应时间集合
- en: '![Graph of Response Times with Outlier](assets/jp2e_0203.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![异常响应时间图](assets/jp2e_0203.png)'
- en: Figure 2-3\. Set of response times with an outlier
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3\. 带有异常值的响应时间集合
- en: Outliers like that can occur for multiple reasons, and they can more easily
    occur in Java applications because of the pause times introduced by GC.^([1](ch02.html#idm45775552311416))
    In performance testing, the usual focus is on the 90th% response time (or even
    the 95th% or 99th% response time; there is nothing magical about 90%). If you
    can focus on only one number, a percentile-based number is the better choice,
    since achieving a smaller number there will benefit a majority of users. But it
    is even better to look at both the average response time and at least one percentile-based
    response time, so you do not miss cases with large outliers.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 类似这样的异常值可能由多种原因引起，在Java应用程序中更容易发生，因为GC引入的暂停时间。^([1](ch02.html#idm45775552311416))
    在性能测试中，通常关注的是第90%的响应时间（甚至是第95%或第99%的响应时间；90%没有什么神奇之处）。如果只能专注于一个数字，基于百分位数的数字是更好的选择，因为在那里实现较小的数字将使大多数用户受益。但更好的做法是同时查看平均响应时间和至少一个基于百分位数的响应时间，这样就不会错过大异常值的情况。
- en: Quick Summary
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速总结
- en: Batch-oriented tests (or any test without a warm-up period) have been infrequently
    used in Java performance testing but can yield valuable results.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面向批处理的测试（或任何没有预热期的测试）在Java性能测试中很少使用，但可以产生有价值的结果。
- en: Other tests can measure either throughput or response time, depending on whether
    the load comes in at a fixed rate (i.e., based on emulating think time in the
    client).
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他测试可以根据负载是否以固定速率到达（即基于在客户端模拟思考时间）来测量吞吐量或响应时间。
- en: Understand Variability
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解变异性
- en: 'The third principle is to understand how test results vary over time. Programs
    that process exactly the same set of data will produce a different answer each
    time they are run. Background processes on the machine will affect the application,
    the network will be more or less congested when the program is run, and so on.
    Good benchmarks also never process exactly the same set of data each time they
    are run; random behavior will be built into the test to mimic the real world.
    This creates a problem: when comparing the result from one run to the result from
    another run, is the difference due to a regression or to the random variation
    of the test?'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个原则是理解测试结果随时间变化的方式。处理完全相同数据集的程序每次运行时会产生不同的答案。机器上的后台进程会影响应用程序，程序运行时网络拥塞程度会有所不同等等。良好的基准测试也不会每次运行时处理完全相同的数据集；测试中将内置随机行为以模拟真实世界。这带来了一个问题：比较一个运行的结果与另一个运行的结果时，差异是由于回归还是由于测试的随机变化？
- en: This problem can be solved by running the test multiple times and averaging
    those results. Then when a change is made to the code being tested, the test can
    be rerun multiple times, the results averaged, and the two averages compared.
    It sounds so easy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过多次运行测试并对结果取平均值来解决此问题。因此，当对正在测试的代码进行更改时，可以多次重新运行测试，取平均值，并比较两个平均值。听起来很简单。
- en: Unfortunately, it isn’t as simple as that. Understanding when a difference is
    a real regression and when it is a random variation is difficult. In this key
    area, science leads the way, but art will come into play.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，事情并非如此简单。理解何时差异是真正的回归，何时是随机变化是困难的。在这一关键领域，科学引领前行，但艺术也会发挥作用。
- en: When averages in benchmark results are compared, it is impossible to know with
    absolute certainty whether the difference in the averages is real or due to random
    fluctuation. The best that can be done is to hypothesize that “the averages are
    the same” and then determine the probability that such a statement is true. If
    the statement is false with a high degree of probability, we are comfortable believing
    the difference in the averages (though we can never be 100% certain).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Testing code for changes like this is called *regression testing*. In a regression
    test, the original code is known as the *baseline*, and the new code is called
    the *specimen*. Take the case of a batch program in which the baseline and specimen
    are each run three times, yielding the times given in [Table 2-2](#TableTTest1).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-2\. Hypothetical times to execute two tests
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Baseline | Specimen |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
- en: '| First iteration | 1.0 second | 0.5 second |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
- en: '| Second iteration | 0.8 second | 1.25 seconds |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
- en: '| Third iteration | 1.2 seconds | 0.5 second |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
- en: '| **Average** | **1 second** | **0.75 second** |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
- en: 'The average of the specimen indicates a 25% improvement in the code. How confident
    can we be that the test really reflects a 25% improvement? Things look good: two
    of the three specimen values are less than the baseline average, and the size
    of the improvement is large. Yet when the analysis described in this section is
    performed on those results, it turns out that the probability of the specimen
    and the baseline having the same performance is 43%. When numbers like these are
    observed, 43% of the time the underlying performance of the two tests are the
    same, and performance is different only 57% of the time. This, by the way, is
    not exactly the same thing as saying that 57% of the time the performance is 25%
    better, but you’ll learn more about that later in this section.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: The reason these probabilities seem different than might be expected is due
    to the large variation in the results. In general, the larger the variation in
    a set of results, the harder it is to guess the probability that the difference
    in the averages is real or due to random chance.^([2](ch02.html#idm45775552239592))
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: This number—43%—is based on the result of *Student’s t-test*, which is a statistical
    analysis based on the series and their variances.^([3](ch02.html#idm45775552237160))
    The *t*-test produces a number called the *p-value*, which refers to the probability
    that the null hypothesis for the test is true.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: The *null hypothesis* in regression testing is the hypothesis that the two tests
    have equal performance. The *p*-value for this example is roughly 43%, which means
    the confidence we can have that the series converge to the same average is 43%.
    Conversely, the confidence we have that the series do not converge to the same
    average is 57%.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: What does it mean to say that 57% of the time, the series do not converge to
    the same average? Strictly speaking, it doesn’t mean that we have 57% confidence
    that there is a 25% improvement in the result—it means only that we have 57% confidence
    that the results are different. There may be a 25% improvement, there may be a
    125% improvement; it is even conceivable that the specimen has worse performance
    than the baseline. The most probable likelihood is that the difference in the
    test is similar to what has been measured (particularly as the *p*-value goes
    down), but certainty can never be achieved.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 说57%的时间系列不会收敛到相同的平均值意味着什么？严格来说，并不意味着我们有57%的置信度，表明结果有25%的改善——它只是意味着我们有57%的置信度，结果是不同的。可能有25%的改善，可能有125%的改善；甚至可能样本比基线表现更差。最有可能的情况是测试中的差异与已测量的相似（特别是*p*-值下降时），但无法确保。
- en: The *t*-test is typically used in conjunction with an *α-value*, which is a
    (somewhat arbitrary) point at which the result is assumed to have statistical
    significance. The *α*-value is commonly set to 0.1—which means that a result is
    considered statistically significant if the specimen and baseline will be the
    same only 10% (0.1) of the time (or conversely, that 90% of the time the specimen
    and baseline differ). Other commonly used *α*-values are 0.05 (95%) or 0.01 (99%).
    A test is considered statistically significant if the *p*-value is larger than
    1 – *α*-value.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*t*-检验通常与*α值*结合使用，*α值*是一个（有些任意的）点，假设结果具有统计显著性。*α*-值通常设置为0.1——这意味着如果在样本和基线相同的情况下只有10%的时间（或者反过来说，90%的时间样本和基线不同），结果被认为具有统计显著性。其他常用的*α*-值有0.05（95%）或0.01（99%）。如果*p*-值大于1
    – *α*-值，则测试被认为具有统计显著性。'
- en: Hence, the proper way to search for regressions in code is to determine a level
    of statistical significance—say, 90%—and then to use the *t*-test to determine
    if the specimen and baseline are different within that degree of statistical significance.
    Care must be taken to understand what it means if the test for statistical significance
    fails. In the example, the *p*-value is 0.43; we cannot say that there is statistical
    significance within a 90% confidence level that the result indicates that the
    averages are different. The fact that the test is not statistically significant
    does not mean that it is an insignificant result; it simply means that the test
    is inconclusive.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在代码中搜索回归的正确方法是确定一个统计显著性水平——比如说，90%——然后使用*t*-检验来确定在该统计显著性水平内样本和基线是否不同。必须注意理解如果统计显著性检验失败意味着什么。在这个例子中，*p*-值为0.43；我们不能说在90%置信水平下这个结果表明平均值不同具有统计显著性。事实上，测试结果不具有统计显著性并不意味着它是无意义的；它只是表明测试结果不明确。
- en: The usual reason a test is statistically inconclusive is that the samples don’t
    have enough data. So far, our example has looked at a series with three results
    in the baseline and the specimen. What if three additional results are added,
    yielding the data in [Table 2-3](#TableTTest2)?
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 测试统计不明确的通常原因是样本数据不足。到目前为止，我们的例子看了一系列包含基线和样本的三个结果的情况。如果再增加三个结果，就会得到表2-3中的数据？
- en: Table 2-3\. Increased sample size of hypothetical times
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-3\. 假设时间的增加样本量
- en: '|  | Baseline | Specimen |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | 基线 | 样本 |'
- en: '| --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| First iteration | 1.0 second | 0.5 second |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 第一次迭代 | 1.0 秒 | 0.5 秒 |'
- en: '| Second iteration | 0.8 second | 1.25 second |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 第二次迭代 | 0.8 秒 | 1.25 秒 |'
- en: '| Third iteration | 1.2 seconds | 0.5 second |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 第三次迭代 | 1.2 秒 | 0.5 秒 |'
- en: '| Fourth iteration | 1.1 seconds | 0.8 second |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 第四次迭代 | 1.1 秒 | 0.8 秒 |'
- en: '| Fifth iteration | 0.7 second | 0.7 second |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 第五次迭代 | 0.7 秒 | 0.7 秒 |'
- en: '| Sixth iteration | 1.2 seconds | 0.75 second |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 第六次迭代 | 1.2 秒 | 0.75 秒 |'
- en: '| **Average** | **1 second** | **0.75 second** |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| **平均值** | **1 秒** | **0.75 秒** |'
- en: 'With the additional data, the *p*-value drops from 0.43 to 0.11: the probability
    that the results are different has risen from 57% to 89%. The averages haven’t
    changed; we just have more confidence that the difference is not due to random
    variation.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 随着额外的数据，*p*-值从0.43下降到0.11：结果不同的概率从57%上升到89%。平均值没有改变；我们只是更有信心，差异不是由于随机变化造成的。
- en: Running additional tests until a level of statistical significance is achieved
    isn’t always practical. It isn’t, strictly speaking, necessary either. The choice
    of the *α*-value that determines statistical significance is arbitrary, even if
    the usual choice is common. A *p*-value of 0.11 is not statistically significant
    within a 90% confidence level, but it is statistically significant within an 89%
    confidence level.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 运行额外的测试直到达到统计显著性水平并非总是切实可行。严格来说，也并非必要。决定统计显著性的*α*值是任意的，即使通常选择是常见的。在90%置信水平内，*p*值为0.11不具有统计显著性，但在89%置信水平内具有统计显著性。
- en: Regression testing is important, but it’s not a black-and-white science. You
    cannot look at a series of numbers (or their averages) and make a judgment that
    compares them without doing some statistical analysis to understand what the numbers
    mean. Yet even that analysis cannot yield a completely definitive answer, because
    of the laws of probabilities. The job of a performance engineer is to look at
    the data, understand the probabilities, and determine where to spend time based
    on all the available data.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 回归测试很重要，但它并不是一门黑白分明的科学。您不能仅仅查看一系列数字（或其平均值）并进行比较，而不进行一些统计分析以理解这些数字的含义。然而，即使进行了分析，也不能得出完全确切的答案，因为概率定律的影响。性能工程师的工作是查看数据，理解概率，并根据所有可用数据决定在哪里花费时间。
- en: Quick Summary
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速摘要
- en: Correctly determining whether results from two tests are different requires
    a level of statistical analysis to make sure that perceived differences are not
    the result of random chance.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确确定两个测试结果是否不同需要进行一定水平的统计分析，以确保所感知的差异不是由于随机机会造成的。
- en: The rigorous way to accomplish that is to use Student’s *t*-test to compare
    the results.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现这一点的严格方法是使用学生的*t*检验来比较结果。
- en: Data from the *t*-test tells us the probability that a regression exists, but
    it doesn’t tell us which regressions should be ignored and which must be pursued.
    Finding that balance is part of the art of performance engineering.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t*-检验的数据告诉我们存在回归的概率，但它并没有告诉我们应该忽略哪些回归，哪些必须追究。找到平衡点是性能工程的一部分艺术。'
- en: Test Early, Test Often
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 早测，多测
- en: Fourth and, finally, performance geeks (including me) like to recommend that
    performance testing be an integral part of the development cycle. In an ideal
    world, performance tests would be run as part of the process when code is checked
    into the central repository; code that introduces performance regressions would
    be blocked from being checked in.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，性能极客（包括我在内）建议性能测试成为开发周期的一个组成部分。在理想的情况下，性能测试应作为代码提交到中央仓库的过程的一部分运行；引入性能回归的代码将被阻止提交。
- en: Some inherent tension exists between that recommendation and other recommendations
    in this chapter—and between that recommendation and the real world. A good performance
    test will encompass a lot of code—at least a medium-sized mesobenchmark. It will
    need to be repeated multiple times to establish confidence that any difference
    found between the old code and the new code is a real difference and not just
    random variation. On a large project, this can take a few days or a week, making
    it unrealistic to run performance tests on code before checking it into a repository.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其他建议以及现实世界之间存在一定的张力。一次良好的性能测试将包含大量代码，至少是一个中等规模的中型基准测试。它需要多次重复以确保在旧代码和新代码之间找到的任何差异是真实的差异，而不仅仅是随机变化。在大型项目中，这可能需要几天甚至一周的时间，因此在将代码提交到代码库之前进行性能测试是不现实的。
- en: 'The typical development cycle does not make things any easier. A project schedule
    often establishes a feature-freeze date: all feature changes to code must be checked
    into the repository at an early point in the release cycle, and the remainder
    of the cycle is devoted to shaking out any bugs (including performance issues)
    in the new release. This causes two problems for early testing:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的开发周期并不会使事情变得更容易。项目进度表通常会设定一个特性冻结日期：所有代码的特性更改必须在发布周期的早期阶段提交到代码库，而剩余的周期则用于解决新发布版本中的任何错误（包括性能问题）。这给早期测试带来了两个问题：
- en: Developers are under time constraints to get code checked in to meet the schedule;
    they will balk at having to spend time fixing a performance issue when the schedule
    has time for that after all the initial code is checked in. The developer who
    checks in code causing a 1% regression early in the cycle will face pressure to
    fix that issue; the developer who waits until the evening of the feature freeze
    can check in code that causes a 20% regression and deal with it later.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发人员必须在时间限制内完成代码审核以满足计划；当计划中有时间供所有初始代码审核后解决性能问题时，他们会反对花时间解决性能问题。在周期早期提交引起1%回归的代码的开发人员将面临修复该问题的压力；等到功能冻结的晚上，可以提交引起20%回归的代码，以后再处理。
- en: 'Performance characteristics of code will change as the code changes. This is
    the same principle that argued for testing the full application (in addition to
    any module-level tests that may occur): heap usage will change, code compilation
    will change, and so on.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着代码的改变，代码的性能特征也会改变。这是与测试完整应用程序的相同原则（除了可能发生的任何模块级测试）：堆使用量将改变，代码编译将改变等等。
- en: 'Despite these challenges, frequent performance testing during development is
    important, even if the issues cannot be immediately addressed. A developer who
    introduces code causing a 5% regression may have a plan to address that regression
    as development proceeds: maybe the code depends on an as-yet-to-be integrated
    feature, and when that feature is available, a small tweak will allow the regression
    to go away. That’s a reasonable position, even though it means that performance
    tests will have to live with that 5% regression for a few weeks (and the unfortunate
    but unavoidable issue that said regression is masking other regressions).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些挑战，开发过程中频繁进行性能测试是重要的，即使问题无法立即解决。引入导致5%回归的代码的开发人员可能有计划在开发过程中解决该回归：也许代码依赖于尚未集成的功能，当该功能可用时，稍作调整即可使回归消失。尽管这意味着性能测试将不得不在几周内忍受这个5%的回归（以及不幸但不可避免的问题，即该回归正在掩盖其他回归），但这是一个合理的立场。
- en: On the other hand, if the new code causes a regression that can be fixed with
    only architectural changes, it is better to catch the regression and address it
    early, before the rest of the code starts to depend on the new implementation.
    It’s a balancing act, requiring analytic and often political skills.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果新代码引起的回归可以仅通过架构更改来修复，最好是在其他代码开始依赖新实现之前尽早捕捉回归并加以解决。这是一种权衡，需要分析和通常还需要政治技巧。
- en: 'Early, frequent testing is most useful if the following guidelines are followed:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遵循以下准则，早期频繁测试是最有用的：
- en: Automate everything
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都要自动化
- en: 'All performance testing should be scripted (or programmed, though scripting
    is usually easier). Scripts must be able to install the new code, configure it
    into the full environment (creating database connections, setting up user accounts,
    and so on), and run the set of tests. But it doesn’t stop there: the scripts must
    be able to run the test multiple times, perform *t*-test analysis on the results,
    and produce a report showing the confidence level that the results are the same,
    and the measured difference if they are not the same.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 所有性能测试都应该是脚本化的（或者编程化，尽管脚本通常更容易）。脚本必须能够安装新代码，将其配置到完整环境中（创建数据库连接，设置用户帐户等），并运行一组测试。但事情并不止于此：脚本必须能够多次运行测试，对结果进行*t*-test分析，并生成报告，显示结果相同的置信水平，以及如果结果不同则测得的差异。
- en: 'The automation must make sure that the machine is in a known state before tests
    are run: it must check that no unexpected processes are running, that the OS configuration
    is correct, and so on. A performance test is repeatable only if the environment
    is the same from run to run; the automation must take care of that.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化必须确保在运行测试之前机器处于已知状态：必须检查是否运行了意外进程，操作系统配置是否正确等等。只有在每次运行时环境相同，性能测试才是可重复的；自动化必须处理这一点。
- en: Measure everything
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 测量一切
- en: 'The automation must gather every conceivable piece of data that will be useful
    for later analysis. This includes system information sampled throughout the run:
    CPU usage, disk usage, network usage, memory usage, and so on. It includes logs
    from the application—both those the application produces, and the logs from the
    garbage collector. Ideally, it can include Java Flight Recorder (JFR) recordings
    (see [Chapter 3](ch03.html#Tools)) or other low-impact profiling information,
    periodic thread stacks, and other heap analysis data like histograms or full heap
    dumps (though the full heap dumps, in particular, take a lot of space and cannot
    necessarily be kept long-term).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化必须收集每一个可能对后续分析有用的数据片段。这包括在整个运行过程中抽样的系统信息：CPU使用率、磁盘使用率、网络使用率、内存使用率等等。它包括应用程序生成的日志以及垃圾收集器的日志。理想情况下，它可以包括Java
    Flight Recorder（JFR）记录（参见[第3章](ch03.html#Tools)）或其他低影响的分析信息，定期的线程堆栈以及像直方图或完整堆转储这样的堆分析数据（尽管特别是完整堆转储占用大量空间，不能长期保留）。
- en: 'The monitoring information must also include data from other parts of the system,
    if applicable: for example, if the program uses a database, include the system
    statistics from the database machine as well as any diagnostic output from the
    database (including performance reports like Oracle’s Automatic Workload Repository,
    or AWR, reports).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 监控信息还必须包括系统其他部分（如果适用）的数据：例如，如果程序使用数据库，则包括数据库机器的系统统计信息以及数据库的任何诊断输出（包括Oracle的自动工作负载存储库，或AWR报告等性能报告）。
- en: 'This data will guide the analysis of any regressions that are uncovered. If
    the CPU usage has increased, it’s time to consult the profile information to see
    what is taking more time. If the time spent in GC has increased, it’s time to
    consult the heap profiles to see what is consuming more memory. If CPU time and
    GC time have decreased, contention somewhere has likely slowed performance: stack
    data can point to particular synchronization bottlenecks (see [Chapter 9](ch09.html#ThreadPerformance)),
    JFR recordings can be used to find application latencies, or database logs can
    point to something that has increased database contention.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据将指导对发现的任何回归的分析。如果CPU使用率增加，那么是时候查看概要信息，看看是什么花费了更多时间。如果GC时间增加，那么是时候查看堆概要信息，看看是什么消耗了更多内存。如果CPU时间和GC时间减少，那么某处的争用可能已经减慢了性能：堆栈数据可以指向特定的同步瓶颈（参见[第9章](ch09.html#ThreadPerformance)），JFR记录可用于查找应用程序延迟，或者数据库日志可以指出增加的数据库争用情况。
- en: When figuring out the source of a regression, it is time to play detective,
    and the more data that is available, the more clues there are to follow. As discussed
    in [Chapter 1](ch01.html#Introduction), it isn’t necessarily the case that the
    JVM is the regression. Measure everything, everywhere, to make sure the correct
    analysis can be done.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当解决回归问题的源头时，就是扮演侦探的时候了，而有更多可用数据时，就有更多线索可以追踪。正如在[第1章](ch01.html#Introduction)中讨论的那样，并非总是JVM导致回归。要全面测量，以确保能进行正确的分析。
- en: Run on the target system
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在目标系统上运行
- en: 'A test that is run on a single-core laptop will behave differently than a test
    run on a machine with 72 cores. That should be clear in terms of threading effects:
    the larger machine is going to run more threads at the same time, reducing contention
    among application threads for access to the CPU. At the same time, the large system
    will show synchronization bottlenecks that would be unnoticed on the small laptop.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在单核笔记本上运行的测试与在具有72个核心的机器上运行的测试会表现出不同的行为。这在线程效果方面应该是显而易见的：更大的机器将同时运行更多线程，减少应用线程之间竞争CPU访问的情况。与此同时，大系统将显示同步瓶颈，而这在小型笔记本上可能会被忽视。
- en: Other performance differences are just as important, even if they are not as
    immediately obvious. Many important tuning flags calculate their defaults based
    on the underlying hardware the JVM is running on. Code is compiled differently
    from platform to platform. Caches—software and, more importantly, hardware—behave
    differently on different systems and under different loads. And so on…
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 其他性能差异同样重要，即使它们并不像立即显而易见。许多重要的调整标志根据JVM运行的底层硬件计算其默认值。代码在不同平台上编译不同。缓存（软件和更重要的硬件）在不同系统和不同负载下的行为也会不同。等等...
- en: Hence, the performance of a particular production environment can never be fully
    known without testing the expected load on the expected hardware. Approximations
    and extrapolations can be made from running smaller tests on smaller hardware,
    and in the real world, duplicating a production environment for testing can be
    quite difficult or expensive. But extrapolations are simply predictions, and even
    in the best case, predictions can be wrong. A large-scale system is more than
    the sum of its parts, and there can be no substitute for performing adequate load
    testing on the target platform.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，除非在预期的硬件上测试预期负载，否则无法完全了解特定生产环境的性能。可以从较小的硬件上运行较小的测试中进行近似和推断，但在现实世界中，为测试复制生产环境可能非常困难或昂贵。但推断只是预测，即使在最佳情况下，预测也可能是错误的。大规模系统不仅仅是其各部分的总和，对目标平台进行充分的负载测试是无法替代的。
- en: Quick Summary
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速总结
- en: Frequent performance testing is important, but it doesn’t occur in a vacuum;
    there are trade-offs to consider regarding the normal development cycle.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 频繁的性能测试很重要，但这并不是孤立进行的；在正常的开发周期中，需要考虑一些权衡。
- en: An automated testing system that collects all possible statistics from all machines
    and programs will provide the necessary clues to any performance regressions.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个自动化测试系统，从所有机器和程序中收集所有可能的统计信息，将为任何性能回归提供必要的线索。
- en: Benchmark Examples
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基准测试示例
- en: Some examples in this book use `jmh` to provide a microbenchmark. In this section,
    we’ll look in depth at how one such microbenchmark is developed as an example
    of how to write your own `jmh` benchmark. But many examples in this book are based
    on variants of a mesobenchmark—a test that is complex enough to exercise various
    JVM features but less complex than a real application. So following our exploration
    of `jmh`, we’ll look through some of the common code examples of the mesobenchmark
    used in later chapters so that those examples have some context.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的一些示例使用`jmh`提供微基准测试。在本节中，我们将深入研究如何开发这样一个微基准测试示例，作为编写自己`jmh`基准测试的示例。但本书中的许多示例都是基于mesobenchmark的变体——这是一个复杂到可以测试各种JVM特性但比实际应用程序复杂度低的测试。因此，在我们探讨完`jmh`之后，我们将查看后续章节中使用的mesobenchmark的一些常见代码示例，以便这些示例有一些背景知识。
- en: Java Microbenchmark Harness
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Java微基准测试工具
- en: '`jmh` is a set of classes that supply a framework for writing benchmarks. The
    *m* in `jmh` used to stand for *microbenchmark*, though now `jmh` advertises itself
    as suitable for nano/micro/milli/macro benchmarks. Its typical usage, though,
    remains small (micro) benchmarks. Although `jmh` was announced in conjunction
    with Java 9, it isn’t really tied to any specific Java release, and no tools in
    the JDK support `jmh`. The class libraries that make up `jmh` are compatible with
    JDK 8 and later releases.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`jmh`是一组供编写基准测试的类。`jmh`中的*m*曾代表*microbenchmark*，尽管现在`jmh`宣称适用于nano/micro/milli/macro基准测试。尽管`jmh`是与Java
    9一同宣布的，但它实际上并未与任何特定的Java版本绑定，JDK中也没有支持`jmh`的工具。构成`jmh`的类库与JDK 8及更高版本兼容。'
- en: '`jmh` takes some of the uncertainty out of writing a good benchmark, but it
    is not a silver bullet; you still must understand what you’re benchmarking and
    how to write good benchmark code. But the features of `jmh` are designed to make
    that easier.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`jmh`消除了编写良好基准测试的一些不确定性，但它并非解决所有问题的灵丹妙药；您仍然必须理解您正在进行基准测试的内容以及如何编写良好的基准测试代码。但`jmh`的特性旨在使这一过程更加简单。'
- en: '`jmh` is used for a few examples in the book, including a test for JVM parameters
    that affect string interning presented in [Chapter 12](ch12.html#Misc). We’ll
    use that example here to understand how to write a benchmark using `jmh`.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中使用`jmh`的几个示例，包括测试影响字符串国际化的JVM参数的测试，该测试在[第12章](ch12.html#Misc)中进行了介绍。我们将在此使用该示例来理解如何使用`jmh`编写基准测试。
- en: 'It is possible to write a `jmh` benchmark from scratch, but it is easier to
    start with a `jmh`-provided main class and write only the benchmark-specific code.
    And while it is possible to get the necessary `jmh` classes by using a variety
    of tools (and even certain IDEs), the basic method is to use Maven. The following
    command will create a Maven project that we can add our benchmark code to:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始编写`jmh`基准测试是可能的，但更容易的是从`jmh`提供的主类开始，并仅编写特定于基准测试的代码。虽然可以使用各种工具（甚至某些集成开发环境）获取必要的`jmh`类，但基本方法是使用Maven。下面的命令将创建一个Maven项目，我们可以向其添加我们的基准测试代码：
- en: '[PRE4]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This creates the Maven project in the *string-intern-benchmark* directory; there,
    it creates a directory with the given `groupId` name, and a skeleton benchmark
    class called `MyBenchmark`. There’s nothing special about that name; you can create
    a different (or several different) classes, since `jmh` will figure out which
    classes to test by looking for an annotation called `Benchmark`.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在 *string-intern-benchmark* 目录中创建 Maven 项目；在那里，它创建了一个以给定 `groupId` 名称命名的目录，并且一个名为
    `MyBenchmark` 的骨架基准类。该名称并不特殊；您可以创建一个不同的（或多个不同的）类，因为 `jmh` 将通过查找称为 `Benchmark`
    的注解来确定要测试的类。
- en: 'We’re interested in testing the performance of the `String.intern()` method,
    so the first benchmark method we would write looks like this:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有兴趣测试 `String.intern()` 方法的性能，因此我们将编写的第一个基准方法如下所示：
- en: '[PRE5]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The basic outline of the `testIntern()` method should make sense: we are testing
    the time to create 10,000 interned strings. The `Blackhole` class used here is
    a `jmh` feature that addresses one of the points about microbenchmarking: if the
    value of an operation isn’t used, the compiler is free to optimize out the operation.
    So we make sure that the values are used by passing them to the `consume()` method
    of the `Blackhole`.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '`testIntern()` 方法的基本概述应该是有意义的：我们正在测试创建 10,000 个 interned 字符串的时间。这里使用的 `Blackhole`
    类是 `jmh` 的一个特性，解决了微基准测试中的一个问题：如果不使用操作的值，编译器可以自由地优化掉该操作。因此，我们通过将它们传递给 `Blackhole`
    的 `consume()` 方法来确保值被使用。'
- en: 'In this example, the `Blackhole` isn’t strictly needed: we’re really interested
    in only the side effects of calling the `intern()` method, which is going to insert
    a string into a global hash table. That state change cannot be compiled away even
    if we don’t use the return value from the `intern()` method itself. Still, rather
    than puzzle through whether it’s necessary to consume a particular value, it’s
    better to be in the habit of making sure the operation will execute as we expect
    and consume calculated values.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`Blackhole` 并不是严格必需的：我们真正感兴趣的只是调用 `intern()` 方法的副作用，它将字符串插入全局哈希表中。即使我们不使用
    `intern()` 方法本身的返回值，这种状态变化也无法被编译器优化掉。但是，与其费力去研究是否有必要消耗特定值，还不如养成确保操作按预期执行并消耗计算值的习惯。
- en: 'To compile and run the benchmark:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 编译并运行基准测试：
- en: '[PRE6]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: From the output, we can see how `jmh` helps avoid the pitfalls that we discussed
    earlier in this chapter. First, see that we execute five warm-up iterations of
    10 seconds each, followed by five measurement iterations (also of 10 seconds each).
    The warm-up iterations allow the compiler to fully optimize the code, and then
    the harness will report information from only the iterations of that compiled
    code.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中可以看出，`jmh` 帮助我们避免了本章前面讨论过的陷阱。首先，我们执行了五次每次 10 秒的预热迭代，然后是五次测量迭代（同样每次 10 秒）。预热迭代允许编译器充分优化代码，然后测试框架将仅报告编译后代码的迭代信息。
- en: Then see that there are different forks (five in all). The harness is repeating
    the test five times, each time in a separate (newly forked) JVM in order to determine
    the repeatability of the results. And each JVM needs to warm up and then measure
    the code. A forked test like this (with warm-up and measurement intervals) is
    called a *trial*. In all, each test takes one hundred seconds for 5 warm-up and
    5 measurement cycles; that is all repeated 5 times, and the total execution time
    is 8:20 minutes.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后看到有不同的 fork（共五个）。测试框架重复测试五次，每次在一个单独的（新 fork 的）JVM 中，以确定结果的可重复性。每个 JVM 需要预热，然后测量代码。像这样的
    forked 测试（带有预热和测量间隔）称为 *trial*。总体来说，每个测试需要 5 次预热和 5 次测量循环，总执行时间为 8 分 20 秒。
- en: 'Finally, we have the summary output: on average, the `testIntern()` method
    executed 177 times per second. With a confidence interval of 99.9%, we can say
    that the statistical average lies between 167 and 187 operations per second. So
    `jmh` also helps us with the necessary statistical analysis we need to understand
    if a particular result is running with acceptable variance.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到了汇总输出：平均而言，`testIntern()` 方法每秒执行 177 次。在 99.9% 的置信区间下，我们可以说统计平均值在每秒 167
    到 187 次操作之间波动。因此，`jmh` 还帮助我们进行必要的统计分析，以了解特定结果是否具有可接受的变化范围。
- en: JMH and parameters
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: JMH 和参数
- en: 'Often you want a range of input for a test; in this example, we’d like to see
    the effect of interning 1 or 10,000 (or maybe even 1 million) strings. Rather
    than hardcoding that value in the `testIntern()` method, we can introduce a parameter:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您希望测试的输入范围；在这个例子中，我们想看看内部化1或10,000个（甚至1百万）字符串的效果。与在`testIntern()`方法中硬编码该值不同，我们可以引入一个参数：
- en: '[PRE7]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now `jmh` will report results for both values of the parameter:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`jmh`将报告两个参数值的结果：
- en: '[PRE8]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Predictably, with a loop size of 10,000, the number of times the loop is run
    per second is reduced by a factor of 10,000\. In fact, the result for 10,000 strings
    is less than something around 283 as we might hope, which is due to the way the
    string intern table scales (which is explained when we use this benchmark in [Chapter 12](ch12.html#Misc)).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 可预见地，循环大小为10,000时，每秒运行的循环次数将减少10,000倍。实际上，10,000个字符串的结果少于我们可能希望的约283，这是由字符串内部化表的缩放方式引起的（这在我们在[第12章](ch12.html#Misc)中使用这个基准时有解释）。
- en: 'It’s usually easier to have a single simple value for the parameter in the
    source code and use that for testing. When you run the benchmark, you can give
    it a list of values to use for each parameter, overriding the value that is hardcoded
    in the Java code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，将源代码中的参数设为单一简单值并用于测试会更容易。当您运行基准测试时，可以为每个参数提供一个值列表，覆盖Java代码中硬编码的值：
- en: '[PRE9]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Comparing tests
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较测试
- en: 'The genesis of this benchmark is to figure out if we can make string interning
    faster by using different JVM tunings. To do that, we’ll run the benchmark with
    different JVM arguments by specifying those arguments on the command line:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基准的起源在于，我们想弄清楚是否可以通过使用不同的JVM调优使字符串内部化速度更快。为了达到这个目的，我们将通过在命令行上指定这些参数来使用不同的JVM参数运行基准测试：
- en: '[PRE10]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Then we can manually inspect and compare the difference the tuning has made
    in our result.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以手动检查和比较调优对结果产生的影响。
- en: 'More commonly, you want to compare two implementations of code. String interning
    is fine, but could we do better by using a simple hash map and managing that?
    To test that, we would define another method in the class and annotate that with
    the `Benchmark` annotation. Our first (and suboptimal) pass at that would look
    like this:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 更常见的情况是，您希望比较两种代码实现方式。字符串内部化是不错的选择，但是如果我们使用一个简单的哈希映射并进行管理，能否更好呢？为了测试这一点，我们将在类中定义另一个方法，并使用`Benchmark`注解进行标注。我们第一次（也是次优的）尝试看起来像这样：
- en: '[PRE11]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`jmh` will run all annotated methods through the same series of warm-up and
    measurement iterations (always in newly forked JVMs) and produce a nice comparison:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`jmh`将通过同一系列的预热和测量迭代（始终在新分叉的JVM中）运行所有带注解的方法，并生成一个很好的比较：'
- en: '[PRE12]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Managing the interned objects by hand has given a nice improvement here (though
    beware: issues with the benchmark as written remain; this isn’t the final word).'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里手动管理内部化的对象确实有了很好的改进（尽管请注意：写法上可能存在问题；这并非最终结论）。
- en: Setup code
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置代码
- en: 'The previous output is abbreviated for this format, but when you run `jmh`
    tests, you’ll see a long caveat before the results are printed. The gist of this
    caveat is “just because you put some code into `jmh`, don’t assume you’ve written
    a good benchmark: test your code and make sure it’s testing what you expect.”'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 由于格式限制，上述输出已经被省略，但当您运行`jmh`测试时，您将在结果打印之前看到一个长长的警告。这个警告的要点是：“只是因为你把一些代码放进了`jmh`，不要假设你写了一个好的基准测试：测试你的代码，确保它测试的是你期望的内容。”
- en: 'Let’s look again at the benchmark definition. We wanted to test how long it
    took to intern 10,000 strings, but what we are testing is the time it takes to
    create (via concatenation) 10,000 strings plus the time it takes to intern the
    resulting strings. The range of those strings is also fairly limited: they are
    the same initial 17 characters followed by an integer. In the same way we pre-created
    input for the handwritten Fibonacci microbenchmark, we should pre-create input
    in this case.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次看看基准定义。我们想测试将10,000个字符串内部化所需的时间，但我们正在测试的是创建（通过连接）10,000个字符串的时间加上内部化所需的时间。这些字符串的范围也相当有限：它们是相同的初始17个字符，后跟一个整数。与我们为手写的斐波那契微基准测试预先创建输入的方式相同，我们也应该在这种情况下预先创建输入。
- en: It could be argued that the range of strings doesn’t matter to this benchmark
    and the concatenation is minor and that hence the original test is completely
    accurate. That is possibly true, but proving that point requires some work. Better
    just to write a benchmark where those questions aren’t an issue than to make assumptions
    about what’s going on.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'We also have to think deeply about what is going on in the test. Essentially,
    the table holding the interned strings is a cache: the interned string might be
    there (in which case it is returned), or it might not (in which case it is inserted).
    And now we have a problem when we compare the implementations: the manually managed
    concurrent hash map is never cleared during a test. That means that during the
    first warm-up cycle, the strings are inserted into the map, and in subsequent
    measurement cycles, the strings are already there: the test has a 100% hit rate
    on the cache.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'The string intern table doesn’t work that way: the keys in the string intern
    table are essentially weak references. Hence, the JVM may clear some or all entries
    at any point in time (since the interned string goes out of scope immediately
    after it is inserted into the table). The cache hit rate in this case is indeterminate,
    but it is likely not anywhere close to 100%. So as it stands now, the intern test
    is going to do more work, since it has to update the internal string table more
    frequently (both to delete and then to re-add entries).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Both of these issues will be avoided if we pre-create the strings into a static
    array and then intern them (or insert them into the hash map). Because the static
    array maintains a reference to the string, the reference in the string table will
    not be cleared. Then both tests will have a 100% hit rate during the measurement
    cycles, and the range of strings will be more comprehensive.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to do this initialization outside the measurement period, which is
    accomplished using the `Setup` annotation:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `Level` value given to the `Setup` annotation controls when the given method
    is executed. `Level` can take one of three values:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '`Level.Trial`'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: The setup is done once, when the benchmark code initializes.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '`Level.Iteration`'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The setup is done before each iteration of the benchmark (each measurement cycle).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '`Level.Invocation`'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: The setup is done before each time the test method is executed.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: A similar `Teardown` annotation could be used in other cases to clear state
    if required.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '`jmh` has many additional options, including measuring single invocations of
    a method or measuring average time instead of throughput, passing additional JVM
    arguments to a forked JVM, controlling thread synchronization, and more. My goal
    isn’t to provide a complete reference for `jmh`; rather, this example ideally
    shows the complexity involved even in writing a simple microbenchmark.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Controlling execution and repeatability
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you have a correct microbenchmark, you’ll need to run it in such a way
    that the results make statistical sense for what you are measuring.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: As you’ve just seen, by default `jmh` will run the target method for a period
    of 10 seconds by executing it as many times as necessary over that interval (so
    in the previous example, it was executed an average of 1,772 times over 10 seconds).
    Each 10-second test is an iteration, and by default there were five warm-up iterations
    (where the results were discarded) and five measurement iterations each time a
    new JVM was forked. And that was all repeated for five trials.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: All of that is done so `jmh` can perform the statistical analysis to calculate
    the confidence interval in the result. In the cases presented earlier, the 99.9%
    confidence interval has a range of about 10%, which may or may not be sufficient
    when comparing to other benchmarks.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get a smaller or larger confidence interval by varying these parameters.
    For example, here are the results from running the two benchmarks with a low number
    of measurement iterations and trials:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'That result makes it look like using the `intern()` method is far worse than
    using a map, but look at the range: it is possible that the real result of the
    first case is close to 330 ops/s, while the real result of the second case is
    close to 200 ops/s. Even if that’s unlikely, the ranges here are too broad to
    conclusively decide which is better.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'That result is from having only two forked trials of two iterations each. If
    we increase that to 10 iterations each, we get a better result:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now the ranges are discrete, and we can confidently conclude that the map technique
    is superior (at least with respect to a test with a 100% cache hit-rate and 10,000
    unchanging strings).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: There is no hard-and-fast rule as to how many iterations, how many forked trials,
    or what length of execution will be needed to get enough data so that the results
    are clear like this. If you are comparing two techniques that have little difference
    between them, you’ll need a lot more iterations and trials. On the other hand,
    if they’re that close together, perhaps you’re better off looking at something
    that will have a greater impact on performance. This again is a place where art
    influences science; at some point, you’ll have to decide for yourself where the
    boundaries lie.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'All of these variables—number of iterations, length of each interval, etc.—are
    controlled via command-line arguments to the standard `jmh` benchmark. Here are
    the most relevant ones:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '`-f 5`'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of forked trials to run (default: 5).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '`-wi 5`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of warm-up iterations per trial (default: 5).'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '`-i 5`'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Number of measurement iterations per trial (default: 5).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '`-r 10`'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Minimum length of time (in seconds) of each iteration; an iteration may run
    longer than this, depending on the actual length of the target method.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Increasing these parameters will generally lower the variability of a result
    until you have the desired confidence range. Conversely, for more stable tests,
    lowering these parameters will generally reduce the time required to run the test.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Quick Summary
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`jmh` is a framework and harness for writing microbenchmarks that provides
    help to properly address the requirements of such benchmarks.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jmh` isn’t a replacement for deeply thinking about writing the code that you
    measure; it’s simply a useful tool in its development.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common Code Examples
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many of the examples in this book are based on a sample application that calculates
    the “historical” high and low price of a stock over a range of dates, as well
    as the standard deviation during that time. *Historical* is in quotes here because
    in the application, all the data is fictional; the prices and the stock symbols
    are randomly generated.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic object within the application is a `StockPrice` object that represents
    the price range of a stock on a given day, along with a collection of option prices
    for that stock:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The sample applications typically deal with a collection of these prices, representing
    the history of the stock over a period of time (e.g., 1 year or 25 years, depending
    on the example):'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The basic implementation of this class loads a set of prices from the database:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The architecture of the samples is designed to be loaded from a database, and
    that functionality will be used in the examples in [Chapter 11](ch11.html#Database).
    However, to facilitate running the examples, most of the time they will use a
    mock entity manager that generates random data for the series. In essence, most
    examples are module-level mesobenchmarks that are suitable for illustrating the
    performance issues at hand—but we would have an idea of the actual performance
    of the application only when the full application is run (as in [Chapter 11](ch11.html#Database)).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: One caveat is that numerous examples are therefore dependent on the performance
    of the random number generator in use. Unlike the microbenchmark example, this
    is by design, as it allows the illustration of several performance issues in Java.
    (For that matter, the goal of the examples is to measure the performance of an
    arbitrary thing, and the performance of the random number generator fits that
    goal. That is quite different from a microbenchmark, where including the time
    for generating random numbers would affect the overall calculation.)
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'The examples also heavily depend on the performance of the `BigDecimal` class,
    which is used to store all the data points. This is a standard choice for storing
    currency data; if the currency data is stored as primitive `double` objects, rounding
    of half-pennies and smaller amounts becomes quite problematic. From the perspective
    of writing examples, that choice is also useful as it allows some “business logic”
    or lengthy calculation to occur—particularly in calculating the standard deviation
    of a series of prices. The standard deviation relies on knowing the square root
    of a `BigDecimal` number. The standard Java API doesn’t supply such a routine,
    but the examples use this method:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This is an implementation of the Babylonian method for estimating the square
    root of a number. It isn’t the most efficient implementation; in particular, the
    initial guess could be much better, which would save some iterations. That is
    deliberate because it allows the calculation to take some time (emulating business
    logic), though it does illustrate the basic point made in [Chapter 1](ch01.html#Introduction):
    often the better way to make Java code faster is to write a better algorithm,
    independent of any Java tuning or Java coding practices that are employed.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: The standard deviation, average price, and histogram of an implementation of
    the `StockPriceHistory` interface are all derived values. In different examples,
    these values will be calculated eagerly (when the data is loaded from the entity
    manager) or lazily (when the method to retrieve the data is called). Similarly,
    the `StockPrice` interface references a `StockOptionPrice` interface, which is
    the price of certain options for the given stock on the given day. Those option
    values can be retrieved from the entity manager either eagerly or lazily. In both
    cases, the definition of these interfaces allows these approaches to be compared
    in different situations.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'These interfaces also fit naturally into a Java REST application: users can
    make a call with parameters indicating the symbol and date range for a stock they
    are interested in. In the standard example, the request will go through a standard
    calling using the Java API for RESTful Web Services (JAX-RS) call that parses
    the input parameters, calls an embedded JPA bean to get the underlying data, and
    forwards the response:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This class can inject different implementations of the history bean (for eager
    or lazy initialization, among other things); it will optionally cache the data
    retrieved from the backend database (or mock entity manager). Those are the common
    options when dealing with the performance of an enterprise application (in particular,
    caching data in the middle tier is sometimes considered the big performance advantage
    of an application server). Examples throughout the book examine those trade-offs
    as well.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance testing involves trade-offs. Making good choices among competing
    options is crucial to successfully tracking the performance characteristics of
    a system.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: Choosing what to test is the first area where experience with the application
    and intuition will be of immeasurable help when setting up performance tests.
    Microbenchmarks are helpful to set a guideline for certain operations. That leaves
    a broad continuum of other tests, from small module-level tests to a large, multitiered
    environment. Tests all along that continuum have merit, and choosing the tests
    along that continuum is one place where experience and intuition will come into
    play. However, in the end there can be no substitute for testing a full application
    as it is deployed in production; only then can the full effect of all performance-related
    issues be understood.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, understanding what is and is not an actual regression in code is
    not always a black-and-white issue. Programs always exhibit random behavior, and
    once randomness is injected into the equation, we will never be 100% certain about
    what data means. Applying statistical analysis to the results can help turn the
    analysis to a more objective path, but even then some subjectivity will be involved.
    Understanding the underlying probabilities and what they mean can help to reduce
    that subjectivity.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Finally, with these foundations in place, an automated testing system can be
    set up to gather full information about everything that occurred during the test.
    With the knowledge of what’s going on and what the underlying tests mean, the
    performance analyst can apply both science and art so that the program can exhibit
    the best possible performance.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch02.html#idm45775552311416-marker)) Not that garbage collection should
    be expected to introduce a hundred-second delay, but particularly for tests with
    small average response times, the GC pauses can introduce significant outliers.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch02.html#idm45775552239592-marker)) And though three data points makes
    it easier to understand an example, it is too small to be accurate for any real
    system.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch02.html#idm45775552237160-marker)) *Student*, by the way, is the pen
    name of the scientist who first published the test; it isn’t named that way to
    remind you of graduate school, where you (or at least I) slept through statistics
    class.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
