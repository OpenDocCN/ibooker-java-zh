<html><head></head><body>
<div id="sbo-rt-content" class="calibre2"><section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3"><div class="preface" id="ch_continuous_delivery">
<h1 class="calibre17" id="9H5K4-2d714b853a094e9a910510217e0e3d73"><span class="keep-together">Chapter 5. </span>Safe, Multicloud Continuous Delivery</h1>


<p class="author1"><a data-type="indexterm" data-primary="continuous delivery (CD)" id="ix_ch05-asciidoc0" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="continuous delivery (CD)" data-secondary="multicloud" id="ix_ch05-asciidoc1" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="Spinnaker" data-secondary="continuous delivery with" id="ix_ch05-asciidoc2" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>The placement of this chapter in the second half of this book should signal the importance of telemetry in achieving safe and effective delivery practices. This may come as a surprise, because every organization ships software with an emphasis on testing as a means of guaranteeing safety, but not every organization measures it actively in a way that is directly related to end-user experience.</p>

<p class="author1">The concepts in this chapter will be introduced with a continuous delivery tool called Spinnaker, but as with earlier chapters, a different tool could achieve similar ends. I’d like to establish a minimum base for what you should expect from a worthy CD tool.</p>

<p class="author1"><a data-type="indexterm" data-primary="Spinnaker" data-secondary="origins" id="idm45139265087400" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Spinnaker is an open source continuous delivery solution that started at Netflix in 2014 to help manage its microservices in AWS. It was preceded at Netflix by a tool called Asgard, which was really just an alternative AWS console organized with application developers in mind, and built for Netflix’s unusual scale of AWS consumption. At one point, I was interacting with an AWS console form that required me to select a security group. The UI element in the console was a plain HTML select (list box) with four visible elements. The available security groups were unsorted in the list box, and there were thousands of them (again, because of Netflix’s broad scale in this account)! Usability issues like this led to Asgard, which in turn led to Spinnaker. Asgard was really just an application inventory with some actions (like the AWS console). Spinnaker was conceived as an inventory <em class="calibre12">plus</em> pipelines.</p>

<p class="author1">In 2015 Spinnaker was open sourced and other, initially IaaS implementations were added to it. At various points, Spinnaker has seen significant contributions from Google, Pivotal, Amazon, and Microsoft, as well as end users like Target. Many of these contributors worked together to write a <a href="https://oreil.ly/fuhPb" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">separate book</a> on the topic of <span class="keep-together">Spinnaker.</span></p>

<p class="author1">The practices described in this chapter are applicable to a wide variety of platforms.</p>






</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Types of Platforms" class="calibre3"><div class="preface" id="idm45139265082424">
<h1 class="calibre19" id="calibre_pb_1">Types of Platforms</h1>

<p class="author1"><a data-type="indexterm" data-primary="continuous delivery (CD)" data-secondary="platform types" id="idm45139265080984" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Different types of platforms have a surprising level of commonality in the high-level concepts that make up a running application. The concepts introduced in this chapter will be mostly platform neutral. Platforms fall into one of the following categories:</p>
<dl class="calibre20">
<dt class="calibre21">Infrastructure as a service (IaaS)</dt>
<dd class="calibre22">
<p class="calibre23"><a data-type="indexterm" data-primary="infrastructure as a service (IaaS)" id="idm45139265078056" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>An infrastructure as a service provides virtualized computing resources as a service. An IaaS provider traditionally is responsible for servers, storage, networking hardware, the hypervisor layer, and APIs and other forms of user interface to manage these resources. Originally, to the extent you were using an IaaS, it was as an alternative to having physical hardware. Provisioning resources on an IaaS involves building virtual machine (VM) images. Deploying to an IaaS requires building VM images at some point in the delivery pipeline.</p>
</dd>
<dt class="calibre21">Container as a service (CaaS)</dt>
<dd class="calibre22">
<p class="calibre23"><a data-type="indexterm" data-primary="CaaS (container as a service)" id="idm45139265075528" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="container as a service (CaaS)" id="idm45139265074760" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="PaaS (platform as a service)" id="idm45139265074072" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="platform as a service (PaaS)" id="idm45139265073384" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>A container as a service is a kind of specialization of an IaaS for container-based workloads rather than VMs. It provides a higher level of abstraction for the deployment of apps as containers. Kubernetes has of course become the de facto standard CaaS offered by public cloud vendors and on-prem. It provides a lot of other services that are outside the scope of this book. Deploying to a CaaS requires the extra step of building containers somewhere in the delivery pipeline (often at build time).</p>
</dd>
<dt class="calibre21">Platform as a service (PaaS)</dt>
<dd class="calibre22">
<p class="calibre23">A platform as a service further abstracts away the details of the underlying infrastructure, generally by allowing you to upload an application binary like a JAR or WAR directly to the PaaS API, which is then responsible for building an image and provisioning it. Contrary to the as-a-service implication of PaaS, sometimes PaaS offerings like Cloud Foundry are layered on top of virtualized infrastructure in customer datacenters. They also can be layered on top of IaaS offerings to provide a further abstraction away from the IaaS resource model, which may serve the purpose of preserving some degree of public cloud provider vendor neutrality or permitting similar delivery and management workflows in a hybrid private/public cloud environment.</p>
</dd>
</dl>

<p class="author1">These abstractions can be provided to you by another company, or you can build this cloud native infrastructure yourself (as a number of large companies have). The key requirement is an elastic, self-serve, and API-driven platform upon which to build.</p>

<p class="author1">A key assumption we will make throughout this chapter is that you are building immutable infrastructure. While nothing about an IaaS, for example, prevents you from building a VM image, launching an instance of it, and dropping an application onto it after launch, we are going to assume that the VM image is “baked” along with the application and any supporting software in such a way that when a new instance is provisioned, the application should start and run.</p>

<p class="author1">A further assumption is that applications deployed in this manner are approximately cloud native. The definition of cloud native varies from source to source, but at a minimum the applications that are amenable to the deployment strategies discussed throughout this chapter are stateless. Other elements of <a href="https://12factor.net/" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">12-Factor apps</a> aren’t as crucial.</p>

<p class="author1">For example, I managed a service at Netflix that routinely took over 40 minutes to start, which doesn’t look good against the disposability criteria, but was otherwise unavoidable. The same service used exceedingly high memory footprint instance types in AWS, of which we had a small reserved pool. These put constraints on my choices: I couldn’t have more than about four instances of this service running at any given time, so I wasn’t going to be doing blue/green deployments with several disabled clusters (described in <a data-type="xref" href="part0010_split_010.html#9H5RH-2d714b853a094e9a910510217e0e3d73" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">“Blue/Green Deployment”</a>).</p>

<p class="author1">To further center the discussion around a common language, let’s discuss the resource building blocks common to all of these platforms.</p>
</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Resource Types" class="calibre3"><div class="preface" id="resource_types">
<h1 class="calibre19" id="calibre_pb_2">Resource Types</h1>

<p class="author1"><a data-type="indexterm" data-primary="continuous delivery (CD)" data-secondary="resource types" id="ix_ch05-asciidoc3" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>To keep our discussion of delivery concepts platform-neutral, we will adopt the abstractions as defined by Spinnaker, which are surprisingly portable across different types of platforms:</p>
<dl class="calibre20">
<dt class="calibre21">Instance</dt>
<dd class="calibre22">
<p class="calibre23"><a data-type="indexterm" data-primary="instance, defined" id="idm45139265059496" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>An instance is a running copy of some microservice (that isn’t on a local developer machine, because I sincerely hope production traffic isn’t finding its way there). The AWS EC2 and Cloud Foundry platforms both call this “instance,” conveniently. In Kubernetes, an instance is a pod.</p>
</dd>
<dt class="calibre21">Server group</dt>
<dd class="calibre22">
<p class="calibre23"><a data-type="indexterm" data-primary="server group, defined" id="idm45139265057208" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>A server group represents a collection of instances, managed together. Platforms have different ways of managing a collection of instances, but they tend to have a responsibility for ensuring that a certain number of instances are running. We generally assume that all of the instances of a server group have the same code and configuration, because they are immutable (except when they aren’t). Server groups can logically be devoid of any instances at all but simply have the potential to scale into a nonzero set of instances. In AWS EC2 a server group is an Auto Scaling Group. In Kubernetes a server group is roughly the combination of a Deployment and ReplicaSet (where a deployment manages rollout of ReplicaSets) or a StatefulSet. In Cloud Foundry, a server group is an Application (not to be confused with Application as defined in this list and as we are going to use the term throughout this chapter).</p>
</dd>
<dt class="calibre21">Cluster</dt>
<dd class="calibre22">
<p class="calibre23"><a data-type="indexterm" data-primary="cluster, defined" id="idm45139265054296" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>A cluster is a set of server groups that may span multiple regions. Within a single region, multiple server groups may represent different versions of the microservice. Clusters do <em class="calibre12">not</em> span cloud providers. You could be running two very similar clusters in different cloud providers, but for our discussion they would be considered distinct. A cluster is a logical concept that doesn’t actually have a correlated resource type in any cloud provider. Even more precisely, it doesn’t span multiple installations of a particular platform. So a cluster does not span multiple Cloud Foundry foundations or Kubernetes clusters. There is no higher-level abstraction in AWS EC2 that represents a collection of Auto Scaling Groups or in Kubernetes that represents a collection of Deployments. Spinnaker manages cluster membership by naming conventions or additional metadata that it places on resources it creates, depending on the platform’s implementation in Spinnaker.</p>
</dd>
<dt class="calibre21">Application</dt>
<dd class="calibre22">
<p class="calibre23"><a data-type="indexterm" data-primary="application, defined" id="idm45139265050904" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>An application is a logical business function and not one particular resource. All of the running instances of an application are included. They may span multiple clusters in multiple regions. They may exist on multiple cloud providers, either because you are transitioning from one provider to another, or because you have some concrete business case for not being locked into one provider, or for any other reason. Wherever there is a running process that represents an instance of this business function, it is part of the grander concept called application.</p>
</dd>
<dt class="calibre21">Load balancer</dt>
<dd class="calibre22">
<p class="calibre23"><a data-type="indexterm" data-primary="load balancing" data-secondary="defined" id="idm45139265048344" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>A load balancer is a component that allocates individual requests to instances in one or more server groups. Most load balancers have a set of strategies or algorithms they can use to allocate traffic. Also, they generally have a health-checking feature that allows the load balancer to determine if a candidate microservice instance is healthy enough to receive traffic. In AWS EC2 a load balancer is an Application Load Balancer (or a legacy Elastic Load Balancer). In Kubernetes, the Service resource is a load balancer. The Cloud Foundry Router is a load balancer.</p>
</dd>
<dt class="calibre21">Firewall</dt>
<dd class="calibre22">
<p class="calibre23"><a data-type="indexterm" data-primary="firewall, defined" id="idm45139265045496" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>A firewall is a set of rules that govern ingress and egress to a set of server groups. In AWS EC2 these are called Security Groups.</p>
</dd>
</dl>

<p class="author1"><a data-type="indexterm" data-primary="Kubernetes, Spinnaker’s implementation of" id="idm45139265060424" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Spinnaker’s Kubernetes implementation is a little unique among the providers. Spinnaker can really deploy any Kubernetes resource because it internally uses <code class="calibre24">kubectl apply</code> and passes the manifest to the Kubernetes cluster. Furthermore, Spinnaker allows you to treat manifests as templates and provide variable substitution. It then maps some Kubernetes objects like ReplicaSets/Deployments/StatefulSets to server groups and Services to load balancers.</p>

<p class="author1"><a data-type="xref" href="part0010_split_002.html#spinnaker_k8s_actions" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-1</a> shows a Spinnaker view of a series of Kubernetes ReplicaSets. Note how this infrastructure view also contains actions like Edit, Scale, Disable, and Delete for the selected resource. In this view, <code class="calibre24">replicaSet helloworldapp-frontend</code> is the “Cluster” resource (an amalgamation in this case of the Kubernetes resource type and name), representing a set of ReplicaSets in one or more Kubernetes namespaces. <code class="calibre24">HELLOWORLDWEBAPP-STAGING</code> is the “Region” corresponding to a Kubernetes namespace of the same name. <code class="calibre24">helloworldapp-frontend-v004</code> is a server group (a ReplicaSet). The individual blocks are the “Instances” corresponding to Kubernetes pods.<a data-type="indexterm" data-startref="ix_ch05-asciidoc3" id="idm45139265039896" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/></p>

<figure class="calibre32"><div id="spinnaker_k8s_actions" class="figure">
<img src="../images/00035.png" alt="srej 0501" class="calibre138"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-1. </span>Spinnaker view of three Kubernetes ReplicaSets with actions highlighted</h6>
</div></figure>
</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Delivery Pipelines" class="calibre3"><div class="preface" id="delivery_definitions">
<h1 class="calibre19" id="calibre_pb_3">Delivery Pipelines</h1>

<p class="author1"><a data-type="indexterm" data-primary="continuous delivery (CD)" data-secondary="delivery pipelines" id="ix_ch05-asciidoc4" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="delivery pipelines" data-secondary="continuous delivery and" id="ix_ch05-asciidoc5" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Spinnaker pipelines are just one of many delivery-focused pipeline solutions on the market both commercially and in OSS. They range from the low-level and heavily opinionated Spring Cloud Pipelines to continuous integration pipelining extended to include delivery building blocks like JenkinsX. For the sake of this chapter, we will stick to Spinnaker pipelines, but if you substitute another pipelining solution, look for some key capabilities:</p>
<dl class="calibre20">
<dt class="calibre21">Platform neutrality</dt>
<dd class="calibre22">
<p class="calibre23">A delivery solution doesn’t have to support every possible vendor to qualify as a platform-neutral solution, but delivery solutions based on, for example, Kubernetes custom resource definitions are guaranteed to be locked into a given platform. With this kind of lock-in, any heterogeny in your deployed environment means you are going to be building to multiple tools. Mixed platform use is exceedingly common in enterprises of sufficient scale (as it should be).</p>
</dd>
<dt class="calibre21">Automated triggers</dt>
<dd class="calibre22">
<p class="calibre23">Pipelines should be able to be automatically triggered by events, especially by changes in artifact inputs. We will discuss more about how artifact triggers help you repave your infrastructure in a safe and controlled way in <a data-type="xref" href="part0010_split_004.html#9H5O7-2d714b853a094e9a910510217e0e3d73" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">“Packaging for the Cloud”</a>.</p>
</dd>
<dt class="calibre21">Scalability</dt>
<dd class="calibre22">
<p class="calibre23">A good pipelining solution accounts for the radically different computational nature of different pipeline stages. “Deploy” stages that are exercising platform API endpoints to provision new resources have very low computational needs, even if the stage may run for several minutes. A single instance of a pipeline execution service can easily run thousands of these in parallel. “Execute script” stages that execute something like a Gradle task are arbitrarily resource-intensive, so the execution of them is best delegated to something like a container scheduler such that the resource utilization of the stage execution doesn’t affect the performance of the pipeline execution service.</p>
</dd>
</dl>

<p class="author1">When continuous integration products are used to perform deployment operations, they generally inefficiently use resources in a significant way. For one financial institution I once visited, performing delivery operations with the CI system Concourse was costing several million dollars per year. For this organization, running 30 <code class="calibre24">m4.large</code> reserved instances in EC2 to support a Spinnaker installation would have cost a little over $15,000 per year. The resource inefficiency can easily swing the other direction though. Stages of arbitrary computational complexity should not be run on host or in process with Spinnaker’s Orca (i.e., pipeline) service.</p>

<p class="author1">The various cloud providers <em class="calibre12">feel</em> very different. The deployable resource is a different level of abstraction for each type.</p>

<p class="author1">Spinnaker pipelines consist of stages that are approximately cloud neutral. That is, the same basic building blocks will be available for every cloud provider implementation, but the configuration of a stage like “Deploy” will vary from platform to <span class="keep-together">platform.</span></p>

<p class="author1"><a data-type="xref" href="part0010_split_003.html#spinnaker_k8s_pipeline_definition" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-2</a> shows the definition of a Spinnaker pipeline that is going to deploy to Kubernetes. Pipelines can be arbitrarily complex, containing parallel stages and multiple triggers defined in the special configuration stage up front.</p>

<figure class="calibre32"><div id="spinnaker_k8s_pipeline_definition" class="figure">
<img src="../images/00001.png" alt="srej 0502" class="calibre139"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-2. </span>A detailed view of a Spinnaker pipeline showing multiple stages</h6>
</div></figure>

<p class="author1">Spinnaker defines several different trigger types. This pipeline is triggered by the publication of a new container image in a Docker registry, as shown in <a data-type="xref" href="part0010_split_003.html#spinnaker_k8s_expected_artifact" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-3</a>.</p>

<figure class="calibre32"><div id="spinnaker_k8s_expected_artifact" class="figure">
<img src="../images/00060.png" alt="srej 0503" class="calibre140"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-3. </span>Spinnaker expected artifact definition</h6>
</div></figure>

<p class="author1"><a data-type="xref" href="part0010_split_003.html#spinnaker_k8s_pipelines" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-4</a> shows the execution history of two Spinnaker pipelines, including the one whose configuration we just saw. The staging pipeline was last executed by a Docker registry trigger (a new container published to a Docker registry). In the other circumstances, the pipeline was manually triggered.</p>

<figure class="calibre32"><div id="spinnaker_k8s_pipelines" class="figure">
<img src="../images/00101.png" alt="srej 0504" class="calibre141"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-4. </span>Spinnaker view of two different delivery pipelines</h6>
</div></figure>

<p class="author1">The first task of any delivery pipeline is to package the application in an immutable unit of deployment that can be stamped out in instances across a server group.<a data-type="indexterm" data-startref="ix_ch05-asciidoc5" id="idm45139265010760" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-startref="ix_ch05-asciidoc4" id="idm45139265010056" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/></p>
</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Packaging for the Cloud" class="calibre3"><div class="preface" id="packaging_for_the_cloud">
<h1 class="calibre19" id="9H5O7-2d714b853a094e9a910510217e0e3d73">Packaging for the Cloud</h1>

<p class="author1"><a data-type="indexterm" data-primary="cloud, packaging for" id="ix_ch05-asciidoc6" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="continuous delivery (CD)" data-secondary="packaging for the cloud" id="ix_ch05-asciidoc7" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="packaging" data-secondary="for the cloud" id="ix_ch05-asciidoc8" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>The different abstractions given by the various types of cloud platforms have tradeoffs in terms of startup time, resource efficiency, and cost. But as we’ll see, there shouldn’t be a significant difference in terms of the effort each requires to package a microservice for deployment.</p>

<p class="author1">Generating a new application from <a href="https://start.spring.io" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">start.spring.io</a> includes the generation of a Gradle or Maven build that can generate a runnable JAR. For PaaS platforms like Cloud Foundry and Heroku, this runnable JAR <em class="calibre12">is</em> the input unit of deployment. It is the responsibility of the cloud provider to take this runnable JAR and containerize or otherwise package it and then provision some underlying resource to run this package on.</p>

<p class="author1">For cloud platforms other than PaaS, the effort required from the application team is surprisingly not much different. The examples included here are implemented with Gradle because open source tooling exists for both IaaS and CaaS uses. There is no reason similar tooling couldn’t be produced for Maven.</p>

<p class="author1">One of the typical value propositions of a PaaS is that you provide just the application binary as an input to deployment processes and let the PaaS manage the operating system and package patching on your behalf, even transparently. In practice, it doesn’t work out quite like this. In the case of Cloud Foundry, the platform is responsible for a certain level of patching that is achieved in a rolling manner, affecting one instance at a time in any server group (a.k.a. “application” in Cloud Foundry parlance). But such patching comes with a certain degree of risk: an update to any part of the operating system could adversely affect an application running on it. So there is this risk/reward trade-off that carefully circumscribes the types of changes the platform is willing to automate on behalf of users. All other patches/updates are applied to the “type” of image that the platform will layer your application on. Cloud Foundry calls these buildpacks. For example, Java version upgrades involve an update to the buildpack. The platform does not automatically update buildpack versions for every running application that used the Java buildpack. It really is up to the organization then to redeploy every application using the Java buildpack to pick up the update.</p>

<p class="author1">For non-PaaS environments, with the extra amount of effort involved in generating another type of artifact (other than a JAR) from your build or having an additional stage in your deployment pipeline comes a dramatically greater degree of control and flexibility over how infrastructure can be patched across your organization. While the type of base image is different between IaaS and CaasS (virtual machine and container image, respectively), the principle of baking your application on top of a base image allows you to define your application binary and the base image it is layered upon as separate inputs to each microservice delivery pipeline. <a data-type="xref" href="part0010_split_004.html#base_image_trigger" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-5</a> shows a hypothetical delivery pipeline for a microservice that first deploys to a test environment, runs tests, and goes through an audit check before finally being deployed to production. Note how Spinnaker supports multiple trigger types in this case: one for a new application binaries and one for a new base image.</p>

<figure class="calibre32"><div id="base_image_trigger" class="figure">
<img src="../images/00115.png" alt="srej 0505" class="calibre142"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-5. </span>Changes to the base image trigger pipelines</h6>
</div></figure>

<p class="author1">In the same organization, a different microservice may have more or fewer stages to verify the fitness of the combination of application artifact and base image before promoting to production. Having changes to the base image <em class="calibre12">trigger</em> delivery pipelines is the ideal balance between safety and speed. Microservices whose delivery pipelines contain all fully automated stages may adopt the new base within minutes, where another service with more stringent manual verification and approval stages takes days. Both types of services adopt the change in a way that is best in line with the responsible team’s unique culture and requirements.</p>








</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Packaging for the Cloud" class="calibre3">
<div class="preface" id="packaging_for_the_cloud">
<section data-type="sect2" data-pdf-bookmark="Packaging for IaaS Platforms" class="calibre3"><div class="preface" id="packaging_iaas">
<h2 class="calibre37" id="calibre_pb_5">Packaging for IaaS Platforms</h2>

<p class="author1"><a data-type="indexterm" data-primary="infrastructure as a service (IaaS)" data-secondary="packaging for" id="idm45139264991768" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="packaging" data-secondary="for IaaS platforms" id="idm45139264990728" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>For an IaaS platform, the immutable unit of deployment is a virtual machine image. In AWS EC2, this image is called an Amazon Machine Image. Creating one is a matter of provisioning an instance of the base image (which contains common opinions for all of your microservices like the Java version, common system dependencies, and monitoring and debugging agents.), installing a system dependency on it containing your application binary, snapshotting the resultant image, and configuring new server groups to use this image as the template when provisioning instances of the microservice.</p>

<p class="author1"><a data-type="indexterm" data-primary="baking" id="idm45139264988792" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>The process of provisioning the instance, installing the system dependency, and snapshotting it is collectively called <em class="calibre12">baking</em>. It isn’t always necessary to even launch a live copy of the base image to bake. HashiCorp’s battle-tested <a href="https://www.packer.io" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Packer</a> provides an open source bakery solution that works for a variety of different IaaS providers.</p>

<p class="author1"><a data-type="xref" href="part0010_split_005.html#iaas_packaging_participants" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-6</a> shows where the boundaries of responsibility are for the build tool, the bakery, and the server group managed by the cloud provider. A Spinnaker pipeline stage is responsible for starting the baking process and for creating the server group with the image resulting from the bake stage. It shows that there is one additional requirement of each microservice’s build, the production of a system dependency, meaning the production of a Debian package on an Ubuntu or Debian base image, an RPM on a Red Hat base image, etc. Ultimately, the bakery will in some way be invoking the operating-system-level package installer to layer your application binary onto the base image (e.g., <code class="calibre24">apt-get install &lt;system-package&gt;</code>).</p>

<figure class="calibre32"><div id="iaas_packaging_participants" class="figure">
<img src="../images/00076.png" alt="srej 0506" class="calibre143"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-6. </span>Participants in IaaS packaging</h6>
</div></figure>

<p class="author1">Producing a Debian or RPM system dependency is fortunately straightforward with the application of a Gradle plug-in from Netflix’s Nebula suite of Gradle plug-ins, as shown in <a data-type="xref" href="part0010_split_005.html#nebula_ospackage" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Example 5-1</a>. This adds a Gradle task called <code class="calibre24">buildDeb</code> that does all the work necessary to output a Debian package for a Spring Boot application. It is a one-line change to the build file!</p>
<div id="nebula_ospackage" data-type="example" class="calibre61">
<h5 class="calibre62"><span class="keep-together">Example 5-1. </span>Using a Nebula Gradle plug-in to produce a Debian package</h5>

<pre data-type="programlisting" class="calibre63">plugins {
  id("org.springframework.boot") version "LATEST"
  id("io.spring.dependency-management") version "LATEST"
  id("nebula.ospackage-application-spring-boot") version "LATEST" <a class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre" id="co_safe__multicloud_continuous_delivery_CO1-1" href="part0010_split_005.html#callout_safe__multicloud_continuous_delivery_CO1-1"><img src="../images/00112.png" alt="1" class="calibre66"/></a>
}

...</pre></div>
<dl class="calibre20">
<dt class="calibre21"><a class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre" id="callout_safe__multicloud_continuous_delivery_CO1-1" href="part0010_split_005.html#co_safe__multicloud_continuous_delivery_CO1-1"><img src="../images/00112.png" alt="1" class="calibre66"/></a></dt>
<dd class="calibre67"><p class="calibre68">Replace <code class="calibre24">LATEST</code> with whatever the latest version is on the <a href="https://oreil.ly/xPGaq" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Gradle plug-in portal</a>, because <code class="calibre24">LATEST</code> isn’t actually valid for Gradle plug-in version specifications.</p></dd>
</dl>

<p class="author1">The ospackage plug-in contains a variety of options for adding start scripts, configuring output locations for configuration files and runnable artifacts, etc. Ultimately though, wherever and whatever happens with these files, there should be enough commonality between microservices in an organization to encapsulate these opinions in a similar way to what Netflix has done with <code class="calibre24">nebula.ospackage-application-spring-boot</code> and distribute them as a build tool plug-in that makes adoption trivial.</p>
</div></section>













</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Packaging for the Cloud" class="calibre3">
<div class="preface" id="packaging_for_the_cloud">
<section data-type="sect2" data-pdf-bookmark="Packaging for Container Schedulers" class="calibre3"><div class="preface" id="idm45139264993192">
<h2 class="calibre37" id="calibre_pb_6">Packaging for Container Schedulers</h2>

<p class="author1"><a data-type="indexterm" data-primary="container schedulers, packaging for" id="idm45139264968936" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="packaging" data-secondary="for container schedulers" id="idm45139264968168" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Preparing a microservice for deployment to a container scheduler like Kubernetes can be similar. Opinionated tooling is again available in open source to package for common frameworks like Spring Boot, as shown in <a data-type="xref" href="part0010_split_006.html#muschko_docker_spring_boot" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Example 5-2</a>. This plug-in also understands how to publish to Docker registries, given a little more configuration (which can easily be encapsulated and shipped as a common build tool plug-in across an organization).</p>
<div id="muschko_docker_spring_boot" data-type="example" class="calibre61">
<h5 class="calibre62"><span class="keep-together">Example 5-2. </span>Using a Nebula Gradle plug-in to produce and publish a Docker image</h5>

<pre data-type="programlisting" class="calibre63">plugins {
  id("org.springframework.boot") version "LATEST"
  id("io.spring.dependency-management") version "LATEST"
  id("com.bmuschko.docker-spring-boot-application") version "LATEST"
}

if (hasProperty("dockerUser") &amp;&amp; hasProperty("dockerPassword")) {
  docker {
    registryCredentials {
      username = dockerUser
      password = dockerPassword
      email = "bot@myorg.com"
    }

    springBootApplication {
      tag = "$dockerUser/${project.name}:${project.version}"
      baseImage = "openjdk:8"
    }
  }
}</pre></div>
</div></section>





</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Packaging for the Cloud" class="calibre3">
<div class="preface" id="packaging_for_the_cloud">
<section data-type="sect2" data-pdf-bookmark="Packaging for Container Schedulers" class="calibre3">
<div class="preface" id="idm45139264993192">
<div data-type="warning" type="warning" class="calibre30"><h1 class="calibre69" id="calibre_pb_7">Rely on Open Source Build Tooling, but Be Careful About Consuming Base Images Without Validation</h1>
<p class="author1">It’s great to have tools like Ben Muschko’s Gradle Docker plug-in for producing an image containing an application built on top of some base. But you should expect that somebody in your organization is validating and creating approved images, known to perform well and be free of known defects and security vulnerabilities. This is applicable to both VM and container images.</p>
</div>

<p class="author1">This approach does have the disadvantage that operating system and other system package updates are part of the base Docker image used to produce the application container image. Propagating a base container image change across the whole organization then requires us to rebuild the application binary, which can be inconvenient. After all, to effect a change to <em class="calibre12">only</em> the base image and not the application code (which may have a set of further source code changes since the last time it was built), we have to check out the application code at the hash of the version in production and rebuild with the new image. This process, since it involves the build again, is fraught with the potential for nonreproducibility in the application binary when all we want to do is update the base image.</p>

<p class="author1"><a data-type="indexterm" data-primary="baking" id="idm45139264959512" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Adding a bake stage to containerized workloads simplifies the build by removing the need to publish a container image at all (just publish the JAR to a Maven artifact repository) and allows for mass updating of the base image, again with the same process and safety guarantees that we received by making the base image an artifact trigger for IaaS-based workloads above. Spinnaker supports baking container images with <a href="https://oreil.ly/JpW3V" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Kaniko</a>, removing the need for container image building/publishing to be part of the build workflow. One of the advantages of doing so is that you can rebake the same application binary on a more recent base (say, when you fix a security vulnerability in the base), operating effectively with an immutable copy of the application code.</p>

<p class="author1">Surprisingly then, the desire for safe base updates across all three cloud abstractions (IaaS, CaaS, and PaaS) leads to a remarkably similar workflow for all three (and similar application developer experience). In effect, ease of deployment is no longer a decision criterion between these level of abstractions, and we are left with considering other differentiators like startup time, vendor lock-in, cost, and security.</p>

<p class="author1">Now that we’ve discussed packaging, let’s turn our attention to deployment strategies that can be used to stand up these packages on your platform.<a data-type="indexterm" data-startref="ix_ch05-asciidoc8" id="idm45139264955816" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-startref="ix_ch05-asciidoc7" id="idm45139264955112" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-startref="ix_ch05-asciidoc6" id="idm45139264954440" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/></p>
</div></section>





</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="The Delete + None Deployment" class="calibre3"><div class="preface" id="idm45139265008888">
<h1 class="calibre19" id="calibre_pb_8">The Delete + None Deployment</h1>

<p class="author1"><a data-type="indexterm" data-primary="continuous delivery (CD)" data-secondary="delete + none deployment" id="idm45139264952728" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="delete + none deployment" id="idm45139264951672" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>If the name <em class="calibre12">delete + none</em> sounds ugly, that’s because I’m about to describe a hack that may only be useful in certain narrow situations but helps set the framework for other deployment strategies to follow.</p>

<p class="author1">The basic idea is to simply delete the existing deployment and deploy a new one. The obvious ramification of this is downtime, however short it may be. The existence of downtime suggests that API compatibility across versions isn’t strictly required, provided you coordinate deployments to all callers of a service with a changing API at the same time.</p>

<p class="author1">Every deployment strategy that follows will be zero-downtime.</p>

<p class="author1">To tie this concept to deployment practices you may be familiar with that are <em class="calibre12">not immutable</em>, a delete + none deployment strategy is in use when, on an always-running virtual machine, a new application version is installed and started (replacing the previously running version). Again, this chapter focuses strictly on immutable <span class="keep-together">deployments,</span> and no other deployment strategy that follows has an obvious mutable counterpart.</p>

<p class="author1">The strategy is also in use when performing a basic <code class="calibre24">cf push</code> on Cloud Foundry and an operation on AWS EC2 that reconfigures an Auto Scaling Group to use a different Amazon Machine Image. The point is, often basic CLI or console-based deployment options do accept downtime and operate more or less with this strategy.</p>

<p class="author1">The next strategy is similar, but with zero downtime.</p>
</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="The Highlander" class="calibre3"><div class="preface" id="idm45139264945480">
<h1 class="calibre19" id="calibre_pb_9">The Highlander</h1>

<p class="author1"><a data-type="indexterm" data-primary="continuous delivery (CD)" data-secondary="Highlander deployment" id="idm45139264944040" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="Highlander deployment strategy" id="idm45139264943096" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Despite the odd name, the Highlander strategy is the most common zero-downtime strategy in practice today. The name comes from a slogan from the <em class="calibre12">Highlander</em> movie: “there can be only one.” In other words, when you deploy a new version of a service, you replace the old version. There can be only one. Only the new version is running at the end of the deployment.</p>

<p class="author1">The Highlander strategy is zero-downtime. In practice it involves deploying a new version of the application and adding it to the load balancer, which causes traffic to be served to both for a short period of time while the old version is destroyed automatically. So maybe the more accurate slogan for this deployment strategy is “there is usually only one.” Required API compatibility across versions follows from the existence of this brief overlap.</p>

<p class="author1">The Highlander model is simple, and its simplicity can make it an attractive option for many services. Since there is only one server group at any given time, there is no need to worry about coordinating to prevent interference from “other” running versions that are not supposed to be in service.</p>

<p class="author1">Going back to a previous version of code under a Highlander strategy involves reinstalling an old version of the microservice (which receives a new server group version number). Therefore, the time to completion for this pseudorollback action is the amount of time it takes to install and initialize the application process.</p>

<p class="author1">The next strategy offers faster rollback at the expense of some coordination and <span class="keep-together">complexity.</span></p>
</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Blue/Green Deployment" class="calibre3"><div class="preface" id="blue_green">
<h1 class="calibre19" id="9H5RH-2d714b853a094e9a910510217e0e3d73">Blue/Green Deployment</h1>

<p class="author1"><a data-type="indexterm" data-primary="blue/green deployment" id="ix_ch05-asciidoc9" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="continuous delivery (CD)" data-secondary="blue/green deployment" id="ix_ch05-asciidoc10" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>The blue/green deployment strategy involves having at least two copies of the microservice provisioned (whether in an enabled or disabled state), involving server groups for old and new versions. At any given time, production traffic is being served from one of these versions. Rolling back is a matter of switching which copy is considered live. Rolling forward to the newer version has the same experience. How this <span class="keep-together">switching</span> logic is achieved is cloud-platform-dependent (but orchestrated by Spinnaker), but at a high level it involves influencing the cloud platform’s load balancer abstraction to send traffic to one version or the other.</p>
</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Blue/Green Deployment" class="calibre3">
<div class="preface" id="blue_green">
<div data-type="note" type="note" class="calibre28"><h1 class="calibre54" id="calibre_pb_11">kubectl apply Is a Specific Kind of Blue/Green by Default</h1>
<p class="author1"><a data-type="indexterm" data-primary="kubectl apply" id="idm45139264931128" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><code class="calibre24">kubectl apply</code> that updates a Kubernetes Deployment (from the CLI and not using Spinnaker) is by default a rolling blue/green deployment, allowing you to roll back to a  ReplicaSet representing a previous version. Because it is a container deployment type, the rollback operation involves pulling back the image. The Kubernetes Deployment resource is implemented as a controller on top of ReplicaSet that manages rolling blue/green deployment and rollback. Spinnaker offers more control for Kubernetes ReplicaSets, enabling blue/green functionality with N disabled versions, canary deployments, etc. So think of a Kubernetes Deployment as a limited, opinionated blue/green deployment strategy.</p>
</div>

<p class="author1">Load balancer switching can have implications for the structure of deployed assets. For example, on Kubernetes, blue/green basically requires that you use the ReplicaSet abstraction. The blue/green strategy requires that <em class="calibre12">running</em> resources are somehow edited to influence traffic. For Kubernetes, we can do this with label manipulation, and this is what the Spinnaker Kubernetes implementation does to achieve blue/green. If we instead tried to edit the Kubernetes Deployment object, it would trigger a rollout. Spinnaker automatically adds special labels to ReplicaSets that indirectly cause them to be considered enabled or disabled and a label selector to the service to only route traffic to enabled ReplicaSets. If you aren’t using Spinnaker, you’ll need to create some sort of similar process that mutates labels in place on ReplicaSets and configure your services to be aware of these labels.</p>

<p class="author1">The colors blue/green imply that there are two server groups and either the blue or the green server group is serving traffic. Blue/green strategies aren’t always binary either, and the coloring should not suggest that these server groups need to be long-lived, mutating with new service versions as they become available.</p>

<p class="author1">A blue/green deployment is more generally a 1:N relationship in any given cluster, where one server group is live and N server groups are not live. A visual representation of such a 1:N blue/green cluster is shown in <a data-type="xref" href="part0010_split_011.html#spinnaker_blue_green_cluster" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-7</a>.</p>

<figure class="calibre32"><div id="spinnaker_blue_green_cluster" class="figure">
<img src="../images/00116.png" alt="srej 0507" class="calibre144"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-7. </span>Spinnaker blue/green cluster</h6>
</div></figure>

<p class="author1">The rollback server group action, shown in <a data-type="xref" href="part0010_split_011.html#spinnaker_server_group_actions" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-8</a>, allows for the selection of any one of these disabled server group versions (V023–V026). At the completion of the rollback, the current live version (V027) will still be around, but disabled.</p>

<figure class="calibre32"><div id="spinnaker_server_group_actions" class="figure">
<img src="../images/00032.png" alt="srej 0508" class="calibre145"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-8. </span>Spinnaker rollback server group action</h6>
</div></figure>

<p class="author1">Depending on what the underlying cloud platform can support, a disabled cluster can retain live running instances that aren’t receiving any traffic, or it may be reduced to zero instances, ready at rollback to be scaled back up. To achieve the fastest form of rollbacks, disabled clusters should be left with active instances. This of course increases the expense of running the service, as you now pay not only for the cost of the set of instances serving live production traffic, but also the set of instances remaining from prior service versions that may potentially be rolled back to.</p>

<p class="author1">Ultimately, you need to evaluate the trade-off between rollback speed and cost, the spectrum of which is shown in <a data-type="xref" href="part0010_split_011.html#cost_rollback_speed_tradeoff" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-9</a>. This should be done on a microservice by microservice basis rather than across the organization. The extra operational cost of maintaining fully scaled disabled server groups in a blue/green deployment of a microservice that needs to run hundreds of live instances is not equivalent to the extra cost for such a service that only needs a handful of instances.</p>

<figure class="calibre32"><div id="cost_rollback_speed_tradeoff" class="figure">
<img src="../images/00079.png" alt="srej 0509" class="calibre146"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-9. </span>The trade-off between operational cost and rollback speed by deployment strategy</h6>
</div></figure>

<p class="author1">When a microservice is not purely RESTful, the blue/green deployment strategy that doesn’t completely scale to zero disabled clusters has implications for the application code itself.</p>

<p class="author1">Consider for example an (at least partially) event-driven microservice that reacts to messages on a Kafka topic or RabbitMQ queue. Shifting the load balancer from one server group to another has no effect on such a service’s connection to their topic/queue. In some way, the application code needs to respond to being placed out of service by some external process, in this case a blue/green deployment.</p>

<p class="author1">Similarly, an application process running on an instance that is part of a disabled server group needs to respond to being placed back <em class="calibre12">in service</em> by an external process such as a Rollback server group action in Spinnaker, in this case reconnecting to queues and beginning to process work again. The Spinnaker AWS implementation of the blue/green deployment strategy is aware of this problem, and when <a href="https://oreil.ly/ODfkK" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Eureka</a> service discovery is also in use, it influences service availability using Eureka’s API endpoints for taking instances in and out of service, as shown in <a data-type="xref" href="part0010_split_011.html#eureka_api_availability" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Table 5-1</a>.</p>

<p class="author1">Notice how this is done on a per-instance basis. Spinnaker’s awareness of what instances exist (through polling the state of deployed environment regularly) helps it build this kind of automation.</p>
<table id="eureka_api_availability" class="calibre40">
<caption class="calibre41"><span class="keep-together">Table 5-1. </span>Eureka API endpoints that externally affect service availability</caption>
<thead class="calibre42">
<tr class="calibre43">
<th class="calibre44">Action</th>
<th class="calibre44">API</th>
<th class="calibre44">Notes</th>
</tr>
</thead>
<tbody class="calibre45">
<tr class="calibre46">
<td class="calibre47"><p class="calibre48">Take instance out of service</p></td>
<td class="calibre47"><p class="calibre48"><code class="calibre132">PUT /eureka/v2/apps/appID/instanceID/status?value=OUT_OF_SERVICE</code></p></td>
<td class="calibre47"/>
</tr>
<tr class="calibre50">
<td class="calibre47"><p class="calibre48">Move instance back into service (remove override)</p></td>
<td class="calibre47"><p class="calibre48"><code class="calibre132">DELETE /eureka/v2/apps/appID/instanceID/status?value=UP</code></p></td>
<td class="calibre47"><p class="calibre48">The value=UP is optional; it is used as a suggestion for the fallback status due to removal of the override</p></td>
</tr>
</tbody>
</table>

<p class="author1">This supposes that your application is using the Eureka service discovery client to register with Eureka. But doing so means that you can add a Eureka status-changed event listener, as shown in <a data-type="xref" href="part0010_split_011.html#eureka_event_listener" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Example 5-3</a>.</p>
<div id="eureka_event_listener" data-type="example" class="calibre61">
<h5 class="calibre62"><span class="keep-together">Example 5-3. </span></h5>

<pre data-type="programlisting" data-code-language="java" class="calibre63"><code class="c">// For an application with a dependency on</code>
<code class="c">// 'org.springframework.cloud:spring-cloud-starter-netflix-eureka-client'</code>
<code class="nd">@Bean</code>
<code class="n">ApplicationInfoManager</code><code class="o">.</code><code class="na">StatusChangeListener</code> <code class="nf">statusChangeListener</code><code class="o">()</code> <code class="o">{</code>
  <code class="k">return</code> <code class="k">new</code> <code class="n">ApplicationInfoManager</code><code class="o">.</code><code class="na">StatusChangeListener</code><code class="o">()</code> <code class="o">{</code>
    <code class="nd">@Override</code>
    <code class="k">public</code> <code class="n">String</code> <code class="nf">getId</code><code class="o">()</code> <code class="o">{</code>
      <code class="k">return</code> <code class="s">"blue.green.listener"</code><code class="o">;</code>
    <code class="o">}</code>

    <code class="nd">@Override</code>
    <code class="k">public</code> <code class="kt">void</code> <code class="nf">notify</code><code class="o">(</code><code class="n">StatusChangeEvent</code> <code class="n">statusChangeEvent</code><code class="o">)</code> <code class="o">{</code>
      <code class="k">switch</code><code class="o">(</code><code class="n">statusChangeEvent</code><code class="o">.</code><code class="na">getStatus</code><code class="o">())</code> <code class="o">{</code>
        <code class="k">case</code> <code class="nd">OUT_OF_SERVICE:</code>
          <code class="c">// Disconnect from queues...</code>
          <code class="k">break</code><code class="o">;</code>
        <code class="k">case</code> <code class="nd">UP:</code>
          <code class="c">// Reconnect to queues...</code>
          <code class="k">break</code><code class="o">;</code>
      <code class="o">}</code>
    <code class="o">}</code>
  <code class="o">};</code>
<code class="o">}</code></pre></div>

<p class="author1">Naturally the same sort of workflow could be achieved with <a href="https://www.consul.io" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Consul</a>, a dynamic configuration server that allows for tagging (i.e., tag by server group name, or cluster), or any other central source of data that has two <span class="keep-together">characteristics:</span></p>

<ul class="printings">
<li class="calibre15">
<p class="calibre18">Application code can respond to change events in near real time via some sort of event listener.</p>
</li>
<li class="calibre15">
<p class="calibre18">The data is groupable by at least server group, cluster, and application, and your application code is able to determine which server group, cluster, and application it belongs to.</p>
</li>
</ul>

<p class="author1">The same requirement to respond to external modifications of a service’s availability is applicable also to microservices using persistent RPC connections like <a href="https://rsocket.io" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">RSocket</a> or streaming/bidirectional <a href="https://oreil.ly/tNORN" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">GRPC</a>, where a disabled server group needs to terminate any persistent RPC connections either outbound or inbound.</p>

<p class="author1">There’s a hidden and significant point embedded in having to listen to discovery status events (or any other external indicator service availability): the application is aware of its participation in service discovery. A goal of service mesh (see <a data-type="xref" href="part0012_split_023.html#BE895-2d714b853a094e9a910510217e0e3d73" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">“Implementation in Service Mesh”</a>) is to take this kind of responsibility away from the application and externalize it to a sidecar process or container, generally for the sake of achieving polyglot support for these patterns quickly. We’ll talk about other problems with this model later, but blue/green deployment of message-driven applications where you’d like to preserve live instances in disabled server groups is an example of where a language-specific binding (in this case for service discovery) is necessary.</p>
</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Blue/Green Deployment" class="calibre3">
<div class="preface" id="blue_green">
<div data-type="tip" class="calibre28"><h1 class="calibre54" id="calibre_pb_12">What’s in a Name?</h1>
<p class="author1"><a data-type="indexterm" data-primary="red/black deployment" data-seealso="blue/green deployment" id="idm45139264816936" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>A blue/green deployment is the same thing as a red/black deployment. They are just different sets of colors, but the techniques have precisely the same meaning.</p>
</div>

<p class="author1">Blue/green deployments are something every team should practice before considering a more complex strategy, such as automated canary analysis.<a data-type="indexterm" data-startref="ix_ch05-asciidoc10" id="idm45139264814984" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-startref="ix_ch05-asciidoc9" id="idm45139264814280" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/></p>
</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Automated Canary Analysis" class="calibre3"><div class="preface" id="automated_canary_analysis">
<h1 class="calibre19" id="9H5VC-2d714b853a094e9a910510217e0e3d73">Automated Canary Analysis</h1>

<p class="author1"><a data-type="indexterm" data-primary="automated canary analysis" id="ix_ch05-asciidoc11" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="canary releases" data-secondary="automated canary analysis" id="ix_ch05-asciidoc12" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="continuous delivery (CD)" data-secondary="automated canary analysis" id="ix_ch05-asciidoc13" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Blue/green deployments achieve a great deal of reliability at a reasonably low cost most of the time. Not every service needs to go any further than this. Yet, there is an additional level of safety we can pursue.</p>

<p class="author1">While blue/green deployments allow you to quickly roll back a code or configuration change that causes unanticipated issues, canary releases provide an additional level of risk reduction by exposing a small subset of users to a new version of the service that runs alongside the existing version.</p>

<p class="author1">Canaries aren’t appropriate for every service. Services with low throughput make it difficult, but not impossible, to send just a small percentage of traffic to a canary server group without prolonging the determination of the canary’s fitness to a long period of time. There isn’t a right amount of time that a canary fitness determination needs to take. It may very well be acceptable for you to run a canary test for days on a relatively low throughput service to make a determination.</p>

<p class="author1">There is a noticeable bias in many engineering teams to understate how much traffic their service actually receives and thus assume that techniques like canary analysis couldn’t possibly work for them. Recall the real-world team mentioned in <a data-type="xref" href="part0005_split_005.html#monitoring_dont_expect_perfection" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">“Learning to Expect Failure”</a>, whose business application was receiving over 1,000 requests per minute. This throughput is significantly higher than most engineers on this team would guess. This is yet another reason that real production telemetry should be the first priority. Building up even a short history of what is happening in production helps you make better decisions about which kinds of techniques, in this case deployment strategies, are appropriate later.</p>
</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Automated Canary Analysis" class="calibre3">
<div class="preface" id="automated_canary_analysis">
<div data-type="tip" class="calibre28"><h1 class="calibre54" id="calibre_pb_14">My Service Can Never Fail Because It Is Too Important</h1>
<p class="author1">Be cautious of a line of reasoning that eschews strategies like automated canary analysis strictly on the basis that a particular microservice is too important to fail. Rather, adopt the mindset that failure is not only possible, but will happen on every service regardless of its importance to the business, and act accordingly.</p>
</div>

<p class="author1">The fitness of a canary deployment is determined by comparing service level indicators of the old and new versions. When there is a significant enough worsening in one or more of these SLIs, all traffic is routed to the stable version, and the canary is aborted.</p>

<p class="author1">Ideally, canary deployments consist of three server groups, as shown in <a data-type="xref" href="part0010_split_014.html#aca_participants" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-10</a>.</p>

<figure class="calibre32"><div id="aca_participants" class="figure">
<img src="../images/00006.png" alt="srej 0510" class="calibre147"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-10. </span>Canary release participants</h6>
</div></figure>

<p class="author1">These canary deployments can be described as follows:</p>
<dl class="calibre20">
<dt class="calibre21">Production</dt>
<dd class="calibre22">
<p class="calibre23">This is the existing server group prior to the canary deployment, containing one or more instances.</p>
</dd>
<dt class="calibre21">Baseline</dt>
<dd class="calibre22">
<p class="calibre23">This server group runs the same version of code and configuration as the production server group. While it may seem counterintuitive to run another copy of the old code at first, we need a baseline that is launched at roughly the same time as the canary because a production server group, by virtue of the fact that it has been running for some amount of time, may have different characteristics like heap consumption or cache contents. It is important to be able to make an accurate comparison between the old code and new, and the best way to do that is to launch copies of each at roughly the same time.</p>
</dd>
<dt class="calibre21">Canary</dt>
<dd class="calibre22">
<p class="calibre23">This server group consists of the new code or configuration.</p>
</dd>
</dl>

<p class="author1">The fitness of the canary is entirely determined by comparing a set of metrics indicators relative only to the baseline (not the production cluster). This implies that applications undergoing canaries are publishing metrics with a <code class="calibre24">cluster</code> common tag so that the canary analysis system can aggregate over indicators coming from instances belonging to the canary and baseline clusters and compare the two aggregates relative to each other.</p>

<p class="author1">This relative comparison is much preferable to testing a canary against a set of fixed thresholds because fixed thresholds tend to make certain assumptions about the amount of throughput going through the system at test time. <a data-type="xref" href="part0010_split_014.html#canary_test_against_fixed_threshold" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-11</a> shows the problem. The application exhibits a higher response time during peak business hours when most traffic is flowing through the system (which is typical). So for the fixed threshold, perhaps we try to set a value that would fail a canary if response time was more than 10% worse than the normative case. During peak business hours, the canary would fail because it has a worse than 10% degradataion from the baseline. But if we ran the canary test after hours, it could be significantly worse than 10% from the baseline and still be under the fixed threshold, which was set to 10% worse than what we expect under <em class="calibre12">different operating conditions</em>. There is a greater chance that a relative comparison would be able to catch a performance degradation whether the test runs during or after business hours.</p>

<figure class="calibre32"><div id="canary_test_against_fixed_threshold" class="figure">
<img src="../images/00045.png" alt="srej 0511" class="calibre148"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-11. </span>Canary test against fixed threshold</h6>
</div></figure>

<p class="author1">On several occassions, I’ve met with organizations about automated delivery practices where the conversation came about because someone heard about canary deployments and the idea sounds so compelling that it stimulates an interest in the topic. Commonly, these organizations didn’t have dimensional metrics instrumentation in place or an automated release process resembling a blue/green deployment. Maybe because of the allure of the safety of a canary deployment, platforms sometimes include canary deployment features. Often they lack baselining and/or comparative measurement, so evaluate platform-provided canary features from this perspective and decide whether giving up one or both still makes sense. I would suggest it doesn’t in many cases.</p>

<p class="author1">In the three-cluster setup (production, baseline, canary), most traffic will go to the production cluster, with a small amount going to the baseline and canary. The canary deployment uses load balancer configuration, service mesh configuration, or whatever other platform feature is available to distribute traffic proportionally.</p>

<p class="author1"><a data-type="xref" href="part0010_split_014.html#spinnaker_aca_infra_when_canary" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-12</a> shows a Spinnaker infrastructure view of the three clusters participating in a canary test. In this case, they are running on a single Kubernetes cluster that has been named “PROD-CLUSTER” in Spinnaker (“cluster” referring to Kubernetes cluster, not as we’ve defined the word in the delivery definitions at the beginning of this chapter).</p>

<p class="author1">Spinnaker integrates with an open source automated canary analysis service, which encapsulates the evaluation of metrics from baseline and canary clusters.</p>

<figure class="calibre32"><div id="spinnaker_aca_infra_when_canary" class="figure">
<img src="../images/00042.png" alt="srej 0512" class="calibre149"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-12. </span>Three clusters of an application undergoing a canary</h6>
</div></figure>








</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Automated Canary Analysis" class="calibre3">
<div class="preface" id="automated_canary_analysis">
<section data-type="sect2" data-pdf-bookmark="Spinnaker with Kayenta" class="calibre3"><div class="preface" id="idm45139264780664">
<h2 class="calibre37" id="9H60U-2d714b853a094e9a910510217e0e3d73">Spinnaker with Kayenta</h2>

<p class="author1"><a data-type="indexterm" data-primary="canary releases" data-secondary="Spinnaker with Kayenta" id="ix_ch05-asciidoc14" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="Kayenta, Spinnaker with" id="ix_ch05-asciidoc15" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="Spinnaker" data-secondary="with Kayenta" data-secondary-sortas="Kayenta" id="ix_ch05-asciidoc16" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a href="https://oreil.ly/f4KZW" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Kayenta</a> is a stand-alone open source automated canary analysis service that is also deeply integrated into Spinnaker by way of pipeline stages and configuration.</p>

<p class="author1">Kayenta determines whether there is a significant difference between the canary and baseline for each metric, yielding a <em class="calibre12">pass</em>, <em class="calibre12">high</em>, or <em class="calibre12">low</em> classification. <em class="calibre12">High</em> and <em class="calibre12">low</em> are both failing conditions. <a data-type="indexterm" data-primary="Mann-Whitney U test" id="idm45139264748024" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="U test" id="idm45139264747320" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Kayenta makes this difference comparatively between the two clusters using a <a href="https://oreil.ly/qLYOS" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Mann-Whitney <em class="calibre12">U</em> test</a>. The implementation of this statistical test is called a judge, and Kayenta could be configured with alternative judges, but they typically involve code that goes beyond what you could achieve through a single query against a metrics system.</p>

<p class="author1"><a data-type="xref" href="part0010_split_015.html#aca_metrics" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-13</a> presents an example of Kayenta’s classification decisions for several metrics. This screenshot is from the original Netflix <a href="https://oreil.ly/ik79d" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">blog</a> on Kayenta. In this case, latency has failed the test.</p>

<figure class="calibre32"><div id="aca_metrics" class="figure">
<img src="../images/00028.png" alt="srej 0513" class="calibre150"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-13. </span>Canary metrics</h6>
</div></figure>

<p class="author1">In Spinnaker, the canary metrics for an application can be defined in the “Canary Configs” tab of the application infrastructure view. In the configuration, as shown in <a data-type="xref" href="part0010_split_015.html#spinnaker_aca_canary_config" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-14</a>, you can define one or more service level indicators. If enough of these indicators fail, the canary will fail.</p>

<figure class="calibre32"><div id="spinnaker_aca_canary_config" class="figure">
<img src="../images/00118.png" alt="srej 0514" class="calibre151"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-14. </span>Canary configuration for an application in Spinnaker</h6>
</div></figure>

<p class="author1"><a data-type="xref" href="part0010_split_015.html#spinnaker_aca_cpu_configuration" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-15</a> shows the configuration of a single indicator, in this case processor utilization. Notice how the configuration contains a metrics query that is specific to a monitoring system that you have configured Kayenta to poll from (in this case, Prometheus). You then indicate very broadly that an increase or decrease (or a <span class="keep-together">deviation</span> either way) is considered bad. We would not like to see significantly higher processor utilization in this case, although a decrease would be welcome.</p>

<p class="author1">On the other hand, a <em class="calibre12">decrease</em> in servicing throughput for an application that should process rather continuously at a certain rate would be a bad sign. The indicator can be marked as critical enough that failure alone should fail the canary.</p>

<figure class="calibre32"><div id="spinnaker_aca_cpu_configuration" class="figure">
<img src="../images/00071.png" alt="srej 0515" class="calibre152"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-15. </span>Processor utilization canary configuration</h6>
</div></figure>

<p class="author1"><a data-type="indexterm" data-primary="delivery pipelines" data-secondary="canary configuration and" id="idm45139264732040" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Once the canary configuration is built, it can be used in pipelines. A characteristic canary deployment pipeline is shown in <a data-type="xref" href="part0010_split_015.html#spinnaker_aca_pipeline" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-16</a>. The “Configuration” stage defines the triggers that begin the process of evaluating the canary. “Set Cluster Name to Canary” sets a variable that is used in the subsequent stage “Deploy Canary” by Spinnaker to name the cluster canary. It is this variable that eventually yields the named canary cluster shown in <a data-type="xref" href="part0010_split_014.html#spinnaker_aca_infra_when_canary" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-12</a>.</p>

<figure class="calibre32"><div id="spinnaker_aca_pipeline" class="figure">
<img src="../images/00022.png" alt="srej 0516" class="calibre153"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-16. </span>A canary deployment pipeline in Spinnaker</h6>
</div></figure>

<p class="author1">In parallel, Spinnaker is retrieving the artifacts that the current production version is based off of and creating a baseline cluster with those artifacts as well. The “Canary Analysis” stage could run for hours or even days, depending on how it is configured. If it passes, we will deploy a new prod cluster (with the same artifact used to created the canary, which may no longer be the latest version available in the artifact repository). In parallel, we can tear down the baseline and canary clusters that are no longer needed. This whole pipeline can be configured in Spinnaker to be run serially so that only one canary is being evaluated at any given time.</p>

<p class="author1">The outcome of a canary run is viewable in a few different ways. Spinnaker presents a “Canary Reports” tab that shows the outcome of the judgment for a canary stage, breaking down each service level indicator that goes into the decision individually. Each indicator can be viewed as a time series graph over the interval in which the canary ran, as in <a data-type="xref" href="part0010_split_015.html#spinnaker_aca_cpu_comparison" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-17</a>.</p>

<figure class="calibre32"><div id="spinnaker_aca_cpu_comparison" class="figure">
<img src="../images/00108.png" alt="srej 0517" class="calibre154"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-17. </span>Time series visualization of CPU utilization in baseline and canary</h6>
</div></figure>
</div></section>













</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Automated Canary Analysis" class="calibre3">
<div class="preface" id="automated_canary_analysis">
<section data-type="sect2" data-pdf-bookmark="Spinnaker with Kayenta" class="calibre3">
<div class="preface" id="idm45139264780664">
<div data-type="note" type="note" class="calibre28"><h1 class="calibre54" id="calibre_pb_16">Current Production Version Isn’t Always Latest</h1>
<p class="author1">Note that the current production version from which a baseline will be created is <em class="calibre12">not</em> always the latest version of the application binary (e.g., JAR or WAR) available in an artifact repository. It may actually be several versions older, in cases where we have attempted to release new versions but they have failed canaries or were otherwise rolled back. One of the values of a stateful continuous delivery solution like Spinnaker is its ability to poll the environment for what is there and to then act upon this information.</p>
</div>

<p class="author1">Alternatively, the comparison for an indicator can be viewed as a bar chart (or histogram), as in <a data-type="xref" href="part0010_split_016.html#spinnaker_aca_latency_histogram_view" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-18</a>.</p>

<figure class="calibre32"><div id="spinnaker_aca_latency_histogram_view" class="figure">
<img src="../images/00062.png" alt="srej 0518" class="calibre155"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-18. </span>Histogram visualization of 99th-percentile latency</h6>
</div></figure>

<p class="author1">Lastly, and perhaps most usefully, the comparison between canary and baseline can be visualized as a <a data-type="indexterm" data-primary="beeswarm plot" id="idm45139264715784" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>beeswarm plot, shown in <a data-type="xref" href="part0010_split_016.html#spinnaker_aca_latency_beeswarm_view" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-19</a>. The canary is judged over time, with Kayenta polling the monitoring system for values for the canary and baseline at a prescribed interval. The individual samples here are shown on the beeswarm plot along with a box-and-whisker plot showing the basic quartiles (min, 25th percentile, median, 75th percentile, and max of all samples).
The median has certainly increased, but as discussed in <a data-type="xref" href="part0006_split_000.html#5N3C4-2d714b853a094e9a910510217e0e3d73" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Chapter 2</a>, measures of centrality like mean and median aren’t really that useful for judging the fitness of a service. This plot really highlights this fact. The max and even 75% latency haven’t changed much at all between versions. So there’s a little more variation in the median, but it probably doesn’t indicate a performance regression at all.</p>

<figure class="calibre32"><div id="spinnaker_aca_latency_beeswarm_view" class="figure">
<img src="../images/00021.png" alt="srej 0519" class="calibre156"/>
<h6 class="calibre34"><span class="keep-together">Figure 5-19. </span>Beeswarm visualization of 99th-percentile latency</h6>
</div></figure>

<p class="author1">The key indicators for canary analysis will be different sometimes than the indicators we use to alert, because they are designed for comparative analysis between two clusters rather than against an absolute measure. Even if a new application version remains underneath a service level objective boundary that we’ve set as the test for an alert, it’s still probably best that the general trajectory of the code doesn’t continue to regress closer and closer to that service level objective.<a data-type="indexterm" data-startref="ix_ch05-asciidoc16" id="idm45139264709560" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-startref="ix_ch05-asciidoc15" id="idm45139264708856" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-startref="ix_ch05-asciidoc14" id="idm45139264708184" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/></p>
</div></section>













</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Automated Canary Analysis" class="calibre3">
<div class="preface" id="automated_canary_analysis">
<section data-type="sect2" data-pdf-bookmark="General-Purpose Canary Metrics for Every Microservice" class="calibre3"><div class="preface" id="idm45139264779720">
<h2 class="calibre37" id="9H63B-2d714b853a094e9a910510217e0e3d73">General-Purpose Canary Metrics for Every Microservice</h2>

<p class="author1"><a data-type="indexterm" data-primary="canary releases" data-secondary="general-purpose canary metrics for every microservice" id="ix_ch05-asciidoc17" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="metrics" data-secondary="general-purpose canary metrics for every microservice" id="ix_ch05-asciidoc18" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Think of the L-USE acronym when considering canary metrics that are useful to start with. In fact, many of the same SLIs that you should alert on for most microservices are also good canary metrics, with a twist.</p>

<p class="author1">Let’s consider a few good canary metrics, beginning with latency. Really any of the signals described in <a data-type="xref" href="part0009_split_000.html#8IL24-2d714b853a094e9a910510217e0e3d73" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Chapter 4</a> are good candidates for canary analysis.</p>










<section data-type="sect3" data-pdf-bookmark="Latency" class="calibre3"><div class="preface" id="idm45139264702072">
<h3 class="calibre51">Latency</h3>

<p class="author1"><a data-type="indexterm" data-primary="canary releases" data-secondary="latency" id="idm45139264700792" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="latency" data-secondary="as canary metric" data-secondary-sortas="canary" id="idm45139264699816" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Latency for some bellwether API endpoint is a good starting point. Constrain the metric to successful outcomes only, since successful outcomes tend to have different latency characteristics than unsuccessful outcomes. Imagine fixing a bug that caused failures for a critical API endpoint in production, only to have the canary fail because the bug caused the API endpoint to fail fast and the canary decides that latency has degraded too much by fixing it!</p>

<p class="author1">In <a data-type="xref" href="part0009_split_000.html#8IL24-2d714b853a094e9a910510217e0e3d73" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Chapter 4</a>, the idea was to measure a timed operation’s decaying <em class="calibre12">maximum</em> latency against a fixed service level objective, that service level objective being a conservative boundary inside engineering’s service level agreement determined with business partners. But maximum latency has a way of being spiky. For example, hypervisor and garbage collection pauses or full connection pools are mostly temporary conditions (and outside of your control) that naturally affect instances at different times. For the purposes of measuring an application’s fitness relative to a service level objective, we want to make sure that performance is still acceptable even under these conditions. But because of the staggered nature in which they occur on different instances, these effects lead to bad <em class="calibre12">comparative</em> measures.</p>

<p class="author1">For canaries, it is best to look at a distribution statistic like 99th-percentile latency, which shaves off the top 1% where these temporary conditions exhibit. 99th percentile (or some other high percentile) is generally a better measure of a piece of code’s performance <em class="calibre12">potential</em> minus temporary environmental factors.</p>

<p class="author1">Recall from <a data-type="xref" href="part0007_split_007.html#6LKA8-2d714b853a094e9a910510217e0e3d73" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">“Histograms”</a> that in order to compute a high-percentile latency across a cluster (and constrained to successful outcomes for a particular endpoint), we need to use a method like percentile approximation based off of histogram data that can be summed across the instances in this cluster and any other tag variation that exists for successful outcomes for this critical API endpoint. Only a handful of monitoring systems at this point support aggregable percentile approximation. If your monitoring system can’t do percentile approximation, do not attempt to aggregate individual percentiles from instances (we showed why the math for this doesn’t work in <a data-type="xref" href="part0007_split_003.html#6LK44-2d714b853a094e9a910510217e0e3d73" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">“Percentiles/Quantiles”</a>). Also, avoid the temptation to use another measure like average. Look at the beeswarm plot in <a data-type="xref" href="part0010_split_016.html#spinnaker_aca_latency_histogram_view" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Figure 5-18</a> to see how measures of centrality like median and mean can vary widely between versions (and realistically even over time with the same version!) without any real change in <span class="keep-together">performance.</span></p>
<blockquote class="pcalibre8 pcalibre9 pcalibre7">
<p class="calibre16">Average: a random number that falls somewhere between the maximum and 1/2 the median. Most often used to ignore reality.</p>
<p data-type="attribution" class="pcalibre10 pcalibre11">Gil Tene</p>
</blockquote>

<p class="author1">To compute an aggregable percentile approximation for Atlas, use the <code class="calibre24">:percentiles</code> function, as in <a data-type="xref" href="part0010_split_017.html#atlas_percentile_latency" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Example 5-4</a>.</p>
<div id="atlas_percentile_latency" data-type="example" class="calibre61">
<h5 class="calibre62"><span class="keep-together">Example 5-4. </span>Atlas percentile latency for canaries</h5>

<pre data-type="programlisting" class="calibre63">name,http.server.requests,:eq,
uri,$ENDPOINT,:eq,
:and,
outcome,SUCCESS,:eq,
:and,
(,99,),:percentiles</pre></div>

<p class="author1">For Prometheus, use the <code class="calibre24">histogram_quantile</code> function, as in <a data-type="xref" href="part0010_split_017.html#prometheus_percentile_latency" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Example 5-5</a>.</p>
<div id="prometheus_percentile_latency" data-type="example" class="calibre61">
<h5 class="calibre62"><span class="keep-together">Example 5-5. </span>Prometheus percentile latency for canaries</h5>

<pre data-type="programlisting" class="calibre63">histogram_quantile(
  0.99,
  rate(
    http_server_requests_seconds_bucket{
      uri="$ENDPOINT",
      outcome="SUCCESS"
    }[2m]
  )
)</pre></div>

<p class="author1">In a similar way, you should include latency metrics for interactions with critical downstream resources like a database. Consider relational database interactions. It is not uncommon for new code to inadvertently cause an existing database index to go unused (increasing latency substantially, as well as load on the database), or for a new index to not perform as well as expected once it gets to production. Regardless of how well we try to replicate and test these new interactions in lower-level environments, there is nothing quite like production.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Error ratio" class="calibre3"><div class="preface" id="idm45139264701480">
<h3 class="calibre51">Error ratio</h3>

<p class="author1"><a data-type="indexterm" data-primary="canary releases" data-secondary="error ratio" id="idm45139264677720" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="error ratio" data-secondary="as canary metric" data-secondary-sortas="canary" id="idm45139264676744" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Error ratio (<a data-type="xref" href="part0010_split_017.html#atlas_error_ratio" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Example 5-6</a> on Atlas and <a data-type="xref" href="part0010_split_017.html#prometheus_error_ratio" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Example 5-7</a> for Prometheus) on some bellwether API endpoint (or all of them) can be incredibly useful as well, as this will determine whether you have introduced semantic regressions into your code that may have not been caught by tests but are causing issues in production.</p>
<div id="atlas_error_ratio" data-type="example" class="calibre61">
<h5 class="calibre62"><span class="keep-together">Example 5-6. </span>Error ratio of HTTP server requests in Atlas</h5>

<pre data-type="programlisting" class="calibre63">name,http.server.requests,:eq,
:dup,
outcome,SERVER_ERROR,:eq,
:div,
uri,$ENDPOINT,:eq,:cq</pre></div>
<div id="prometheus_error_ratio" data-type="example" class="calibre61">
<h5 class="calibre62"><span class="keep-together">Example 5-7. </span>Error ratio of HTTP server requests in Prometheus</h5>

<pre data-type="programlisting" class="calibre63">sum(
  rate(
    http_server_requests_seconds_count{outcome="SERVER_ERROR", uri="$ENDPOINT"}[2m]
  )
) /
sum(
  rate(
    http_server_requests_seconds_count{uri="$ENDPOINT"}[2m]
  )
)</pre></div>

<p class="author1">Carefully consider whether to include more than one API endpoint in a single canary signal. Suppose you had two separate API endpoints that receive significantly different throughput, one receiving 1,000 requests/second and the other receiving 10 requests/second. Because our service isn’t perfect (what is?), the old code is failing the high throughput endpoint at a fixed rate of 3 requests/second, but all requests to the low throughput endpoint succeed. Now imagine we make a code change that causes 3 out of 10 requests to the low throughput endpoint to fail but doesn’t change the error ratio of the other endpoint. If these endpoints are considered together by the canary judge, the judge will probably pass the regression because it only resulted in a small uptick in error ratio (0.3% to 0.6%). Considered separately, however, the judge would certainly fail the error ratio on the low throughput endpoint (0% to 33%).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Heap saturation" class="calibre3"><div class="preface" id="idm45139264667768">
<h3 class="calibre51">Heap saturation</h3>

<p class="author1"><a data-type="indexterm" data-primary="canary releases" data-secondary="heap saturation" id="idm45139264666328" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="heap saturation" id="idm45139264665352" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="heap utilization" data-secondary="as canary metric" data-secondary-sortas="canary" id="idm45139264664680" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Heap utilization can be compared in two ways: for total consumption relative to the maximum heap and allocation performance.</p>

<p class="author1">Total consumption is determined by dividing used by max, shown in <a data-type="xref" href="part0010_split_017.html#atlas_heap_canary" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Example 5-8</a> and <a data-type="xref" href="part0010_split_017.html#prometheus_heap_canary" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Example 5-9</a>.</p>
<div id="atlas_heap_canary" data-type="example" class="calibre61">
<h5 class="calibre62"><span class="keep-together">Example 5-8. </span>Atlas heap consumption canary metric</h5>

<pre data-type="programlisting" class="calibre63">name,jvm.memory.used,:eq,
name,jvm.memory.max,:eq,
:div</pre></div>
<div id="prometheus_heap_canary" data-type="example" class="calibre61">
<h5 class="calibre62"><span class="keep-together">Example 5-9. </span>Prometheus heap consumption canary metric</h5>

<pre data-type="programlisting" class="calibre63">jvm_memory_used / jvm_memory_max</pre></div>

<p class="author1">Allocation performance can be measured by dividing allocations by promotions, shown in Examples <a data-type="xref" data-xrefstyle="select:labelnumber" href="part0010_split_017.html#atlas_allocation_canary" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">5-10</a> and <a data-type="xref" data-xrefstyle="select:labelnumber" href="part0010_split_017.html#prometheus_allocation_canary" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">5-11</a>.</p>
<div id="atlas_allocation_canary" data-type="example" class="calibre61">
<h5 class="calibre62"><span class="keep-together">Example 5-10. </span>Atlas allocation performance canary metric</h5>

<pre data-type="programlisting" class="calibre63">name,jvm.gc.memory.allocated,:eq,
name,jvm.gc.memory.promoted,:eq,
:div</pre></div>
<div id="prometheus_allocation_canary" data-type="example" class="calibre61">
<h5 class="calibre62"><span class="keep-together">Example 5-11. </span>Prometheus allocation performance canary metric</h5>

<pre data-type="programlisting" class="calibre63">jvm_gc_memory_allocated / jvm_gc_memory_promoted</pre></div>
</div></section>













<section data-type="sect3" data-pdf-bookmark="CPU utilization" class="calibre3"><div class="preface" id="idm45139264649768">
<h3 class="calibre51">CPU utilization</h3>

<p class="author1"><a data-type="indexterm" data-primary="canary releases" data-secondary="CPU utilization" id="idm45139264648360" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-primary="CPU utilization" id="idm45139264647384" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Process CPU utilization can be compared rather simply, shown in <a data-type="xref" href="part0010_split_017.html#atlas_cpu_canary" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Example 5-12</a> and <a data-type="xref" href="part0010_split_017.html#prometheus_cpu_canary" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre">Example 5-13</a>.</p>
<div id="atlas_cpu_canary" data-type="example" class="calibre61">
<h5 class="calibre62"><span class="keep-together">Example 5-12. </span>Atlas CPU utilization canary metric</h5>

<pre data-type="programlisting" class="calibre63">name,process.cpu.usage,:eq</pre></div>
<div id="prometheus_cpu_canary" data-type="example" class="calibre61">
<h5 class="calibre62"><span class="keep-together">Example 5-13. </span>Prometheus CPU utilization canary metric</h5>

<pre data-type="programlisting" class="calibre63">process_cpu_usage</pre></div>

<p class="author1">Add canary metrics incrementally, since failed canary tests block the production path, potentially slowing down the delivery of features and bug fixes unnecessarily. Canary failures should be tuned to block dangerous regressions<a data-type="indexterm" data-startref="ix_ch05-asciidoc18" id="idm45139264640312" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-startref="ix_ch05-asciidoc17" id="idm45139264639608" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>.<a data-type="indexterm" data-startref="ix_ch05-asciidoc13" id="idm45139264638808" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-startref="ix_ch05-asciidoc12" id="idm45139264638104" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-startref="ix_ch05-asciidoc11" id="idm45139264637432" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/></p>
</div></section>



</div></section>





</div></section>













</div></section></div>



  

<div id="sbo-rt-content" class="calibre2">
<section data-type="chapter" type="chapter" data-pdf-bookmark="Chapter 5. Safe, Multicloud Continuous Delivery" class="calibre3">
<div class="preface" id="ch_continuous_delivery">
<section data-type="sect1" data-pdf-bookmark="Summary" class="calibre3"><div class="preface" id="idm45139264812696">
<h1 class="calibre19" id="calibre_pb_18">Summary</h1>

<p class="author1">This chapter introduced continuous delivery concepts at a high level with Spinnaker as its example system. You don’t need to rush to adopt Spinnaker in order to find a way to gain some of the benefits. For many enterprises, I believe clearing two hurdles would go a long way in improving release success:</p>
<dl class="calibre20">
<dt class="calibre21">Blue/green capability</dt>
<dd class="calibre22">
<p class="calibre23">Have a blue/green deployment strategy that supports <em class="calibre12">N</em> active disabled clusters for fast rollback and takes into account the unique needs of event-driven applications (as switching a load balancer isn’t enough to effectively take an event-driven application out of service).</p>
</dd>
<dt class="calibre21">Deployed asset inventory</dt>
<dd class="calibre22">
<p class="calibre23"><a data-type="indexterm" data-primary="deployed asset inventory" id="idm45139264631944" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/>Have some means of querying the live state of your deployed assets. It’s easier (and likely more accurate) to accomplish this by actually polling the state of your deployed environments periodically than by trying to make every possible mutating action pass through some central system like a CI server and trying to rebuild the state of the system from all the individual mutations that have occurred.</p>
</dd>
</dl>

<p class="author1">A further goal would be to have sufficient access and quality controls in place in your delivery system (again, whether that is Spinnaker or something else) to allow for some variation in deployments between teams. Some deployments, especially for static assets or internal tools, aren’t going to benefit from blue/green deployment significantly. Others may release frequently enough that multiple disabled server groups in a blue/green deployment strategy are needed. Some start up fast enough that having active instances in disabled clusters represents a cost inefficiency. A platform engineering team thinking in terms of “guardrails not gates” will favor allowing this pipeline diversity over organizational consistency, maximizing the safety/cost trade-off uniquely for each team.</p>

<p class="author1">In the next chapter, we’ll use the presence of a deployed asset inventory as an assumption for building an artifact provenance chain for your deployed assets that reaches down to the source code running in each environment.<a data-type="indexterm" data-startref="ix_ch05-asciidoc2" id="idm45139264628616" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-startref="ix_ch05-asciidoc1" id="idm45139264627912" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/><a data-type="indexterm" data-startref="ix_ch05-asciidoc0" id="idm45139264627240" class="pcalibre2 pcalibre3 pcalibre1 pcalibre4 pcalibre"/></p>
</div></section>







</div></section></div>



  </body></html>