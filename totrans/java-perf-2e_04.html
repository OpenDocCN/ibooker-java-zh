<html><head></head><body><section data-pdf-bookmark="Chapter 4. Working with the JIT Compiler" data-type="chapter" epub:type="chapter"><div class="chapter" id="JustInTimeCompilation">&#13;
<h1><span class="label">Chapter 4. </span>Working with the JIT Compiler</h1>&#13;
&#13;
&#13;
<p><a data-primary="just-in-time (JIT) compiler" data-type="indexterm" id="ix_ch04-asciidoc0"/>The <em>just-in-time (JIT) compiler</em> is the heart of the Java Virtual Machine;&#13;
nothing controls the performance of your application more than the JIT&#13;
compiler.</p>&#13;
&#13;
<p>This chapter covers the compiler in depth. It starts with&#13;
information on how the compiler works and discusses the advantages and&#13;
disadvantages of using a JIT compiler.  Until JDK 8 came along,&#13;
you had to choose between two Java compilers. Today, those two&#13;
compilers still exist but work in concert with each other, though in rare cases choosing one is necessary.&#13;
Finally, we’ll look at&#13;
some intermediate and advanced tunings of the compiler. If an application&#13;
is running slowly without any obvious reason, those sections can help you&#13;
determine whether the compiler is at fault.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Just-in-Time Compilers: An Overview" data-type="sect1"><div class="sect1" id="idm45775556355592">&#13;
<h1>Just-in-Time Compilers: An Overview</h1>&#13;
&#13;
<p><a data-primary="just-in-time (JIT) compiler" data-secondary="overview" data-type="indexterm" id="ix_ch04-asciidoc1"/>We’ll start with some introductory material; feel free to skip ahead if you understand the&#13;
basics of just-in-time compilation.</p>&#13;
&#13;
<p>Computers—and more specifically CPUs—can execute only a relatively&#13;
few, specific instructions, which are called <em>machine&#13;
code</em>. All programs that the CPU executes must therefore be translated into&#13;
these instructions.</p>&#13;
&#13;
<p><a data-primary="compiled languages" data-type="indexterm" id="idm45775556351064"/>Languages like C++ and Fortran are called <em>compiled languages</em> because&#13;
their programs are delivered as binary (compiled) code: the&#13;
program is written, and then a static compiler produces a binary. The&#13;
assembly code in that binary is targeted to a particular CPU.&#13;
Complementary CPUs can execute the same binary: for example, AMD and Intel CPUs&#13;
share a basic, common set of assembly language instructions, and later&#13;
versions of <span class="keep-together">CPUs almost</span> always can execute the same set of instructions&#13;
as previous versions of that CPU. The reverse is not always true; new&#13;
versions of CPUs often introduce instructions that will not run on&#13;
older versions of CPUs.</p>&#13;
&#13;
<p><a data-primary="interpreted languages" data-type="indexterm" id="idm45775556348264"/>Languages like PHP and Perl, on the other hand, are interpreted.  The same&#13;
program source code can be run on any CPU as long as the machine has&#13;
the correct interpreter (that is, the program called <code>php</code> or <code>perl</code>).  The&#13;
interpreter translates each line of the program into binary code as&#13;
that line is executed.</p>&#13;
&#13;
<p>Each system has advantages and disadvantages.&#13;
Programs written in interpreted languages are portable: you can take the&#13;
same code and drop it on any machine with the appropriate interpreter, and&#13;
it will run. However, it might run slowly. As a simple case, consider&#13;
what happens in a loop: the interpreter will retranslate each line of&#13;
code when it is executed in the loop. The compiled code doesn’t need to&#13;
repeatedly make that translation.</p>&#13;
&#13;
<p>A good compiler&#13;
takes several factors into account when it produces a binary. One simple example&#13;
is the order of the binary statements: not all assembly language&#13;
instructions take the same amount of time to execute. A statement that&#13;
adds the values stored in two registers might execute in one cycle, but&#13;
retrieving (from main memory) the values needed for the addition may take&#13;
multiple cycles.</p>&#13;
&#13;
<p>Hence, a good compiler will produce a binary that executes the statement&#13;
to load the data, executes other instructions, and then—when the&#13;
data is available—executes the addition. An interpreter that is looking&#13;
at only one line of code at a time doesn’t have enough information to&#13;
produce that kind of code; it will request the data from memory, wait for&#13;
it to become available, and&#13;
then execute the addition. Bad compilers will do the same thing, by the&#13;
way, and it is not necessarily the case that even the best compiler can&#13;
prevent the occasional wait for an instruction to complete.</p>&#13;
&#13;
<p>For these (and other) reasons, interpreted code will almost always be&#13;
measurably slower than compiled code:&#13;
compilers have enough information about the program to provide&#13;
optimizations to the binary code that an interpreter simply&#13;
cannot <span class="keep-together">perform.</span></p>&#13;
&#13;
<p>Interpreted code does have the advantage of&#13;
portability. A binary compiled for an ARM CPU obviously cannot run on&#13;
an Intel CPU. But a binary that uses the latest AVX instructions of&#13;
Intel’s Sandy Bridge processors cannot run on older Intel processors&#13;
either. Hence, commercial software is commonly compiled to a&#13;
fairly old version of a processor and does not take advantage of the newest&#13;
instructions available to it. Various tricks around this exist,&#13;
including shipping a binary with multiple shared libraries that&#13;
execute performance-sensitive code and come with&#13;
versions for various flavors of a CPU.</p>&#13;
&#13;
<p>Java attempts to find a middle ground here. Java applications&#13;
are compiled—but instead of being compiled into a specific binary for&#13;
a specific CPU, they are compiled into an intermediate low-level language.&#13;
<a data-primary="Java bytecode" data-type="indexterm" id="idm45775556340408"/>This language (known as <em>Java bytecode</em>) is then run by the <code>java</code>&#13;
binary (in the same way that an interpreted PHP script is run by the <code>php</code>&#13;
binary). This gives Java the platform independence of an interpreted&#13;
<span class="keep-together">language.</span> Because it is executing an idealized binary code, the&#13;
<code>java</code> program is able to compile the code into the platform binary as the&#13;
code executes. This compilation occurs as the program is executed: it&#13;
happens “just in time.”</p>&#13;
&#13;
<p>This compilation is still subject to platform dependencies. JDK 8, for example,&#13;
cannot generate code for the latest instruction set of Intel’s Skylake&#13;
processors, though JDK 11 can. I’ll have more to say about that in <a data-type="xref" href="#advance-compiler-tuning">“Advanced Compiler Flags”</a>.</p>&#13;
&#13;
<p>The manner in which the Java Virtual Machine compiles this code as&#13;
it executes is the focus of this chapter.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="HotSpot Compilation" data-type="sect2"><div class="sect2" id="HotspotCompilation">&#13;
<h2>HotSpot Compilation</h2>&#13;
&#13;
<p><a data-primary="HotSpot Compilation" data-type="indexterm" id="idm45775556333080"/><a data-primary="just-in-time (JIT) compiler" data-secondary="HotSpot compilation" data-type="indexterm" id="idm45775556332376"/>As discussed in <a data-type="xref" href="ch01.html#Introduction">Chapter 1</a>, the Java implementation discussed in this book&#13;
is Oracle’s HotSpot JVM.&#13;
This name (HotSpot) comes from the approach it takes toward compiling the&#13;
code. In a typical program, only a small subset of code is executed frequently,&#13;
and the performance of an application depends primarily on how fast those&#13;
sections of code are executed. These critical sections are known as the&#13;
hot spots of the application; the more the section of code is executed,&#13;
the hotter that section is said to be.</p>&#13;
&#13;
<p>Hence, when the JVM executes code, it does not begin compiling the&#13;
code immediately. There are two basic reasons for this. First, if the code&#13;
is going to be executed only once, then compiling it is essentially a wasted&#13;
effort; it will be faster to interpret the Java bytecodes than to compile them&#13;
and execute (only once) the compiled code.</p>&#13;
&#13;
<p>But if the code in question is a frequently called method or a loop that&#13;
runs many iterations, then compiling it is worthwhile: the&#13;
cycles it takes to compile the code will be outweighed by the savings in&#13;
multiple executions of the faster compiled code. That&#13;
trade-off is one reason that the compiler executes the interpreted code&#13;
first—the compiler can figure out which methods are called frequently&#13;
enough to warrant their compilation.</p>&#13;
&#13;
<p><a data-primary="optimization" data-secondary="HotSpot compilation and" data-type="indexterm" id="idm45775556328072"/>The second reason is one of optimization: the more times that the JVM&#13;
executes a particular method or loop, the more information it has about&#13;
that code. This allows the JVM to make numerous optimizations when it&#13;
compiles the code.</p>&#13;
&#13;
<p>Those optimizations&#13;
(and ways to affect them) are discussed&#13;
later in this chapter, but for a simple example,&#13;
consider the&#13;
<code class="keep-together">equals()</code>&#13;
method. This method exists in every&#13;
Java object (because it is inherited from the&#13;
<code class="keep-together">Object</code>&#13;
class) and <span class="keep-together">is often</span>&#13;
overridden. When the interpreter encounters the statement&#13;
<span class="keep-together"><code>b = obj1.equals(obj2)</code>,</span>&#13;
it must look up the type (class) of&#13;
<code class="keep-together">obj1</code>&#13;
in order to know&#13;
which&#13;
<code class="keep-together">equals()</code>&#13;
method to execute. This dynamic lookup can be somewhat&#13;
time-consuming.</p>&#13;
&#13;
<p>Over time, say the JVM notices that each time this statement&#13;
is executed,&#13;
<code class="keep-together">obj1</code>&#13;
is of type&#13;
<code class="keep-together">java.lang.String</code>.&#13;
Then the JVM can produce&#13;
compiled code that directly calls the&#13;
<code class="keep-together">String.equals()</code>&#13;
method.&#13;
Now the code is&#13;
faster not only because it is compiled but also because it&#13;
can skip the lookup of which method to call.</p>&#13;
&#13;
<p>It’s not quite as simple as&#13;
that; it is possible the next time the code is executed that&#13;
<code class="keep-together">obj1</code>&#13;
refers to something other than a&#13;
<code class="keep-together">String</code>.&#13;
The JVM will create&#13;
compiled code that deals with that possibility, which will involve&#13;
deoptimizing and then reoptimizing the code in question (you’ll see an example&#13;
in <a data-type="xref" href="#Deoptimization">“Deoptimization”</a>). Nonetheless, the overall&#13;
compiled code here will be faster (at least as long as&#13;
<code class="keep-together">obj1</code>&#13;
continues&#13;
to refer to a&#13;
<span class="keep-together"><code>String</code>)</span>&#13;
because it skips the lookup of which&#13;
method to execute. That kind of optimization can be made only after running&#13;
the code for a while and observing what it does: this is the second reason&#13;
JIT compilers wait to compile sections of code.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775556313720">&#13;
<h5>Registers and Main Memory</h5>&#13;
<p><a data-primary="just-in-time (JIT) compiler" data-secondary="registry values and main memory" data-type="indexterm" id="idm45775556312312"/><a data-primary="main memory, registry values versus" data-type="indexterm" id="idm45775556311256"/>One of the most important optimizations a compiler can make involves when&#13;
to use values from main memory and when to store values in a register.&#13;
Consider this code:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code> <code class="kd">class</code> <code class="nc">RegisterTest</code> <code class="o">{</code>&#13;
    <code class="kd">private</code> <code class="kt">int</code> <code class="n">sum</code><code class="o">;</code>&#13;
&#13;
    <code class="kd">public</code> <code class="kt">void</code> <code class="nf">calculateSum</code><code class="o">(</code><code class="kt">int</code> <code class="n">n</code><code class="o">)</code> <code class="o">{</code>&#13;
        <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">n</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
	    <code class="n">sum</code> <code class="o">+=</code> <code class="n">i</code><code class="o">;</code>&#13;
	<code class="o">}</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>At some point, the <code>sum</code> instance variable must reside in main&#13;
memory, but retrieving a value from main memory is an expensive operation&#13;
that takes multiple cycles to complete. If the value of <code>sum</code> were to be&#13;
retrieved from (and stored back to) main memory on every iteration of this&#13;
loop, performance would be dismal. Instead, the compiler will load a register&#13;
with the initial value of <code>sum</code>, perform the loop using that value in the&#13;
register, and then (at an indeterminate point in time) store the final&#13;
result from the register back to main memory.</p>&#13;
&#13;
<p>This kind of optimization is very effective, but it means that the semantics of&#13;
thread synchronization (see <a data-type="xref" href="ch09.html#ThreadPerformance">Chapter 9</a>) are crucial to the&#13;
behavior of the application. One thread cannot see the value of a variable&#13;
stored in the register used by another thread; synchronization makes it&#13;
possible to know exactly when the register is stored to main memory and&#13;
available to other threads.</p>&#13;
&#13;
<p>Register usage is a general optimization of the compiler, and typically&#13;
the JIT will aggressively use registers. We’ll discuss this more in-depth&#13;
in <a data-type="xref" href="#EscapeAnalysis">“Escape Analysis”</a>.</p>&#13;
</div></aside>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Java is designed to take advantage of the platform independence of scripting languages and the native performance of compiled languages.</p>&#13;
</li>&#13;
<li>&#13;
<p>A Java class file is compiled into an intermediate language (Java bytecodes) that is then further compiled into assembly language by the JVM.</p>&#13;
</li>&#13;
<li>&#13;
<p>Compilation of the bytecodes into assembly language performs optimizations that greatly improve performance.<a data-startref="ix_ch04-asciidoc1" data-type="indexterm" id="idm45775556262680"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tiered Compilation" data-type="sect1"><div class="sect1" id="TieredCompilation">&#13;
<h1>Tiered Compilation</h1>&#13;
&#13;
<p><a data-primary="just-in-time (JIT) compiler" data-secondary="tiered compilation" data-type="indexterm" id="idm45775556260088"/><a data-primary="tiered compilation" data-type="indexterm" id="idm45775556259144"/>Once upon a time, the JIT compiler came in two flavors, and you had to install&#13;
different versions of the JDK depending on which compiler you wanted to use.&#13;
These compilers are known as the&#13;
<code class="keep-together">client</code>&#13;
and&#13;
<code class="keep-together">server</code>&#13;
compilers. In 1996, this&#13;
was an important distinction; in 2020, not so much. Today, all shipping JVMs&#13;
include both compilers (though in common usage, they are usually referred to&#13;
as <code>server</code> JVMs).</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="compiler-flags-aside">&#13;
<h5>Compiler Flags</h5>&#13;
<p><a data-primary="compiler flags" data-type="indexterm" id="idm45775556254344"/><a data-primary="flags" data-secondary="compiler flags" data-type="indexterm" id="idm45775556253640"/><a data-primary="just-in-time (JIT) compiler" data-secondary="compiler flags" data-type="indexterm" id="idm45775556252696"/>In older versions of Java, you would specify which compiler you wanted to use&#13;
via a flag that didn’t follow the normal convention for JVM flags: you would&#13;
use&#13;
<code class="keep-together">-client</code>&#13;
for the client compiler and either&#13;
<code class="keep-together">-server</code> or&#13;
<code class="keep-together">-d64</code> for the server compiler.</p>&#13;
&#13;
<p>Because developers don’t change scripts unnecessarily, you are&#13;
bound to run across scripts and other command lines that specify either&#13;
<code class="keep-together">-client</code>&#13;
or&#13;
<code class="keep-together">-server</code>. But just remember that&#13;
since JDK 8, those flags don’t do anything. That is also true of many&#13;
earlier JDK versions: if you specified&#13;
<code class="keep-together">-client</code>&#13;
for a JVM that supported only the server compiler, you’d get the server&#13;
compiler anyway.</p>&#13;
&#13;
<p>On the other hand, be aware that the old <code>-d64</code> argument (which was&#13;
essentially an&#13;
alias for&#13;
<code class="keep-together">-server</code>)&#13;
has been removed from JDK 11 and will cause an error. Using that argument&#13;
is a no-op on JDK 8.</p>&#13;
</div></aside>&#13;
&#13;
<p>Despite being called server JVMs, the distinction between client and server&#13;
compilers persists; both compilers are available to and used by the JVM.&#13;
So knowing this difference is important in understanding how the&#13;
compiler works.</p>&#13;
&#13;
<p>Historically, JVM developers (and even some tools) sometimes&#13;
referred to the compilers&#13;
by the names <code>C1</code> (compiler 1, client compiler) and <code>C2</code> (compiler 2,&#13;
server <span class="keep-together">compiler).</span> Those names are more apt now, since any distinction between&#13;
a client and server computer is long gone, so we’ll adopt those names&#13;
throughout.</p>&#13;
&#13;
<p>The primary difference between the two compilers is their aggressiveness&#13;
in compiling code. The C1 compiler begins compiling sooner than the&#13;
C2 compiler does. This means that during the beginning of code&#13;
execution, the C1 compiler will be faster, because it will have compiled&#13;
correspondingly more code than the C2 compiler.</p>&#13;
&#13;
<p>The engineering trade-off here is the knowledge the C2 compiler&#13;
gains while it waits: that knowledge allows the C2 compiler&#13;
to make better optimizations in the&#13;
compiled code.  Ultimately, code produced by the C2 compiler will be&#13;
faster than that produced by the C1 compiler.  From a user’s&#13;
perspective, the benefit to that trade-off is based on how long the program&#13;
will run and how important the startup time of the program is.</p>&#13;
&#13;
<p>When these compilers were separate, the obvious question was&#13;
why there needed to be a choice at all: couldn’t&#13;
the JVM start with the C1 compiler and then use the C2 compiler as&#13;
code gets hotter? That technique is known as <em>tiered compilation</em>, and it&#13;
is the technique all JVMs now use.&#13;
<a data-primary="-XX:-TieredCompilation" data-type="indexterm" id="idm45775556238840"/>It can be explicitly disabled with the&#13;
<span class="keep-together"><code>-XX:-TieredCompilation</code></span>&#13;
flag (the default value of which is <code>true</code>); in <a data-type="xref" href="#advance-compiler-tuning">“Advanced Compiler Flags”</a>, we’ll&#13;
discuss the ramifications of doing that.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Common Compiler Flags" data-type="sect1"><div class="sect1" id="idm45775556235864">&#13;
<h1>Common Compiler Flags</h1>&#13;
&#13;
<p>Two commonly used flags affect the JIT compiler; we’ll look&#13;
at them in this section.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tuning the Code Cache" data-type="sect2"><div class="sect2" id="JITCodeCache">&#13;
<h2>Tuning the Code Cache</h2>&#13;
&#13;
<p><a data-primary="code cache tuning" data-type="indexterm" id="ix_ch04-asciidoc3"/><a data-primary="just-in-time (JIT) compiler" data-secondary="code cache tuning" data-type="indexterm" id="ix_ch04-asciidoc4"/><a data-primary="tuning" data-secondary="code cache" data-type="indexterm" id="ix_ch04-asciidoc5"/>When the JVM compiles code, it holds the set of assembly-language instructions&#13;
in the code cache. The code cache has a fixed size, and once it has filled&#13;
up, the JVM is not able to compile any additional code.</p>&#13;
&#13;
<p>It is easy to see the potential issue here if the code cache is too small.&#13;
Some hot methods will get compiled, but others will not: the application will&#13;
end up running a lot of (very slow) interpreted code.</p>&#13;
&#13;
<p>When the code cache fills up, the JVM spits out this&#13;
warning:</p>&#13;
&#13;
<pre data-type="programlisting">Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full.&#13;
         Compiler has been disabled.&#13;
Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the&#13;
         code cache size using -XX:ReservedCodeCacheSize=</pre>&#13;
&#13;
<p>It is sometimes easy to miss this message;&#13;
another way to determine if the&#13;
compiler has ceased to compile code is to follow the output of the&#13;
compilation log discussed later in this section.</p>&#13;
&#13;
<p>There really isn’t a good mechanism to figure out how much code&#13;
cache a particular application needs. Hence, when you need to increase&#13;
the code cache size, it is sort of a hit-and-miss operation; a typical&#13;
option is to simply double or quadruple the default.</p>&#13;
&#13;
<p><a data-primary="-XX:ReservedCodeCacheSize=&lt;MB&gt;" data-type="indexterm" id="idm45775556192840"/><a data-primary="flags" data-secondary="code cache tuning" data-type="indexterm" id="idm45775556191976"/>The maximum size of the code cache is set via the&#13;
<span class="keep-together"><code>-XX:ReservedCodeCacheSize=</code><em><code>N</code></em></span> flag (where <code><em>N</em></code> is the default just mentioned for the particular compiler). The code cache is managed like most memory in the&#13;
JVM: <a data-primary="-XX:InitialCodeCacheSize=&lt;MB&gt;" data-type="indexterm" id="idm45775556188968"/>there is an initial size (specified <span class="keep-together">by <code>-XX:InitialCodeCacheSize=</code><em><code>N</code></em><code>).</code></span> Allocation of the code cache size starts at the initial size and increases as the cache fills up. The initial size of the code cache is 2,496 KB, and the default maximum size is 240 MB. Resizing the cache happens in <span class="keep-together">the background and doesn’t really affect performance, so setting the</span> <code class="keep-together">ReservedCodeCacheSize</code> size (i.e., setting the maximum code cache size) is all that is generally needed.</p>&#13;
&#13;
<p>Is there a disadvantage to specifying a really large value for the maximum&#13;
code cache&#13;
size so that it never runs out of space? It depends on the resources&#13;
available on the target machine. If a 1 GB code cache size is specified,&#13;
the JVM&#13;
will reserve 1 GB of native memory. That memory isn’t allocated until&#13;
needed, but it is still reserved, which means that sufficient&#13;
virtual memory must be available on your machine to satisfy the reservation.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775556184040">&#13;
<h5>Reserved Versus Allocated Memory</h5>&#13;
<p><a data-primary="memory" data-secondary="reserved versus allocated" data-type="indexterm" id="idm45775556182872"/><a data-primary="memory" data-seealso="native memory" data-type="indexterm" id="idm45775556181832"/>It is important to understand the distinction between how the JVM <em>reserves</em>&#13;
memory and how it <em>allocates</em> memory. This difference applies to the code&#13;
cache, the Java heap, and various other native memory structures of the JVM.</p>&#13;
&#13;
<p>For details on this subject, see <a data-type="xref" href="ch08.html#Footprint">“Footprint”</a>.</p>&#13;
</div></aside>&#13;
&#13;
<p>In addition, if you still have an old Windows machine with a&#13;
32-bit JVM, the total process size cannot exceed 4 GB. That includes the Java heap, space for&#13;
all the&#13;
code of the JVM itself (including its native libraries and thread stacks),&#13;
any native memory&#13;
the application allocates (either directly or via the New I/O [NIO] libraries), and&#13;
of course the code cache.</p>&#13;
&#13;
<p>Those are the reasons the code cache is not unbounded and sometimes requires&#13;
tuning for large applications. On 64-bit machines with sufficient memory,&#13;
setting&#13;
the value too high is unlikely to have a practical effect on the application:&#13;
the application won’t run out of process space memory, and the&#13;
extra memory reservation&#13;
will generally be accepted by the operating system.</p>&#13;
&#13;
<p class="pagebreak-before">In Java 11, the code cache is segmented into three parts:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Nonmethod code</p>&#13;
</li>&#13;
<li>&#13;
<p>Profiled code</p>&#13;
</li>&#13;
<li>&#13;
<p>Nonprofiled code</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>By default, the code cache is sized the same way (up to 240 MB), and you&#13;
can still adjust the total size of the code cache by using the&#13;
<code class="keep-together">ReservedCodeCacheSize</code>&#13;
flag. In that case, the nonmethod code segment is allocated space according&#13;
to the number of compiler threads (see <a data-type="xref" href="#CompilationThreads">“Compilation Threads”</a>); on a machine&#13;
with four CPUs, it will be about 5.5 MB. The other two segments then equally&#13;
divide the remaining total code cache—for example, about 117.2 MB each on&#13;
the machine with four CPUs (yielding 240 MB total).</p>&#13;
&#13;
<p>You’ll rarely need to tune these segments individually, but if so, the flags are as <span class="keep-together">follows:</span></p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><code>-XX:NonNMethodCodeHeapSize=<em>N</em></code>: for the nonmethod code</p>&#13;
</li>&#13;
<li>&#13;
<p><code>-XX:ProfiledCodeHapSize=<em>N</em></code> for the profiled code</p>&#13;
</li>&#13;
<li>&#13;
<p><code>-XX:NonProfiledCodeHapSize=<em>N</em></code> for the nonprofiled code</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The size of the code cache (and the JDK 11 segments) can be monitored in real&#13;
time by using <code>jconsole</code> and selecting&#13;
the Memory Pool Code Cache chart on the Memory panel. You can also enable&#13;
Java’s Native Memory Tracking feature as described in <a data-type="xref" href="ch08.html#NativeMemory">Chapter 8</a>.</p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>The code cache is a resource with a defined maximum size that affects the total amount of compiled code the JVM can run.</p>&#13;
</li>&#13;
<li>&#13;
<p>Very large applications can use up the entire code cache in its default configuration; monitor the code cache and increase its size if necessary.<a data-startref="ix_ch04-asciidoc5" data-type="indexterm" id="idm45775556159576"/><a data-startref="ix_ch04-asciidoc4" data-type="indexterm" id="idm45775556158872"/><a data-startref="ix_ch04-asciidoc3" data-type="indexterm" id="idm45775556158200"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Inspecting the Compilation Process" data-type="sect2"><div class="sect2" id="PrintCompilation">&#13;
<h2>Inspecting the Compilation Process</h2>&#13;
&#13;
<p><a data-primary="flags" data-secondary="compilation inspection" data-type="indexterm" id="ix_ch04-asciidoc6"/><a data-primary="just-in-time (JIT) compiler" data-secondary="inspecting the compilation process" data-type="indexterm" id="ix_ch04-asciidoc7"/>The second flag isn’t a tuning per se: it&#13;
will not improve the performance of an application. <a data-primary="-XX:+PrintCompilation" data-type="indexterm" id="idm45775556152632"/>Rather,&#13;
the <code class="keep-together">-XX:+PrintCompilation</code>&#13;
flag (which by default is <code>false</code>) gives us visibility into the workings of&#13;
the compiler (though we’ll also look at tools that provide similar&#13;
information).</p>&#13;
&#13;
<p>If&#13;
<code class="keep-together">PrintCompilation</code>&#13;
is enabled, every time a method (or loop)&#13;
is compiled, the JVM prints out a line with information about what&#13;
has just been compiled.</p>&#13;
&#13;
<p>Most lines of the compilation log have the following format:</p>&#13;
&#13;
<pre data-type="programlisting">timestamp compilation_id attributes (tiered_level) method_name size deopt</pre>&#13;
&#13;
<p>The timestamp here is the time after the compilation has finished (relative to&#13;
0, which is when the JVM started).</p>&#13;
&#13;
<p>The <code>compilation_id</code> is an internal task ID. Usually, this&#13;
number will simply increase monotonically, but sometimes&#13;
you may see an out-of-order compilation ID. This happens most frequently when&#13;
there are multiple compilation threads and indicates that compilation&#13;
threads are running faster or slower relative to each other. Don’t&#13;
conclude, though, that one particular compilation task was somehow&#13;
inordinately slow: it is usually just a function of thread scheduling.</p>&#13;
&#13;
<p>The <code>attributes</code> field is a series of five characters that indicates&#13;
the state of the code being compiled. If a particular attribute applies to&#13;
the given compilation,&#13;
the character shown in the following list is printed; otherwise, a space is printed for&#13;
that attribute. Hence, the five-character attribute string may appear as&#13;
two or more items separated by spaces. The various attributes are as follows:</p>&#13;
<dl>&#13;
<dt><code>%</code></dt>&#13;
<dd>&#13;
<p>The compilation is OSR.</p>&#13;
</dd>&#13;
<dt><code>s</code></dt>&#13;
<dd>&#13;
<p>The method is synchronized.</p>&#13;
</dd>&#13;
<dt><code>!</code></dt>&#13;
<dd>&#13;
<p>The method has an exception handler.</p>&#13;
</dd>&#13;
<dt><code>b</code></dt>&#13;
<dd>&#13;
<p>Compilation occurred in blocking mode.</p>&#13;
</dd>&#13;
<dt><code>n</code></dt>&#13;
<dd>&#13;
<p>Compilation occurred for a wrapper to a native method.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p><a data-primary="on-stack replacement (OSR) compilation" data-type="indexterm" id="idm45775556136840"/><a data-primary="OSR (on-stack replacement) compilation" data-type="indexterm" id="idm45775556136120"/>The first of these attributes refers to <em>on-stack replacement</em> (OSR).&#13;
JIT compilation is an asynchronous process: when the JVM decides that a&#13;
certain method should be compiled, that method is placed in a queue. Rather&#13;
than wait for the compilation, the JVM then continues interpreting the method,&#13;
and the next time the method is called, the JVM will execute the compiled&#13;
version of the method (assuming the compilation has finished, of course).</p>&#13;
&#13;
<p>But consider a long-running loop. The JVM will notice that the loop itself&#13;
should be compiled and will queue that code for compilation.&#13;
But that isn’t sufficient: the JVM has to have the&#13;
ability to start executing the compiled version of the loop while the loop&#13;
is still running—it would be inefficient to wait until the loop and&#13;
enclosing method exit (which may not even happen).&#13;
Hence, when the code for the loop has finished compiling,&#13;
the JVM replaces the code&#13;
(on stack), and the next iteration of the loop will execute the much faster&#13;
compiled version of the code.&#13;
This is OSR.</p>&#13;
&#13;
<p>The next two attributes should be self-explanatory. The blocking&#13;
flag will never be printed by default in current versions of Java;&#13;
it indicates that compilation&#13;
did not occur in the background (see <a data-type="xref" href="#CompilationThreads">“Compilation Threads”</a> for more details). Finally, the native attribute indicates that the JVM generated&#13;
compiled code to facilitate the call into a native method.</p>&#13;
&#13;
<p>If tiered compilation has been disabled, the next&#13;
field (<code>tiered_level</code>)&#13;
will be&#13;
blank. Otherwise, it will be a number indicating which tier has completed&#13;
compilation.</p>&#13;
&#13;
<p>Next comes the name of the method being compiled (or the method containing&#13;
the loop being compiled for OSR), which is printed as <code>ClassName::method</code>.</p>&#13;
&#13;
<p>Next is the <code>size</code> (in bytes) of the code being compiled. This is&#13;
the size of the Java byte<span class="keep-together">codes</span>, not the size of the compiled code (so,&#13;
unfortunately, this can’t be used to predict how large to size the code&#13;
cache).</p>&#13;
&#13;
<p>Finally, in some cases a message at the end of the compilation&#13;
line will indicate that some sort of deoptimization has occurred; these&#13;
are typically the phrases <code>made not entrant</code> or <code>made zombie</code>. See&#13;
<a data-type="xref" href="#Deoptimization">“Deoptimization”</a> for more details.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775556125752">&#13;
<h5>Inspecting Compilation with jstat</h5>&#13;
<p><a data-primary="-XX:+PrintCompilation" data-type="indexterm" id="idm45775556124344"/><a data-primary="jstat" data-type="indexterm" id="idm45775556123640"/>Seeing the compilation log requires that the program&#13;
be started with the&#13;
<code class="keep-together">-XX:+PrintCompilation</code>&#13;
flag. If the program was started without that flag, you can&#13;
get limited visibility into the working of the compiler by using&#13;
<code>jstat</code>.</p>&#13;
&#13;
<p><code>jstat</code> has two options to provide information about the compiler. The&#13;
<code class="keep-together">-compiler</code>&#13;
option supplies summary information about the number of methods&#13;
compiled (here 5003 is the process ID of the program to be&#13;
inspected):</p>&#13;
<pre data-type="programlisting">&#13;
% <strong>jstat -compiler 5003</strong>&#13;
Compiled Failed Invalid   Time   FailedType FailedMethod&#13;
     206      0       0     1.97          0&#13;
</pre>&#13;
&#13;
<p>Note this also lists the number of methods that failed to compile&#13;
and the name of the last method that failed to compile; if profiles or&#13;
other information lead you to suspect that&#13;
a method is slow because it hasn’t been compiled, this is an easy way to&#13;
verify that hypothesis.</p>&#13;
&#13;
<p>Alternately, you can use the&#13;
<code class="keep-together">-printcompilation</code>&#13;
option to get information&#13;
about the last method that is compiled. Because&#13;
<code class="keep-together">jstat</code>&#13;
takes an optional&#13;
argument to repeat its operation, you can see over time which methods are&#13;
being compiled. In this example, <code>jstat</code> repeats the information&#13;
for process ID 5003 every second (1,000 ms):</p>&#13;
<pre data-type="programlisting">&#13;
% <strong>jstat -printcompilation 5003 1000</strong>&#13;
Compiled  Size  Type Method&#13;
     207     64    1 java/lang/CharacterDataLatin1 toUpperCase&#13;
     208      5    1 java/math/BigDecimal$StringBuilderHelper getCharArray&#13;
</pre>&#13;
</div></aside>&#13;
&#13;
<p>The compilation log may also include a line that looks like this:</p>&#13;
&#13;
<pre data-type="programlisting">timestamp compile_id COMPILE SKIPPED: reason</pre>&#13;
&#13;
<p>This line (with the literal text <code>COMPILE SKIPPED</code>) indicates that something&#13;
has gone wrong with the compilation of the given method. In two&#13;
cases this is expected, depending on the reason specified:</p>&#13;
<dl>&#13;
<dt>Code cache filled</dt>&#13;
<dd>&#13;
<p>The size of the code cache needs to be increased using the&#13;
<code class="keep-together">ReservedCodeCache</code>&#13;
flag.</p>&#13;
</dd>&#13;
<dt>Concurrent classloading</dt>&#13;
<dd>&#13;
<p>The class was modified as it was being compiled. The JVM will compile it again later; you should expect to see the method recompiled later in the log.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>In all cases (except the cache being filled), the compilation should be&#13;
reattempted. If it is not, an error prevents&#13;
compilation of the code. This is often a bug in the compiler, but the usual&#13;
remedy in all cases is to refactor the code into something simpler&#13;
that the compiler can handle.</p>&#13;
&#13;
<p>Here are a few lines of output from enabling <code>PrintCompilation</code> on the&#13;
stock REST application:</p>&#13;
&#13;
<pre data-type="programlisting">  28015  850       4     net.sdo.StockPrice::getClosingPrice (5 bytes)&#13;
  28179  905  s    3     net.sdo.StockPriceHistoryImpl::process (248 bytes)&#13;
  28226   25 %     3     net.sdo.StockPriceHistoryImpl::&lt;init&gt; @ 48 (156 bytes)&#13;
  28244  935       3     net.sdo.MockStockPriceEntityManagerFactory$\&#13;
                             MockStockPriceEntityManager::find (507 bytes)&#13;
  29929  939       3     net.sdo.StockPriceHistoryImpl::&lt;init&gt; (156 bytes)&#13;
 106805 1568   !   4     net.sdo.StockServlet::processRequest (197 bytes)</pre>&#13;
&#13;
<p>This output includes only a few of the stock-related methods (and not&#13;
necessarily all of the lines related to a particular method).&#13;
A few interesting things to note: the first such method wasn’t&#13;
compiled until 28 seconds after the server was started, and 849&#13;
methods were compiled before it. In this case, all those other methods were&#13;
methods of the server or JDK (filtered out of this output).&#13;
The server&#13;
took about 2 seconds to start;&#13;
the remaining 26 seconds before anything else was compiled were essentially&#13;
idle as the application server waited for requests.</p>&#13;
&#13;
<p>The remaining lines are included to point out interesting features. The <code class="keep-together">process()</code>&#13;
method is synchronized, so the attributes include an <code>s</code>. Inner classes are compiled just like any other class and appear in the output with the usual Java nomenclature: <code class="keep-together">outer-classname$inner-classname</code>.&#13;
The <code class="keep-together">processRequest()</code>&#13;
method shows up with the exception handler as expected.</p>&#13;
&#13;
<p>Finally, recall the implementation of the <code>StockPriceHistoryImpl</code>&#13;
constructor, which contains a large loop:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code> <code class="nf">StockPriceHistoryImpl</code><code class="o">(</code><code class="n">String</code> <code class="n">s</code><code class="o">,</code> <code class="n">Date</code> <code class="n">startDate</code><code class="o">,</code> <code class="n">Date</code> <code class="n">endDate</code><code class="o">)</code> <code class="o">{</code>&#13;
    <code class="n">EntityManager</code> <code class="n">em</code> <code class="o">=</code> <code class="n">emf</code><code class="o">.</code><code class="na">createEntityManager</code><code class="o">();</code>&#13;
    <code class="n">Date</code> <code class="n">curDate</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Date</code><code class="o">(</code><code class="n">startDate</code><code class="o">.</code><code class="na">getTime</code><code class="o">());</code>&#13;
    <code class="n">symbol</code> <code class="o">=</code> <code class="n">s</code><code class="o">;</code>&#13;
    <code class="k">while</code> <code class="o">(!</code><code class="n">curDate</code><code class="o">.</code><code class="na">after</code><code class="o">(</code><code class="n">endDate</code><code class="o">))</code> <code class="o">{</code>&#13;
         <code class="n">StockPrice</code> <code class="n">sp</code> <code class="o">=</code> <code class="n">em</code><code class="o">.</code><code class="na">find</code><code class="o">(</code><code class="n">StockPrice</code><code class="o">.</code><code class="na">class</code><code class="o">,</code> <code class="k">new</code> <code class="n">StockPricePK</code><code class="o">(</code><code class="n">s</code><code class="o">,</code> <code class="n">curDate</code><code class="o">));</code>&#13;
         <code class="k">if</code> <code class="o">(</code><code class="n">sp</code> <code class="o">!=</code> <code class="kc">null</code><code class="o">)</code> <code class="o">{</code>&#13;
            <code class="k">if</code> <code class="o">(</code><code class="n">firstDate</code> <code class="o">==</code> <code class="kc">null</code><code class="o">)</code> <code class="o">{</code>&#13;
                <code class="n">firstDate</code> <code class="o">=</code> <code class="o">(</code><code class="n">Date</code><code class="o">)</code> <code class="n">curDate</code><code class="o">.</code><code class="na">clone</code><code class="o">();</code>&#13;
            <code class="o">}</code>&#13;
            <code class="n">prices</code><code class="o">.</code><code class="na">put</code><code class="o">((</code><code class="n">Date</code><code class="o">)</code> <code class="n">curDate</code><code class="o">.</code><code class="na">clone</code><code class="o">(),</code> <code class="n">sp</code><code class="o">);</code>&#13;
            <code class="n">lastDate</code> <code class="o">=</code> <code class="o">(</code><code class="n">Date</code><code class="o">)</code> <code class="n">curDate</code><code class="o">.</code><code class="na">clone</code><code class="o">();</code>&#13;
        <code class="o">}</code>&#13;
        <code class="n">curDate</code><code class="o">.</code><code class="na">setTime</code><code class="o">(</code><code class="n">curDate</code><code class="o">.</code><code class="na">getTime</code><code class="o">()</code> <code class="o">+</code> <code class="n">msPerDay</code><code class="o">);</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p><a data-primary="on-stack replacement (OSR) compilation" data-type="indexterm" id="idm45775556096632"/><a data-primary="OSR (on-stack replacement) compilation" data-type="indexterm" id="idm45775556096024"/>The loop is executed more often than the constructor itself, so the&#13;
loop is subject to OSR compilation. Note that it took a while for that&#13;
method to be compiled; its compilation ID is 25, but it doesn’t appear until&#13;
other methods in the 900 range are being compiled. (It’s easy to read OSR&#13;
lines like this example as 25% and wonder about the other 75%, but remember that&#13;
the number is the compilation ID, and the % just signifies OSR compilation.)&#13;
That is typical of OSR compilation; the stack replacement is harder to set&#13;
up, but other compilation can continue in the meantime.<a data-startref="ix_ch04-asciidoc7" data-type="indexterm" id="idm45775556094696"/><a data-startref="ix_ch04-asciidoc6" data-type="indexterm" id="idm45775555976728"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tiered Compilation Levels" data-type="sect2"><div class="sect2" id="idm45775556156232">&#13;
<h2>Tiered Compilation Levels</h2>&#13;
&#13;
<p><a data-primary="just-in-time (JIT) compiler" data-secondary="tiered compilation levels" data-type="indexterm" id="idm45775555975064"/><a data-primary="tiered compilation" data-secondary="levels" data-type="indexterm" id="idm45775555974104"/>The compilation log for a program using tiered&#13;
compilation prints the tier level at which each&#13;
method is compiled. In the sample output, code was compiled&#13;
either at level 3 or 4, even though we’ve discussed only two compilers&#13;
(plus the interpreter) so far. It turns out that there are five levels of compilation, because the C1&#13;
compiler has three levels. So the levels of compilation are as follows:</p>&#13;
<dl>&#13;
<dt>0</dt>&#13;
<dd>&#13;
<p>Interpreted code</p>&#13;
</dd>&#13;
<dt>1</dt>&#13;
<dd>&#13;
<p>Simple C1 compiled code</p>&#13;
</dd>&#13;
<dt>2</dt>&#13;
<dd>&#13;
<p>Limited C1 compiled code</p>&#13;
</dd>&#13;
<dt>3</dt>&#13;
<dd>&#13;
<p>Full C1 compiled code</p>&#13;
</dd>&#13;
<dt>4</dt>&#13;
<dd>&#13;
<p>C2 compiled code</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>A typical compilation log shows that&#13;
most methods are first compiled at level 3: full C1 compilation. (All methods&#13;
start at level 0, of course, but that doesn’t appear in the log.) If a&#13;
method&#13;
runs often enough, it will get&#13;
compiled at level 4 (and the level 3 code will be made not entrant).&#13;
This is the most frequent path: the C1&#13;
compiler waits to compile something until it has information about how the&#13;
code is used that it can leverage to perform optimizations.</p>&#13;
&#13;
<p>If the C2 compiler queue is full, methods will be pulled from the&#13;
C2 queue and compiled at level 2, which is the level at which the C1&#13;
compiler uses the invocation and back-edge counters (but doesn’t require&#13;
profile feedback). That gets the method compiled&#13;
more quickly; the method will later be compiled at level 3 after the C1&#13;
compiler&#13;
has gathered profile information, and finally compiled at level 4 when the&#13;
C2 compiler queue is less busy.</p>&#13;
&#13;
<p>On the other hand, if the C1 compiler queue is full, a method that is&#13;
scheduled for compilation at level 3 may become eligible for level 4&#13;
compilation while still waiting to be compiled at level 3. In that case, it&#13;
is quickly compiled to level 2 and then transitioned to level 4.</p>&#13;
&#13;
<p>Trivial methods may start in either level 2 or 3 but then go to level 1 because of&#13;
their trivial nature. If the C2 compiler for some reason cannot compile&#13;
the code, it will also go to level 1. And, of course, when code is deoptimized, it goes to level 0.</p>&#13;
&#13;
<p>Flags control some of this behavior, but expecting results&#13;
when tuning at this level is optimistic. The best case for performance&#13;
happens when methods are compiled as expected: tier 0 → tier 3 → tier 4.&#13;
If methods frequently get compiled into tier 2 and extra CPU cycles are&#13;
available, consider increasing the number of compiler threads; that will&#13;
reduce the size of the C2 compiler queue. If no extra CPU cycles are&#13;
available, all you can do is attempt to reduce the size of the&#13;
application.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deoptimization" data-type="sect2"><div class="sect2" id="Deoptimization">&#13;
<h2>Deoptimization</h2>&#13;
&#13;
<p><a data-primary="deoptimization" data-type="indexterm" id="ix_ch04-asciidoc8"/><a data-primary="just-in-time (JIT) compiler" data-secondary="deoptimization" data-type="indexterm" id="ix_ch04-asciidoc9"/>The discussion of the output of the&#13;
<code class="keep-together">PrintCompilation</code>&#13;
flag mentioned&#13;
two cases of the compiler deoptimizing the code. <em>Deoptimization</em> means&#13;
that the compiler has to “undo” a previous compilation. The effect is that the performance of the application will be reduced—at&#13;
least until the compiler can recompile the code in question.</p>&#13;
&#13;
<p>Deoptimization occurs in two cases: when code is <code>made not entrant</code> and when code is <code>made zombie</code>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Not entrant code" data-type="sect3"><div class="sect3" id="idm45775555954744">&#13;
<h3>Not entrant code</h3>&#13;
&#13;
<p><a data-primary="deoptimization" data-secondary="not-entrant code" data-type="indexterm" id="ix_ch04-asciidoc10"/><a data-primary="not-entrant code" data-type="indexterm" id="ix_ch04-asciidoc11"/>Two things cause code to be made not entrant. One is due&#13;
to the way classes and interfaces work, and one is an implementation detail&#13;
of tiered compilation.</p>&#13;
&#13;
<p>Let’s look at the first case. Recall that the stock application has an interface <code class="keep-together">StockPriceHistory</code>.&#13;
In the sample code, this interface has two implementations: a <span class="keep-together">basic one (<code>StockPriceHistoryImpl</code>)</span> and one that adds logging&#13;
(<code>Stock​PriceHistoryLogger</code>) to each operation. In the REST code, the implementation used is based on the <code>log</code> parameter of the URL:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="n">StockPriceHistory</code> <code class="n">sph</code><code class="o">;</code>&#13;
<code class="n">String</code> <code class="n">log</code> <code class="o">=</code> <code class="n">request</code><code class="o">.</code><code class="na">getParameter</code><code class="o">(</code><code class="s">"log"</code><code class="o">);</code>&#13;
<code class="k">if</code> <code class="o">(</code><code class="n">log</code> <code class="o">!=</code> <code class="kc">null</code> <code class="o">&amp;&amp;</code> <code class="n">log</code><code class="o">.</code><code class="na">equals</code><code class="o">(</code><code class="s">"true"</code><code class="o">))</code> <code class="o">{</code>&#13;
    <code class="n">sph</code> <code class="o">=</code> <code class="k">new</code> <code class="n">StockPriceHistoryLogger</code><code class="o">(...);</code>&#13;
<code class="o">}</code>&#13;
<code class="k">else</code> <code class="o">{</code>&#13;
    <code class="n">sph</code> <code class="o">=</code> <code class="k">new</code> <code class="n">StockPriceHistoryImpl</code><code class="o">(...);</code>&#13;
<code class="o">}</code>&#13;
<code class="c1">// Then the JSP makes calls to:</code>&#13;
<code class="n">sph</code><code class="o">.</code><code class="na">getHighPrice</code><code class="o">();</code>&#13;
<code class="n">sph</code><code class="o">.</code><code class="na">getStdDev</code><code class="o">();</code>&#13;
<code class="c1">// and so on</code></pre>&#13;
&#13;
<p>If a bunch of calls are made to <em>http://localhost:8080/StockServlet</em>&#13;
(that is, without the <code>log</code> parameter), the compiler will see that the&#13;
actual type of the <code>sph</code> object is&#13;
<code class="keep-together">StockPriceHistoryImpl</code>.&#13;
It will then&#13;
inline code and perform other optimizations based on that knowledge.</p>&#13;
&#13;
<p>Later, say a call is made to&#13;
<em>http://localhost:8080/StockServlet?log=true</em>. Now the assumption the&#13;
compiler made regarding the type of the <code>sph</code> object is incorrect;&#13;
the previous optimizations&#13;
are no longer valid. This generates a deoptimization trap, and the previous&#13;
optimizations are discarded. If a lot of additional calls are made&#13;
with logging enabled, the JVM will quickly end up compiling that code and&#13;
making new optimizations.</p>&#13;
&#13;
<p>The compilation log for that scenario will include lines&#13;
such as the following:</p>&#13;
&#13;
<pre data-type="programlisting"> 841113   25 %           net.sdo.StockPriceHistoryImpl::&lt;init&gt; @ -2 (156 bytes)&#13;
                                 made not entrant&#13;
 841113  937  s          net.sdo.StockPriceHistoryImpl::process (248 bytes)&#13;
                                 made not entrant&#13;
1322722   25 %           net.sdo.StockPriceHistoryImpl::&lt;init&gt; @ -2 (156 bytes)&#13;
                                 made zombie&#13;
1322722  937  s          net.sdo.StockPriceHistoryImpl::process (248 bytes)&#13;
                                 made zombie</pre>&#13;
&#13;
<p>Note that both the OSR-compiled constructor and the standard-compiled methods&#13;
have been made not entrant, and some time much later, they are made zombie.</p>&#13;
&#13;
<p>Deoptimization sounds like a bad thing, at least in terms of performance,&#13;
but that isn’t necessarily the case. <a data-type="xref" href="#TableDeopt">Table 4-1</a> shows the operations&#13;
per second that the REST server achieves under deoptimization scenarios.</p>&#13;
<table id="TableDeopt">&#13;
<caption><span class="label">Table 4-1. </span>Throughput of server with deoptimization</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Scenario</th>&#13;
<th>OPS</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Standard implementation</p></td>&#13;
<td><p>24.4</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Standard implementation after deopt</p></td>&#13;
<td><p>24.4</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Logging implementation</p></td>&#13;
<td><p>24.1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Mixed impl</p></td>&#13;
<td><p>24.3</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>The standard implementation will give us 24.4 OPS.&#13;
Suppose that immediately after that test, a test is run that triggers the&#13;
<code>StockPriceHistoryLogger</code> path—that is the scenario that ran to produce&#13;
the deoptimization examples&#13;
just listed. The full output of&#13;
<code class="keep-together">PrintCompilation</code>&#13;
shows that all the methods of the&#13;
<code class="keep-together">StockPriceHistoryImpl</code>&#13;
class&#13;
get deoptimized when the requests for the logging implementation are started.&#13;
But after deoptimization, if the path that uses the&#13;
<code class="keep-together">StockPriceHistoryImpl</code>&#13;
implementation is rerun, that code will get recompiled (with slightly different&#13;
assumptions), and&#13;
we will still end up still seeing about 24.4 OPS (after&#13;
another warm-up period).</p>&#13;
&#13;
<p>That’s the best case, of course. What happens if the calls are intermingled&#13;
such that the compiler can never really assume which path the code will take?&#13;
Because of the extra logging, the path that includes the logging gets&#13;
about 24.1 OPS through the server. If operations are mixed,&#13;
we get about 24.3 OPS: just about what would be expected from an&#13;
average. So aside from&#13;
a momentary point where the trap is processed, deoptimization&#13;
has not affected the performance in any significant way.</p>&#13;
&#13;
<p>The second thing that can cause code to be made not entrant is the&#13;
way tiered compilation works. When&#13;
code is compiled by the C2 compiler, the JVM must replace&#13;
the code already compiled by the C1 compiler. It does this by marking&#13;
the old code as not entrant and using the same&#13;
deoptimization mechanism to substitute the newly compiled (and more efficient)&#13;
code.  Hence, when a program is run with tiered compilation,&#13;
the compilation log will show a slew of methods that are&#13;
made not entrant.&#13;
Don’t panic: this “deoptimization” is, in fact, making the code that much&#13;
faster.</p>&#13;
&#13;
<p>The way to detect this is to pay attention to the tier level in&#13;
the compilation log:</p>&#13;
&#13;
<pre data-type="programlisting">  40915   84 %     3       net.sdo.StockPriceHistoryImpl::&lt;init&gt; @ 48 (156 bytes)&#13;
  40923 3697       3       net.sdo.StockPriceHistoryImpl::&lt;init&gt; (156 bytes)&#13;
  41418   87 %     4       net.sdo.StockPriceHistoryImpl::&lt;init&gt; @ 48 (156 bytes)&#13;
  41434   84 %     3       net.sdo.StockPriceHistoryImpl::&lt;init&gt; @ -2 (156 bytes)&#13;
                                      made not entrant&#13;
  41458 3749       4       net.sdo.StockPriceHistoryImpl::&lt;init&gt; (156 bytes)&#13;
  41469 3697       3       net.sdo.StockPriceHistoryImpl::&lt;init&gt; (156 bytes)&#13;
                                      made not entrant&#13;
  42772 3697       3       net.sdo.StockPriceHistoryImpl::&lt;init&gt; (156 bytes)&#13;
                                      made zombie&#13;
  42861   84 %     3       net.sdo.StockPriceHistoryImpl::&lt;init&gt; @ -2 (156 bytes)&#13;
                                      made zombie</pre>&#13;
&#13;
<p>Here, the constructor is first OSR-compiled at level 3 and then fully&#13;
compiled also at level 3. A second later, the OSR code becomes eligible for&#13;
level 4 compilation, so it is compiled at level 4 and the level 3 OSR&#13;
code is made not entrant. The same process then occurs for the standard&#13;
compilation, and finally the level 3 code becomes a <span class="keep-together">zombie.</span><a data-startref="ix_ch04-asciidoc11" data-type="indexterm" id="idm45775555812808"/><a data-startref="ix_ch04-asciidoc10" data-type="indexterm" id="idm45775555812104"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Deoptimizing zombie code" data-type="sect3"><div class="sect3" id="idm45775555954088">&#13;
<h3>Deoptimizing zombie code</h3>&#13;
&#13;
<p><a data-primary="deoptimization" data-secondary="zombie code" data-type="indexterm" id="idm45775555810008"/><a data-primary="zombie code" data-type="indexterm" id="idm45775555809032"/>When the compilation log reports that it has made zombie code, it&#13;
is saying that it has reclaimed previous code that was made&#13;
not entrant.&#13;
In the preceding example, after a test was run with the&#13;
<code class="keep-together">StockPriceHistoryLogger</code>&#13;
implementation, the code for the&#13;
<code class="keep-together">StockPriceHistoryImpl</code>&#13;
class was made not entrant. But objects of the&#13;
<code class="keep-together">StockPriceHistoryImpl</code>&#13;
class remained. Eventually all those&#13;
objects were reclaimed by GC. When that happened, the compiler&#13;
noticed that the methods of that class were now eligible to be marked as&#13;
zombie code.</p>&#13;
&#13;
<p>For performance, this is a good thing. Recall that the compiled code&#13;
is held in a fixed-size code cache; when zombie methods are identified,&#13;
the code in question can be removed from the code cache, making&#13;
room for other classes to be compiled (or limiting the amount of memory the&#13;
JVM will need to allocate later).</p>&#13;
&#13;
<p>The possible downside is that if the code for the class is made zombie&#13;
and then later reloaded and heavily used again, the JVM will need&#13;
to recompile and reoptimize the code. Still, that’s exactly what happened in&#13;
the previous scenario, where the test was run without logging, then&#13;
with logging, and then without logging; performance in that case was not&#13;
noticeably affected. In general, the small recompilations that occur when&#13;
zombie code&#13;
is recompiled will not have a measurable effect on most <span class="keep-together">applications.</span></p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>The best way to gain visibility into how code is being compiled is by enabling&#13;
<code class="keep-together">PrintCompilation</code>.</p>&#13;
</li>&#13;
<li>&#13;
<p>Output from enabling <code>PrintCompilation</code> can be used to make sure that compilation is proceeding as expected.</p>&#13;
</li>&#13;
<li>&#13;
<p>Tiered compilation can operate at five distinct levels among the two compilers.</p>&#13;
</li>&#13;
<li>&#13;
<p>Deoptimization is the process by which the JVM replaces previously compiled code. This usually happens in the context of C2 code replacing C1 code, but it can happen because of changes in the execution profile of an application.<a data-startref="ix_ch04-asciidoc9" data-type="indexterm" id="idm45775555797464"/><a data-startref="ix_ch04-asciidoc8" data-type="indexterm" id="idm45775555796760"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Advanced Compiler Flags" data-type="sect1"><div class="sect1" id="advance-compiler-tuning">&#13;
<h1>Advanced Compiler Flags</h1>&#13;
&#13;
<p><a data-primary="flags" data-secondary="advanced compiler flags" data-type="indexterm" id="ix_ch04-asciidoc12"/><a data-primary="just-in-time (JIT) compiler" data-secondary="advanced compiler flags" data-type="indexterm" id="ix_ch04-asciidoc13"/>This section covers a few other flags that affect the compiler.&#13;
Mostly, this gives you a chance to understand even better how the compiler works;&#13;
these flags should not generally be used. On the other hand, another reason&#13;
they are included here is that they were once common enough to be in wide&#13;
usage, so if you’ve encountered them and wonder what they do, this section&#13;
should answer those questions.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Compilation Thresholds" data-type="sect2"><div class="sect2" id="CompileThreshold">&#13;
<h2>Compilation Thresholds</h2>&#13;
&#13;
<p><a data-primary="advanced compiler flags" data-secondary="compilation thresholds" data-type="indexterm" id="ix_ch04-asciidoc14"/><a data-primary="compilation thresholds" data-type="indexterm" id="ix_ch04-asciidoc15"/><a data-primary="just-in-time (JIT) compiler" data-secondary="compilation thresholds" data-type="indexterm" id="ix_ch04-asciidoc16"/>This chapter has been somewhat vague in defining just what triggers&#13;
the compilation of code.&#13;
The major factor is how often the code is&#13;
executed; once it is executed a certain number of times, its compilation&#13;
threshold is reached, and the compiler deems that it has enough information&#13;
to compile the code.</p>&#13;
&#13;
<p>Tunings affect these thresholds.&#13;
However, this section is really designed to give you better insight&#13;
into how the compiler works (and introduce some terms); in current JVMs,&#13;
tuning the threshold never really makes sense.</p>&#13;
&#13;
<p>Compilation is based on two counters in the JVM: the number of times the&#13;
method has been called, <a data-primary="branching back (compiler)" data-type="indexterm" id="idm45775555752168"/>and the number of times any loops in the method&#13;
have branched back. <em>Branching back</em> can effectively be thought of as the&#13;
number of times a loop has completed execution, either because it reached&#13;
the end of the loop itself or because it executed a branching statement&#13;
like <code>continue</code>.</p>&#13;
&#13;
<p><a data-primary="standard compilation" data-type="indexterm" id="idm45775555749992"/>When the JVM executes a Java method, it checks the sum of those two counters&#13;
and decides whether the method is eligible for compilation. If it&#13;
is, the method is queued for compilation (see <a data-type="xref" href="#CompilationThreads">“Compilation Threads”</a> for&#13;
more details about queuing). This kind of compilation has no official name&#13;
but is often called <em>standard</em> <span class="keep-together"><em>compilation</em>.</span></p>&#13;
&#13;
<p>Similarly, every time a loop&#13;
completes an execution, the branching counter is incremented and inspected.&#13;
If the branching counter has exceeded its individual threshold, the loop&#13;
(and not the entire method) becomes eligible for <span class="keep-together">compilation</span>.</p>&#13;
&#13;
<p><a data-primary="tuning" data-secondary="compilation thresholds" data-type="indexterm" id="idm45775555745112"/>Tunings affect these thresholds.&#13;
<a data-primary="-XX:CompileThreshold=&lt;N&gt;" data-type="indexterm" id="idm45775555743848"/><a data-primary="lambda/anonymous classloading" data-type="indexterm" id="idm45775555743160"/>When tiered compilation is disabled,&#13;
standard compilation is triggered by the value of the&#13;
<span class="keep-together"><code>-XX:CompileThreshold=</code><em><code>N</code></em></span>&#13;
flag. The default value of <em><code>N</code></em> is 10,000. Changing the value&#13;
of the <code>CompileThreshold</code> flag will cause the compiler to choose to compile the&#13;
code sooner (or later) than it normally would have. Note, however, that&#13;
although there is one flag here, the threshold is calculated by adding the&#13;
sum of the back-edge loop counter plus the method entry counter.</p>&#13;
&#13;
<p>You can often find recommendations to change the&#13;
<code class="keep-together">CompileThreshold</code>&#13;
flag, and several publications of&#13;
Java benchmarks use this flag (e.g., frequently after 8,000 iterations).&#13;
Some applications still ship with that flag set by default.</p>&#13;
&#13;
<p>But remember that I said this flag works when tiered compilation is&#13;
disabled—which means that when tiered compilation is enabled (as it normally&#13;
is), this flag does nothing at all. Use of this flag is really just a holdover&#13;
from JDK 7 and earlier days.</p>&#13;
&#13;
<p>This flag used to be recommended for two reasons: first, lowering&#13;
it would improve startup time for an application using the C2 compiler,&#13;
since code would get compiled more quickly (and usually with the same&#13;
effectiveness). Second, it could cause some methods to get compiled&#13;
that otherwise never would have been compiled.</p>&#13;
&#13;
<p>That last point is an interesting quirk: if a program runs forever, wouldn’t&#13;
we expect all of its code to get compiled eventually? That’s not&#13;
how it works, because the counters the compilers use&#13;
increase&#13;
as methods and loops are executed, but they also decrease over time.&#13;
Periodically (specifically, when the JVM reaches a safepoint), the value of&#13;
each counter is reduced.</p>&#13;
&#13;
<p>Practically speaking, this means that the&#13;
counters are a relative measure of the <em>recent</em> hotness of the method or loop.&#13;
One side effect is that somewhat frequently executed code may never&#13;
be compiled by the C2 compiler, even for programs that run forever.&#13;
<a data-primary="lukewarm compilation" data-type="indexterm" id="idm45775555735304"/>These methods are&#13;
sometimes called <em>lukewarm</em> (as opposed to hot). Before tiered compilation,&#13;
this was one case where&#13;
reducing the compilation threshold was <span class="keep-together">beneficial.</span></p>&#13;
&#13;
<p>Today, however, even the lukewarm methods will be compiled, though perhaps&#13;
they could be ever-so-slightly improved if we could get them compiled by the C2&#13;
compiler rather than the C1 compiler. <a data-primary="-XX:Tier3InvocationThreshold=N" data-type="indexterm" id="idm45775555732776"/><a data-primary="-XX:Tier4InvocationThreshold=N" data-type="indexterm" id="idm45775555732008"/>Little practical benefit&#13;
exists, but if you’re really interested, try changing the flags&#13;
<code class="keep-together">-XX:Tier3InvocationThreshold=<em>N</em></code>&#13;
(default 200) to get C1 to compile a method more quickly, and&#13;
<code class="keep-together">-XX:Tier4InvocationThreshold=<em>N</em></code>&#13;
(default 5000) to get C2 to compile a method more quickly. Similar&#13;
flags are available for the back-edge threshold.</p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>The thresholds at which methods (or loops) get compiled are set via tunable parameters.</p>&#13;
</li>&#13;
<li>&#13;
<p>Without tiered compilation, it sometimes made sense to adjust those thresholds, but with tiered compilation, this tuning is no longer recommended.<a data-startref="ix_ch04-asciidoc16" data-type="indexterm" id="idm45775555725896"/><a data-startref="ix_ch04-asciidoc15" data-type="indexterm" id="idm45775555725192"/><a data-startref="ix_ch04-asciidoc14" data-type="indexterm" id="idm45775555724520"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Compilation Threads" data-type="sect2"><div class="sect2" id="CompilationThreads">&#13;
<h2>Compilation Threads</h2>&#13;
&#13;
<p><a data-primary="advanced compiler flags" data-secondary="compilation threads" data-type="indexterm" id="ix_ch04-asciidoc17"/><a data-primary="just-in-time (JIT) compiler" data-secondary="compilation threads" data-type="indexterm" id="ix_ch04-asciidoc18"/><a data-primary="threads" data-secondary="compilation threads" data-type="indexterm" id="ix_ch04-asciidoc19"/><a data-type="xref" href="#CompileThreshold">“Compilation Thresholds”</a> mentioned that when a method (or loop) becomes&#13;
eligible for compilation, it is queued for compilation. That queue is processed&#13;
by one or more background threads.</p>&#13;
&#13;
<p>These queues are not strictly first in, first out; methods whose invocation&#13;
counters are higher have priority. So even when a program starts execution&#13;
and has lots of code to compile, this priority ordering helps ensure&#13;
that the most important code will be compiled first. (This is another&#13;
reason the compilation ID in the <code>PrintCompilation</code> output can appear&#13;
out of order.)</p>&#13;
&#13;
<p>The C1 and C2 compilers have different queues, each of which is&#13;
processed by (potentially multiple) different threads. The number of threads&#13;
is based on a complex formula of logarithms, but&#13;
<a data-type="xref" href="#TableCompilerThreads">Table 4-2</a> lists the details.</p>&#13;
<table id="TableCompilerThreads">&#13;
<caption><span class="label">Table 4-2. </span>Default number of C1 and C2 compiler threads for tiered compilation</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>CPUs</th>&#13;
<th>C1 threads</th>&#13;
<th>C2 threads</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>1</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>2</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>1</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>4</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>2</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>8</p></td>&#13;
<td><p>1</p></td>&#13;
<td><p>2</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>16</p></td>&#13;
<td><p>2</p></td>&#13;
<td><p>6</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>32</p></td>&#13;
<td><p>3</p></td>&#13;
<td><p>7</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>64</p></td>&#13;
<td><p>4</p></td>&#13;
<td><p>8</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>128</p></td>&#13;
<td><p>4</p></td>&#13;
<td><p>10</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p><a data-primary="-XX:CICompilerCount=&lt;N&gt;" data-type="indexterm" id="idm45775555692696"/>The number of compiler threads can be&#13;
adjusted by setting the&#13;
<span class="keep-together"><code>-XX:CICompilerCount=</code><em><code>N</code></em></span>&#13;
flag. That is&#13;
the total number of threads the JVM will use to process the queue(s);&#13;
for tiered compilation, one-third (but at least one) will be&#13;
used to process the C1 compiler queue, and the remaining threads (but also&#13;
at least one) will be used to process the C2 compiler queue. The default value of that flag is the sum of the two columns in&#13;
the preceding table.</p>&#13;
&#13;
<p>If tiered compilation is disabled, only the given number of C2&#13;
compiler threads are started.</p>&#13;
&#13;
<p>When might you consider adjusting this value? Because the default value is&#13;
based on the&#13;
number of CPUs, <a data-primary="Docker container" data-type="indexterm" id="idm45775555689128"/>this is one case where running with an older version of&#13;
JDK 8 inside a Docker&#13;
container can cause the automatic tuning to go awry. In such a circumstance,&#13;
you will need to manually set this flag to the desired value (using the&#13;
targets in <a data-type="xref" href="#TableCompilerThreads">Table 4-2</a> as a guideline based on the number of CPUs&#13;
assigned to the Docker container).</p>&#13;
&#13;
<p>Similarly, if a program is run on a&#13;
single-CPU virtual machine, having only one compiler thread might be&#13;
slightly beneficial: limited CPU is available, and having fewer&#13;
threads contending for that resource will help performance in many&#13;
circumstances. However, that advantage is limited only to the initial&#13;
warm-up period; after that, the number of eligible methods to be compiled&#13;
won’t really cause contention for the CPU. When the stock batching application&#13;
was run on a single-CPU machine and the number of compiler threads was limited&#13;
to one,&#13;
the initial calculations were about 10% faster (since they didn’t have to&#13;
compete for CPU as often). The more&#13;
iterations that were run, the smaller the overall effect of that initial&#13;
benefit, until all hot methods were compiled and the benefit was eliminated.</p>&#13;
&#13;
<p>On the other hand, the number of threads can easily&#13;
overwhelm the system, particularly if multiple JVMs are run at once (each&#13;
of which will start many compilation threads). Reducing the number of&#13;
threads in that case can help overall throughput (though again with the&#13;
possible cost that the warm-up period will last longer).</p>&#13;
&#13;
<p>Similarly, if lots of extra CPU cycles are available, then theoretically the&#13;
program will benefit—at least during its warm-up period—when the number of compiler threads is increased.&#13;
In real life, that benefit is extremely&#13;
hard to come by. Further, if all that excess CPU is available, you’re&#13;
much better&#13;
off trying something that takes advantage of the available CPU cycles&#13;
during the entire execution of&#13;
the application (rather than just compiling faster at the beginning).</p>&#13;
&#13;
<p><a data-primary="-XX:+BackgroundCompilation" data-type="indexterm" id="idm45775555684072"/>One other setting that applies to the compilation threads is the value of&#13;
the&#13;
<code class="keep-together">-XX:+BackgroundCompilation</code>&#13;
flag, which by default is <code>true</code>.&#13;
That setting means that the queue is processed&#13;
asynchronously as just described. But that flag can be set to <code>false</code>,&#13;
in which case when a method is eligible for compilation, code that wants&#13;
to execute it will wait until it is in fact compiled (rather than continuing&#13;
to execute in the interpreter). Background compilation is also disabled when&#13;
<code>-Xbatch</code> is specified.</p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Compilation occurs asynchronously for methods that are placed on the compilation queue.</p>&#13;
</li>&#13;
<li>&#13;
<p>The queue is not strictly ordered; hot methods are compiled before other methods in the queue. This is another reason compilation IDs can appear out of order in the compilation log.<a data-startref="ix_ch04-asciidoc19" data-type="indexterm" id="idm45775555677672"/><a data-startref="ix_ch04-asciidoc18" data-type="indexterm" id="idm45775555676968"/><a data-startref="ix_ch04-asciidoc17" data-type="indexterm" id="idm45775555676296"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Inlining" data-type="sect2"><div class="sect2" id="idm45775555722936">&#13;
<h2>Inlining</h2>&#13;
&#13;
<p><a data-primary="advanced compiler flags" data-secondary="inlining" data-type="indexterm" id="idm45775555673800"/><a data-primary="inlining" data-type="indexterm" id="idm45775555672824"/><a data-primary="just-in-time (JIT) compiler" data-secondary="inlining" data-type="indexterm" id="idm45775555672152"/>One of the most important optimizations the compiler makes is to&#13;
inline methods. Code that follows good object-oriented design&#13;
often contains attributes that are accessed via getters&#13;
(and perhaps setters):</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code> <code class="kd">class</code> <code class="nc">Point</code> <code class="o">{</code>&#13;
    <code class="kd">private</code> <code class="kt">int</code> <code class="n">x</code><code class="o">,</code> <code class="n">y</code><code class="o">;</code>&#13;
&#13;
    <code class="kd">public</code> <code class="kt">void</code> <code class="nf">getX</code><code class="o">()</code> <code class="o">{</code> <code class="k">return</code> <code class="n">x</code><code class="o">;</code> <code class="o">}</code>&#13;
    <code class="kd">public</code> <code class="kt">void</code> <code class="nf">setX</code><code class="o">(</code><code class="kt">int</code> <code class="n">i</code><code class="o">)</code>  <code class="o">{</code> <code class="n">x</code> <code class="o">=</code> <code class="n">i</code><code class="o">;</code> <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>The overhead for invoking a method call like this is quite high, especially&#13;
relative to the amount of code in the method. In fact, in the early days&#13;
of Java, performance tips often argued against this sort of&#13;
encapsulation precisely because of the performance impact of all those&#13;
method calls.  Fortunately, JVMs now routinely perform code inlining for&#13;
these kinds of methods. Hence, you can write this code:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="n">Point</code> <code class="n">p</code> <code class="o">=</code> <code class="n">getPoint</code><code class="o">();</code>&#13;
<code class="n">p</code><code class="o">.</code><code class="na">setX</code><code class="o">(</code><code class="n">p</code><code class="o">.</code><code class="na">getX</code><code class="o">()</code> <code class="o">*</code> <code class="mi">2</code><code class="o">);</code></pre>&#13;
&#13;
<p>The compiled code will essentially execute this:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="n">Point</code> <code class="n">p</code> <code class="o">=</code> <code class="n">getPoint</code><code class="o">();</code>&#13;
<code class="n">p</code><code class="o">.</code><code class="na">x</code> <code class="o">=</code> <code class="n">p</code><code class="o">.</code><code class="na">x</code> <code class="o">*</code> <code class="mi">2</code><code class="o">;</code></pre>&#13;
&#13;
<p>Inlining is enabled by default. <a data-primary="-XX:-Inline" data-type="indexterm" id="idm45775555621768"/>It can be disabled using the&#13;
<code class="keep-together">-XX:-Inline</code>&#13;
flag, though it is such an important performance boost that you would never&#13;
actually do that (for example, disabling inlining reduces the performance of&#13;
the stock batching test by over 50%). Still, because inlining is so important,&#13;
and perhaps because we have many other knobs to turn, recommendations&#13;
are often&#13;
made regarding tuning the inlining behavior of the JVM.</p>&#13;
&#13;
<p>Unfortunately, there is no basic visibility into how the JVM inlines&#13;
code. <a data-primary="-XX:+PrintInlining" data-type="indexterm" id="idm45775555619608"/>If you compile the JVM from source, you can produce&#13;
a debug version that includes the flag&#13;
<span class="keep-together"><code>-XX:+PrintInlining</code></span>.&#13;
That flag provides all sorts of information about the inlining&#13;
decisions that the compiler makes.) The best that can be done is to&#13;
look at profiles of the code, and if any simple methods near the top&#13;
of the profiles seem like they should be inlined, experiment&#13;
with inlining flags.</p>&#13;
&#13;
<p>The basic decision about whether to inline a method depends on how hot&#13;
it is and its size. The JVM determines if a method is hot (i.e., called&#13;
frequently)&#13;
based on an internal calculation; it is not directly subject to&#13;
any tunable parameters. If a method is eligible for inlining because it&#13;
is called frequently, it will be inlined only if its bytecode size is&#13;
<a data-primary="-XX:MaxFreqInlineSize=N" data-type="indexterm" id="idm45775555540456"/>less than 325 bytes (or whatever is specified as the&#13;
<span class="keep-together"><code>-XX:MaxFreqInlineSize=</code><em><code>N</code></em></span>&#13;
flag). Otherwise, it is eligible for inlining only&#13;
if it is smaller than 35 bytes<a data-primary="-XX:MaxInlineSize=N" data-type="indexterm" id="idm45775555538408"/> (or whatever is specified as the&#13;
<span class="keep-together"><code>-XX:MaxInlineSize=</code><em><code>N</code></em></span>&#13;
flag).</p>&#13;
&#13;
<p>Sometimes you will see recommendations that the value of the&#13;
<span class="keep-together"><code>MaxInlineSize</code></span>&#13;
flag be increased so that more methods are inlined.&#13;
One often overlooked aspect of this relationship is that setting the&#13;
<code>MaxInlineSize</code> value higher than 35 means that a method might be inlined when&#13;
it is first called. However, if the method is called frequently—in&#13;
which case its performance matters much more—then it would have been&#13;
inlined eventually (assuming its size is less than 325 bytes).&#13;
Otherwise, the net effect of&#13;
tuning the&#13;
<span class="keep-together"><code>MaxInlineSize</code></span>&#13;
flag is that it&#13;
might reduce the warm-up time needed for a test, but it is unlikely that&#13;
it will have a big impact on a long-running application.</p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Inlining is the most beneficial optimization the compiler can make, particularly for object-oriented code where attributes are well encapsulated.</p>&#13;
</li>&#13;
<li>&#13;
<p>Tuning the inlining flags is rarely needed, and recommendations to do so often fail to account for the relationship between normal inlining and frequent inlining. Make sure to account for both cases when investigating the effects of inlining.</p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Escape Analysis" data-type="sect2"><div class="sect2" id="EscapeAnalysis">&#13;
<h2>Escape Analysis</h2>&#13;
&#13;
<p><a data-primary="-XX:+DoEscapeAnalysis" data-type="indexterm" id="idm45775555527880"/><a data-primary="advanced compiler flags" data-secondary="escape analysis" data-type="indexterm" id="idm45775555527176"/><a data-primary="escape analysis" data-type="indexterm" id="idm45775555526232"/><a data-primary="just-in-time (JIT) compiler" data-secondary="escape analysis" data-type="indexterm" id="idm45775555525560"/><a data-primary="optimization" data-secondary="escape analysis" data-type="indexterm" id="idm45775555524648"/>The C2 compiler performs aggressive optimizations if&#13;
escape analysis is enabled&#13;
<span class="keep-together">(<code>-XX:+DoEscapeAnalysis</code></span>,&#13;
which is <code>true</code> by default).&#13;
For example,&#13;
consider this class to work with factorials:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code> <code class="kd">class</code> <code class="nc">Factorial</code> <code class="o">{</code>&#13;
    <code class="kd">private</code> <code class="n">BigInteger</code> <code class="n">factorial</code><code class="o">;</code>&#13;
    <code class="kd">private</code> <code class="kt">int</code> <code class="n">n</code><code class="o">;</code>&#13;
    <code class="kd">public</code> <code class="nf">Factorial</code><code class="o">(</code><code class="kt">int</code> <code class="n">n</code><code class="o">)</code> <code class="o">{</code>&#13;
        <code class="k">this</code><code class="o">.</code><code class="na">n</code> <code class="o">=</code> <code class="n">n</code><code class="o">;</code>&#13;
    <code class="o">}</code>&#13;
    <code class="kd">public</code> <code class="kd">synchronized</code> <code class="n">BigInteger</code> <code class="nf">getFactorial</code><code class="o">()</code> <code class="o">{</code>&#13;
        <code class="k">if</code> <code class="o">(</code><code class="n">factorial</code> <code class="o">==</code> <code class="kc">null</code><code class="o">)</code>&#13;
            <code class="n">factorial</code> <code class="o">=</code> <code class="o">...;</code>&#13;
        <code class="k">return</code> <code class="n">factorial</code><code class="o">;</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>To store the first 100 factorial values in an array, this&#13;
code would be used:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="n">ArrayList</code><code class="o">&lt;</code><code class="n">BigInteger</code><code class="o">&gt;</code> <code class="n">list</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ArrayList</code><code class="o">&lt;</code><code class="n">BigInteger</code><code class="o">&gt;();</code>&#13;
<code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="mi">100</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
    <code class="n">Factorial</code> <code class="n">factorial</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Factorial</code><code class="o">(</code><code class="n">i</code><code class="o">);</code>&#13;
    <code class="n">list</code><code class="o">.</code><code class="na">add</code><code class="o">(</code><code class="n">factorial</code><code class="o">.</code><code class="na">getFactorial</code><code class="o">());</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>The&#13;
<code class="keep-together">factorial</code>&#13;
object is referenced only inside that loop; no other&#13;
code can ever access that object. Hence, the JVM is free to perform optimizations on that object:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>It needn’t get a synchronization lock when calling the&#13;
<code class="keep-together">getFactorial()</code>&#13;
method.</p>&#13;
</li>&#13;
<li>&#13;
<p>It needn’t store the field <code>n</code> in memory; it can keep that value in a&#13;
register. Similarly, it can store the&#13;
<code class="keep-together">factorial</code>&#13;
object reference in a&#13;
register.</p>&#13;
</li>&#13;
<li>&#13;
<p>In fact, it needn’t allocate an actual factorial object at all; it can just keep track of the individual fields of the object.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This kind of optimization is sophisticated: it is simple enough in&#13;
this example, but these optimizations are possible even with more-complex&#13;
code. Depending on the code usage, not all optimizations will necessarily&#13;
apply. But escape analysis can determine which of those optimizations are&#13;
possible and make the necessary changes in the compiled code.</p>&#13;
&#13;
<p>Escape analysis is enabled by default. In rare cases, it will get things&#13;
wrong. That is usually unlikely, and in current JVMs, it is&#13;
rare indeed. Still, because there were once some high-profile bugs,&#13;
you’ll sometimes see recommendations for disabling escape analysis.&#13;
Those are likely not appropriate any longer, though as with all aggressive&#13;
compiler optimizations, it’s not out of the question that&#13;
disabling this feature could lead to more stable code.&#13;
If you find this to be the case, simplifying the code in question is&#13;
the best course of action: simpler code will compile better. (It&#13;
is a bug, however, and should be reported.)</p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Escape analysis is the most sophisticated of the optimizations the compiler can perform. This is the kind of optimization that frequently causes microbenchmarks to go awry.</p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="CPU-Specific Code" data-type="sect2"><div class="sect2" id="UseAVX">&#13;
<h2>CPU-Specific Code</h2>&#13;
&#13;
<p><a data-primary="advanced compiler flags" data-secondary="CPU-specific code" data-type="indexterm" id="idm45775555358040"/><a data-primary="just-in-time (JIT) compiler" data-secondary="CPU-specific code" data-type="indexterm" id="idm45775555357064"/>I mentioned earlier that one advantage of the JIT compiler is that&#13;
it could emit code for different processors depending on where it was running. This&#13;
presumes that the JVM is built with the knowledge of the newer processor,&#13;
of course.</p>&#13;
&#13;
<p><a data-primary="Intel chips" data-type="indexterm" id="idm45775555355512"/>That is exactly what the compiler does for Intel chips. In 2011, Intel&#13;
introduced <a data-primary="Advanced Vector Extensions (AVX2)" data-type="indexterm" id="idm45775555354680"/><a data-primary="AVX2 (Advanced Vector Extensions)" data-type="indexterm" id="idm45775555354040"/>Advanced Vector Extensions (AVX2) for the Sandy Bridge (and later)&#13;
chips. JVM support for those instructions soon followed. Then in 2016 Intel&#13;
extended this to include AVX-512 instructions; those are present on Knights&#13;
Landing and subsequent chips. Those instructions are not supported in JDK 8&#13;
but are supported in JDK 11.</p>&#13;
&#13;
<p>Normally, this feature isn’t something you worry about; the JVM will detect&#13;
the CPU that it is running on and select the appropriate instruction set.&#13;
But as with all new features, sometimes things go awry.</p>&#13;
&#13;
<p>Support for AVX-512 instructions was first introduced in JDK 9, though it&#13;
was not enabled by default. In a couple of false starts, it&#13;
was enabled by default and then disabled by default.&#13;
In JDK 11, those instructions were enabled by default. However, beginning in&#13;
JDK 11.0.6, those instructions are again disabled by default.&#13;
Hence, even in&#13;
JDK 11, this is still a work in progress. (This, by the way, is not unique&#13;
to Java; many programs have struggled to get the support of the AVX-512&#13;
instructions exactly right.)</p>&#13;
&#13;
<p>So it is that on some newer Intel hardware, running some programs, you may&#13;
find that an earlier instruction set works much better.&#13;
The kinds of applications that benefit from the new instruction set typically&#13;
involve more scientific calculations than Java programs often do.</p>&#13;
&#13;
<p><a data-primary="-XX:UseAVX=&lt;N&gt;" data-type="indexterm" id="idm45775555350344"/>These instruction sets are selected with the&#13;
<span class="keep-together"><code>-XX:UseAVX=</code><em><code>N</code></em></span>&#13;
argument, where <em><code>N</code></em> is as <span class="keep-together">follows:</span></p>&#13;
<dl>&#13;
<dt>0</dt>&#13;
<dd>&#13;
<p>Use no AVX instructions.</p>&#13;
</dd>&#13;
<dt>1</dt>&#13;
<dd>&#13;
<p>Use Intel AVX level 1 instructions (for Sandy Bridge and later processors).</p>&#13;
</dd>&#13;
<dt>2</dt>&#13;
<dd>&#13;
<p>Use Intel AVX level 2 instructions (for Haswell and later processors).</p>&#13;
</dd>&#13;
<dt>3</dt>&#13;
<dd>&#13;
<p>Use Intel AVX-512 instructions (for Knights Landing and later processors).</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The default value for this flag will depend on the processor running the JVM;&#13;
the JVM will detect the CPU and pick the highest supported value it can.&#13;
Java 8 has no support for a level of 3, so 2 is the value you’ll&#13;
see used on most processors. In Java 11 on newer Intel processors,&#13;
the default is to use 3 in versions up to 11.0.5, and&#13;
2 in later versions.</p>&#13;
&#13;
<p>This is one of the reasons I mentioned in <a data-type="xref" href="ch01.html#Introduction">Chapter 1</a> that it is a good&#13;
idea to use the latest versions of Java 8 or Java 11, since important fixes&#13;
like this are in those latest versions. If you must use an earlier version of&#13;
Java 11 on the latest Intel processors, try setting the&#13;
<span class="keep-together"><code>-XX:UseAVX=2</code></span>&#13;
flag, which in many cases will give you a performance boost.</p>&#13;
&#13;
<p>Speaking of code maturity: for completeness, I’ll mention that the <span class="keep-together"><code>-XX:UseSSE=<em>N</em></code></span>&#13;
flag supports Intel Streaming SIMD Extensions (SSE) one to four. These extensions are for the Pentium line of processors. Tuning this flag in&#13;
2010 made some sense as all the permutations of its use were being worked out.&#13;
Today, we can generally rely on the robustness of that flag.<a data-startref="ix_ch04-asciidoc13" data-type="indexterm" id="idm45775555336344"/><a data-startref="ix_ch04-asciidoc12" data-type="indexterm" id="idm45775555335704"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tiered Compilation Trade-offs" data-type="sect1"><div class="sect1" id="NoTieredCompilation">&#13;
<h1>Tiered Compilation Trade-offs</h1>&#13;
&#13;
<p><a data-primary="just-in-time (JIT) compiler" data-secondary="tiered compilation trade-offs" data-type="indexterm" id="ix_ch04-asciidoc22"/><a data-primary="tiered compilation" data-secondary="trade-offs" data-type="indexterm" id="ix_ch04-asciidoc23"/>I’ve mentioned a few times that the JVM works differently when tiered&#13;
compilation is disabled. Given the performance advantages it provides,&#13;
is there ever a reason to turn it off?</p>&#13;
&#13;
<p>One such reason might be when running in a memory-constrained environment. <a data-primary="Docker container" data-type="indexterm" id="idm45775555329976"/>Sure,&#13;
your 64-bit machine probably has a ton of memory, but you may be running in&#13;
a Docker container with a small memory limit or in a cloud virtual machine&#13;
that just doesn’t have quite enough memory. Or you may be running dozens of&#13;
JVMs on your large machine. In those cases, you may want to reduce the&#13;
memory footprint of your application.</p>&#13;
&#13;
<p><a data-type="xref" href="ch08.html#NativeMemory">Chapter 8</a> provides general recommendations about this, but&#13;
in this section we’ll look at the effect of tiered compilation on the code&#13;
cache.</p>&#13;
&#13;
<p><a data-type="xref" href="#TableCodeCache">Table 4-3</a> shows the result of starting NetBeans on my system, which has&#13;
a couple dozen projects that will be opened at startup.</p>&#13;
<table id="TableCodeCache">&#13;
<caption><span class="label">Table 4-3. </span>Effect of tiered compilation on the code cache</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Compiler mode</th>&#13;
<th>Classes compiled</th>&#13;
<th>Committed code cache</th>&#13;
<th>Startup time</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>+TieredCompilation</p></td>&#13;
<td><p>22,733</p></td>&#13;
<td><p>46.5 MB</p></td>&#13;
<td><p>50.1 seconds</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>-TieredCompilation</p></td>&#13;
<td><p>5,609</p></td>&#13;
<td><p>10.7 MB</p></td>&#13;
<td><p>68.5 seconds</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>The C1 compiler compiled about four times as many classes and predictably&#13;
required about four times as much memory for the code cache. In absolute&#13;
terms, saving 34 MB in this example is unlikely to make a huge difference.&#13;
Saving 300 MB in a program that compiles 200,000 classes might be a different&#13;
choice on some platforms.</p>&#13;
&#13;
<p>What do we lose by disabling tiered compilation? As the table shows, we do&#13;
spend more time to start the application and load all project classes.&#13;
But what about a long-running program, where you’d expect all the hot spots&#13;
to get compiled?</p>&#13;
&#13;
<p>In that case, given a sufficiently long warm-up period, execution should be&#13;
about the same when tiered compilation is disabled.&#13;
<a data-type="xref" href="#TableLongServer">Table 4-4</a> shows the performance of our stock REST server&#13;
after warm-up periods of 0,&#13;
60, and 300 seconds.</p>&#13;
<table id="TableLongServer">&#13;
<caption><span class="label">Table 4-4. </span>Throughput of server applications with tiered compilation</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Warm-up period</th>&#13;
<th><code>-XX:-TieredCompilation</code></th>&#13;
<th><code>-XX:+TieredCompilation</code></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>0 seconds</p></td>&#13;
<td><p>23.72</p></td>&#13;
<td><p>24.23</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>60 seconds</p></td>&#13;
<td><p>23.73</p></td>&#13;
<td><p>24.26</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>300 seconds</p></td>&#13;
<td><p>24.42</p></td>&#13;
<td><p>24.43</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>The measurement period is 60 seconds, so even when there is&#13;
no warm-up, the compilers had an opportunity to get enough information to&#13;
compile the hot spots; hence,&#13;
there is little difference even when there is no warm-up period.&#13;
(Also, a lot of code was compiled&#13;
during the startup of the server.)&#13;
Note that in the end, tiered compilation is still able to eke out a&#13;
small advantage (albeit one that is unlikely to be noticeable). We discussed&#13;
the reason for that when discussing compilation thresholds: there will always&#13;
be a small number of methods that are compiled by the C1 compiler when&#13;
tiered compilation is used that won’t be compiled by the C2 compiler.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775555278360">&#13;
<h5>The javac Compiler</h5>&#13;
<p><a data-primary="javac compiler" data-type="indexterm" id="idm45775555276920"/><a data-primary="just-in-time (JIT) compiler" data-secondary="javac compiler" data-type="indexterm" id="idm45775555276216"/>In performance terms, compilation is really about the JIT built into the&#13;
JVM. Recall, though, that the Java code first is compiled into bytecodes;&#13;
that occurs via the <code>javac</code> process. So we’ll end this section by&#13;
mentioning a few points about it.</p>&#13;
&#13;
<p>Most important is that the <code>javac</code> compiler—with one exception—doesn’t&#13;
really affect performance at all. In particular:</p>&#13;
<ul>&#13;
<li> The <code>-g</code> option to include additional debugging information doesn’t affect <span class="keep-together">performance.</span></li>&#13;
<li> Using the <code>final</code> keyword in your Java program doesn’t produce faster compiled code.</li>&#13;
<li> Recompiling with newer <code>javac</code> versions doesn’t (usually) make programs any faster.</li>&#13;
</ul>&#13;
&#13;
<p>These three points have been general recommendations for years, and then along&#13;
came JDK 11. JDK 11 introduces a new way of doing string concatenation that&#13;
can be faster than previous versions, but it requires that code be&#13;
recompiled in order to&#13;
take advantage of it. That is the exception to the rule here; in general,&#13;
you never need to recompile to bytecodes in order to take advantage of new&#13;
features. More details about this are given in <a data-type="xref" href="ch12.html#StringHandling">“Strings”</a>.<a data-startref="ix_ch04-asciidoc23" data-type="indexterm" id="idm45775555268296"/><a data-startref="ix_ch04-asciidoc22" data-type="indexterm" id="idm45775555267592"/><a data-startref="ix_ch04-asciidoc20" data-type="indexterm" id="idm45775555266920"/></p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The GraalVM" data-type="sect1"><div class="sect1" id="idm45775555334440">&#13;
<h1>The GraalVM</h1>&#13;
&#13;
<p><a data-primary="GraalVM" data-type="indexterm" id="idm45775555264808"/><a data-primary="Java Virtual Machine (JVM)" data-secondary="GraalVM and" data-type="indexterm" id="idm45775555264104"/><a data-primary="just-in-time (JIT) compiler" data-secondary="GraalVM" data-type="indexterm" id="idm45775555263144"/>The <em>GraalVM</em> is a new virtual machine. It provides a means to run Java code, of course, but also code from&#13;
many other languages. This universal virtual machine can also run&#13;
JavaScript, Python, Ruby, R, and traditional JVM bytecodes from Java and&#13;
other languages that compile to JVM bytecodes (e.g., Scala, Kotlin, etc.).&#13;
Graal comes in two editions: a full open source Community Edition (CE) and a commercial Enterprise Edition (EE). Each edition has binaries that support either Java 8 or Java 11.</p>&#13;
&#13;
<p>The GraalVM has two important contributions to JVM performance. First,&#13;
an add-on technology allows the GraalVM to produce fully&#13;
native binaries; we’ll examine that in the next section.</p>&#13;
&#13;
<p>Second, the GraalVM can run in a mode as a regular JVM, but it contains&#13;
a new implementation of the C2 compiler. This compiler is written in Java&#13;
(as opposed to the traditional C2 compiler, which is written in C++).</p>&#13;
&#13;
<p>The traditional JVM contains a version of the GraalVM JIT, depending on when the JVM was built. These JIT releases come from the CE version of GraalVM, which are slower than the EE version; they are also typically out-of-date compared to versions of GraalVM that you can download directly.</p>&#13;
&#13;
<p>Within the JVM, using the GraalVM compiler is considered experimental,&#13;
<a data-primary="-XX:+EnableJVMCI" data-type="indexterm" id="idm45775555258520"/><a data-primary="-XX:+UnlockExperimentalVMOptions" data-type="indexterm" id="idm45775555257816"/><a data-primary="-XX:+UseJVMCICompiler" data-type="indexterm" id="idm45775555257176"/>so to enable it, you need to supply these flags:&#13;
<span class="keep-together"><code>-XX:+UnlockExperimentalVMOptions</code></span>,&#13;
<span class="keep-together"><code>-XX:+EnableJVMCI</code></span>,&#13;
and&#13;
<span class="keep-together"><code>-XX:+UseJVMCICompiler</code></span>.&#13;
The default for all those flags is <code>false</code>.</p>&#13;
&#13;
<p><a data-type="xref" href="#TableGraal">Table 4-5</a> shows the performance of the standard Java 11 compiler,&#13;
the Graal compiler from EE version 19.2.1, and the GraalVM embedded in Java&#13;
11 and 13.</p>&#13;
<table id="TableGraal">&#13;
<caption><span class="label">Table 4-5. </span>Performance of Graal compiler</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>JVM/compiler</th>&#13;
<th>OPS</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>JDK 11/Standard C2</p></td>&#13;
<td><p>20.558</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>JDK 11/Graal JIT</p></td>&#13;
<td><p>14.733</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Graal 1.0.0b16</p></td>&#13;
<td><p>16.3</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Graal 19.2.1</p></td>&#13;
<td><p>26.7</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>JDK 13/Standard C2</p></td>&#13;
<td><p>21.9</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>JDK 13/Graal JIT</p></td>&#13;
<td><p>26.4</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>This is once again the performance of our REST server (though on slightly&#13;
different hardware than before, so the baseline OPS is only 20.5 OPS instead&#13;
of 24.4).</p>&#13;
&#13;
<p>It’s interesting to note the&#13;
progression here: JDK 11 was built with a pretty early version of the&#13;
Graal compiler, so the performance of that compiler lags the C2&#13;
compiler. The Graal compiler&#13;
improved through its early access builds,&#13;
though even its latest early access (1.0) build wasn’t as fast as the standard&#13;
VM. Graal versions in late 2019 (released as production version 19.2.1), though,&#13;
got substantially faster. The&#13;
early access release of JDK 13 has one of those later builds and achieves close to&#13;
the same performance with the Graal compiler, even while its C2 compiler is&#13;
only modestly improved since JDK 11.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Precompilation" data-type="sect1"><div class="sect1" id="idm45775555265656">&#13;
<h1>Precompilation</h1>&#13;
&#13;
<p><a data-primary="just-in-time (JIT) compiler" data-secondary="precompilation" data-type="indexterm" id="ix_ch04-asciidoc24"/><a data-primary="precompilation" data-type="indexterm" id="ix_ch04-asciidoc25"/>We began this chapter by discussing the philosophy behind a just-in-time&#13;
compiler. Although it has its advantages, code is still subject to a warm-up&#13;
period before it executes. What if in our environment a traditional&#13;
compiled model would work better: an embedded system without the extra&#13;
memory the JIT requires, or a program that completes before having a chance&#13;
to warm up?</p>&#13;
&#13;
<p>In this section, we’ll look at two experimental features that address that&#13;
scenario. Ahead-of-time compilation is an experimental feature of the&#13;
standard JDK 11, and the ability to produce a fully native binary is a&#13;
feature of the Graal VM.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Ahead-of-Time Compilation" data-type="sect2"><div class="sect2" id="AOTC">&#13;
<h2>Ahead-of-Time Compilation</h2>&#13;
&#13;
<p><a data-primary="ahead-of-time (AOT) compilation" data-type="indexterm" id="ix_ch04-asciidoc26"/><a data-primary="AOT (ahead-of-time) compilation" data-type="indexterm" id="ix_ch04-asciidoc27"/><a data-primary="precompilation" data-secondary="ahead-of-time compilation" data-type="indexterm" id="ix_ch04-asciidoc28"/><em>Ahead-of-time (AOT) compilation</em> was first available in JDK 9 for Linux only, but in JDK 11&#13;
it is available on all platforms. From a performance standpoint, it is still&#13;
a work in progress, but this section will give you a sneak peek at&#13;
it.<sup><a data-type="noteref" href="ch04.html#idm45775555226680" id="idm45775555226680-marker">1</a></sup></p>&#13;
&#13;
<p>AOT compilation allows you to compile some (or all) of your application&#13;
in advance of running it. This compiled code becomes a shared library that&#13;
the JVM uses when starting the application. In theory, this means the JIT&#13;
needn’t be involved,&#13;
at least in the startup of your application: your code should initially run&#13;
at least as well as the C1 compiled code without having to wait for that code&#13;
to be compiled.</p>&#13;
&#13;
<p>In practice, it’s a little different: the startup time of the application is&#13;
greatly affected by the size of the shared library (and hence the time to load&#13;
that shared library into the JVM). That means a simple application like a&#13;
“Hello, world” application won’t run any faster when you use AOT compilation&#13;
(in fact, it may run slower depending on the choices made to precompile the&#13;
shared library). AOT compilation is targeted toward&#13;
something like a REST server that has a relatively long startup time.&#13;
That way, the time to load the shared library is offset by the long startup&#13;
time, and AOT produces a benefit. But remember as well that AOT compilation&#13;
is an experimental feature, and smaller programs may see benefits from it as&#13;
the technology evolves.</p>&#13;
&#13;
<p><a data-primary="jaotc" data-type="indexterm" id="idm45775555222808"/>To use AOT compilation, you use the <code>jaotc</code> tool to produce a shared library&#13;
containing the compiled classes that you select. Then that shared library&#13;
is loaded into the JVM via a runtime argument.</p>&#13;
&#13;
<p>The <code>jaotc</code> tool has several options, but the way that you’ll&#13;
produce the best library is something like this:</p>&#13;
&#13;
<pre data-type="programlisting">$ jaotc --compile-commands=/tmp/methods.txt \&#13;
    --output JavaBaseFilteredMethods.so \&#13;
    --compile-for-tiered \&#13;
    --module java.base</pre>&#13;
&#13;
<p>This command will use a set of compile commands to produce a&#13;
compiled version of the <em>java.base</em> module in the given output file.&#13;
You have the option of AOT&#13;
compiling a module, as we’ve done here, or a set of classes.</p>&#13;
&#13;
<p>The time to load the shared library depends on its size, which is a factor&#13;
of the number of methods in the library. You can load multiple shared&#13;
libraries that pre-compile different parts of code as well, which may be&#13;
easier to manage but has the same performance, so we’ll concentrate&#13;
on a single library.</p>&#13;
&#13;
<p>While you might be tempted to precompile everything, you’ll obtain better&#13;
performance if you judiciously precompile only subsets of&#13;
the code. That’s why this recommendation is to compile only the <em>java.base</em>&#13;
module.</p>&#13;
&#13;
<p>The compile commands (in the <em>/tmp/methods.txt</em> file in this example) also&#13;
serve to limit the data that is compiled into the shared library. That file&#13;
contains lines that look like this:</p>&#13;
&#13;
<pre data-type="programlisting">compileOnly java.net.URI.getHost()Ljava/lang/String;</pre>&#13;
&#13;
<p>This line tells <code>jaotc</code> that when it compiles the <code>java.net.URI</code> class, it&#13;
should include only the <code>getHost()</code> method. We can have other lines&#13;
referencing other methods from that class to include their compilation as well;&#13;
in the end, only the methods listed in the file will be included in the&#13;
shared library.</p>&#13;
&#13;
<p>To create the list of compile commands, we need a list of every&#13;
method that the application actually uses. To do that, we run the application&#13;
like this:</p>&#13;
&#13;
<pre data-type="programlisting">$ java -XX:+UnlockDiagnosticVMOptions -XX:+LogTouchedMethods \&#13;
      -XX:+PrintTouchedMethodsAtExit &lt;other arguments&gt;</pre>&#13;
&#13;
<p>When the program exits, it will print lines of each method the program&#13;
used in a format like this:</p>&#13;
&#13;
<pre data-type="programlisting">java/net/URI.getHost:()Ljava/lang/String;</pre>&#13;
&#13;
<p>To produce the <em>methods.txt</em> file, save those lines, prepend each with&#13;
the <code>compileOnly</code> directive, and remove&#13;
the colon immediately preceding the method arguments.</p>&#13;
&#13;
<p>The classes that are precompiled by <code>jaotc</code> will use a form of the C1&#13;
compiler, so in a long-running program, they will not be optimally compiled.&#13;
So the final option that we’ll need is <code>--compile-for-tiered</code>. That&#13;
option arranges the shared library so that its methods are still eligible&#13;
to be compiled by the C2 compiler.</p>&#13;
&#13;
<p>If you are using AOT compilation for a short-lived program, it’s fine to&#13;
leave out this argument, but remember that the target set of applications&#13;
is a server.  If we don’t&#13;
allow the precompiled methods to become eligible for C2 compilation, the&#13;
warm performance of the server will be slower than what is ultimately&#13;
possible.</p>&#13;
&#13;
<p><a data-primary="-XX:+PrintCompilation" data-type="indexterm" id="idm45775555206328"/>Perhaps unsurprisingly, if you run your application with a library that&#13;
has tiered compilation enabled and use the&#13;
<span class="keep-together"><code>-XX:+PrintCompilation</code></span>&#13;
flag, you see the same code replacement technique we observed before:&#13;
the AOT compilation will appear as another tier in the output, and&#13;
you’ll see the AOT methods get made not entrant and replaced as the JIT&#13;
compiles them.</p>&#13;
&#13;
<p>Once the library has been created, you use it with your application&#13;
like this:</p>&#13;
&#13;
<pre data-type="programlisting">$ java -XX:AOTLibrary=/path/to/JavaBaseFilteredMethods.so &lt;other args&gt;</pre>&#13;
&#13;
<p><a data-primary="-XX:+PrintAOT" data-type="indexterm" id="idm45775555202952"/>If you want to make sure that the library is being used, include the&#13;
<code class="keep-together">-XX:+PrintAOT</code>&#13;
flag in your JVM arguments; that flag is <code>false</code> by default. Like the&#13;
<code class="keep-together">-XX:+PrintCompilation</code>&#13;
flag, the&#13;
<code class="keep-together">-XX:+PrintAOT</code>&#13;
flag will produce output whenever a precompiled method is used by the JVM.&#13;
A typical line looks like this:</p>&#13;
&#13;
<pre data-type="programlisting">    373  105     aot[ 1]   java.util.HashSet.&lt;init&gt;(I)V</pre>&#13;
&#13;
<p>The first column here is the milliseconds since the program started, so it&#13;
took 373 milliseconds until the constructor of the <code>HashSet</code> class was loaded&#13;
from the shared library and began&#13;
execution. The second column is an ID assigned to the method, and the third&#13;
column tells us which library the method was loaded from. The index (1 in this&#13;
example) is also printed by this flag:</p>&#13;
&#13;
<pre data-type="programlisting">18    1     loaded    /path/to/JavaBaseFilteredMethods.so  aot library</pre>&#13;
&#13;
<p><em>JavaBaseFilteredMethods.so</em> is the first (and only) library loaded in&#13;
this example, so&#13;
its index is 1 (the second column) and subsequent references to <code>aot</code> with that&#13;
index refer to this library.<a data-startref="ix_ch04-asciidoc28" data-type="indexterm" id="idm45775555195256"/><a data-startref="ix_ch04-asciidoc27" data-type="indexterm" id="idm45775555194552"/><a data-startref="ix_ch04-asciidoc26" data-type="indexterm" id="idm45775555193880"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="GraalVM Native Compilation" data-type="sect2"><div class="sect2" id="idm45775555231672">&#13;
<h2>GraalVM Native Compilation</h2>&#13;
&#13;
<p><a data-primary="GraalVM" data-type="indexterm" id="ix_ch04-asciidoc29"/><a data-primary="precompilation" data-secondary="GraalVM native compilation" data-type="indexterm" id="ix_ch04-asciidoc30"/>AOT compilation was beneficial for relatively large programs but didn’t&#13;
help (and could hinder) small, quick-running programs. That is because&#13;
it’s still an experimental feature and because its&#13;
architecture has the JVM load the shared library.</p>&#13;
&#13;
<p>The GraalVM, on the other hand, can produce full native executables that&#13;
run without the JVM. These executables are ideal for short-lived programs.&#13;
If you ran the examples, you may have noticed references in some things&#13;
(like ignored errors) to GraalVM classes: AOT compilation uses GraalVM as its&#13;
foundation. This is an Early Adopter feature of the GraalVM;&#13;
it can be used in production with the appropriate license but is not subject&#13;
to warranty.</p>&#13;
&#13;
<p>The GraalVM produces binaries that start up quite fast, particularly when&#13;
comparing them to the running programs in the JVM. However, in this mode&#13;
the GraalVM does&#13;
not optimize code as aggressively as the C2 compiler, so given a sufficiently&#13;
long-running application, the traditional JVM will win out in the end. Unlike&#13;
AOT compilation, the GraalVM native binary does not compile classes using C2&#13;
during execution.</p>&#13;
&#13;
<p>Similarly, the memory footprint of a native program produced from the GraalVM&#13;
starts out significantly smaller than a traditional JVM. However, by the&#13;
time a program runs and expands the heap, this memory advantage fades.</p>&#13;
&#13;
<p>Limitations also exist on which Java features can be used in a program&#13;
compiled into native code. These limitations include the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Dynamic class loading (e.g., by calling <code>Class.forName()</code>).</p>&#13;
</li>&#13;
<li>&#13;
<p>Finalizers.</p>&#13;
</li>&#13;
<li>&#13;
<p>The Java Security Manager.</p>&#13;
</li>&#13;
<li>&#13;
<p>JMX and JVMTI (including JVMTI profiling).</p>&#13;
</li>&#13;
<li>&#13;
<p>Use of reflection often requires special coding or configuration.</p>&#13;
</li>&#13;
<li>&#13;
<p>Use of dynamic proxies often requires special configuration.</p>&#13;
</li>&#13;
<li>&#13;
<p>Use of JNI requires special coding or configuration.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>We can see all of this in action by using a demo program from the GraalVM project&#13;
that recursively counts the files in a directory. With a few files to count,&#13;
the native program produced by the GraalVM is quite small and fast, but as&#13;
more work&#13;
is done and the JIT kicks in, the traditional JVM compiler generates better&#13;
code optimizations and is faster, as we see in <a data-type="xref" href="#TableAOTList">Table 4-6</a>.</p>&#13;
<table id="TableAOTList">&#13;
<caption><span class="label">Table 4-6. </span>Time to count files with native and JIT-compiled code</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Number of files</th>&#13;
<th>Java 11.0.5</th>&#13;
<th>Native application</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>7</p></td>&#13;
<td><p>217 ms (36K)</p></td>&#13;
<td><p>4 ms (3K)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>271</p></td>&#13;
<td><p>279 ms (37K)</p></td>&#13;
<td><p>20 ms (6K)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>169,000</p></td>&#13;
<td><p>2.3 s (171K)</p></td>&#13;
<td><p>2.1 s (249K)</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>1.3 million</p></td>&#13;
<td><p>19.2 s (212K)</p></td>&#13;
<td><p>25.4 s (269K)</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>The times here are the time to count the files; the total footprint of the&#13;
run (measured at completion) is given in parentheses.</p>&#13;
&#13;
<p>Of course, the GraalVM itself is rapidly evolving, and the optimizations&#13;
within its native code can be expected to improve over time as well<a data-startref="ix_ch04-asciidoc30" data-type="indexterm" id="idm45775555163544"/><a data-startref="ix_ch04-asciidoc29" data-type="indexterm" id="idm45775555162840"/>.<a data-startref="ix_ch04-asciidoc25" data-type="indexterm" id="idm45775555162040"/><a data-startref="ix_ch04-asciidoc24" data-type="indexterm" id="idm45775555161336"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45775555192952">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>This chapter contains a lot of background about how the compiler&#13;
works. This is so you can understand some of the general&#13;
recommendations made in <a data-type="xref" href="ch01.html#Introduction">Chapter 1</a> regarding small methods and simple&#13;
code, and the effects of the compiler on microbenchmarks that were&#13;
described in <a data-type="xref" href="ch02.html#SampleApplications">Chapter 2</a>. In particular:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Don’t be afraid of small methods—and, in particular, getters and setters—because they are easily inlined. If you have a feeling that the method&#13;
overhead can be expensive, you’re correct in theory (we showed that removing&#13;
inlining significantly degrades performance). But it’s not the case in practice, since the compiler fixes that problem.</p>&#13;
</li>&#13;
<li>&#13;
<p>Code that needs to be compiled sits in a compilation queue. The more code&#13;
in the queue, the longer the program will take to achieve optimal performance.</p>&#13;
</li>&#13;
<li>&#13;
<p>Although you can (and should) size the code cache, it is still a finite&#13;
resource.</p>&#13;
</li>&#13;
<li>&#13;
<p>The simpler the code, the more optimizations that can be performed on it.&#13;
Profile feedback and escape analysis can yield much faster code, but complex&#13;
loop structures and large methods limit their effectiveness.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Finally, if you profile your code and find some surprising methods at the&#13;
top of your profile—methods you expect shouldn’t be there—you can use&#13;
the information here to look into what the compiler is doing and to make&#13;
sure it can handle the way your code is written.<a data-startref="ix_ch04-asciidoc0" data-type="indexterm" id="idm45775555152072"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45775555226680"><sup><a href="ch04.html#idm45775555226680-marker">1</a></sup> One benefit of AOC compilation is faster startup, but application class data sharing gives—at least for now—a better benefit in terms of startup performance and is a fully supported feature; see <a data-type="xref" href="ch12.html#ClassDataSharing">“Class Data Sharing”</a> for more details.</p></div></div></section></body></html>