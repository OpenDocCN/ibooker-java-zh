<html><head></head><body><section data-pdf-bookmark="Chapter 13. Observing Reactive and &#10;Event-Driven Architectures" data-type="chapter" epub:type="chapter"><div class="chapter" id="observability">&#13;
<h1><span class="label">Chapter 13. </span>Observing Reactive and &#13;
<span class="keep-together">Event-Driven Architectures</span></h1>&#13;
&#13;
&#13;
<p><a data-primary="observability" data-type="indexterm" id="ix_observability-adoc0"/>So far, we’ve focused on how to develop reactive systems.&#13;
What we haven’t discussed is how to ensure that all the components of our reactive system are functioning as we expect them to.&#13;
This is the focus of the chapter:&#13;
how we monitor and observe our reactive and event-driven architecture.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Why Is Observability Important?" data-type="sect1"><div class="sect1" id="idm45358813624112">&#13;
<h1>Why Is Observability Important?</h1>&#13;
&#13;
<p><a data-primary="observability" data-secondary="importance of" data-type="indexterm" id="idm45358813622752"/>When an application is a single deployment, or <em>monolith</em>,&#13;
we have a relatively easy time observing how the application is performing.&#13;
Everything we need to observe is in one place.&#13;
Whether it’s checking logs for errors,&#13;
monitoring the utilization of CPU and memory,&#13;
or any other aspect,&#13;
it’s all accessible.</p>&#13;
&#13;
<p>With a reactive and event-driven architecture,&#13;
instead of one deployment, it’s often several, dozens, or even hundreds.&#13;
We’re no longer dealing with a single place to view the information we need to monitor and observe,&#13;
but many places!&#13;
Observability tooling provides a means for us to gather this information and provide a single place to view it again.</p>&#13;
&#13;
<p><a data-primary="telemetry" data-type="indexterm" id="idm45358813619856"/>However, we need to gather the necessary information, or telemetry,&#13;
from the components in the event-driven architecture to enable a singular view.&#13;
<em>Telemetry</em> consists of any information we gather from processes for the purpose of observing a system.&#13;
The most common types of telemetry are as follows:</p>&#13;
<dl>&#13;
<dt>Logs</dt>&#13;
<dd>&#13;
<p>Textual messages often written to console output, logfiles, or exported to specific log-processing systems.&#13;
We can also provide more structured logging in a JSON format to facilitate more accurate data extraction.</p>&#13;
</dd>&#13;
<dt>Metrics</dt>&#13;
<dd>&#13;
<p>A single metric measures a specific piece of information, such as HTTP server requests.&#13;
Various types of metrics are available: counter, gauge, timer, and histogram, to name a few.</p>&#13;
</dd>&#13;
<dt>Traces</dt>&#13;
<dd>&#13;
<p>Represents a single request through a system,&#13;
broken into specific operations.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>When running distributed systems utilizing reactive or event-driven architecture,&#13;
we need solid telemetry produced from the components to support sufficient reasoning about the system.&#13;
Without being able to reason about the system based on what we can observe from the outside,&#13;
our reactive system is not truly observable.</p>&#13;
&#13;
<p>Let’s clarify some terms.&#13;
<em>Monitoring</em> and <em>observability</em> can be conflated to mean the same thing.&#13;
Though there are overlaps, they do mean different things.&#13;
Monitoring focuses on specific metrics and measuring them against specific goals, service-level objectives (SLOs),&#13;
and alerting operations when those goals are not met.&#13;
Monitoring is also called <em>known unknowns</em>,&#13;
as we know what data, or metrics, to measure to see a problem,&#13;
but we don’t know what might cause a specific problem.&#13;
<em>Unknown unknowns</em> refers to observability,&#13;
because we don’t know what will cause a problem,&#13;
and when one occurs, it requires observation of a system from its outputs to determine the cause.</p>&#13;
&#13;
<p>Kubernetes is a great place to run reactive systems, as it provides the mechanism to monitor, scale, and repair a system gracefully.&#13;
However, we need to provide information for Kubernetes to do that properly,&#13;
such as with health checks.&#13;
Health checks can serve many purposes;&#13;
for our needs, the readiness and liveness probes in Kubernetes can utilize them.&#13;
<em>Readiness probes</em> let Kubernetes know a container is ready to begin accepting requests,&#13;
and <em>liveness probes</em> let Kubernetes know if a container needs to be restarted because of unrecoverable failures when communicating with Kafka.</p>&#13;
&#13;
<p>Throughout the rest of the chapter, we explain how to effectively monitor and observe reactive systems.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Health with Messaging" data-type="sect1"><div class="sect1" id="idm45358813607504">&#13;
<h1>Health with Messaging</h1>&#13;
&#13;
<p><a data-primary="health checks" data-type="indexterm" id="ix_observability-adoc1"/><a data-primary="Kubernetes" data-secondary="health checks with messaging" data-type="indexterm" id="ix_observability-adoc2"/><a data-primary="messaging" data-secondary="health checks with" data-type="indexterm" id="ix_observability-adoc3"/><a data-primary="observability" data-secondary="health with messaging" data-type="indexterm" id="ix_observability-adoc4"/>Kubernetes utilizes health checks to determine the state of a container.&#13;
If containers don’t provide health checks,&#13;
Kubernetes is unable to determine the state of a container.&#13;
This may result in users experiencing errors caused by deadlocked containers that cannot be stopped or by containers that are not ready to process requests.</p>&#13;
&#13;
<p class="pagebreak-before less_space">We can implement three types of health checks for our containers:</p>&#13;
<dl>&#13;
<dt>Liveness Probe</dt>&#13;
<dd>&#13;
<p><a data-primary="liveness probe" data-type="indexterm" id="idm45358813598304"/>This probe lets Kubernetes know a container should be restarted.&#13;
If we can write a meaningful health check,&#13;
it’s a good way to catch situations of application deadlock or connection issues with external systems.&#13;
We can possibly resolve intermittent issues by allowing a clean slate by restarting the container.&#13;
The probe is periodically run based on the frequency we define.&#13;
We want to ensure that the frequency is not too large,&#13;
so we prevent containers being stuck for long periods of time, but not too small either, as that would increase resource consumption.</p>&#13;
</dd>&#13;
<dt>Readiness Probe</dt>&#13;
<dd>&#13;
<p><a data-primary="readiness probe" data-type="indexterm" id="idm45358813595744"/>This probe informs Kubernetes when a container is ready to begin receiving traffic from a service.&#13;
We can use this type of health check to provide enough time for HTTP servers and connections to external systems to be available before we begin accepting requests.&#13;
This prevents users from experiencing errors because the container was not ready to process a request.&#13;
This probe executes only once during the life of a container.&#13;
The readiness probe is necessary to effectively allow scaling up without causing undue user errors.</p>&#13;
</dd>&#13;
<dt>Startup Probe</dt>&#13;
<dd>&#13;
<p><a data-primary="startup probe" data-type="indexterm" id="idm45358813593216"/>A recent health check addition, this probe has a similar purpose as the liveness probe.&#13;
However, this probe allows us to set a different wait period before declaring the container unhealthy.&#13;
This is especially beneficial in situations where a container could take a very long time to be alive,&#13;
possibly due to connecting with legacy systems.&#13;
We’re able to set a shorter time-out for a <em>Liveness Probe</em>,&#13;
while allowing a much longer time-out for a <em>Startup Probe</em>.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Each of these probes supports HTTP, TCP, or commands run inside the container itself.&#13;
Nothing prevents other protocols from being used for probes,&#13;
but they’re currently not implemented in Kubernetes.&#13;
Which probe we use for an application will depend on whether there are HTTP endpoints we can utilize for the probes,&#13;
or whether we need custom commands within the container.&#13;
Quarkus has an extension for <a href="https://github.com/smallrye/smallrye-health">SmallRye Health</a> to develop health checks available over HTTP.</p>&#13;
&#13;
<p>How do these probes relate to a reactive application?&#13;
Readiness indicates that a Reactive Messaging connector, such as Kafka,&#13;
has successfully connected to the broker, or backend, there were no failures,&#13;
and optionally the topic we intend to use exists in the broker.&#13;
In this state, the connector is ready to begin sending or receiving messages.&#13;
Verifying the presence of any topics is disabled by default because it’s a lengthy operation requiring use of the admin client. Enabling topic verification is done by setting <code>health-readiness-topic-verification: true</code>.</p>&#13;
&#13;
<p>Liveness should fail when the Reactive Messaging connector has experienced an unrecoverable failure or a disconnection from the broker.&#13;
These types of transient failures can disappear after a restart of the container.&#13;
For example, the application may connect to another broker.</p>&#13;
&#13;
<p>As we covered in <a data-type="xref" href="ch11.html#_kafka_101">“Apache Kafka”</a>,&#13;
Kafka has built-in resilience.&#13;
The last committed offset is not updated until a consumer has successfully processed the record,&#13;
ensuring that records are not forgotten if a consumer fails while processing it.&#13;
Also, Kafka is able to rebalance consumers, within the same consumer group, if any of them fail.&#13;
Any consumer(s) that might crash while processing records from a partition will be replaced with other consumers from the same group.&#13;
When using Kubernetes health checks,&#13;
the consumers will be rebalanced when containers stop,&#13;
and re-balanced again when Kubernetes has started new instances of the containers.</p>&#13;
&#13;
<p>It is now time to see how it all works with an example.&#13;
We will take the example from <a data-type="xref" href="ch11.html#event-bus">Chapter 11</a> and extend it.&#13;
We want to customize the consumer to highlight the behaviors of health checks.&#13;
We will have a specific process service, <code>processor-health</code>.&#13;
You can find the complete code in the <em>chapter-13</em> directory.</p>&#13;
&#13;
<p>First we need to add the extension for SmallRye Health to the <em>pom.xml</em> of each service, as shown in <a data-type="xref" href="#dependency-health-support">Example 13-1</a>.</p>&#13;
<div data-type="example" id="dependency-health-support">&#13;
<h5><span class="label">Example 13-1. </span>Dependency for the health support (<em>chapter-13/processor-health/pom.xml</em>)</h5>&#13;
&#13;
<pre data-code-language="xml" data-type="programlisting"><code class="nt">&lt;dependency&gt;</code>&#13;
  <code class="nt">&lt;groupId&gt;</code>io.quarkus<code class="nt">&lt;/groupId&gt;</code>&#13;
  <code class="nt">&lt;artifactId&gt;</code>quarkus-smallrye-health<code class="nt">&lt;/artifactId&gt;</code>&#13;
<code class="nt">&lt;/dependency&gt;</code></pre></div>&#13;
&#13;
<p>To generate the necessary Kubernetes deployment YAML,&#13;
including the liveness and readiness probes,&#13;
we need the Kubernetes extension.&#13;
In this case, though,&#13;
we use the minikube extension as we’re deploying to it; see <a data-type="xref" href="#dependency-minikube-deployment-feature">Example 13-2</a>.</p>&#13;
<div data-type="example" id="dependency-minikube-deployment-feature">&#13;
<h5><span class="label">Example 13-2. </span>Dependency for the minikube deployment feature (<em>chapter-13/processor-health/pom.xml</em>)</h5>&#13;
&#13;
<pre data-code-language="xml" data-type="programlisting"><code class="nt">&lt;dependency&gt;</code>&#13;
  <code class="nt">&lt;groupId&gt;</code>io.quarkus<code class="nt">&lt;/groupId&gt;</code>&#13;
  <code class="nt">&lt;artifactId&gt;</code>quarkus-minikube<code class="nt">&lt;/artifactId&gt;</code>&#13;
<code class="nt">&lt;/dependency&gt;</code></pre></div>&#13;
&#13;
<p>Run <code>mvn clean package</code> in the <em>/chapter-13</em> directory to generate the deployment YAML.&#13;
Take a look in the <em>/target/kubernetes</em> directory of one of the modules and view the generated YAML.&#13;
We see the desired liveness and readiness probes added to the deployment specification.</p>&#13;
&#13;
<p>By default, the period between each liveness probe request is 30 seconds.&#13;
Let’s reduce it to 10 seconds to enable Kubernetes to restart our &#13;
<span class="keep-together">consumer</span>, <code>processor-health</code>,&#13;
sooner if there are problems by modifying <code>application.properties</code> (<a data-type="xref" href="#configure-liveness-probe">Example 13-3</a>).</p>&#13;
<div data-type="example" id="configure-liveness-probe">&#13;
<h5><span class="label">Example 13-3. </span>Configure the liveness probe (<em>chapter-13/processor-health/src/main/resources/application.properties</em>)</h5>&#13;
&#13;
<pre data-code-language="properties" data-type="programlisting"><code class="na">quarkus.kubernetes.liveness-probe.period</code><code class="o">=</code><code class="s">10s</code></pre></div>&#13;
&#13;
<p><a data-type="xref" href="#observability:nack">Example 13-4</a> shows how to modify <code>Processor</code> to simulate failures.</p>&#13;
<div data-type="example" id="observability:nack">&#13;
<h5><span class="label">Example 13-4. </span>Processor to nack every eighth message received (<em>chapter-13/processor-health/src/main/java/org/acme/Processor.java</em>)</h5>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="nd">@Incoming</code><code class="o">(</code><code class="s">"ticks"</code><code class="o">)</code><code>&#13;
</code><code class="nd">@Outgoing</code><code class="o">(</code><code class="s">"processed"</code><code class="o">)</code><code>&#13;
</code><code class="nd">@Acknowledgment</code><code class="o">(</code><code class="n">Acknowledgment</code><code class="o">.</code><code class="na">Strategy</code><code class="o">.</code><code class="na">MANUAL</code><code class="o">)</code><code>                            </code><a class="co" href="#callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-1" id="co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code class="n">Message</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code><code> </code><code class="nf">process</code><code class="o">(</code><code class="n">Message</code><code class="o">&lt;</code><code class="n">Long</code><code class="o">&gt;</code><code> </code><code class="n">message</code><code class="o">)</code><code> </code><code class="kd">throws</code><code> </code><code class="n">Exception</code><code> </code><code class="o">{</code><code>          </code><a class="co" href="#callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-2" id="co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
</code><code>    </code><code class="k">if</code><code> </code><code class="o">(</code><code class="n">count</code><code class="o">+</code><code class="o">+</code><code> </code><code class="o">%</code><code> </code><code class="mi">8</code><code> </code><code class="o">=</code><code class="o">=</code><code> </code><code class="mi">0</code><code class="o">)</code><code> </code><code class="o">{</code><code>                                                </code><a class="co" href="#callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-3" id="co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>      </code><code class="n">message</code><code class="o">.</code><code class="na">nack</code><code class="o">(</code><code class="k">new</code><code> </code><code class="n">Throwable</code><code class="o">(</code><code class="s">"Random failure to process a record."</code><code class="o">)</code><code class="o">)</code><code>&#13;
</code><code>          </code><code class="o">.</code><code class="na">toCompletableFuture</code><code class="o">(</code><code class="o">)</code><code class="o">.</code><code class="na">join</code><code class="o">(</code><code class="o">)</code><code class="o">;</code><code>&#13;
</code><code>      </code><code class="k">return</code><code> </code><code class="kc">null</code><code class="o">;</code><code>&#13;
</code><code>    </code><code class="o">}</code><code>&#13;
</code><code>    </code><code class="n">String</code><code> </code><code class="n">value</code><code> </code><code class="o">=</code><code> </code><code class="n">String</code><code class="o">.</code><code class="na">valueOf</code><code class="o">(</code><code class="n">message</code><code class="o">.</code><code class="na">getPayload</code><code class="o">(</code><code class="o">)</code><code class="o">)</code><code class="o">;</code><code>&#13;
</code><code>    </code><code class="n">value</code><code> </code><code class="o">+</code><code class="o">=</code><code> </code><code class="s">" consumed in pod ("</code><code> </code><code class="o">+</code><code> </code><code class="n">InetAddress</code><code class="o">.</code><code class="na">getLocalHost</code><code class="o">(</code><code class="o">)</code><code class="o">.</code><code class="na">getHostName</code><code class="o">(</code><code class="o">)</code><code> </code><code class="o">+</code><code> </code><code class="s">")"</code><code class="o">;</code><code>&#13;
</code><code>    </code><code class="n">message</code><code class="o">.</code><code class="na">ack</code><code class="o">(</code><code class="o">)</code><code class="o">.</code><code class="na">toCompletableFuture</code><code class="o">(</code><code class="o">)</code><code class="o">.</code><code class="na">join</code><code class="o">(</code><code class="o">)</code><code class="o">;</code><code>                            </code><a class="co" href="#callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-4" id="co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-4"><img alt="4" src="assets/4.png"/></a><code>&#13;
</code><code>    </code><code class="k">return</code><code> </code><code class="n">message</code><code class="o">.</code><code class="na">withPayload</code><code class="o">(</code><code class="n">value</code><code class="o">)</code><code class="o">;</code><code>&#13;
</code><code class="o">}</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-1" id="callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Use manual acknowledgment, so we can nack messages explicitly.</p></dd>&#13;
<dt><a class="co" href="#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-2" id="callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>We need to change the method signature to use <code>Message</code> instead of <code>Long</code> to use manual acknowledgment.</p></dd>&#13;
<dt><a class="co" href="#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-3" id="callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Every eighth message should be nacked, and we return a <code>null</code> instead.</p></dd>&#13;
<dt><a class="co" href="#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-4" id="callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO1-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Explicitly ack a message.</p></dd>&#13;
</dl>&#13;
&#13;
<p>As the default <code>failure-strategy</code> is <code>fail</code>,&#13;
when we nack a message, the processing of messages fails.&#13;
This message failure will cause the health check of the consumer to also fail,&#13;
triggering a container restart once the next liveness probe runs. Refer to <a data-type="xref" href="ch11.html#bus::install-kafka">“Kafka on Kubernetes”</a>, or the <em>README</em> of <em>/chapter-13</em>, to start minikube and deploy Kafka. Then, run <code>m⁠v⁠n⁠ v⁠e⁠r⁠i⁠f⁠y⁠ ‑D⁠q⁠u⁠a⁠r⁠k⁠u⁠s⁠.​k⁠u⁠b⁠e⁠r⁠n⁠e⁠t⁠e⁠s⁠.d⁠e⁠p⁠l⁠o⁠y⁠=t⁠r⁠u⁠e</code> for each of the three services:&#13;
ticker, viewer, processor. Verify that all three services are running with <code>kubectl get pods</code>.</p>&#13;
&#13;
<p>With the services deployed,&#13;
we can see the overall health check status by accessing <code>/q/health</code> of a service.&#13;
We get the response shown in <a data-type="xref" href="#observability:health-output">Example 13-5</a> for the <code>processor-health</code> service.</p>&#13;
<div data-type="example" id="observability:health-output">&#13;
<h5><span class="label">Example 13-5. </span>Reactive application health check with no errors</h5>&#13;
&#13;
<pre data-code-language="json" data-type="programlisting"><code class="p">{</code><code>&#13;
    </code><code class="nt">"status"</code><code class="p">:</code><code> </code><code class="s2">"UP"</code><code class="p">,</code><code>&#13;
    </code><code class="nt">"checks"</code><code class="p">:</code><code> </code><code class="p">[</code><code>&#13;
        </code><code class="p">{</code><code>&#13;
            </code><code class="nt">"name"</code><code class="p">:</code><code> </code><code class="s2">"SmallRye Reactive Messaging - liveness check"</code><code class="p">,</code><code>&#13;
            </code><code class="nt">"status"</code><code class="p">:</code><code> </code><code class="s2">"UP"</code><code class="p">,</code><code>&#13;
            </code><code class="nt">"data"</code><code class="p">:</code><code> </code><code class="p">{</code><code>&#13;
                </code><code class="nt">"ticks"</code><code class="p">:</code><code> </code><code class="s2">"[OK]"</code><code class="p">,</code><code>                     </code><a class="co" href="#callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO2-1" id="co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO2-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
                </code><code class="nt">"processed"</code><code class="p">:</code><code> </code><code class="s2">"[OK]"</code><code>&#13;
            </code><code class="p">}</code><code>&#13;
        </code><code class="p">}</code><code class="p">,</code><code>&#13;
        </code><code class="p">{</code><code>&#13;
            </code><code class="nt">"name"</code><code class="p">:</code><code> </code><code class="s2">"SmallRye Reactive Messaging - readiness check"</code><code class="p">,</code><code>&#13;
            </code><code class="nt">"status"</code><code class="p">:</code><code> </code><code class="s2">"UP"</code><code class="p">,</code><code>&#13;
            </code><code class="nt">"data"</code><code class="p">:</code><code> </code><code class="p">{</code><code>&#13;
                </code><code class="nt">"ticks"</code><code class="p">:</code><code> </code><code class="s2">"[OK]"</code><code class="p">,</code><code>&#13;
                </code><code class="nt">"processed"</code><code class="p">:</code><code> </code><code class="s2">"[OK]"</code><code>&#13;
            </code><code class="p">}</code><code>&#13;
        </code><code class="p">}</code><code>&#13;
    </code><code class="p">]</code><code>&#13;
</code><code class="p">}</code></pre>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO2-1" id="callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>The data within the check shows the channels we’re connected to and their respective status.</p></dd>&#13;
</dl></div>&#13;
&#13;
<p>We saw, when viewing the generated deployment YAML, that there are also <code>/q/health/live</code> and <code>/q/health/ready</code> endpoints.&#13;
These represent the liveness and readiness probes, respectively.&#13;
Access them in a browser, or via <code>curl</code>,&#13;
to see the specific checks of each probe.</p>&#13;
&#13;
<p>Open up the <em>VIEWER_URL</em>, from the terminal, in a browser.&#13;
Based on the producer we’ve defined,&#13;
we will see seven messages with the same processor pod name,&#13;
before it hit the message that we nacked.&#13;
There will be a pause while Kubernetes restarts the container;&#13;
then we will see another seven messages,&#13;
and this sequence repeats.</p>&#13;
&#13;
<p>If we take a look at the pods in Kubernetes,&#13;
we can see that the container for the processor service has been restarted, as shown in <a data-type="xref" href="#use-kubectl-to-list-pods">Example 13-6</a>.</p>&#13;
<div class="pagebreak-before less_space" data-type="example" id="use-kubectl-to-list-pods">&#13;
<h5><span class="label">Example 13-6. </span>Use <code>kubectl</code> to list pods</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">&gt; kubectl get pods&#13;
NAME                                       READY   STATUS    RESTARTS   AGE&#13;
observability-processor-5cffd8c755-d5578   1/1     Running   <code class="m">2</code>          84s&#13;
observability-ticker-bd8f6f5bb-hqtpj       1/1     Running   <code class="m">0</code>          2m&#13;
observability-viewer-786dd8bc84-zbjp4      1/1     Running   <code class="m">0</code>          3m</pre></div>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>After several restarts in a short amount of time,&#13;
the pod will be in the state of <code>CrashLoopBackoff</code>, which will slowly increase the delay between pod restarts.&#13;
As we don’t have a “happy” container for at least 10 minutes,&#13;
we end up in a state where the pod will not restart for a while.&#13;
That’s not a problem for these examples, but is worth noting.</p>&#13;
</div>&#13;
&#13;
<p>When viewing the health checks at <code>/q/health</code>,&#13;
it can be difficult to “catch” the failed health check before the container restarts.&#13;
To make it easier,&#13;
we can modify the <code>quarkus.kubernetes.liveness-probe.period</code> of the processor service to a large period of time,&#13;
like <code>100s</code>.&#13;
With a longer period, we give ourselves a chance to view the failed health check before the container restarts, as shown in <a data-type="xref" href="#observability:check-fail">Example 13-7</a>.</p>&#13;
<div data-type="example" id="observability:check-fail">&#13;
<h5><span class="label">Example 13-7. </span>Reactive application health check with errors</h5>&#13;
&#13;
<pre data-code-language="json" data-type="programlisting"><code class="p">{</code><code>&#13;
    </code><code class="nt">"status"</code><code class="p">:</code><code> </code><code class="s2">"DOWN"</code><code class="p">,</code><code>&#13;
    </code><code class="nt">"checks"</code><code class="p">:</code><code> </code><code class="p">[</code><code>&#13;
        </code><code class="p">{</code><code>&#13;
            </code><code class="nt">"name"</code><code class="p">:</code><code> </code><code class="s2">"SmallRye Reactive Messaging - liveness check"</code><code class="p">,</code><code>&#13;
            </code><code class="nt">"status"</code><code class="p">:</code><code> </code><code class="s2">"DOWN"</code><code class="p">,</code><code>                                               </code><a class="co" href="#callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-1" id="co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
            </code><code class="nt">"data"</code><code class="p">:</code><code> </code><code class="p">{</code><code>&#13;
                </code><code class="nt">"ticks"</code><code class="p">:</code><code> </code><code class="s2">"[KO] - Random failure to process a record."</code><code class="p">,</code><code>      </code><a class="co" href="#callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-2" id="co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
                </code><code class="nt">"processed"</code><code class="p">:</code><code> </code><code class="s2">"[KO] - Multiple exceptions caught:&#13;
                    [Exception 0] java.util.concurrent.CompletionException:&#13;
                      java.lang.Throwable: Random failure to process a record.&#13;
                    [Exception 1] io.smallrye.reactive.messaging.ProcessingException:&#13;
                      SRMSG00103: Exception thrown when calling the method&#13;
                      org.acme.Processor#process"</code><code>&#13;
            </code><code class="p">}</code><code>&#13;
        </code><code class="p">}</code><code class="p">,</code><code>&#13;
        </code><code class="p">{</code><code>&#13;
            </code><code class="nt">"name"</code><code class="p">:</code><code> </code><code class="s2">"SmallRye Reactive Messaging - readiness check"</code><code class="p">,</code><code>&#13;
            </code><code class="nt">"status"</code><code class="p">:</code><code> </code><code class="s2">"UP"</code><code class="p">,</code><code>&#13;
            </code><code class="nt">"data"</code><code class="p">:</code><code> </code><code class="p">{</code><code>&#13;
                </code><code class="nt">"ticks"</code><code class="p">:</code><code> </code><code class="s2">"[OK] - no subscription yet,&#13;
                    so no connection to the Kafka broker yet"</code><code>   </code><a class="co" href="#callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-3" id="co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
                </code><code class="s2">"processed"</code><code class="p">:</code><code> </code><code class="s2">"[OK]"</code><code>&#13;
            </code><code class="p">}</code><code>&#13;
        </code><code class="p">}</code><code>&#13;
    </code><code class="p">]</code><code>&#13;
</code><code class="p">}</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-1" id="callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Liveness check is <code>DOWN</code>, causing the entire health check to be <code>DOWN</code>.</p></dd>&#13;
<dt><a class="co" href="#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-2" id="callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>The <code>ticks</code> channel is not OK, showing the failure from the exception sent in <code>nack</code>.</p></dd>&#13;
<dt><a class="co" href="#co_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-3" id="callout_observing_reactive_and___span_class__keep_together__event_driven_architectures__span__CO3-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>There is no subscription, because the <code>process</code> method has failed and is no longer subscribing.&#13;
The <code>ticks</code> channel is still OK; it’s just waiting for a subscription.</p></dd>&#13;
</dl>&#13;
&#13;
<p>We can now check the health of our applications&#13;
and utilize them in the container orchestration of Kubernetes.&#13;
Next, we see how our reactive applications can generate metrics for monitoring&#13;
and utilize those metrics for autoscaling.<a data-startref="ix_observability-adoc4" data-type="indexterm" id="idm45358813065120"/><a data-startref="ix_observability-adoc3" data-type="indexterm" id="idm45358813026912"/><a data-startref="ix_observability-adoc2" data-type="indexterm" id="idm45358813026304"/><a data-startref="ix_observability-adoc1" data-type="indexterm" id="idm45358813025696"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Metrics with Messaging" data-type="sect1"><div class="sect1" id="idm45358813606592">&#13;
<h1>Metrics with Messaging</h1>&#13;
&#13;
<p><a data-primary="messaging" data-secondary="metrics with" data-type="indexterm" id="ix_observability-adoc5"/><a data-primary="observability" data-secondary="metrics with messaging" data-type="indexterm" id="ix_observability-adoc6"/>Metrics are a critical part of our applications,&#13;
even more so with reactive applications.&#13;
Metrics can feed monitoring systems for alerting operations and SREs of problems in applications.&#13;
Before delving into how we can do that,&#13;
let’s explain some monitoring-related terms:</p>&#13;
<dl>&#13;
<dt>SLA (service-level agreement)</dt>&#13;
<dd>&#13;
<p><a data-primary="SLA (service-level agreement)" data-type="indexterm" id="idm45358813020016"/>A contract between a service provider and its customers as to the availability, performance, etc. of the service.</p>&#13;
</dd>&#13;
<dt>SLO (service-level objective)</dt>&#13;
<dd>&#13;
<p><a data-primary="SLO (service-level objective)" data-type="indexterm" id="idm45358813018128"/>A goal for a service provider to reach.&#13;
SLOs are internal goals used to help prevent a service provider from breaking an SLA with its customers.&#13;
Developers define rules, or thresholds, for the SLOs of a service to alert Operations or SREs when we’re at risk of breaking SLAs.</p>&#13;
</dd>&#13;
<dt>SLI (service-level indicator)</dt>&#13;
<dd>&#13;
<p><a data-primary="SLI (service-level indicator)" data-type="indexterm" id="idm45358813016080"/>A specific measurement used in measuring an SLO.&#13;
These are the metrics we generate from a reactive application.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>If an organization doesn’t define SLAs, SLOs, and SLIs,&#13;
that’s OK.&#13;
It’s still beneficial to gather metrics from a reactive application to at least define the thresholds indicating when everything is OK and when it is not. A “good” metric for a particular reactive application can differ depending on the specific use case.</p>&#13;
&#13;
<p class="pagebreak-before less_space">However,&#13;
all reactive systems should be gathering and monitoring certain metrics:</p>&#13;
<dl>&#13;
<dt>Queue length</dt>&#13;
<dd>&#13;
<p>If the queue of messages waiting to be processed is too large,&#13;
it impacts the speed at which messages flow through the system.&#13;
If messages aren’t flowing fast enough,&#13;
a time-sensitive reactive application, such as stock trading, will see delays and problems as a result.&#13;
High queue length is an indication we need to increase the number of consumers within a consumer group.&#13;
It may also indicate that we need to increase the number of partitions for a topic if we’re already at the maximum number of consumers.</p>&#13;
</dd>&#13;
<dt>Processing time</dt>&#13;
<dd>&#13;
<p>When a consumer takes too long to process a message,&#13;
it will likely cause an increase in queue length.&#13;
Long processing times can also indicate other issues with a reactive application,&#13;
dependent on what work a consumer does.&#13;
We could see network latency issues because of another service we’re interacting with,&#13;
database contention, or any other number of possible problems.</p>&#13;
</dd>&#13;
<dt>Messages processed in a time window</dt>&#13;
<dd>&#13;
<p>This metric provides an overall understanding of the throughput of a reactive application.&#13;
Knowing the actual number of messages processed is likely less important than monitoring variations.&#13;
A significant drop could indicate a problem in messages not being received,&#13;
or large numbers of customers leaving the application too early.</p>&#13;
</dd>&#13;
<dt>Ack-to-nack ratio</dt>&#13;
<dd>&#13;
<p>We want this metric to be as high as possible,&#13;
as it means we’re not seeing many failures in the messages we process.&#13;
If too many failures occur,&#13;
we need to investigate whether it’s due to upstream systems providing invalid data,&#13;
or failures in the processor to handle different data types properly.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>All of these metrics we’ve discussed are great for detecting possible bottlenecks in a reactive application.&#13;
We may see several of these metrics go in a bad direction at the same time—definitely a sign we have a problem in processing messages!&#13;
We can also define basic rules for detecting bottlenecks.&#13;
When using HTTP, or request/reply, we should check the response time and success rate.&#13;
High response times or low success rates would indicate a problem needing investigation.&#13;
For messaging applications, the number of <em>in-flight</em>, not yet processed, messages is a key measurement to track.</p>&#13;
&#13;
<p>We’ve covered a lot of theory,&#13;
but what do we need to do to capture these metrics?&#13;
The key change is to add the dependency for Micrometer,<sup><a data-type="noteref" href="ch13.html#idm45358813004512" id="idm45358813004512-marker">1</a></sup>&#13;
<a data-primary="Prometheus" data-type="indexterm" id="idm45358813003152"/>and in this case we want the metrics available in the Prometheus format.</p>&#13;
&#13;
<p><em>Micrometer</em> is the preferred metrics solution for Quarkus because it offers key benefits for developers:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Ability to switch the monitoring backend from Prometheus, to Datadog, to Splunk, to New Relic,&#13;
and many others, without needing to modify existing code-creating metrics.&#13;
All that’s required is a dependency change to use a different &#13;
<span class="keep-together">registry</span>!</p>&#13;
</li>&#13;
<li>&#13;
<p>Provides <code>MeterBinder</code> implementations for many of the frameworks used in Quarkus.&#13;
These provide metrics for frameworks such as JAX-RS, Vert.x, and Hibernate,&#13;
without developers needing to specifically code metrics themselves.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>To expose the metrics in the Prometheus format, add the dependency in <a data-type="xref" href="#dependency-micrometer-prometheus-support">Example 13-8</a> to your application.</p>&#13;
<div data-type="example" id="dependency-micrometer-prometheus-support">&#13;
<h5><span class="label">Example 13-8. </span>Dependency for the Micrometer Prometheus support (<em>chapter-13/viewer/pom.xml</em>)</h5>&#13;
&#13;
<pre data-code-language="xml" data-type="programlisting"><code class="nt">&lt;dependency&gt;</code>&#13;
  <code class="nt">&lt;groupId&gt;</code>io.quarkus<code class="nt">&lt;/groupId&gt;</code>&#13;
  <code class="nt">&lt;artifactId&gt;</code>quarkus-micrometer-registry-prometheus<code class="nt">&lt;/artifactId&gt;</code>&#13;
<code class="nt">&lt;/dependency&gt;</code></pre></div>&#13;
&#13;
<p>With this dependency, we have an endpoint showing all the metrics of an application at <code>/q/metrics</code>.&#13;
When using Prometheus in Kubernetes,&#13;
we then need only a <code>ServiceMonitor</code> to inform Prometheus of this endpoint for it to scrape the metrics.&#13;
For this example, we won’t be utilizing Prometheus and Grafana,&#13;
two common tools for monitoring metrics.&#13;
Plenty of documentation online explains how to set them up in Kubernetes for readers to view the metrics in these tools.</p>&#13;
&#13;
<p>If minikube is not still running from the earlier health example, follow the instructions in the <em>README</em> of <em>/chapter-13</em> to start it, deploy Kafka,&#13;
and build and deploy the three services.&#13;
Verify that they’re running with <code>kubectl get pods</code>, and then open the URL of the viewer.&#13;
Once you’ve seen messages appear,&#13;
open up the metrics endpoint for the processor.&#13;
The URL can be found with <code>minikube service --url observability-processor</code> and then add <code>/q/metrics</code> to the end.</p>&#13;
&#13;
<p>You will see metrics such those shown in <a data-type="xref" href="#observability:metrics">Example 13-9</a>.</p>&#13;
<div data-type="example" id="observability:metrics">&#13;
<h5><span class="label">Example 13-9. </span>Sample of metrics for a reactive application</h5>&#13;
&#13;
<pre data-code-language="properties" data-type="programlisting"><code class="c"># HELP kafka_consumer_fetch_manager_records_consumed_total The total number of</code>&#13;
<code class="c"># records consumed</code>&#13;
<code class="err">kafka_consumer_fetch_manager_records_consumed_total</code> <code class="err">\</code>&#13;
  <code class="na">{client_id</code><code class="o">=</code><code class="s">"kafka-consumer-ticks",kafka_version="2.8.0",} 726.0</code>&#13;
<code class="c"># HELP kafka_consumer_response_total The total number of responses received</code>&#13;
<code class="err">kafka_consumer_response_total</code>&#13;
  <code class="na">{client_id</code><code class="o">=</code><code class="s">"kafka-consumer-ticks",kafka_version="2.8.0",} 123.0</code>&#13;
<code class="c"># HELP kafka_consumer_fetch_manager_fetch_latency_avg The average time taken for</code>&#13;
<code class="c"># a fetch request.</code>&#13;
<code class="err">kafka_consumer_fetch_manager_fetch_latency_avg</code> <code class="err">\</code>&#13;
  <code class="na">{client_id</code><code class="o">=</code><code class="s">"kafka-consumer-ticks",kafka_version="2.8.0",} 485.6222222222222</code>&#13;
<code class="c"># HELP kafka_consumer_fetch_manager_records_consumed_rate</code>&#13;
<code class="c"># The average number of records consumed per second</code>&#13;
<code class="err">kafka_consumer_fetch_manager_records_consumed_rate</code> <code class="err">\</code>&#13;
  <code class="na">{client_id</code><code class="o">=</code><code class="s">"kafka-consumer-ticks",kafka_version="2.8.0",} 15.203870076019351</code>&#13;
<code class="c"># HELP kafka_consumer_coordinator_assigned_partitions</code>&#13;
<code class="c"># The number of partitions currently assigned to this consumer</code>&#13;
<code class="err">kafka_consumer_coordinator_assigned_partitions</code> <code class="err">\</code>&#13;
  <code class="na">{client_id</code><code class="o">=</code><code class="s">"kafka-consumer-ticks",kafka_version="2.8.0",} 3.0</code>&#13;
<code class="c"># HELP kafka_producer_response_rate The number of responses received per second</code>&#13;
<code class="err">kafka_producer_response_rate</code> <code class="err">\</code>&#13;
  <code class="na">{client_id</code><code class="o">=</code><code class="s">"kafka-producer-processed",kafka_version="2.8.0",} 3.8208002687156233</code>&#13;
<code class="c"># HELP kafka_producer_request_rate The number of requests sent per second</code>&#13;
<code class="err">kafka_producer_request_rate</code> <code class="err">\</code>&#13;
  <code class="na">{client_id</code><code class="o">=</code><code class="s">"kafka-producer-processed",kafka_version="2.8.0",} 3.820639852212612</code>&#13;
<code class="c"># HELP kafka_producer_record_send_rate The average number of records sent per second.</code>&#13;
<code class="err">kafka_producer_record_send_rate</code> <code class="err">\</code>&#13;
  <code class="na">{client_id</code><code class="o">=</code><code class="s">"kafka-producer-processed",kafka_version="2.8.0",} 15.230982251500022</code>&#13;
<code class="c"># HELP kafka_producer_record_send_total The total number of records sent.</code>&#13;
<code class="err">kafka_producer_record_send_total</code> <code class="err">\</code>&#13;
  <code class="na">{client_id</code><code class="o">=</code><code class="s">"kafka-producer-processed",kafka_version="2.8.0",}\</code>&#13;
<code class="s">  726.0</code>&#13;
<code class="c"># HELP kafka_producer_response_total The total number of responses received</code>&#13;
<code class="err">kafka_producer_response_total</code> <code class="err">\</code>&#13;
  <code class="na">{client_id</code><code class="o">=</code><code class="s">"kafka-producer-processed",kafka_version="2.8.0",} \</code>&#13;
<code class="s">  182.0</code>&#13;
<code class="c"># HELP kafka_producer_request_total The total number of requests sent</code>&#13;
<code class="err">kafka_producer_request_total</code> <code class="err">\</code>&#13;
  <code class="na">{client_id</code><code class="o">=</code><code class="s">"kafka-producer-processed",kafka_version="2.8.0",} \</code>&#13;
<code class="s">  182.0</code>&#13;
<code class="c"># HELP kafka_producer_request_latency_avg The average request latency in ms</code>&#13;
<code class="err">kafka_producer_request_latency_avg</code> <code class="err">\</code>&#13;
  <code class="na">{client_id</code><code class="o">=</code><code class="s">"kafka-producer-processed",kafka_version="2.8.0",} 10.561797752808989</code></pre></div>&#13;
&#13;
<p><a data-type="xref" href="#observability:metrics">Example 13-9</a> shows a condensed version of the metrics generated from the processor service,&#13;
as the complete version would require much more space!</p>&#13;
&#13;
<p>Metrics would also enable us to develop a Kubernetes operator to autoscale the consumers of a reactive system.&#13;
The operator can use the Kafka admin API to measure the number of messages that have not been consumed.&#13;
If there are fewer consumers than the number of partitions,&#13;
the operator can scale up the number of replicas for a consumer to process more messages in the same time.&#13;
When the number of un-consumed messages drops below a threshold,&#13;
the operator can then scale back consumers from within the consumer group.<a data-startref="ix_observability-adoc6" data-type="indexterm" id="idm45358812828144"/><a data-startref="ix_observability-adoc5" data-type="indexterm" id="idm45358812827536"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Distributed Tracing with Messaging" data-type="sect1"><div class="sect1" id="idm45358813024832">&#13;
<h1>Distributed Tracing with Messaging</h1>&#13;
&#13;
<p><a data-primary="distributed tracing" data-type="indexterm" id="ix_observability-adoc7"/><a data-primary="messaging" data-secondary="distributed tracing with" data-type="indexterm" id="ix_observability-adoc8"/><a data-primary="observability" data-secondary="distributed tracing with messaging" data-type="indexterm" id="ix_observability-adoc9"/><em>Distributed tracing</em> is an extremely important part of observability for reactive systems.&#13;
When we have a single application deployment,&#13;
all the interactions usually occur within the same process.&#13;
Additionally,&#13;
reactive systems have the complexity of one service not knowing where, or often when,&#13;
another service will consume the message they’ve created.&#13;
With nonreactive systems,&#13;
we’re usually able to infer the connections by reading the code to see where outgoing HTTP calls are made.&#13;
That is not possible with a reactive system built around messaging.</p>&#13;
&#13;
<p>This is where distributed tracing shines,&#13;
connecting the many dots—services—in a system across space and time to provide an overall perspective on the message flows.&#13;
For the example, we will be using the OpenTelemetry extension,&#13;
with an exporter to send the captured traces to <a data-primary="Jaeger" data-type="indexterm" id="idm45358812819968"/>Jaeger.<sup><a data-type="noteref" href="ch13.html#idm45358812819136" id="idm45358812819136-marker">2</a></sup></p>&#13;
&#13;
<p>First, though, let’s cover some terminology:</p>&#13;
<dl>&#13;
<dt>Span</dt>&#13;
<dd>&#13;
<p>A single operation within a trace (defined next).&#13;
Many spans can be created within a single service,&#13;
depending on the level of detail you want to collect.&#13;
A span can have parent or child spans associated with it,&#13;
representing a chain of execution.</p>&#13;
</dd>&#13;
<dt>Trace</dt>&#13;
<dd>&#13;
<p>A collection of operations, or spans,&#13;
representing a single request processed by an application and its components.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>When using Reactive Messaging in Quarkus for Kafka or AMQP,&#13;
spans are automatically created when messages are consumed and when they’re produced.&#13;
This is done by the extension propagating the existing trace and span into the headers of any produced message,&#13;
which is extracted when consuming it.&#13;
This process allows OpenTelemetry to chain together the spans across multiple services in different processes&#13;
to provide a singular view of the flow with Jaeger.</p>&#13;
&#13;
<p>Let’s update the example for distributed tracing!&#13;
We add the Quarkus extension for OpenTelemetry to each service in <em>pom.xml</em>, as shown in <a data-type="xref" href="#jaeger-exporter-open-telemetry-dependency">Example 13-10</a>.</p>&#13;
<div data-type="example" id="jaeger-exporter-open-telemetry-dependency">&#13;
<h5><span class="label">Example 13-10. </span>Jaeger exporter for OpenTelemetry dependency</h5>&#13;
&#13;
<pre data-code-language="xml" data-type="programlisting"><code class="nt">&lt;dependency&gt;</code>&#13;
  <code class="nt">&lt;groupId&gt;</code>io.quarkus<code class="nt">&lt;/groupId&gt;</code>&#13;
  <code class="nt">&lt;artifactId&gt;</code>quarkus-opentelemetry-exporter-jaeger<code class="nt">&lt;/artifactId&gt;</code>&#13;
<code class="nt">&lt;/dependency&gt;</code></pre></div>&#13;
&#13;
<p>For each service to be able to send the gathered spans to Jaeger,&#13;
we also need to update <code>application.properties</code> for each service with the URL of the collector (<a data-type="xref" href="#jaeger-collector-endpoint">Example 13-11</a>).</p>&#13;
<div data-type="example" id="jaeger-collector-endpoint">&#13;
<h5><span class="label">Example 13-11. </span>Jaeger collector endpoint</h5>&#13;
&#13;
<pre data-code-language="properties" data-type="programlisting"><code class="na">quarkus.opentelemetry.tracer.exporter.jaeger.endpoint</code><code class="o">=</code>&#13;
<code class="na">http</code><code class="o">:</code><code class="s">//simplest-collector.jaeger:14250</code></pre></div>&#13;
&#13;
<p>To simplify the deployment of Jaeger,&#13;
we will deploy the <em>all-in-one</em> image, as shown in <a data-type="xref" href="#install-jaeger-all-in-one">Example 13-12</a>.</p>&#13;
<div data-type="example" id="install-jaeger-all-in-one">&#13;
<h5><span class="label">Example 13-12. </span>Install Jaeger all-in-one</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">kubectl create ns jaeger&#13;
kubectl apply -f deploy/jaeger/jaeger-simplest.yaml -n jaeger</pre></div>&#13;
&#13;
<p>Details of the Kubernetes deployment and service for Jaeger can be examined by reviewing <em>/deploy/jaeger/jaeger-simplest.yaml</em>.&#13;
The key point to note is the service exposing port 14250 for collecting spans,&#13;
which is the port we set in <a data-type="xref" href="#jaeger-collector-endpoint">Example 13-11</a>.</p>&#13;
&#13;
<p>Retrieve the URL for the Jaeger UI, <code>minikube service --url jaeger-ui -n jaeger</code>,&#13;
and open it in a browser.&#13;
We see the initial page to search for traces,&#13;
but without any services in the drop-down to search for, as nothing is running yet.</p>&#13;
&#13;
<p>Follow the <em>README</em> for <em>/chapter-13</em> to rebuild and redeploy the three services:&#13;
ticker, processor, viewer.&#13;
Once they’re deployed,&#13;
open up the viewer URL, <code>minikube service --url observability-viewer</code>,&#13;
in a browser to begin receiving messages.</p>&#13;
&#13;
<p>Once messages are appearing,&#13;
go back to the Jaeger UI and refresh the page.&#13;
There will now be four services to select from;&#13;
choose <code>observability-ticker</code>,&#13;
the first service in the reactive stream.&#13;
Click the Find Traces button to retrieve the traces for the service.&#13;
Select one of the traces from the list to open a view containing all the details of the spans (<a data-type="xref" href="#image:jaeger-ui-trace">Figure 13-1</a>).</p>&#13;
&#13;
<figure><div class="figure" id="image:jaeger-ui-trace">&#13;
<img alt="Jaeger UI showing reactive system trace" src="assets/rsij_1301.png"/>&#13;
<h6><span class="label">Figure 13-1. </span>Jaeger UI showing reactive system trace</h6>&#13;
</div></figure>&#13;
&#13;
<p>In this example, we have four spans within a single trace.&#13;
There is a span for each step in the reactive stream that first produces a message from the ticker services, then consumes and produces a message in the processor service, and finally consumes the message in the viewer service.&#13;
In the Jaeger UI, explore the data captured within the spans for each step.&#13;
In <a data-type="xref" href="#image:jaeger-ui-trace">Figure 13-1</a>, we see the details of the <code>ticks send</code> span,&#13;
including the type of span, producer,&#13;
and the details of where the message was sent.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Though <em>ticks send</em> is grammatically incorrect for past tense,&#13;
the name of the span is dictated by the semantic conventions of OpenTelemetry in <em>blob/main/specification/trace/semantic_conventions/messaging.md#span-name.</em>&#13;
The span name is a combination of the destination, <em>ticks</em>, and the operation type, <em>send</em>.</p>&#13;
</div>&#13;
&#13;
<p>So far, you’ve seen how to utilize traces when message flows are almost instantaneous between services.&#13;
However,&#13;
a benefit of reactive systems is being able to decouple components within the system according to time.&#13;
In other words,&#13;
a message flow can take hours, days, weeks, months, or even years to be completed, with messages waiting in a topic, for instance, for a lengthy amount of time before being consumed.&#13;
It’s also possible for the same message to be processed by a different consumer,&#13;
such as an audit process, some time after it was originally consumed.&#13;
Let’s simulate a delayed scenario and see how the traces work.</p>&#13;
&#13;
<p>To start, let’s clear out the existing services with <code>kubectl delete all --all -n default</code>.&#13;
To ensure that we’re starting with a clean slate,&#13;
we should also delete and re-create the existing Kafka topics, as shown in <a data-type="xref" href="#update-application-deployments">Example 13-13</a>.</p>&#13;
<div data-type="example" id="update-application-deployments">&#13;
<h5><span class="label">Example 13-13. </span>Update the application deployments</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">kubectl delete kafkatopics -n kafka processed&#13;
kubectl delete kafkatopics -n kafka ticks&#13;
kubectl apply -f deploy/kafka/ticks.yaml&#13;
kubectl apply -f deploy/kafka/processed.yaml</pre></div>&#13;
&#13;
<p>To simulate delayed processing,&#13;
let’s deploy the ticker service and then remove it again after 20–30 seconds to have a reasonable number of messages produced (<a data-type="xref" href="#deploy-remove-ticker-application">Example 13-14</a>).</p>&#13;
<div data-type="example" id="deploy-remove-ticker-application">&#13;
<h5><span class="label">Example 13-14. </span>Deploy and remove the ticker application</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code class="nb">cd </code>ticker&#13;
mvn verify -Dquarkus.kubernetes.deploy<code class="o">=</code><code class="nb">true</code>&#13;
<code class="c"># After 20-30 seconds</code>&#13;
kubectl delete all --all -n default</pre></div>&#13;
&#13;
<p>Search again for traces of the <code>observability-ticker</code> service and you’ll see traces with only a single span.&#13;
The only span in every trace is the one from the ticker service.&#13;
For the processor to receive messages from before it was running,&#13;
we need to update <code>application.properties</code> to indicate we want the earliest messages; see <a data-type="xref" href="#configure-first-offset-new-consumer-groups">Example 13-15</a>.</p>&#13;
<div data-type="example" id="configure-first-offset-new-consumer-groups">&#13;
<h5><span class="label">Example 13-15. </span>Configure the first offset to read for new consumer groups</h5>&#13;
&#13;
<pre data-code-language="properties" data-type="programlisting"><code class="na">mp.messaging.incoming.ticks.auto.offset.reset</code><code class="o">=</code><code class="s">earliest</code></pre></div>&#13;
&#13;
<p>With the change made,&#13;
deploy the viewer and processor services,&#13;
and open the viewer URL in a browser to receive the messages.&#13;
Once the messages have been received by the viewer,&#13;
go back to the Jaeger UI and search for the traces again.&#13;
We see the traces that previously had only a single span&#13;
now have all four spans!&#13;
We successfully processed messages after some time, and Jaeger was able to associate the spans with the right trace.</p>&#13;
&#13;
<p>In a real production system,&#13;
whether the preceding process works would depend on the retention of tracing data.&#13;
If we retain tracing data for a year&#13;
but want to process messages older than that,&#13;
Jaeger will consider them as traces with only the spans from today.&#13;
Any spans from the same trace will no longer be present for Jaeger to properly link them for visualization.<a data-startref="ix_observability-adoc9" data-type="indexterm" id="idm45358812696176"/><a data-startref="ix_observability-adoc8" data-type="indexterm" id="idm45358812695568"/><a data-startref="ix_observability-adoc7" data-type="indexterm" id="idm45358812649440"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45358812825984">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>This chapter detailed the importance of observability for reactive systems in Kubernetes.&#13;
Observability is the key to ensuring the resiliency and elasticity of reactive systems.&#13;
Health checks help systems to be resilient,&#13;
by triggering the restart of services that are not healthy.&#13;
Specific metrics of a reactive system can be used to provide elasticity by&#13;
scaling up and down consumers as needed,&#13;
dependent on message queue size, for instance.</p>&#13;
&#13;
<p>We covered observability in Kubernetes with health checks, metrics, and distributed tracing.&#13;
What we’ve covered only scratches the surface of observability for reactive systems,&#13;
but provides sufficient detail for developers to delve deeper themselves.&#13;
Though we can provide general guidelines for observability of reactive systems,&#13;
specifics of what is desired will depend heavily on the use cases of the system.<a data-startref="ix_observability-adoc0" data-type="indexterm" id="idm45358812646080"/></p>&#13;
&#13;
<p>We’ve reached the end of <a data-type="xref" href="part04.html#patterns-part">Part IV</a>,&#13;
where we covered patterns of Reactive Messaging and its support of event buses, connecting messages to/from HTTP endpoints, and observing reactive systems.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45358813004512"><sup><a href="ch13.html#idm45358813004512-marker">1</a></sup> <a href="https://micrometer.io">Micrometer</a> provides a facade over the instrumentation clients of popular monitoring systems, such as Prometheus.</p><p data-type="footnote" id="idm45358812819136"><sup><a href="ch13.html#idm45358812819136-marker">2</a></sup> <a href="https://opentelemetry.io">OpenTelemetry</a> is a CNCF project combining OpenCensus and OpenTracing into a single project for the collection of telemetry signals. <a href="https://www.jaegertracing.io">Jaeger</a> is a CNCF project for collecting and visualizing traces.</p></div></div></section></body></html>