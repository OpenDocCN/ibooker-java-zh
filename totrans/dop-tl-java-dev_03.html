<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. An Introduction to Containers"><div class="chapter" id="thinking_in_containers">
<h1><span class="label">Chapter 3. </span>An Introduction to Containers</h1>


<p class="byline">Melissa McKay</p>
  <blockquote data-type="epigraph" epub:type="epigraph">
    <p>Any fool can know. The point is to understand.</p>
    <p data-type="attribution">Albert Einstein</p>
  </blockquote>
  <blockquote data-type="epigraph" epub:type="epigraph">
    <p>If you know the why, you can live any how.</p>
    <p data-type="attribution">Friedrich Nietzsche</p>
  </blockquote>

<p>At the time of this writing, the use of containers<a data-type="indexterm" data-primary="containers" data-secondary="about" id="idm45310223592304"/> in production and other environments is growing exponentially, and best practices around containerizing applications are still being discussed and defined. As we home in on efficiency improvements and consider specific use cases, techniques and patterns have evolved that come highly recommended by the blogosphere and professional practitioners through experience. And as expected, a fair share of patterns and common uses have evolved, as well as antipatterns that I hope this chapter will help you recognize and avoid.</p>

<p>My own trial-and-error introduction to containers felt like stirring up a hornet’s nest (oh, the stings!). I was undeniably unprepared. Containerization on the surface is deceptively simple. Knowing what I know now about how to develop and deploy with containers, especially within the Java ecosystem, I hope to pass this knowledge on in a way that will help prevent similar pain for you. This chapter outlines the essential concepts you will need to successfully containerize your applications and discusses <em>why</em> you would even want to do such a thing.</p>

<p><a data-type="xref" href="ch04.xhtml#dissecting_the_monolith">Chapter 4</a> discusses the bigger picture of microservices,<a data-type="indexterm" data-primary="deployment" data-secondary="containers" id="idm45310223589184"/><a data-type="indexterm" data-primary="containers" data-secondary="deployment" id="idm45310223588240"/><a data-type="indexterm" data-primary="microservices" data-secondary="deployment" data-see="deployment" id="idm45310223587296"/> but here we will start with learning about one of the basic building blocks of microservice deployments that you will no doubt encounter if you haven’t already: the container. Note that the concept of microservices, an architectural concern, <em>does not imply the use of containers</em>; rather, it’s the concern of <em>deploying</em> these services, especially in a cloud native environment, that usually begins the conversation around containerization.</p>

<p>Let’s start with considering <em>why</em> we would use a container. The best way to do that is to back up and get some context on how we got here to begin with. Patience is a virtue. If you persevere, going through this history lesson will also naturally lead you to a clearer understanding of <em>what</em> a container actually is.</p>






<section data-type="sect1" data-pdf-bookmark="Understanding the Problem"><div class="sect1" id="idm45310223582992">
<h1>Understanding the Problem</h1>

<p>I’m certain I’m not alone in experiencing the company<a data-type="indexterm" data-primary="containers" data-secondary="problem to be solved" id="idm45310223581248"/> of an “elephant in the room.” Despite the looming frame, deafening noise, and potential for dangerous consequences when ignored, this elephant-sized subject is just allowed to roam, unchallenged. I’ve witnessed it. I’m guilty of it. I’ve even had the distinct pleasure of <em>being</em> said elephant.</p>

<p>In the context of containerization, I’m going to make the argument that we need to address <em>two</em> elephants in the room—in the form of two questions: <em>What is a container?</em> and <em>Why would we use a container?</em> Those sound simple enough. How could anyone miss these basic starting points?</p>

<p>Perhaps it’s because the microservice movement tends to lead into discussions about deploying containers more now than ever, and we’re suffering from the fear of missing out. Maybe it’s because a container implementation is expected by default with the exceedingly popular Kubernetes ride, and “our K8s cluster” is the cool new phrase to include in our conversations. It might even just be that we are suffering such an onslaught of new technologies and tools in the DevOps ecosystem that, as a developer (a Java developer, no less), if we stop to ask questions, we fear getting left behind. Whatever the reasons may be, before we can even get into the details of how to build and use containers, these <em>what</em> and <em>why</em> questions must be addressed.</p>

<p>I’m deeply grateful for the incredible colleagues and mentors I’ve had the privilege of working with over the years. I frequently recall, from the formative years of my career, sage advice that has become a mantra of mine. It’s simple; always begin and then proceed working on any project with a constant, repeating question in mind: <em>What is the problem you are trying to solve?</em> The success of your solution will be measured by how well it meets this requirement—that it indeed solves the original problem.</p>

<p>Carefully consider whether you are solving the right problem to begin with. Be especially vigilant to reject problem statements that are actually implementation instructions in disguise, like this one: <em>Improve the performance of your application by breaking it into containerized microservices</em>. You will be better served by a problem statement like this: <em>To decrease the time it takes for customers to complete their objectives, improve the performance of the application by 5%</em>. Note that the latter statement includes a tangible metric to gauge success and is not restricted to a microservices implementation.</p>

<p>This same principle applies to your day-to-day choices in what tools you use, what frameworks and languages you choose to code within, how you choose to design a system, and even how you package and deploy your software to production. What problem are you solving with the choices you’ve made? And how do you know if you’ve chosen the best tool for the job? One way is to understand the problem the particular tool under review is intended to solve. And the best way to do that is to look at its history. This practice should be in place for every tool you pick up to use. I guarantee that you will make better decisions knowing its history, and you will benefit from skirting known pitfalls or, at the very least, have some justification for accepting any disadvantages and moving forward anyway.</p>

<p>My plan is not to completely bore you with historical details, but you should know some basic information and important milestones before jumping into containerizing every bit of code put in front of you. By understanding more about the original problem and the solutions that have come out of it, you’ll be able to intelligently explain why you are choosing to deploy with containers.</p>

<p>I don’t want to go all the way back to the Big Bang, but I’m going to go back more than 50 years, mostly to make the point that virtualization and containerization are not new. In fact, this concept has been worked on and improved for more than half a century. I’ve picked out some points to highlight that will bring us up to speed quickly. This is not intended to be a deep technical manual on any of the topics mentioned—rather, just enough material to wrap your mind around the progress that has been made over time and how we’ve ended up where we are today.</p>

<p>Let’s begin.</p>








<section data-type="sect2" data-pdf-bookmark="The History of Containers"><div class="sect2" id="idm45310223571168">
<h2>The History of Containers</h2>

<p>In the 1960s and ’70s, computing resources<a data-type="indexterm" data-primary="containers" data-secondary="problem to be solved" data-tertiary="history of containers" id="ch03-hist"/><a data-type="indexterm" data-primary="Unix in history of containers" id="ch03-hist2"/><a data-type="indexterm" data-primary="history" data-secondary="containers" id="ch03-hist3"/> were in general exceptionally limited and expensive (by today’s standards). It took a long time for processes to complete (again, by today’s standards), and it was common for a computer to be dedicated for a long period of time to a single task for a single user. Efforts were begun to improve the sharing of compute resources and address the bottlenecks and inefficiency brought by these limitations. But just being able to share resources was not enough. A need arose for a method of sharing resources without getting in each other’s way or having one person inadvertently cause an entire system to crash for everyone. Both hardware and software that advanced virtualization technology started to trickle in. <a data-type="indexterm" data-primary="chroot (Unix)" id="idm45310223565520"/>One development in software is <code>chroot</code>, which is where we’ll begin.</p>

<p>In 1979, during the development of the seventh edition of Unix, <code>chroot</code> was developed and then in 1982 was added to the Berkeley Software Distribution (BSD). This system command changed the apparent root directory for a process and its children, which resulted in a limited view of the filesystem in order to provide an environment for testing a different distribution, for example. Although a step in the right direction, <code>chroot</code> was just a start on the path to providing the isolation of applications required from us today. In 2000, FreeBSD expanded the concept and introduced the more sophisticated <code>jail</code> command and utility in FreeBSD 4.0. Its features (improved in the later 5.1 and 7.2 releases) help further isolate filesystems, users, and networks, and include the ability to assign an IP address to each <code>jail</code>.</p>

<p>In 2004, Solaris containers and zones brought us ahead even further by giving an application full user, process, and filesystem space and access to system hardware. Google jumped in with its <em>process containers</em> in 2006, later renamed <em>cgroups</em>, which centered around isolating and limiting the resource usage of a process. In 2008, <em>cgroups</em> were merged into the Linux kernel, which, along with Linux namespaces, <a data-type="indexterm" data-primary="Linux Containers (LXC; IBM)" id="idm45310223559360"/><a data-type="indexterm" data-primary="LXC (Linux Containers; IBM)" id="idm45310223558592"/>led to IBM’s development of Linux Containers (LXC).</p>

<p>Now things get even more interesting.<a data-type="indexterm" data-primary="Docker" data-secondary="history of containers" id="idm45310223557328"/><a data-type="indexterm" data-primary="Google" data-secondary="containers" id="idm45310223556352"/><a data-type="indexterm" data-primary="lmctfy (Let Me Contain That For You) Google project" id="idm45310223555408"/><a data-type="indexterm" data-primary="Let Me Contain That For You (lmctfy) Google project" id="idm45310223554640"/> Docker became open source in 2013. That same year, Google offered its Let Me Contain That For You (lmctfy) open source project, which gave applications the ability to create and manage their own subcontainers. And from there, we saw the use of containers explode—Docker containers specifically. Initially, Docker used LXC as its default execution environment, <a data-type="indexterm" data-primary="libcontainer project" id="idm45310223553424"/>but in 2014 Docker chose to swap out its use of the LXC toolset for launching containers with <em>libcontainer</em>, a native solution written in Go. Soon after, the lmctfy project ceased active development with the intention of joining forces and migrating the core concepts to the libcontainer project.</p>

<p>A lot more happened during this period of time. I’m intentionally skipping over additional details about other projects, organizations, and specifications that were developed because I want to get to a specific event in 2015. This event is especially important because it will give you some insight into some of the activity and motivations behind shifts in the market, especially concerning Docker.</p>

<p>On June 22, 2015, the establishment of<a data-type="indexterm" data-primary="OCI (Open Container Initiative)" data-secondary="establishment of" id="idm45310223550944"/><a data-type="indexterm" data-primary="Linux Foundation" data-secondary="Open Container Initiative" id="idm45310223549904"/> the <a href="https://oreil.ly/Vsr6U">Open Container Initiative (OCI)</a> was announced. This is an organization under the <a href="https://oreil.ly/J5ioU">Linux Foundation</a> with the goal of creating open standards for container runtimes and image specification. Docker is a heavy contributor, but Docker’s announcement of this new organization listed participants including Apcera, Amazon Web Services (AWS), Cisco, CoreOS, EMC, Fujitsu, Google, Goldman Sachs, HP, Huawei Technologies, IBM, Intel, Joyent, Pivotal Software, the Linux Foundation, Mesosphere, Microsoft, Rancher Labs, Red Hat, and VMware. Clearly, the development of containers and the ecosystem around them has reached a significant point to glean this much attention, and has evolved to where establishing some common ground will be beneficial to all parties involved.</p>

<p>When the formation of the OCI was announced, <a data-type="indexterm" data-primary="OCI (Open Container Initiative)" data-secondary="Runtime Specification" id="idm45310223546832"/><a data-type="indexterm" data-primary="OCI (Open Container Initiative)" data-secondary="Image Format Specification" id="idm45310223545840"/><a data-type="indexterm" data-primary="resources for learning" data-secondary="OCI Image Format Specification" id="idm45310223544864"/><a data-type="indexterm" data-primary="resources for learning" data-secondary="OCI Runtime Specification" id="idm45310223543904"/><a data-type="indexterm" data-primary="Open Container Initiative" data-see="OCI" id="idm45310223542944"/><a data-type="indexterm" data-primary="runtimes" data-secondary="OCI Runtime Specification" id="idm45310223541984"/>Docker also announced its intention to donate its base container format and runtime, runC. In quick succession, <em>runC</em> became the reference implementation for the <a href="https://oreil.ly/lLia7">OCI Runtime Specification</a>, and the Docker v2 Schema 2 image format, donated in April 2016, became the basis for the <a href="https://oreil.ly/mmPu4">OCI Image Format Specification</a>. <a href="https://oreil.ly/y6QwF">Version 1.0 of these specifications</a> were both released in July 2017.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><em>runC</em> is a repackage of libcontainer, <a data-type="indexterm" data-primary="libcontainer project" data-secondary="runC as repackage of" id="idm45310223536800"/>which meets the requirements of the OCI runtime specification. In fact, as of this writing, the <a href="https://oreil.ly/hbUaP">source code for runC</a> contains a directory called <em>libcontainer</em>.</p>
</div>

<p>In tandem with developments in the container ecosystem, orchestration of these systems was also under rapid development. <a data-type="indexterm" data-primary="Google" data-secondary="Kubernetes v1.0 released" id="idm45310223446480"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="about" data-tertiary="v1.0 release" id="idm45310223445440"/><a data-type="indexterm" data-primary="Cloud Native Computing Foundation (CNCF)" id="idm45310223444224"/><a data-type="indexterm" data-primary="Linux Foundation" data-secondary="Cloud Native Computing Foundation" id="idm45310223443520"/><a data-type="indexterm" data-primary="Google" data-secondary="Cloud Native Computing Foundation" id="idm45310223442560"/>On July 21, 2015, one month after the OCI was established, Google released Kubernetes v1.0. Along with this release, the <a href="https://www.cncf.io">Cloud Native Computing Foundation (CNCF)</a> was established in partnership with Google and the Linux Foundation. <a data-type="indexterm" data-primary="Google" data-secondary="containers" data-tertiary="Container Runtime Interface" id="idm45310223440672"/><a data-type="indexterm" data-primary="Container Runtime Interface (CRI)" id="idm45310223439440"/><a data-type="indexterm" data-primary="runtimes" data-secondary="kubelet alternative container runtimes" id="idm45310223438752"/><a data-type="indexterm" data-primary="CRI" data-see="Container Runtime Interface" id="idm45310223437792"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="kubelet" data-tertiary="alternative container runtimes" id="idm45310223436832"/>Another important step taken by Google and released with v1.5 of Kubernetes in December 2016 was the development of the Container Runtime Interface (CRI), which created the level of abstraction needed to allow the Kubernetes machine daemon, <em>kubelet</em>, to support alternative low-level container runtimes. <a data-type="indexterm" data-primary="containerd" id="idm45310223434768"/><a data-type="indexterm" data-primary="Docker" data-secondary="history of containers" data-tertiary="containerd" id="idm45310223434064"/>In March 2017, Docker, also a member of the CNCF, contributed its CRI-compatible runtime <em>containerd</em> that it had developed in order to integrate runC into Docker v1.11.</p>

<p>In February 2021, Docker donated yet another reference implementation to the CNCF. <a data-type="indexterm" data-primary="Docker" data-secondary="history of containers" data-tertiary="container images pushed and pulled" id="idm45310223432096"/><a data-type="indexterm" data-primary="container images" data-secondary="history of containers" id="idm45310223430880"/><a data-type="indexterm" data-primary="OCI (Open Container Initiative)" data-secondary="distribution specification" id="idm45310223429936"/><a data-type="indexterm" data-primary="images" data-see="container images" id="idm45310223428960"/>This contribution was centered around the distribution of images (pushing and pulling container images). Three months later, in May 2021, the OCI released version 1.0 of the <a href="https://oreil.ly/JfGvb">OCI Distribution Spec</a> based on the Docker Registry HTTP API V2 protocol.</p>

<p>Today, the use of containers and orchestration systems<a data-type="indexterm" data-primary="deployment" data-secondary="containers" id="idm45310223426496"/><a data-type="indexterm" data-primary="containers" data-secondary="deployment" id="idm45310223425520"/> like Kubernetes is typical fare for cloud native deployments. Containers are an important factor in keeping deployments flexible among a variety of hosts and play a huge role in scaling distributed applications. Cloud providers including AWS, Google Cloud, Microsoft Azure, and others are continuously bulking up their offerings using shared infrastructure and pay-per-use storage.</p>

<p>Congratulations for getting through that bit of history! In a few paragraphs, we spanned more than 50 years of development and advancement. You were introduced to a lot of the projects that have evolved into our solutions as well as some of the common terms used in the context of containers and their deployment. You’ve also learned how much Docker has contributed to the state of containers today—which makes this a perfect time to get a solid understanding of the container ecosystem, the technical details behind containers, and the implementation components that come into play.</p>

<p>But wait! Before we dive into that, let’s discuss that second elephant. You learned a lot about <em>what</em> happened, but <em>why</em> did the industry shift in this way?<a data-type="indexterm" data-startref="ch03-hist" id="idm45310223422272"/><a data-type="indexterm" data-startref="ch03-hist2" id="idm45310223421568"/><a data-type="indexterm" data-startref="ch03-hist3" id="idm45310223420896"/></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Why Containers?"><div class="sect2" id="idm45310223570512">
<h2>Why Containers?</h2>

<p>Knowing what containers are and how to describe them is not enough.<a data-type="indexterm" data-primary="containers" data-secondary="problem to be solved" data-tertiary="why containers" id="idm45310223418528"/> To talk intelligently about them, you should have some understanding of <em>why</em> they are used. What are the advantages of using containers? Some of this may seem obvious, given what you now know about containers and their history, but it’s worth going in depth before jumping into the fray. Project changes and any introduction of a new tech stack should always be intentional with a thoughtful cost-benefit analysis. Following the crowd is not a good enough reason in and of itself.</p>

<p>Your first question is likely along the lines<a data-type="indexterm" data-primary="DevOps" data-secondary="containers for deployment" id="idm45310223416256"/><a data-type="indexterm" data-primary="containers" data-secondary="about" id="idm45310223415216"/> of <em>why are containers a developer’s concern?</em>—a valid question, indeed. If containers are simply a method of deployment, it seems that this should be in the wheelhouse of operations. It is here that we approach a blurry line between development and operations, an argument for a DevOps mindset. Packaging your app into a container involves more thought and foresight from the developer’s perspective than you may initially think. After you’ve learned some of the best practices and some of the problems encountered by others’ experience, you will find yourself considering the packaging <em>while</em> developing your application. Certain aspects of the process will drive the decisions you make about how your application or service uses memory, how it uses the filesystem, how you plug in observability hooks, how you allow for different configurations, and how you communicate with other services (such as databases). These are just a few examples. Ultimately, it will depend on how your team is organized, but on a DevOps team, I would expect that as a developer, knowing how to build and maintain container images and to understand the container environment will be valuable.</p>

<p>I recently had the opportunity to be part of a panel discussion for the Cloud and DevOps international track at The Developer’s Conference titled “Cloud Efficiency and Simplicity: What Will the Future Bring?” As part of this discussion, we talked about the current state of technologies available and where we would expect more simplification. I introduced the following question/analogy to the discussion: <em>How many of us would be driving cars today if we were expected to build our own?</em>
We are still in very early stages of so many technologies in this area. The market is ripe for manufacturers of full-featured products that allow our software and services to take full advantage of the scalability, availability, and resilience that the cloud has to offer, packaged in a way that reduces complexity. However, we are still in the middle of designing the individual pieces and parts that would be used to build something like this.</p>

<p>Containers are a huge step in this direction, providing a useful level of abstraction between the packaging of an application and the infrastructure where it will be deployed. I anticipate a time when developers will no longer need to be involved in the details at the level of containers, but for now, <em>we should be</em>. At the very least, we should have a seat at the table to make sure that development concerns are addressed moving forward. To that end, and to satisfy any remaining doubts about why you should even broach the subject of containers, let’s learn more.</p>

<p>Think about all it takes to package, deploy, and run your Java application. To begin development, you install a particular version of the Java Development Kit (JDK) to your development machine. <a data-type="indexterm" data-primary="dependencies" data-secondary="packaging a Java application" id="idm45310223410224"/>Then you might install a dependency manager such as Apache Maven or Gradle to pull in all of the needed third-party libraries you choose to use in your app and package it up into a WAR or a JAR file. At this point, it might be ready to deploy… <em>somewhere</em>.</p>

<p>And here the problems begin. What is installed on the production server—what version of the Java runtime, what application server (for example, JBoss, Apache Tomcat, WildFly)? Are other processes running on the production server that might interfere with your application’s performance? Does your application require root access for any reason, and is your application user set up appropriately with the correct permissions? Does your app require access to external services like a database or APIs for alive or well checks? Before any of these questions can be answered, do you even have access to a dedicated production server to begin with, or do you need to begin the process of requesting one to be provisioned for your application? And then what happens when your application is strained with heavy activity—are you able to scale quickly and automatically, or must you begin the provisioning process all over again?</p>

<p>Given these issues, it’s easy to see why<a data-type="indexterm" data-primary="virtual machines (VMs)" id="idm45310223407536"/><a data-type="indexterm" data-primary="containers" data-secondary="about" data-tertiary="virtual machines versus" id="idm45310223406832"/> virtualization using virtual machines (VMs) became such an attractive option. VMs provide more flexibility when it comes to isolating application processes, and the ability to snapshot a VM can provide consistency in deployments. <a data-type="indexterm" data-primary="OS" data-secondary="VMs versus containers" id="idm45310223405248"/>However, VM images are large and not easy to move around because they include an entire OS, which contributes to their overall bulk.</p>

<p>More than a few times when first introducing fellow developers to containers, I’ve gotten the response, “Oh! So a container is like a VM?” While it’s convenient to think of <em>containers</em> as analogous to VMs, an important distinction exists. VMs (VMware vSphere, Microsoft Hyper-V, and others) are an abstraction of the hardware, emulating a complete server. In a sense, the entire operating system is included in a VM. <a data-type="indexterm" data-primary="hypervisor" id="idm45310223402784"/>VMs are managed by a software layer called a <em>hypervisor</em>, which divides and allocates the host’s resources to the VMs as required.</p>

<p>Containers, on the other hand, are not as heavy as a traditional VM. Rather than include an entire OS, a Linux container, for example, can be thought of as a Linux distribution that shares the host operating system. As shown in <a data-type="xref" href="#vm_vs_container">Figure 3-1</a>, VMs and containers are <a data-type="indexterm" data-primary="Java Virtual Machine (JVM)" id="idm45310223400016"/><a data-type="indexterm" data-primary="virtual machines (VMs)" data-secondary="Java Virtual Machines versus" id="idm45310223399280"/><a data-type="indexterm" data-primary="VMs" data-see="virtual machines" id="idm45310223398320"/>different levels of abstraction, as is the Java Virtual Machine (JVM).</p>

<p>Where does the JVM fit in all of this? It gets confusing when terms like <em>virtual machine</em> are overloaded. <a data-type="indexterm" data-primary="runtimes" data-secondary="Java Runtime Environment" id="idm45310223396256"/><a data-type="indexterm" data-primary="Java Runtime Environment (JRE)" id="idm45310223395232"/>The JVM is a completely different abstraction altogether and is a <em>process</em> virtual machine as opposed to a <em>system</em> virtual machine. Its primary concern is to provide the Java Runtime Environment (or JRE, the implementation of the JVM) for a Java application. The JVM virtualizes the host’s processor(s) for the purpose of executing Java bytecode.</p>

<figure><div id="vm_vs_container" class="figure">
<img src="Images/dtjd_0301.png" alt="dtjd 0301" width="600" height="336"/>
<h6><span class="label">Figure 3-1. </span>VMs versus containers</h6>
</div></figure>

<p>Containers are a lightweight solution that promises<a data-type="indexterm" data-primary="dependencies" data-secondary="containers" id="idm45310223390880"/> to solve most of the issues around application consistency, process isolation, and OS-level dependencies. This method of packaging a service or application can utilize caching mechanisms that drastically reduce the time it takes to get an application deployed and up and running. Rather than having to wait for custom provisioning and setup, containers can be deployed to existing infrastructure—whether that’s an available dedicated server, an existing VM on premises in a private data center, or cloud resources.</p>

<p>Even if you choose not to utilize containers in production,<a data-type="indexterm" data-primary="development using containers" id="idm45310223389392"/> you are well advised to consider a couple of other use cases around development and test environments.</p>

<p>A big challenge in onboarding a new developer to a team is the time spent setting up their local development environment. It is generally understood that it’s going to take some time to get a developer to the point where they can contribute their first bug fix or improvement. While some companies dictate the development tools (consistency is often believed to improve support efforts and therefore efficiency), developers have more choices today than ever. I’m of the opinion that forcing a specific toolset on developers when they are already accustomed to something different actually has the opposite effect. Frankly, in many cases, it simply just isn’t necessary anymore—especially now that we can utilize containers.</p>

<p>Containers help keep the runtime environment consistent, and when configured correctly, can easily be launched<a data-type="indexterm" data-primary="testing" data-secondary="containers for" id="idm45310223387456"/> in dev, test, or production modes. The risk of your service or application behaving differently in these environments because of a missing dependency is greatly reduced since the environment is shipped along with your application in a container image.</p>

<p>This portability improves a developer’s ability to sanity-test changes in a local environment as well as the ability to deploy the same version of the code that’s in production in order to reproduce a bug. Integration testing with containers also comes with the added benefit of being able to reproduce as close as possible a production environment. <a data-type="indexterm" data-primary="databases" data-secondary="containers matching version" id="idm45310223385328"/>For example, instead of using an in-memory database for integration tests, you can now launch containers that match the version of the database used in production. Using a project like TestContainers for this purpose will prevent irregularities in behavior due to slightly different SQL syntax or other differences between database software versions. Using containers in this way improves efficiency by circumventing the complications of installing new software or multiple versions of the same software to your local machine.</p>

<p>If we’ve learned anything about containers thus far, it is that they are likely here to stay in one form or another. This section began with an illustration of the exponential increase in container usage over the last several years, and the toolsets being continuously developed and improved around the container ecosystem have gained a solid foothold in both development and operations processes. Apart from a huge, and as of yet unknown, advancement in a completely different direction (remember, containers have over 50 years of history behind them), you are well advised to learn about the container ecosystem and how to exploit this technology to your full advantage.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Intro to Container Anatomy"><div class="sect1" id="idm45310223582496">
<h1>Intro to Container Anatomy</h1>

<p>My first experience with containers as a developer<a data-type="indexterm" data-primary="containers" data-secondary="anatomy of" id="idm45310223382288"/> was via a project, developed by a third-party contractor, that my team was now responsible to further develop and maintain. Aside from bringing the initial codebase into our internal GitHub organization, a lot of setup needed to happen to establish our internal DevOps environment around the project—setting up our continuous integration and deployment (CI/CD) pipeline as well as our development and test environments, and, of course, our deployment process.</p>

<p>I compare this experience to clearing my desk (even more so after days of neglect). I’m about to reveal entirely too much about my personal habits here, but it’s worth doing to make this point. The most time-consuming bit of clearing my desk is a stack of papers and mail that invariably grows to the point of falling over. It’s terribly convenient to rush into the house with these items and, because of other urgent tasks on my mind, set them down on the kitchen counter…frequently on top of an existing stack of papers, with the promise that I’ll get to it later. The problem is, I never know what’s going to be in there. The stack could contain bills that need to be paid, important papers that need filing, or invites or letters that need responding to and thought put toward scheduling on our family calendar. I often dread the amount of time I anticipate it will take to get through it, which only leads to a larger stack of neglected correspondence.</p>

<p>For the project my team was responsible for, my first step was to metaphorically clear the desk. <a data-type="indexterm" data-primary="Dockerfile" data-secondary="inherited project" id="idm45310223514560"/>The Dockerfile that I found in the source code was the equivalent of tackling that dreaded stack of papers. Although getting through it and learning the concepts was necessary, I felt like I was getting derailed from the task at hand. Learning a new technology when starting a new project sometimes doesn’t get the amount of time it should be allotted during project planning, even though it adds variables and inherent risk to the project timeline. This does <em>not</em> mean that new technology should never be introduced. Developers absolutely need to learn new things as the industry grows and changes, but it’s best to mitigate risk by either limiting the amount of new tech introduced to a project or being up-front about the variability of the timeline.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>A <em>Dockerfile</em> is a text file that<a data-type="indexterm" data-primary="Dockerfile" data-secondary="about" id="idm45310223511216"/><a data-type="indexterm" data-primary="container images" data-secondary="Dockerfile" id="idm45310223510208"/> contains instructions providing the blueprint for your container. This file is typically named <em>Dockerfile</em>, and although originally specific to Docker, because of its wide use, other image-building tools support using Dockerfiles to build a container image (such as Buildah, kaniko, and BuildKit).</p>
</div>

<p>The information available here is not meant to be a regurgitation of documentation that’s already out there (for example, the<a data-type="indexterm" data-primary="Docker" data-secondary="getting started guide link" id="idm45310223507744"/><a data-type="indexterm" data-primary="resources for learning" data-secondary="Docker" data-tertiary="getting started guide" id="idm45310223506752"/> online <a href="https://oreil.ly/Tez72">Docker getting started guide</a> is exceptional). Instead, I hope to peel this onion in a way that will orient you on the basics and give you immediate value and enough detail to better estimate what it’s going to take to get your own desk cleared and ready for business. You now have quite a bit of information under your belt about containers and how they came to be. This next section covers the terminology and functionality that you will be exposed to as a developer.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45310223504592">
<h5>Key Container and Image Terminology</h5>
<p>The world of containers has its own lexicon, and you will encounter the following terms frequently:<a data-type="indexterm" data-primary="containers" data-secondary="about" data-tertiary="terminology" id="ch03-term"/><a data-type="indexterm" data-primary="container images" data-secondary="about" data-tertiary="terminology" id="ch03-term2"/></p>
<dl>
<dt>Container</dt>
<dd>
<p>The encapsulation of an application and all of its required<a data-type="indexterm" data-primary="containers" data-secondary="about" data-tertiary="definition" id="idm45310223497952"/><a data-type="indexterm" data-primary="OS" data-secondary="container definition" id="idm45310223496704"/><a data-type="indexterm" data-primary="dependencies" data-secondary="containers" id="idm45310223495760"/> dependencies and system resources running within an isolated “space” on a host machine. Containers share the host machine’s operating system and kernel, but utilize low-level features that allow isolation between processes running inside the container and other processes on the same host. Containers enable the portability of an <span class="keep-together">application</span> or service between computing environments without the risk of changes in behavior because of differing dependency sets.</p>
</dd>
<dt>Container image</dt>
<dd>
<p>An immutable, executable binary that<a data-type="indexterm" data-primary="container images" data-secondary="about" data-tertiary="definition" id="idm45310223492432"/><a data-type="indexterm" data-primary="dependencies" data-secondary="container images" id="idm45310223491184"/><a data-type="indexterm" data-primary="filesystem" data-secondary="container image definition" id="idm45310223490240"/><a data-type="indexterm" data-primary="environment in container image" id="idm45310223489232"/> provides all the dependencies and configuration required for creating a container. It encompasses all the environment configuration and explicitly defines all the resources to which a container will have access after it is launched. Images can be thought of as a snapshot of a complete filesystem stored as an archive that can be unpacked and run within the context of a collection of root filesystem changes.</p>
</dd>
<dt>Base image</dt>
<dd>
<p>Images can inherit from other images,<a data-type="indexterm" data-primary="base image" id="idm45310223486704"/><a data-type="indexterm" data-primary="container images" data-secondary="base image" id="idm45310223486000"/><a data-type="indexterm" data-primary="container images" data-secondary="about" id="idm45310223485056"/><a data-type="indexterm" data-primary="Dockerfile" data-secondary="base image scratch" id="idm45310223484112"/><a data-type="indexterm" data-primary="scratch command in Dockerfile" id="idm45310223483168"/> and many are built from an initial set of dependencies and configurations that come from a base image. Commonly used base images can describe a base operating system and/or include a specific package or set of dependencies. A base image is not based on any other image and uses the command <code>scratch</code> as the first line of the image’s Dockerfile.</p>

<p>An image that is based on another will specify the image<a data-type="indexterm" data-primary="parent images" id="idm45310223481136"/><a data-type="indexterm" data-primary="container images" data-secondary="parent image" id="idm45310223480432"/><a data-type="indexterm" data-primary="Dockerfile" data-secondary="parent image" id="idm45310223479488"/> it inherits from, also known as a <em>parent image</em>, in the first line of the Dockerfile. A parent image is not required to be a base image.</p>
</dd>
<dt>Image ID</dt>
<dd>
<p>When an image is built, it is assigned<a data-type="indexterm" data-primary="image ID" id="idm45310223476560"/> a unique ID in the form of a SHA-256 hash calculated from the contents of the image metadata configuration file.</p>
</dd>
<dt>Image digest</dt>
<dd>
<p>A unique ID in the form<a data-type="indexterm" data-primary="image digest" id="idm45310223474320"/> of a SHA-256 hash calculated from the contents of an image manifest file.</p>
</dd>
<dt>Image manifest</dt>
<dd>
<p>A JSON file that contains<a data-type="indexterm" data-primary="image manifest" id="idm45310223472128"/> metadata about a container image. It contains the image digests of the image metadata configuration file and all of the image layers.</p>
</dd>
<dt>Image layer</dt>
<dd>
<p>Images are composed of<a data-type="indexterm" data-primary="image layer" id="idm45310223469872"/> image layers. Image layers are intermediate images that are generated from each command specified in a Dockerfile. As the commands are executed during a build, a corresponding layer is created that consists of the changes made from the previous layer. Beginning with a base layer, subsequent layers are stacked sequentially, and each layer consists of a delta of changes from the previous layer.</p>
</dd>
<dt>Image tag</dt>
<dd>
<p>An alias used to point to a <a data-type="indexterm" data-primary="image tag" id="idm45310223467264"/>specific image binary within an image repository. The tag can be set to any text but is generally used to indicate a specific version of a named image. A tag is unique to an image binary; however, an image binary can have multiple tags. This capability is commonly used along with semantic versioning to tag the latest minor and/or patch version as the latest major version available.</p>

<p>Be aware that image tags<a data-type="indexterm" data-primary="image tag" data-secondary="caution about" id="idm45310223465600"/> are not always consistently used across all projects and aren’t immutable, meaning that a tag can potentially be moved from one binary to another intentionally—or even by mistake. Today’s tagged 3.2.1 version of an image pulled from a public container registry is not guaranteed to be an identical binary to tomorrow’s 3.2.1 version of that image.</p>
</dd>
<dt>Image repository (image name)</dt>
<dd>
<p>Stores all the versions of an image, making them available for distribution. <a data-type="indexterm" data-primary="image repository" id="idm45310223462832"/><a data-type="indexterm" data-primary="image repository" data-secondary="image name" id="idm45310223462032"/>The name of an image repository is usually referred to as the <em>image name</em>.</p>
</dd>
<dt>Container registry</dt>
<dd>
<p>A library of container images that stores<a data-type="indexterm" data-primary="image repository" data-secondary="container registry definition" id="idm45310223458848"/> collections of image repositories.<a data-type="indexterm" data-primary="container registry" id="idm45310223457728"/> Often you may hear the terms <em>Docker registry</em> and <em>container registry</em> used interchangeably; however, be aware that a container registry might not support all image formats specific to both Docker and OCI images.<a data-type="indexterm" data-startref="ch03-term" id="idm45310223455888"/><a data-type="indexterm" data-startref="ch03-term2" id="idm45310223455184"/></p>
</dd>
</dl>
</div></aside>








<section data-type="sect2" data-pdf-bookmark="Docker Architecture and the Container Runtime"><div class="sect2" id="idm45310223498992">
<h2>Docker Architecture and the Container Runtime</h2>

<p>Just like Kleenex is a brand of tissue, Docker is a <em>brand</em> of container.<a data-type="indexterm" data-primary="Docker" data-secondary="architecture" id="ch03-arch"/><a data-type="indexterm" data-primary="Docker Desktop" data-secondary="Docker architecture" id="ch03-arch2"/><a data-type="indexterm" data-primary="containers" data-secondary="anatomy of" data-tertiary="Docker architecture" id="ch03-arch3"/> The Docker company developed an entire technology stack around containerization. So even though the terms <em>Docker container</em> and <em>Docker image</em> have been somewhat genericized, when you install something like Docker Desktop to your development machine, you are getting more than just the ability to run containers. You’re getting an entire container platform that makes building, running, and managing them easy and convenient for developers.</p>

<p>It is important to understand that<a data-type="indexterm" data-primary="Docker" data-secondary="about" id="idm45310222601008"/> installing Docker is not required for building container images or running containers. It is simply a widely used and convenient tool for doing so. In much the same way that you can package a Java project without using Maven or Gradle, you can build a container image without using Docker or a Dockerfile. My advice to a developer new to containers would be to take advantage of the toolset Docker provides and then experiment with other options or methods to get a good feel for a comparison. Even if you choose to utilize other tools instead of or in addition to Docker, a lot of time and effort was spent on engineering a good developer experience, and this alone scores big points for including Docker Desktop in your development environment.</p>

<p>With Docker, you get an isolated environment<a data-type="indexterm" data-primary="OS" data-secondary="Docker" id="idm45310222599520"/> in which a user/application can operate, sharing the host system’s OS/kernel without interfering with the operation of another isolated environment on the same system (a container). Docker enables you to do the following:</p>

<ul>
<li>
<p>Define a container (an image format)</p>
</li>
<li>
<p>Build an image of a container</p>
</li>
<li>
<p>Manage container images</p>
</li>
<li>
<p>Distribute/share container images</p>
</li>
<li>
<p>Create a container environment</p>
</li>
<li>
<p>Launch/run a container (a container runtime)</p>
</li>
<li>
<p>Manage the lifecycle of container instances</p>
</li>
</ul>

<p>The container landscape contains much more than Docker, but many of the container toolset alternatives focus on a subset of these items. Beginning with learning how Docker operates is helpful in understanding and evaluating these alternatives.</p>

<p>A lot of pictures and diagrams are readily available that describe the Docker architecture. An image search online will most likely result in a version of <a data-type="xref" href="#docker_architecture">Figure 3-2</a>. This diagram does a fairly good job of showing how Docker works on your development machine—the Docker CLI is the interface available to you to send commands to the Docker daemon to build images, retrieve requested images from an <a data-type="indexterm" data-primary="Docker Hub" id="idm45310222589184"/>external registry (by default, this is Docker Hub), manage these images in local storage, and then use these images to launch and run containers on your machine.</p>

<figure><div id="docker_architecture" class="figure">
<img src="Images/dtjd_0302.png" alt="Docker architecture" width="600" height="311"/>
<h6><span class="label">Figure 3-2. </span>Docker architecture</h6>
</div></figure>

<p>One of the more confusing concepts when first introduced to this landscape is the focus on one<a data-type="indexterm" data-primary="Docker" data-secondary="about" data-tertiary="container runtime" id="ch03-runt"/><a data-type="indexterm" data-primary="runtimes" data-secondary="Docker container runtime" id="ch03-runt2"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="Docker container runtime" id="ch03-runt3"/> aspect of the Docker ecosystem: the <em>container runtime</em>. To reiterate, this is just one part of the entire tech stack Docker offers, but because orchestration frameworks like Kubernetes require this portion of functionality to launch and run containers, it is often spoken of as a separate entity from Docker (and in the case of alternative container runtimes, it is).</p>

<p>The topic of container runtimes deserves this section all to itself, because it can be one of the most confusing aspects to someone new to the world of containers. Even more confusing is that container runtimes fall into two different categories, low-level or high-level, depending on what features are implemented. And just to keep you on your toes, some overlap can occur in that feature set.</p>

<p>This is a good spot to present a visual on how container runtimes fit together with what you’ve learned earlier about the OCI and projects like containerd and runC. <a data-type="xref" href="#container_runtimes">Figure 3-3</a> illustrates the relationship between older and newer versions of Docker, high-level and low-level runtimes, and where Kubernetes fits in.</p>

<figure><div id="container_runtimes" class="figure">
<img src="Images/dtjd_0303.png" alt="Container runtimes" width="600" height="330"/>
<h6><span class="label">Figure 3-3. </span>Runtimes in the container ecosystem</h6>
</div></figure>
<div data-type="tip"><h6>Tip</h6>
<p>One of the best explanations I’ve come across<a data-type="indexterm" data-primary="Docker" data-secondary="about" data-tertiary="container runtime explained" id="idm45310222576096"/><a data-type="indexterm" data-primary="resources for learning" data-secondary="Docker" data-tertiary="container runtime explained" id="idm45310222574784"/><a data-type="indexterm" data-primary="runtimes" data-secondary="Docker container runtime" data-tertiary="explained" id="idm45310222573552"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="Docker container runtime" data-tertiary="explained" id="idm45310222572320"/> that really gets into the details of container runtimes along with a historical perspective is a <a href="https://oreil.ly/Y2Fow">blog series</a> composed by Ian Lewis, a developer advocate on the Google Cloud Platform Team.</p>
</div>

<p>Prior to version 1.11 (released in 2016), Docker could be described as a monolithic application that wrapped up the entire feature set required of a runtime, plus other management tools. Docker did quite a bit of reorganizing its codebase over the last several years, developing abstractions and pulling out discrete functionality. <a data-type="indexterm" data-primary="OCI (Open Container Initiative)" data-secondary="Runtime Specification" id="idm45310222569648"/><a data-type="indexterm" data-primary="runtimes" data-secondary="OCI Runtime Specification" id="idm45310222568656"/>The runC project that was contributed by Docker to the OCI came out of this effort. This was the first and, for some time, the <em>only</em> implementation of a low-level container runtime that implemented the OCI Runtime Specification.</p>

<p>Other runtimes are out there, and as of this writing<a data-type="indexterm" data-primary="OCI (Open Container Initiative)" data-secondary="runtimes available list" id="idm45310222566096"/><a data-type="indexterm" data-primary="runtimes" data-secondary="available runtimes list" id="idm45310222565104"/><a data-type="indexterm" data-primary="resources for learning" data-secondary="OCI Runtime Specification" data-tertiary="list of runtimes" id="idm45310222564160"/> this is an active space, so be sure to reference <a href="https://oreil.ly/Vro14">the current list maintained by the OCI</a> for the most up-to-date information. <a data-type="indexterm" data-primary="runtimes" data-secondary="crun runtime" id="idm45310222562032"/><a data-type="indexterm" data-primary="runtimes" data-secondary="railcar runtime" id="idm45310222561088"/><a data-type="indexterm" data-primary="crun runtime" id="idm45310222560144"/><a data-type="indexterm" data-primary="railcar runtime" id="idm45310222559472"/>Notable low-level runtime projects include <em>crun</em>, an implementation in C led by Red Hat; and <em>railcar</em>, an implementation in Rust led by Oracle, although this project is now archived.<a data-type="indexterm" data-startref="ch03-runt" id="idm45310222557744"/><a data-type="indexterm" data-startref="ch03-runt2" id="idm45310222557040"/><a data-type="indexterm" data-startref="ch03-runt3" id="idm45310222556368"/></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45310222555568">
<h5>The Status of CoreOS rkt</h5>
<p>CoreOS was acquired by Red Hat at the beginning of 2018. <a data-type="indexterm" data-primary="Cloud Native Computing Foundation (CNCF)" id="idm45310222554256"/><a data-type="indexterm" data-primary="CoreOS rkt status" id="idm45310222553456"/><a data-type="indexterm" data-primary="rkt (CoreOS) status" id="idm45310222552784"/>Prior to that, <em>rkt</em> (a CoreOS initiative) had been accepted to the CNCF as an incubating project and looked to be a promising competitor to Docker’s containerd project. However, since the CoreOS acquisition, the development of the project went dormant. In mid-2019, rkt was archived by the CNCF, and in February 2020, the project was ended.</p>

<p>It is still possible to use rkt containers, as the code is still <a href="https://oreil.ly/GqSMj">available on GitHub</a>, but all maintenance and development activity has come to a stop.</p>
</div></aside>

<p>Developing a specification is a challenging feat,<a data-type="indexterm" data-primary="runtimes" data-secondary="OCI Runtime Specification" data-tertiary="developing" id="idm45310222549760"/><a data-type="indexterm" data-primary="OCI (Open Container Initiative)" data-secondary="Runtime Specification" data-tertiary="developing" id="idm45310222548448"/> and collaboration on the OCI Runtime Specification wasn’t any less challenging. Figuring out the boundaries—what <em>should</em> and what <em>should not</em> be included in the specification—took time before the release of version 1.0. It’s clear, however, that just implementing the OCI Runtime Specification isn’t enough to drive adoption of an implementation. Additional features are needed to make a low-level runtime usable for developers since we are concerned with much more than just the launching and running of a container.</p>

<p>This leads us to higher-level runtimes<a data-type="indexterm" data-primary="runtimes" data-secondary="OCI Runtime Specification" data-tertiary="containerd and cri-o" id="idm45310222545312"/><a data-type="indexterm" data-primary="OCI (Open Container Initiative)" data-secondary="Runtime Specification" data-tertiary="containerd and cri-o" id="idm45310222544000"/><a data-type="indexterm" data-primary="containerd" id="idm45310222542768"/><a data-type="indexterm" data-primary="cri-o" id="idm45310222542096"/> like <em>containerd</em> and <em>cri-o</em>, the two primary players as of this writing that include solutions for many of the concerns around container orchestration, including image management and distribution. Both of these runtimes implement the CRI (which eases the path to a Kubernetes deployment) and delegate low-level container activities to OCI-compliant low-level runtimes (for example, runC).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45310222539920">
<h5>Kubernetes Deprecation of the  Docker Container Runtime</h5>
<p>Kubernetes announced that in release 1.20, <a data-type="indexterm" data-primary="Kubernetes" data-secondary="Docker container runtime" data-tertiary="deprecation of" id="idm45310222538144"/><a data-type="indexterm" data-primary="Docker" data-secondary="about" data-tertiary="container runtime deprecated" id="idm45310222536880"/><a data-type="indexterm" data-primary="runtimes" data-secondary="Docker container runtime" data-tertiary="deprecated" id="idm45310222535648"/>support for the Docker runtime would be deprecated and removed in a future release (as of this writing, removal is slated for release 1.24). <a href="https://oreil.ly/W7h1N">Here’s information directly from the 1.20 changelog</a>:</p>
<blockquote>
<p>Docker support in the kubelet is now deprecated and will be removed in a future release. <a data-type="indexterm" data-primary="dockershim module (Kubernetes)" id="idm45310222532560"/><a data-type="indexterm" data-primary="Container Runtime Interface (CRI)" data-secondary="kubelet dockershim module" id="idm45310221710768"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="kubelet" data-tertiary="dockershim module" id="idm45310221709792"/>The kubelet uses a module called “dockershim,” which implements CRI support for Docker, and it has seen maintenance issues in the Kubernetes community. We encourage you to evaluate moving to a container runtime that is a full-fledged implementation of CRI (v1alpha1 or v1 compliant) as they become available.</p></blockquote>

<p>What does this mean? Is Docker no longer a viable toolset for Kubernetes deployments? Does this mean you can no longer use Docker Desktop or that you should invest any more time in learning Docker? I’m so glad you asked. Let’s pull this apart into the related deployment and development concerns.</p>

<p>Kubernetes is an orchestration framework that<a data-type="indexterm" data-primary="Kubernetes" data-secondary="kubelet" data-tertiary="about" id="idm45310221707296"/><a data-type="indexterm" data-primary="Kubernetes" data-secondary="about" id="idm45310221706048"/><a data-type="indexterm" data-primary="containers" data-secondary="Kubernetes overview" id="idm45310221705104"/> manages the deployment and scaling of containers based on a given configuration. To accomplish this, a node agent called <em>kubelet</em> runs on each node and manages the configured containers—which means that kubelet must communicate with a container runtime.</p>

<p>The difficulty started when Kubernetes was challenged to support alternative container runtimes in addition to the Docker runtime. Kubernetes supported Docker as a runtime via the <em>dockershim</em> module. Remember, Docker is an entire tech stack that includes more than just the runtime. The dockershim module was how CRI support was implemented for the Docker runtime, but since Docker successfully pulled out containerd as a CRI-compatible runtime (and now even Docker itself uses containerd), it no longer makes sense to keep this custom implementation for Docker.</p>

<p>Even though Kubernetes has made changes in favor of supporting multiple container runtimes, Docker images that you build can still be used in Kubernetes cluster. It is still worth learning Docker.<a data-type="indexterm" data-startref="ch03-arch" id="idm45310221701328"/><a data-type="indexterm" data-startref="ch03-arch2" id="idm45310221700624"/><a data-type="indexterm" data-startref="ch03-arch3" id="idm45310221699952"/></p>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Docker on Your Machine"><div class="sect2" id="docker_on_your_machine">
<h2>Docker on Your Machine</h2>

<p>The second most important thing to understand<a data-type="indexterm" data-primary="Docker" data-secondary="installation information" id="ch03-install"/><a data-type="indexterm" data-primary="containers" data-secondary="anatomy of" data-tertiary="Docker installation information" id="ch03-install2"/><a data-type="indexterm" data-primary="containers" data-secondary="about" id="idm45310221694016"/><a data-type="indexterm" data-primary="container images" data-secondary="about" id="idm45310221693072"/><a data-type="indexterm" data-primary="filesystem" data-secondary="container image definition" id="idm45310221692128"/> about containers is that they are not magic. Containers utilize a combination of existing Linux features (as covered at the beginning of this chapter). Container implementations vary in the details, but a container image, in a sense, is simply a tarball of a complete filesystem, and a running container is a Linux process that is constrained to provide a level of isolation from other processes running on a host. The implementation of a Docker container, for example, primarily involves these three ingredients:</p>

<ul>
<li>
<p>Namespaces</p>
</li>
<li>
<p>cgroups</p>
</li>
<li>
<p>A union filesystem</p>
</li>
</ul>

<p>But what does a container look like on your local filesystem? First, let’s figure out where Docker is storing things on our development machine. Then let’s take a look at a real Docker image pulled from Docker Hub.</p>

<p>After installing Docker Desktop, <a data-type="indexterm" data-primary="Docker Desktop" data-secondary="installation information" id="ch03-instinfo"/><a data-type="indexterm" data-primary="filesystem" data-secondary="Docker Desktop installation information" id="ch03-instinfo2"/>running the command <code>docker info</code> from a terminal will provide you with detailed information about your installation. This output includes information about where your images and containers are stored with the label <code>Docker Root Dir</code>. The following example output (truncated for brevity) indicates that the Docker root directory is <em>/var/lib/docker</em>:</p>

<pre data-type="programlisting">$ docker info
Client:
 Context:    default
 Debug Mode: false
 Plugins:
  app: Docker App (Docker Inc., v0.9.1-beta3)
  buildx: Build with BuildKit (Docker Inc., v0.5.1-docker)
  compose: Docker Compose (Docker Inc., 2.0.0-beta.1)
  scan: Docker Scan (Docker Inc., v0.8.0)

Server:
 Containers: 5
  Running: 0
  Paused: 0
  Stopped: 5
 Images: 62
 Server Version: 20.10.6
 Storage Driver: overlay2
…
 Docker Root Dir: /var/lib/docker
…</pre>

<p>This result is from an existing Docker Desktop (version 3.3.3) installation on macOS Big Sur. A quick listing of <em>/var/lib/docker</em> shows the following:</p>

<pre data-type="programlisting">$ ls /var/lib/docker
ls: /var/lib/docker: No such file or directory</pre>

<p>According to the previous output, 5 stopped containers and 62 images are on this system, so how is it that this directory doesn’t exist? Is the output incorrect? You can check another place for the image and container storage location, as shown in <a data-type="xref" href="#docker_desktop_preferences">Figure 3-4</a>, a screenshot of the Preferences section available in the Mac version of the Docker Desktop UI.</p>

<p>However, this location is completely different. A reasonable explanation for this exists, and note that depending on your operating system, your installation may be slightly different. The reason this matters at all is that Docker Desktop for Mac requires a Linux environment to run Linux containers, and to that end, a minimal Linux virtual machine is instantiated during installation. This means that the Docker root directory referred to in the earlier output is actually referencing a directory within this Linux VM.</p>

<figure><div id="docker_desktop_preferences" class="figure">
<img src="Images/dtjd_0304.png" alt="Docker Desktop Preferences" width="600" height="385"/>
<h6><span class="label">Figure 3-4. </span>Docker Desktop Preferences</h6>
</div></figure>

<p>But wait…what if you’re on Windows? Because containers are sharing the host’s operating system, Windows-based containers require a Windows environment to run, and Linux-based containers require a Linux environment. Docker Desktop (version 3.3.3) is a marked improvement from earlier versions (a.k.a Docker Toolbox) in that no additional supporting software is required to run Linux-based containers. In the old days, to run Docker on a Mac, you would need to install something like VirtualBox and boot2docker to get everything up and functioning as expected. Today, Docker Desktop handles the necessary virtualization behind the scenes. Docker Desktop also supports Windows containers via Hyper-V on Windows 10 and Linux containers on Windows 10 via Windows Subsystem for Linux 2 (WSL 2). To run Windows containers on macOS, however, VirtualBox is still required.<a data-type="indexterm" data-startref="ch03-instinfo" id="idm45310221675792"/><a data-type="indexterm" data-startref="ch03-instinfo2" id="idm45310221675088"/></p>

<p>Now that you know we need to access<a data-type="indexterm" data-primary="Docker Desktop" data-secondary="installation information" data-tertiary="Docker images" id="ch03-img"/><a data-type="indexterm" data-primary="filesystem" data-secondary="Docker images" id="ch03-img2"/><a data-type="indexterm" data-primary="container images" data-secondary="pulling a Docker image" id="ch03-img3"/> the Linux virtual machine to get to this Docker root directory, let’s pull a Docker image by using the command <strong><code>docker pull <em>IMAGE NAME</em></code></strong> and see what it looks like on the filesystem:</p>

<pre data-type="programlisting">$ docker pull openjdk
Using default tag: latest
latest: Pulling from library/openjdk
5a581c13a8b9: Pull complete
26cd02acd9c2: Pull complete
66727af51578: Pull complete
Digest: sha256:05eee0694a2ecfc3e94d29d420bd8703fa9dcc64755962e267fd5dfc22f23664
Status: Downloaded newer image for openjdk:latest
docker.io/library/openjdk:latest</pre>

<p>The command <strong><code>docker images</code></strong> lists all of the images stored locally. You can see from its output that two versions of the <em>openjdk</em> image are stored. The one we pulled in the previous command brought in the image with the tag <code>latest</code>. This is the default behavior, but we could have specified a specific <em>openjdk</em> image version like this: <strong><code>docker pull openjdk:11-jre</code></strong>:</p>

<pre data-type="programlisting">$ docker images
REPOSITORY        TAG             IMAGE ID          CREATED        SIZE
...
openjdk           latest          de085dce79ff     10 days ago     467MB
openjdk           11-jre          b2552539e2dd     4 weeks ago     301MB
...</pre>

<p>You can learn more details about the latest <em>openjdk</em> image by running the <code><strong>docker inspect</strong></code> command using the <em>image ID</em>:</p>

<pre data-type="programlisting">$ docker inspect de085dce79ff
[
    {
        "Id": "sha256:de085dce79ff...",
        "RepoTags": [
            "openjdk:latest"
        ],
...
        "Architecture": "amd64",
        "Os": "linux",
        "Size": 467137618,
        "VirtualSize": 467137618,
        "GraphDriver": {
            "Data": {
                "LowerDir": "/var/lib/docker/overlay2/581137...ca8c47/diff:/var
                /lib/docker/overlay2/7f7929...8f8cb4/diff",
                "MergedDir": "/var/lib/docker/overlay2/693641...940d82/merged",
                "UpperDir": "/var/lib/docker/overlay2/693641...940d82/diff",
                "WorkDir": "/var/lib/docker/overlay2/693641...940d82/work"
            },
            "Name": "overlay2"
        },
        "RootFS": {
            "Type": "layers",
            "Layers": [
                "sha256:1a3adb4bd0a7...",
                "sha256:046fa1e6609c...",
                "sha256:a8a84740beab..."
            ]
        },
...</pre>

<p>The <code>docker inspect</code> command spits out a ton of interesting information. But what I want to highlight here is the <code>GraphDriver</code> section, which contains the paths to the directories where all the layers that belong to this image live.</p>

<p>Docker images are composed of layers that correspond to instructions in the Dockerfile that was used to build the image originally. These layers translate into directories and can be shared across images in order to save space.</p>

<p>Note the <code>LowerDir</code>, <code>MergedDir</code>, and <code>UpperDir</code> sections. The <code>LowerDir</code> section contains all the directories, or layers, that were used to build the original image. These are <em>read-only</em>. The <em>UpperDir</em> directory contains all the content that has been modified while the container is running. If modifications are needed for a read-only layer in <em>LowerDir</em>, then that layer is copied into the <em>UpperDir</em> where it can be written to. This is called a <em>copy-on-write</em> operation.</p>

<p>It’s important to remember that the data in <em>UpperDir</em> is ephemeral data that lives only as long as the container lives.  In fact, if you have data that you intend to keep, you should utilize the volume features of Docker and mount a location that will stick around even after the container dies. For example, a database-driven application running in a container will likely utilize a volume mounted to the container for the database data.</p>

<p>Lastly, the <code>MergedDir</code> section is kind of like a virtual directory that combines everything from <em>LowerDir</em> and <em>Upper Dir</em>. The way the Union File System works is that any edited layers that were copied into <em>UpperDir</em> will overlay layers in <em>LowerDir</em>.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Notice all the references to directories within <em>/var/lib/docker</em>, the Docker root directory. If you monitor the size of this directory, you will notice that the more images and containers you create and run, the storage space required for this directory will increase substantially over time. Consider mounting a dedicated drive, and make sure that you are cleaning up unused images and containers regularly. Also, make sure that containerized apps aren’t continuously producing unmanaged data files or other artifacts. For example, utilize log shipping and or log rotation to manage logs generated by your container and its running processes.</p>
</div>

<p>Any number of containers can be launched using the same image. Each container will be created with the image blueprint and will run independently. In the context of Java, think of a container image as a Java class, and a container as a Java object instantiated from that class.</p>

<p>Containers can be stopped and later restarted without being re-created. <a data-type="indexterm" data-primary="Docker Desktop" data-secondary="installation information" data-tertiary="listing containers" id="idm45310221647600"/><a data-type="indexterm" data-primary="containers" data-secondary="listing" id="idm45310221646336"/>To list containers on your system, use the <code>docker ps -a</code> command. Note that the <code>-a</code> flag will display both stopped containers as well as containers currently running:</p>

<pre data-type="programlisting">$ docker ps -a
CONTAINER ID   IMAGE     COMMAND     STATUS                      NAMES
9668ba978683   openjdk   "tail -f"   Up 19 seconds               vibrant_jang
582ad818a57b   openjdk   "jshell"    Exited (0) 14 minutes ago   zealous_wilson</pre>

<p>If you navigate to the Docker root directory, you will see a subdirectory named <em>containers</em>. Within this directory, you will find additional subdirectories named after the <em>container ID</em> of each container on your system. Stopped containers will retain their state and data in these directories so that they can be restarted if needed. When a container is removed using the <code>docker rm <em>CONTAINER NAME</em></code>, its correlated directory will be deleted.</p>
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>Remember to regularly remove unused containers<a data-type="indexterm" data-primary="containers" data-secondary="removing unused containers" id="idm45310221639776"/><a data-type="indexterm" data-primary="containers" data-secondary="deployment" data-tertiary="removing unused containers" id="idm45310221638736"/><a data-type="indexterm" data-primary="deployment" data-secondary="containers" data-tertiary="removing unused containers" id="idm45310221637504"/> from your systems (<em>remove</em>, not just stop). I personally witnessed a scenario with this part of the deployment process missing. Every time new images were released, the old containers were stopped and new containers were launched based on the new images. This was an oversight that quickly used up hard drive space and eventually prevented new deployments. The following Docker command is useful to clean up unused containers in bulk:</p>

<p><code>docker container prune</code></p>
</div>

<pre data-type="programlisting">docker-desktop:~# ls /var/lib/docker/
builder     containers  overlay2    swarm   volumes
buildkit    image       plugins     tmp
containerd  network     runtimes    trust

docker-desktop:~# ls /var/lib/docker/containers/
9668ba978683b37445defc292198bbc7958da593c6bb3cef6d7f8272bbae1490
582ad818a57b8d125903201e1bcc7693714f51a505747e4219c45b1e237e15cb</pre>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If you are using Mac for development, remember that your containers are running in a tiny VM that you will need to first access before you can see the Docker root directory contents. For example, on a Mac, you can access and navigate this directory by interactively running a container in privileged mode that has <em>nsenter</em> installed (you may need to run this with <code>sudo</code>):</p>

<pre data-type="programlisting">docker run -it --privileged --pid=host debian \
nsenter -t 1 -m -u -n -i sh</pre>

<p>Later versions of Windows (10+) now have the capability of running Linux containers natively using Windows Subsystem for Linux (WSL). The default Docker root directory for Windows 11 Home can be found here in File Explorer:<a data-type="indexterm" data-startref="ch03-install" id="idm45310222682032"/><a data-type="indexterm" data-startref="ch03-install2" id="idm45310222681328"/><a data-type="indexterm" data-startref="ch03-img" id="idm45310222680656"/><a data-type="indexterm" data-startref="ch03-img2" id="idm45310222679984"/><a data-type="indexterm" data-startref="ch03-img3" id="idm45310222679312"/></p>

<p><em>\\wsl.localhost\docker-desktop-data\version-pack-data\</em> <span class="keep-together"><em>community\docker\</em></span></p>
</div>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Basic Tagging and Image Version Management"><div class="sect2" id="idm45310221698112">
<h2>Basic Tagging and Image Version Management</h2>

<p>After working with images awhile, you will see<a data-type="indexterm" data-primary="containers" data-secondary="anatomy of" data-tertiary="image version management" id="ch03-vermgmt"/><a data-type="indexterm" data-primary="version control systems" data-secondary="container image version management" id="ch03-vermgmt2"/><a data-type="indexterm" data-primary="container images" data-secondary="version management" id="ch03-vermgmt3"/><a data-type="indexterm" data-primary="tag with container image version" id="ch03-vermgmt4"/> that identifying them and versioning them are a bit different from the way you version your Java software. Working with build tools like Maven has gotten most Java developers accustomed to standard semantic versioning and always specifying dependency versions (or at least accepting of the version Maven chooses to pull within a particular dependency tree). These guardrails are a little more relaxed in other package managers like npm, where a dependency version can be specified as a range in order to allow for ease and flexibility in updating dependencies.</p>

<p>Image versioning can become a stumbling block if not well understood. No guardrails exist (at least not the kind that Java developers are used to). Flexibility in tagging an image is preferred over any enforcement of good practices. However, just because you <em>can</em>, doesn’t mean you <em>should</em>, and just as with proper versioning of Java libraries and packages, it is best to start out of the gate with a naming and versioning scheme that makes sense and follows an accepted pattern.</p>

<p>Container image names and versions follow a specific format, including multiple components that you rarely see in complete form in examples and tutorials. Most example code and Dockerfiles you find when scouring the internet identify images in an abbreviated format.</p>

<p>It is easiest to visualize image management<a data-type="indexterm" data-primary="Docker Hub" data-secondary="Docker image pulled from" id="idm45310222666416"/> as a directory structure, where the name of an image (such as <em>openjdk</em>) is a directory containing all the versions available for this image. Images are usually identified by a <em>name</em> and a version, known as a <em>tag</em>. But these two components are composed of subcomponents that have an assumed default value if not specified, and often, even a tag is omitted in commands. For example, the simplest command for pulling the <em>openjdk</em> Docker image might take the following form:</p>

<pre data-type="programlisting">docker pull openjdk</pre>

<p>What is this command actually giving us? Aren’t there several versions of the <em>openjdk</em> image that you could use? Indeed, yes, and if you are concerned with having repeatable builds, you will immediately spot this ambiguity as a potential problem.</p>

<p>The first step is to include the image tag in this command, which represents a version. The following command implies that I would be pulling version 11 of the <em>openjdk</em> image:</p>

<pre data-type="programlisting">docker pull openjdk:11</pre>

<p>So what was I pulling previously, if not 11? A special tag called <code>latest</code> is implied by default if a tag is not specified. This tag is intended to point to the latest version of the image available, but that might not always be the case. At any point, a tag can be updated to point to a different version of an image, and in some cases, you might find that the tag <code>latest</code> has not been set to point to anything at all.</p>

<p>It is easy to stumble over the nomenclature as well, notably <em>tag</em>, which can mean something different in different contexts. The term <em>tag</em> can mean a specific version, or it can also mean the full <em>image tag</em>, which includes all the components of identification together, including the image <em>name</em>.</p>

<p>Here’s the complete format of a Docker image tag with all possible components:</p>
<pre data-type="programlisting">[ <em>registry</em> [ :<em>port</em> ] / ] <em>name</em> [ :<em>tag</em> ]
</pre>

<p>The only required component is the image <em>name</em>, also known as the <em>image repository</em>. If <em>tag</em> is not specified, then <em>latest</em> is assumed. If the registry is not specified, Docker Hub is the default registry. The following command is an example of how to reference an image on a registry other than Docker Hub:<a data-type="indexterm" data-startref="ch03-vermgmt" id="idm45310222650368"/><a data-type="indexterm" data-startref="ch03-vermgmt2" id="idm45310222649664"/><a data-type="indexterm" data-startref="ch03-vermgmt3" id="idm45310222648992"/><a data-type="indexterm" data-startref="ch03-vermgmt4" id="idm45310222648320"/></p>

<pre data-type="programlisting">docker pull artifactory-prod.jfrog.io/openjdk:11</pre>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Image and Container Layers"><div class="sect2" id="idm45310222646672">
<h2>Image and Container Layers</h2>

<p>To build efficient containers, having a thorough understanding<a data-type="indexterm" data-primary="container images" data-secondary="layers" id="ch03-layr"/><a data-type="indexterm" data-primary="containers" data-secondary="layers" id="ch03-layr2"/><a data-type="indexterm" data-primary="layers in containers and images" id="ch03-layr3"/><a data-type="indexterm" data-primary="image layer" data-secondary="layers in containers and images" id="ch03-layr4"/> of layers is essential. The details behind how you build the source of your containers—your container <em>images</em>—greatly impact their size and performance, and some approaches have security implications, making this concept even more important to master.</p>

<p>Basically, Docker images are built by establishing a base layer and then subsequently making small changes until you arrive at your desired final state. Each layer represents a set of changes including, but not limited to, the creation of users and related permissions, modifications to configuration or application settings, and updates to existing packages or adding/removing packages. These changes all amount to additions, modifications, or the removal of sets of files in the resulting filesystem. Layers are stacked on top of each other, each one being a delta of the changes from the previous layer, and each one identified by a SHA-256 hash digest of its contents. As discussed in <a data-type="xref" href="#docker_on_your_machine">“Docker on Your Machine”</a>, these layers are stored within the root Docker directory.</p>










<section data-type="sect3" data-pdf-bookmark="Visualizing layers"><div class="sect3" id="idm45310222638128">
<h3>Visualizing layers</h3>

<p>One good way to really visualize layers is to<a data-type="indexterm" data-primary="dive tool for layer visualization" id="idm45310222636528"/><a data-type="indexterm" data-primary="resources for learning" data-secondary="layer visualization tool" id="idm45310222635856"/><a data-type="indexterm" data-primary="container images" data-secondary="layers" data-tertiary="visualization tool" id="idm45310222634896"/><a data-type="indexterm" data-primary="containers" data-secondary="layers" data-tertiary="visualization tool" id="idm45310222633680"/><a data-type="indexterm" data-primary="image layer" data-secondary="visualization tool" id="idm45310222632464"/><a data-type="indexterm" data-primary="layers in containers and images" data-secondary="visualization tool" id="idm45310222631520"/> use the command-line tool <code>dive</code>, available on <a href="https://oreil.ly/M2ZBZ">GitHub</a>. <a data-type="xref" href="#dive-openjdk-screenshot">Figure 3-5</a> shows a screenshot of the tool in action using the official latest <em>openjdk</em> image pulled from Docker Hub. The left pane displays details about the three layers that compose the <em>openjdk</em> image. The right pane highlights the changes each layer applies to the filesystem of the image.</p>

<figure><div id="dive-openjdk-screenshot" class="figure">
<img src="Images/dtjd_0305.png" alt="Dive OpenJDK" width="600" height="388"/>
<h6><span class="label">Figure 3-5. </span><code>dive</code> with openjdk</h6>
</div></figure>

<p>The <code>dive</code> tool is useful in showing you what the filesystem would look like if you were to launch a container based on the <em>openjdk</em> image. As you move through each subsequent layer, you can see the changes made to the initial filesystem. The most important part to convey here is that subsequent layers may obfuscate parts of the filesystem of the previous layer (in the case of any moves or deletions of files), but the original layer still exists in its original form.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Leveraging layer cache"><div class="sect3" id="idm45310222623280">
<h3>Leveraging layer cache</h3>

<p>Utilizing image layers speeds up image requests, <a data-type="indexterm" data-primary="container images" data-secondary="layers" data-tertiary="leveraging layer cache" id="idm45310222621712"/><a data-type="indexterm" data-primary="containers" data-secondary="layers" data-tertiary="leveraging layer cache" id="idm45310222620464"/><a data-type="indexterm" data-primary="image layer" data-secondary="leveraging layer cache" id="idm45310222619248"/><a data-type="indexterm" data-primary="layers in containers and images" data-secondary="leveraging layer cache" id="idm45310222618304"/><a data-type="indexterm" data-primary="storage of images via layers" id="idm45310222617392"/>builds, and pushes. It’s a clever way to decrease the amount of storage required for images. This strategy allows for identical image layers to be shared across multiple images, and reduces the amount of time and bandwidth needed for pulling or pushing images that are already cached locally or stored in the registry.</p>

<p>If you’re using Docker, your system will keep an internal cache of all the images you’ve either requested from external registries or built yourself. When new images are pushed and pulled, comparisons of each of the image layers are made between your local cache and the registry, and decisions are made about whether to push or pull individual layers, increasing efficiency.</p>

<p>Anyone who has ever struggled with their internal Maven repository (haven’t we all at some point?), or with any caching mechanism for that matter, is very aware that the efficiency and performance improvements internal cache provides also come with caveats. Sometimes what you have stored in cache is <em>not</em> what you intended to use. Using stale cache can easily happen in active development and local testing if you aren’t mindful of how and when your local image cache is used.</p>

<p>For example, the commands <code>docker run openjdk</code> and <code>docker pull openjdk</code> behave differently where cache is concerned. The former searches for the specified image in your local cache with the tag <code>latest</code>. If the image exists, the search will be considered satisfied, and a new container based on the cached image will be launched. The latter command will go a step further and update the <em>openjdk</em> image on your system if an update exists in the remote registry it was retrieved from.</p>

<p>Another common mistake is assuming a command in a Dockerfile will run again when an image is rebuilt. This is common with <code>RUN</code> commands such as <code>RUN apt-get update</code>. If this line in the Dockerfile doesn’t change at all, as it would if you were to specify package names along with specific versions, then the initial layer built with this command will live in your cache. It will not get built again. This is not a bug, but a feature of the cache to speed up build processes. If a layer is determined to be already built, the layer will not be built again.</p>

<p>In an attempt to avoid stale cache, you might be tempted to combine commands on one line (producing one layer) in a Dockerfile in order for changes to be more easily recognized and acted on more frequently. The problem with this approach is that by squashing too much into a single layer, you lose the benefit of the cache altogether.</p>
<div data-type="tip"><h6>Tip</h6>
<p>As a developer, be conscientious regarding your local cache. And beyond local development, consider how your continuous integration, build servers, and automated integration testing is using cache. Ensuring that all systems are consistent in this way will help keep you from chasing unexplained and intermittent failures.<a data-type="indexterm" data-startref="ch03-layr" id="idm45310222608368"/><a data-type="indexterm" data-startref="ch03-layr2" id="idm45310222607664"/><a data-type="indexterm" data-startref="ch03-layr3" id="idm45310222606992"/><a data-type="indexterm" data-startref="ch03-layr4" id="idm45310222606320"/></p>
</div>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Best Image Build Practices and Container Gotchas"><div class="sect1" id="idm45310222605264">
<h1>Best Image Build Practices and Container Gotchas</h1>

<p>After some time spent building and playing with images, you’re going to discover that you can shoot yourself in the foot in a lot of places in even the most basic build process. The following is a set of practices to keep in mind as you start on your image-building journey. You will discover more, but these are the most important.</p>








<section data-type="sect2" data-pdf-bookmark="Respect the Docker Context and .dockerignore File"><div class="sect2" id="idm45310222603280">
<h2>Respect the Docker Context and .dockerignore File</h2>

<p>You don’t want to have certain things<a data-type="indexterm" data-primary="container images" data-secondary="best practices" data-tertiary="respect context and .dockerignore" id="idm45310224168336"/><a data-type="indexterm" data-primary=".dockerignore file" data-primary-sortas="dockerignore" id="idm45310224167120"/> in your production Docker image—things like your development environment configuration, keys, your <em>.git</em> directory, or other sensitive hidden directories. When you run the command to build a Docker image, you provide the <em>context</em>, or the location of files you want to make available to the build process.</p>

<p>The following is a contrived Dockerfile example:</p>

<pre data-type="programlisting">FROM ubuntu

WORKDIR /myapp

COPY . /myapp

EXPOSE 8080

ENTRYPOINT ["start.sh"]</pre>

<p>See the <code>COPY</code> instruction? Depending on what you sent in as the context, this could be problematic. It could be copying <em>everything</em> from your working directory into the Docker image you build, which will end up in any container launched from this image.</p>

<p>Make sure to use a <em>.dockerignore</em> file to exclude files from the context that you don’t want showing up unintentionally. You can use it to avoid accidentally adding any user-specific files or secrets that you might have stored locally. In fact, you can greatly reduce the size of the context (and the time it takes to build) by excluding anything the build doesn’t require access to:</p>

<pre data-type="programlisting"># Ignore these files in my project
**/*.md
!README.md
passwords.txt
.git
logs/
*/temp
**/test/</pre>

<p>The <em>.dockerignore</em> matching format follows <a href="https://oreil.ly/sCjIv">Go’s <code>filepath.Match</code> rules</a>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Use Trusted Base Images"><div class="sect2" id="idm45310224157936">
<h2>Use Trusted Base Images</h2>

<p>Whether you choose to use images that include<a data-type="indexterm" data-primary="container images" data-secondary="best practices" data-tertiary="use trusted base images" id="idm45310224156336"/><a data-type="indexterm" data-primary="container images" data-secondary="base image" data-tertiary="use trusted base images" id="idm45310224155088"/><a data-type="indexterm" data-primary="base image" data-secondary="use trusted base images" id="idm45310224153872"/><a data-type="indexterm" data-primary="Docker Hub" data-secondary="use trusted base images" id="idm45310224152928"/> OpenJDK, Oracle JDK, GraalVM, or other images that include a web server or a database, make sure you are using trusted images as parent images, or creating your own from scratch.</p>

<p>Docker Hub proclaims to be the world’s largest library for publicly available container images, with over 100,000 images from software vendors, open source projects, and the community. <em>Not all of these images should be trusted to use as base images.</em></p>

<p class="pagebreak-before">Docker Hub includes a set of curated images labeled “Docker Official Images” that are suitable for use as base images (note that distribution of these requires an agreement with Docker). These details are from the <a href="https://oreil.ly/TO8Po">online Docker docs on official images</a>:</p>
<blockquote>
<p>Docker, Inc. sponsors a dedicated team that is responsible for reviewing and publishing all content in the Docker Official Images. This team works in collaboration with upstream software maintainers, security experts, and the broader Docker community.</p>
</blockquote>

<p>As important as understanding what Java dependencies are brought into your project and the depth of your dependency tree, so is understanding what your base image is bringing in under that one little <code>FROM</code> line at the top of your Dockerfile. The inheritance structure of Dockerfiles can easily obfuscate how much your base image is dragging along with it in the form of additional libraries and packages you don’t need or possibly even malevolent content.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Specify Package Versions and Keep Up with Updates"><div class="sect2" id="idm45310224146528">
<h2>Specify Package Versions and Keep Up with Updates</h2>

<p>Given the caveats discussed earlier about<a data-type="indexterm" data-primary="container images" data-secondary="best practices" data-tertiary="package versions and updates" id="idm45310224145104"/> caching on top of the desire to maintain repeatable builds, specify versions in your Dockerfile just as you would in your Java project. Avoid broken builds and unexpected behavior from new versions or unexpected updates.</p>

<p>That said, it’s easy to get complacent with updating versions if they never force you to look at them because of a failed build or test. Regularly audit your project for needed updates and make these updates intentional. This should be part of your regular project planning. I advise that this activity be separate from any other feature development or bug fixing in order to eliminate unrelated moving parts in your development lifecycle.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Keep Your Images Small"><div class="sect2" id="idm45310224142368">
<h2>Keep Your Images Small</h2>

<p>It is easy for images to become very large, very fast.<a data-type="indexterm" data-primary="container images" data-secondary="best practices" data-tertiary="keep images small" id="idm45310224140800"/><a data-type="indexterm" data-primary="container images" data-secondary="minimal image size" id="idm45310224139552"/><a data-type="indexterm" data-primary="attack surface" data-secondary="minimal base images" id="idm45310224138608"/> Monitor size increases in your automated builds and set up notifications for unusual size changes. Gluttonous disk storage packages can easily sneak in via updates to a base image or be unintentionally included in a <code>COPY</code> statement.</p>

<p>Utilize multistage builds to keep your images small. A multistage build can be set up by creating a Dockerfile that uses multiple <code>FROM</code> statements, which begin a build stage with a different base image. By using multistage builds, you can avoid including things like build tools or package managers that are not needed (and really should not be included) in a production image. For example, the following Dockerfile shows a two-stage build. The first uses a base image that includes Maven. After the Maven build is complete, the required JAR file is copied to the second stage, which uses an image that does <em>not</em> include Maven:</p>

<pre data-type="programlisting">###################
# First build stage
###################

FROM maven:3.8.4-openjdk-11-slim as build

COPY .mvn .mvn
COPY mvnw .
COPY pom.xml .
COPY src src

RUN ./mvnw package

####################
# Second build stage
####################

FROM openjdk:11-jre-slim-buster

COPY --from=build target/my-project-1.0.jar .

EXPOSE 8080

ENTRYPOINT ["java", "-jar", "my-project-1.0.jar"]</pre>

<p>This is also a good way to implement the use of a custom <em>distroless</em> image, an image that has been stripped of everything (including a shell) but the absolute essentials for running your application.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Beware of External Resources"><div class="sect2" id="idm45310224132688">
<h2>Beware of External Resources</h2>

<p>I have often seen requests to external resources<a data-type="indexterm" data-primary="container images" data-secondary="best practices" data-tertiary="beware of external resources" id="idm45310224131328"/><a data-type="indexterm" data-primary="Dockerfile" data-secondary="beware of external resources" id="idm45310224130064"/> within Dockerfiles in the form of <code>wget</code> commands for installation of proprietary software or even external requests for shell scripts that perform a custom installation. These terrify me. More than general suspicion and paranoia are involved here. Even if the external resource is trusted, the more you relinquish control to parts of your build to external parties, the more likely you are to suffer build failures that are out of your control to fix.</p>

<p>The first response I often get when making this observation is this: “There’s nothing to worry about because once you’ve built your image, it’s cached or stored within a base image, and you won’t have to ever make the request again.”</p>

<p>This is absolutely true. Once you have your base image stored, or your image layer cached, you’re good to go. But the first time a new build node (with zero cache) is put into play, or even when a new developer joins your team, building that image might fail. When you need to build a new version of the base image, your build might fail. Why? Because time and time again, external managers of resources will move them, restrict access to them, or simply <em>dispose of them</em>.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Protect Your Secrets"><div class="sect2" id="idm45310224126400">
<h2>Protect Your Secrets</h2>

<p>I include this because in addition to not moving<a data-type="indexterm" data-primary="container images" data-secondary="best practices" data-tertiary="protect your secrets" id="idm45310224124976"/><a data-type="indexterm" data-primary="secrets protected" id="idm45310224123728"/> secrets into your image in the first place, don’t think that using a command in a Dockerfile to remove them from a base image or any other previous layer is good enough. I’ve seen this before as a hack to “fix” a base layer that couldn’t be rebuilt right away.</p>

<p>Now that you know how layering works, you know that a subsequent layer deleting items does not actually remove them from the underlying layer. You can’t see them if you were to <code>exec</code> into a running container based on that image, but they are still there. They exist on the system the image is stored on, they exist anywhere a container based on that image is launched, and they also exist in the image registry you’ve chosen for long-term storage. This is close to the equivalent of checking your passwords into source control. Do not put secrets into images to begin with.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Know Your Outputs"><div class="sect2" id="idm45310224120976">
<h2>Know Your Outputs</h2>

<p>Numerous factors can cause a container to continuously<a data-type="indexterm" data-primary="container images" data-secondary="best practices" data-tertiary="know your outputs" id="idm45310224119584"/><a data-type="indexterm" data-primary="log file outputs" id="idm45310224118336"/><a data-type="indexterm" data-primary="logging" data-secondary="know your outputs" id="idm45310224117664"/> grow while it’s running. One of the most common is not dealing with log files appropriately. Ensure that your application is logging to a volume where you can implement a log-rotating solution. Given the ephemeral nature of containers, it doesn’t make sense to keep logs that you would use for troubleshooting or for compliance stored within the container (on the Docker host).</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm45310224115936">
<h1>Summary</h1>

<p>Much of this chapter was about exploring Docker. And this is an excellent place to start. Once you are comfortable with images and containers, you can branch out to other tools available in the ecosystem. Depending on the operating system and build utilities you’ve chosen for your project, tools such as <a href="https://buildah.io">Buildah</a>, <a href="https://podman.io">Podman</a>, or <a href="https://bazel.build">Bazel</a> might work well for you. You might also choose to use a Maven plug-in such as <a href="https://oreil.ly/pwGsw">Jib</a> to build your container image.</p>

<p>One word of caution: whichever tool you choose, understand how your images and containers are built so you don’t suffer the consequences of bulky and/or insecure images and containers when you are ready to deploy.</p>
</div></section>







</div></section></div></body></html>