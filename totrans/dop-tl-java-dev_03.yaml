- en: Chapter 3\. An Introduction to Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Melissa McKay
  prefs: []
  type: TYPE_NORMAL
- en: Any fool can know. The point is to understand.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Albert Einstein
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you know the why, you can live any how.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Friedrich Nietzsche
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: At the time of this writing, the use of containers in production and other environments
    is growing exponentially, and best practices around containerizing applications
    are still being discussed and defined. As we home in on efficiency improvements
    and consider specific use cases, techniques and patterns have evolved that come
    highly recommended by the blogosphere and professional practitioners through experience.
    And as expected, a fair share of patterns and common uses have evolved, as well
    as antipatterns that I hope this chapter will help you recognize and avoid.
  prefs: []
  type: TYPE_NORMAL
- en: My own trial-and-error introduction to containers felt like stirring up a hornet’s
    nest (oh, the stings!). I was undeniably unprepared. Containerization on the surface
    is deceptively simple. Knowing what I know now about how to develop and deploy
    with containers, especially within the Java ecosystem, I hope to pass this knowledge
    on in a way that will help prevent similar pain for you. This chapter outlines
    the essential concepts you will need to successfully containerize your applications
    and discusses *why* you would even want to do such a thing.
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 4](ch04.xhtml#dissecting_the_monolith) discusses the bigger picture
    of microservices, but here we will start with learning about one of the basic
    building blocks of microservice deployments that you will no doubt encounter if
    you haven’t already: the container. Note that the concept of microservices, an
    architectural concern, *does not imply the use of containers*; rather, it’s the
    concern of *deploying* these services, especially in a cloud native environment,
    that usually begins the conversation around containerization.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with considering *why* we would use a container. The best way to
    do that is to back up and get some context on how we got here to begin with. Patience
    is a virtue. If you persevere, going through this history lesson will also naturally
    lead you to a clearer understanding of *what* a container actually is.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I’m certain I’m not alone in experiencing the company of an “elephant in the
    room.” Despite the looming frame, deafening noise, and potential for dangerous
    consequences when ignored, this elephant-sized subject is just allowed to roam,
    unchallenged. I’ve witnessed it. I’m guilty of it. I’ve even had the distinct
    pleasure of *being* said elephant.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of containerization, I’m going to make the argument that we
    need to address *two* elephants in the room—in the form of two questions: *What
    is a container?* and *Why would we use a container?* Those sound simple enough.
    How could anyone miss these basic starting points?'
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps it’s because the microservice movement tends to lead into discussions
    about deploying containers more now than ever, and we’re suffering from the fear
    of missing out. Maybe it’s because a container implementation is expected by default
    with the exceedingly popular Kubernetes ride, and “our K8s cluster” is the cool
    new phrase to include in our conversations. It might even just be that we are
    suffering such an onslaught of new technologies and tools in the DevOps ecosystem
    that, as a developer (a Java developer, no less), if we stop to ask questions,
    we fear getting left behind. Whatever the reasons may be, before we can even get
    into the details of how to build and use containers, these *what* and *why* questions
    must be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: 'I’m deeply grateful for the incredible colleagues and mentors I’ve had the
    privilege of working with over the years. I frequently recall, from the formative
    years of my career, sage advice that has become a mantra of mine. It’s simple;
    always begin and then proceed working on any project with a constant, repeating
    question in mind: *What is the problem you are trying to solve?* The success of
    your solution will be measured by how well it meets this requirement—that it indeed
    solves the original problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Carefully consider whether you are solving the right problem to begin with.
    Be especially vigilant to reject problem statements that are actually implementation
    instructions in disguise, like this one: *Improve the performance of your application
    by breaking it into containerized microservices*. You will be better served by
    a problem statement like this: *To decrease the time it takes for customers to
    complete their objectives, improve the performance of the application by 5%*.
    Note that the latter statement includes a tangible metric to gauge success and
    is not restricted to a microservices implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: This same principle applies to your day-to-day choices in what tools you use,
    what frameworks and languages you choose to code within, how you choose to design
    a system, and even how you package and deploy your software to production. What
    problem are you solving with the choices you’ve made? And how do you know if you’ve
    chosen the best tool for the job? One way is to understand the problem the particular
    tool under review is intended to solve. And the best way to do that is to look
    at its history. This practice should be in place for every tool you pick up to
    use. I guarantee that you will make better decisions knowing its history, and
    you will benefit from skirting known pitfalls or, at the very least, have some
    justification for accepting any disadvantages and moving forward anyway.
  prefs: []
  type: TYPE_NORMAL
- en: My plan is not to completely bore you with historical details, but you should
    know some basic information and important milestones before jumping into containerizing
    every bit of code put in front of you. By understanding more about the original
    problem and the solutions that have come out of it, you’ll be able to intelligently
    explain why you are choosing to deploy with containers.
  prefs: []
  type: TYPE_NORMAL
- en: I don’t want to go all the way back to the Big Bang, but I’m going to go back
    more than 50 years, mostly to make the point that virtualization and containerization
    are not new. In fact, this concept has been worked on and improved for more than
    half a century. I’ve picked out some points to highlight that will bring us up
    to speed quickly. This is not intended to be a deep technical manual on any of
    the topics mentioned—rather, just enough material to wrap your mind around the
    progress that has been made over time and how we’ve ended up where we are today.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin.
  prefs: []
  type: TYPE_NORMAL
- en: The History of Containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the 1960s and ’70s, computing resources were in general exceptionally limited
    and expensive (by today’s standards). It took a long time for processes to complete
    (again, by today’s standards), and it was common for a computer to be dedicated
    for a long period of time to a single task for a single user. Efforts were begun
    to improve the sharing of compute resources and address the bottlenecks and inefficiency
    brought by these limitations. But just being able to share resources was not enough.
    A need arose for a method of sharing resources without getting in each other’s
    way or having one person inadvertently cause an entire system to crash for everyone.
    Both hardware and software that advanced virtualization technology started to
    trickle in. One development in software is `chroot`, which is where we’ll begin.
  prefs: []
  type: TYPE_NORMAL
- en: In 1979, during the development of the seventh edition of Unix, `chroot` was
    developed and then in 1982 was added to the Berkeley Software Distribution (BSD).
    This system command changed the apparent root directory for a process and its
    children, which resulted in a limited view of the filesystem in order to provide
    an environment for testing a different distribution, for example. Although a step
    in the right direction, `chroot` was just a start on the path to providing the
    isolation of applications required from us today. In 2000, FreeBSD expanded the
    concept and introduced the more sophisticated `jail` command and utility in FreeBSD
    4.0\. Its features (improved in the later 5.1 and 7.2 releases) help further isolate
    filesystems, users, and networks, and include the ability to assign an IP address
    to each `jail`.
  prefs: []
  type: TYPE_NORMAL
- en: In 2004, Solaris containers and zones brought us ahead even further by giving
    an application full user, process, and filesystem space and access to system hardware.
    Google jumped in with its *process containers* in 2006, later renamed *cgroups*,
    which centered around isolating and limiting the resource usage of a process.
    In 2008, *cgroups* were merged into the Linux kernel, which, along with Linux
    namespaces, led to IBM’s development of Linux Containers (LXC).
  prefs: []
  type: TYPE_NORMAL
- en: Now things get even more interesting. Docker became open source in 2013\. That
    same year, Google offered its Let Me Contain That For You (lmctfy) open source
    project, which gave applications the ability to create and manage their own subcontainers.
    And from there, we saw the use of containers explode—Docker containers specifically.
    Initially, Docker used LXC as its default execution environment, but in 2014 Docker
    chose to swap out its use of the LXC toolset for launching containers with *libcontainer*,
    a native solution written in Go. Soon after, the lmctfy project ceased active
    development with the intention of joining forces and migrating the core concepts
    to the libcontainer project.
  prefs: []
  type: TYPE_NORMAL
- en: A lot more happened during this period of time. I’m intentionally skipping over
    additional details about other projects, organizations, and specifications that
    were developed because I want to get to a specific event in 2015\. This event
    is especially important because it will give you some insight into some of the
    activity and motivations behind shifts in the market, especially concerning Docker.
  prefs: []
  type: TYPE_NORMAL
- en: On June 22, 2015, the establishment of the [Open Container Initiative (OCI)](https://oreil.ly/Vsr6U)
    was announced. This is an organization under the [Linux Foundation](https://oreil.ly/J5ioU)
    with the goal of creating open standards for container runtimes and image specification.
    Docker is a heavy contributor, but Docker’s announcement of this new organization
    listed participants including Apcera, Amazon Web Services (AWS), Cisco, CoreOS,
    EMC, Fujitsu, Google, Goldman Sachs, HP, Huawei Technologies, IBM, Intel, Joyent,
    Pivotal Software, the Linux Foundation, Mesosphere, Microsoft, Rancher Labs, Red
    Hat, and VMware. Clearly, the development of containers and the ecosystem around
    them has reached a significant point to glean this much attention, and has evolved
    to where establishing some common ground will be beneficial to all parties involved.
  prefs: []
  type: TYPE_NORMAL
- en: When the formation of the OCI was announced, Docker also announced its intention
    to donate its base container format and runtime, runC. In quick succession, *runC*
    became the reference implementation for the [OCI Runtime Specification](https://oreil.ly/lLia7),
    and the Docker v2 Schema 2 image format, donated in April 2016, became the basis
    for the [OCI Image Format Specification](https://oreil.ly/mmPu4). [Version 1.0
    of these specifications](https://oreil.ly/y6QwF) were both released in July 2017.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*runC* is a repackage of libcontainer, which meets the requirements of the
    OCI runtime specification. In fact, as of this writing, the [source code for runC](https://oreil.ly/hbUaP)
    contains a directory called *libcontainer*.'
  prefs: []
  type: TYPE_NORMAL
- en: In tandem with developments in the container ecosystem, orchestration of these
    systems was also under rapid development. On July 21, 2015, one month after the
    OCI was established, Google released Kubernetes v1.0\. Along with this release,
    the [Cloud Native Computing Foundation (CNCF)](https://www.cncf.io) was established
    in partnership with Google and the Linux Foundation. Another important step taken
    by Google and released with v1.5 of Kubernetes in December 2016 was the development
    of the Container Runtime Interface (CRI), which created the level of abstraction
    needed to allow the Kubernetes machine daemon, *kubelet*, to support alternative
    low-level container runtimes. In March 2017, Docker, also a member of the CNCF,
    contributed its CRI-compatible runtime *containerd* that it had developed in order
    to integrate runC into Docker v1.11.
  prefs: []
  type: TYPE_NORMAL
- en: In February 2021, Docker donated yet another reference implementation to the
    CNCF. This contribution was centered around the distribution of images (pushing
    and pulling container images). Three months later, in May 2021, the OCI released
    version 1.0 of the [OCI Distribution Spec](https://oreil.ly/JfGvb) based on the
    Docker Registry HTTP API V2 protocol.
  prefs: []
  type: TYPE_NORMAL
- en: Today, the use of containers and orchestration systems like Kubernetes is typical
    fare for cloud native deployments. Containers are an important factor in keeping
    deployments flexible among a variety of hosts and play a huge role in scaling
    distributed applications. Cloud providers including AWS, Google Cloud, Microsoft
    Azure, and others are continuously bulking up their offerings using shared infrastructure
    and pay-per-use storage.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations for getting through that bit of history! In a few paragraphs,
    we spanned more than 50 years of development and advancement. You were introduced
    to a lot of the projects that have evolved into our solutions as well as some
    of the common terms used in the context of containers and their deployment. You’ve
    also learned how much Docker has contributed to the state of containers today—which
    makes this a perfect time to get a solid understanding of the container ecosystem,
    the technical details behind containers, and the implementation components that
    come into play.
  prefs: []
  type: TYPE_NORMAL
- en: But wait! Before we dive into that, let’s discuss that second elephant. You
    learned a lot about *what* happened, but *why* did the industry shift in this
    way?
  prefs: []
  type: TYPE_NORMAL
- en: Why Containers?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Knowing what containers are and how to describe them is not enough. To talk
    intelligently about them, you should have some understanding of *why* they are
    used. What are the advantages of using containers? Some of this may seem obvious,
    given what you now know about containers and their history, but it’s worth going
    in depth before jumping into the fray. Project changes and any introduction of
    a new tech stack should always be intentional with a thoughtful cost-benefit analysis.
    Following the crowd is not a good enough reason in and of itself.
  prefs: []
  type: TYPE_NORMAL
- en: Your first question is likely along the lines of *why are containers a developer’s
    concern?*—a valid question, indeed. If containers are simply a method of deployment,
    it seems that this should be in the wheelhouse of operations. It is here that
    we approach a blurry line between development and operations, an argument for
    a DevOps mindset. Packaging your app into a container involves more thought and
    foresight from the developer’s perspective than you may initially think. After
    you’ve learned some of the best practices and some of the problems encountered
    by others’ experience, you will find yourself considering the packaging *while*
    developing your application. Certain aspects of the process will drive the decisions
    you make about how your application or service uses memory, how it uses the filesystem,
    how you plug in observability hooks, how you allow for different configurations,
    and how you communicate with other services (such as databases). These are just
    a few examples. Ultimately, it will depend on how your team is organized, but
    on a DevOps team, I would expect that as a developer, knowing how to build and
    maintain container images and to understand the container environment will be
    valuable.
  prefs: []
  type: TYPE_NORMAL
- en: 'I recently had the opportunity to be part of a panel discussion for the Cloud
    and DevOps international track at The Developer’s Conference titled “Cloud Efficiency
    and Simplicity: What Will the Future Bring?” As part of this discussion, we talked
    about the current state of technologies available and where we would expect more
    simplification. I introduced the following question/analogy to the discussion:
    *How many of us would be driving cars today if we were expected to build our own?*
    We are still in very early stages of so many technologies in this area. The market
    is ripe for manufacturers of full-featured products that allow our software and
    services to take full advantage of the scalability, availability, and resilience
    that the cloud has to offer, packaged in a way that reduces complexity. However,
    we are still in the middle of designing the individual pieces and parts that would
    be used to build something like this.'
  prefs: []
  type: TYPE_NORMAL
- en: Containers are a huge step in this direction, providing a useful level of abstraction
    between the packaging of an application and the infrastructure where it will be
    deployed. I anticipate a time when developers will no longer need to be involved
    in the details at the level of containers, but for now, *we should be*. At the
    very least, we should have a seat at the table to make sure that development concerns
    are addressed moving forward. To that end, and to satisfy any remaining doubts
    about why you should even broach the subject of containers, let’s learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Think about all it takes to package, deploy, and run your Java application.
    To begin development, you install a particular version of the Java Development
    Kit (JDK) to your development machine. Then you might install a dependency manager
    such as Apache Maven or Gradle to pull in all of the needed third-party libraries
    you choose to use in your app and package it up into a WAR or a JAR file. At this
    point, it might be ready to deploy… *somewhere*.
  prefs: []
  type: TYPE_NORMAL
- en: And here the problems begin. What is installed on the production server—what
    version of the Java runtime, what application server (for example, JBoss, Apache
    Tomcat, WildFly)? Are other processes running on the production server that might
    interfere with your application’s performance? Does your application require root
    access for any reason, and is your application user set up appropriately with
    the correct permissions? Does your app require access to external services like
    a database or APIs for alive or well checks? Before any of these questions can
    be answered, do you even have access to a dedicated production server to begin
    with, or do you need to begin the process of requesting one to be provisioned
    for your application? And then what happens when your application is strained
    with heavy activity—are you able to scale quickly and automatically, or must you
    begin the provisioning process all over again?
  prefs: []
  type: TYPE_NORMAL
- en: Given these issues, it’s easy to see why virtualization using virtual machines
    (VMs) became such an attractive option. VMs provide more flexibility when it comes
    to isolating application processes, and the ability to snapshot a VM can provide
    consistency in deployments. However, VM images are large and not easy to move
    around because they include an entire OS, which contributes to their overall bulk.
  prefs: []
  type: TYPE_NORMAL
- en: More than a few times when first introducing fellow developers to containers,
    I’ve gotten the response, “Oh! So a container is like a VM?” While it’s convenient
    to think of *containers* as analogous to VMs, an important distinction exists.
    VMs (VMware vSphere, Microsoft Hyper-V, and others) are an abstraction of the
    hardware, emulating a complete server. In a sense, the entire operating system
    is included in a VM. VMs are managed by a software layer called a *hypervisor*,
    which divides and allocates the host’s resources to the VMs as required.
  prefs: []
  type: TYPE_NORMAL
- en: Containers, on the other hand, are not as heavy as a traditional VM. Rather
    than include an entire OS, a Linux container, for example, can be thought of as
    a Linux distribution that shares the host operating system. As shown in [Figure 3-1](#vm_vs_container),
    VMs and containers are different levels of abstraction, as is the Java Virtual
    Machine (JVM).
  prefs: []
  type: TYPE_NORMAL
- en: Where does the JVM fit in all of this? It gets confusing when terms like *virtual
    machine* are overloaded. The JVM is a completely different abstraction altogether
    and is a *process* virtual machine as opposed to a *system* virtual machine. Its
    primary concern is to provide the Java Runtime Environment (or JRE, the implementation
    of the JVM) for a Java application. The JVM virtualizes the host’s processor(s)
    for the purpose of executing Java bytecode.
  prefs: []
  type: TYPE_NORMAL
- en: '![dtjd 0301](Images/dtjd_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. VMs versus containers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Containers are a lightweight solution that promises to solve most of the issues
    around application consistency, process isolation, and OS-level dependencies.
    This method of packaging a service or application can utilize caching mechanisms
    that drastically reduce the time it takes to get an application deployed and up
    and running. Rather than having to wait for custom provisioning and setup, containers
    can be deployed to existing infrastructure—whether that’s an available dedicated
    server, an existing VM on premises in a private data center, or cloud resources.
  prefs: []
  type: TYPE_NORMAL
- en: Even if you choose not to utilize containers in production, you are well advised
    to consider a couple of other use cases around development and test environments.
  prefs: []
  type: TYPE_NORMAL
- en: A big challenge in onboarding a new developer to a team is the time spent setting
    up their local development environment. It is generally understood that it’s going
    to take some time to get a developer to the point where they can contribute their
    first bug fix or improvement. While some companies dictate the development tools
    (consistency is often believed to improve support efforts and therefore efficiency),
    developers have more choices today than ever. I’m of the opinion that forcing
    a specific toolset on developers when they are already accustomed to something
    different actually has the opposite effect. Frankly, in many cases, it simply
    just isn’t necessary anymore—especially now that we can utilize containers.
  prefs: []
  type: TYPE_NORMAL
- en: Containers help keep the runtime environment consistent, and when configured
    correctly, can easily be launched in dev, test, or production modes. The risk
    of your service or application behaving differently in these environments because
    of a missing dependency is greatly reduced since the environment is shipped along
    with your application in a container image.
  prefs: []
  type: TYPE_NORMAL
- en: This portability improves a developer’s ability to sanity-test changes in a
    local environment as well as the ability to deploy the same version of the code
    that’s in production in order to reproduce a bug. Integration testing with containers
    also comes with the added benefit of being able to reproduce as close as possible
    a production environment. For example, instead of using an in-memory database
    for integration tests, you can now launch containers that match the version of
    the database used in production. Using a project like TestContainers for this
    purpose will prevent irregularities in behavior due to slightly different SQL
    syntax or other differences between database software versions. Using containers
    in this way improves efficiency by circumventing the complications of installing
    new software or multiple versions of the same software to your local machine.
  prefs: []
  type: TYPE_NORMAL
- en: If we’ve learned anything about containers thus far, it is that they are likely
    here to stay in one form or another. This section began with an illustration of
    the exponential increase in container usage over the last several years, and the
    toolsets being continuously developed and improved around the container ecosystem
    have gained a solid foothold in both development and operations processes. Apart
    from a huge, and as of yet unknown, advancement in a completely different direction
    (remember, containers have over 50 years of history behind them), you are well
    advised to learn about the container ecosystem and how to exploit this technology
    to your full advantage.
  prefs: []
  type: TYPE_NORMAL
- en: Intro to Container Anatomy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My first experience with containers as a developer was via a project, developed
    by a third-party contractor, that my team was now responsible to further develop
    and maintain. Aside from bringing the initial codebase into our internal GitHub
    organization, a lot of setup needed to happen to establish our internal DevOps
    environment around the project—setting up our continuous integration and deployment
    (CI/CD) pipeline as well as our development and test environments, and, of course,
    our deployment process.
  prefs: []
  type: TYPE_NORMAL
- en: I compare this experience to clearing my desk (even more so after days of neglect).
    I’m about to reveal entirely too much about my personal habits here, but it’s
    worth doing to make this point. The most time-consuming bit of clearing my desk
    is a stack of papers and mail that invariably grows to the point of falling over.
    It’s terribly convenient to rush into the house with these items and, because
    of other urgent tasks on my mind, set them down on the kitchen counter…frequently
    on top of an existing stack of papers, with the promise that I’ll get to it later.
    The problem is, I never know what’s going to be in there. The stack could contain
    bills that need to be paid, important papers that need filing, or invites or letters
    that need responding to and thought put toward scheduling on our family calendar.
    I often dread the amount of time I anticipate it will take to get through it,
    which only leads to a larger stack of neglected correspondence.
  prefs: []
  type: TYPE_NORMAL
- en: For the project my team was responsible for, my first step was to metaphorically
    clear the desk. The Dockerfile that I found in the source code was the equivalent
    of tackling that dreaded stack of papers. Although getting through it and learning
    the concepts was necessary, I felt like I was getting derailed from the task at
    hand. Learning a new technology when starting a new project sometimes doesn’t
    get the amount of time it should be allotted during project planning, even though
    it adds variables and inherent risk to the project timeline. This does *not* mean
    that new technology should never be introduced. Developers absolutely need to
    learn new things as the industry grows and changes, but it’s best to mitigate
    risk by either limiting the amount of new tech introduced to a project or being
    up-front about the variability of the timeline.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A *Dockerfile* is a text file that contains instructions providing the blueprint
    for your container. This file is typically named *Dockerfile*, and although originally
    specific to Docker, because of its wide use, other image-building tools support
    using Dockerfiles to build a container image (such as Buildah, kaniko, and BuildKit).
  prefs: []
  type: TYPE_NORMAL
- en: The information available here is not meant to be a regurgitation of documentation
    that’s already out there (for example, the online [Docker getting started guide](https://oreil.ly/Tez72)
    is exceptional). Instead, I hope to peel this onion in a way that will orient
    you on the basics and give you immediate value and enough detail to better estimate
    what it’s going to take to get your own desk cleared and ready for business. You
    now have quite a bit of information under your belt about containers and how they
    came to be. This next section covers the terminology and functionality that you
    will be exposed to as a developer.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Architecture and the Container Runtime
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like Kleenex is a brand of tissue, Docker is a *brand* of container. The
    Docker company developed an entire technology stack around containerization. So
    even though the terms *Docker container* and *Docker image* have been somewhat
    genericized, when you install something like Docker Desktop to your development
    machine, you are getting more than just the ability to run containers. You’re
    getting an entire container platform that makes building, running, and managing
    them easy and convenient for developers.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to understand that installing Docker is not required for building
    container images or running containers. It is simply a widely used and convenient
    tool for doing so. In much the same way that you can package a Java project without
    using Maven or Gradle, you can build a container image without using Docker or
    a Dockerfile. My advice to a developer new to containers would be to take advantage
    of the toolset Docker provides and then experiment with other options or methods
    to get a good feel for a comparison. Even if you choose to utilize other tools
    instead of or in addition to Docker, a lot of time and effort was spent on engineering
    a good developer experience, and this alone scores big points for including Docker
    Desktop in your development environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'With Docker, you get an isolated environment in which a user/application can
    operate, sharing the host system’s OS/kernel without interfering with the operation
    of another isolated environment on the same system (a container). Docker enables
    you to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Define a container (an image format)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build an image of a container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manage container images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distribute/share container images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a container environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launch/run a container (a container runtime)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manage the lifecycle of container instances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The container landscape contains much more than Docker, but many of the container
    toolset alternatives focus on a subset of these items. Beginning with learning
    how Docker operates is helpful in understanding and evaluating these alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: A lot of pictures and diagrams are readily available that describe the Docker
    architecture. An image search online will most likely result in a version of [Figure 3-2](#docker_architecture).
    This diagram does a fairly good job of showing how Docker works on your development
    machine—the Docker CLI is the interface available to you to send commands to the
    Docker daemon to build images, retrieve requested images from an external registry
    (by default, this is Docker Hub), manage these images in local storage, and then
    use these images to launch and run containers on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker architecture](Images/dtjd_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Docker architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'One of the more confusing concepts when first introduced to this landscape
    is the focus on one aspect of the Docker ecosystem: the *container runtime*. To
    reiterate, this is just one part of the entire tech stack Docker offers, but because
    orchestration frameworks like Kubernetes require this portion of functionality
    to launch and run containers, it is often spoken of as a separate entity from
    Docker (and in the case of alternative container runtimes, it is).'
  prefs: []
  type: TYPE_NORMAL
- en: The topic of container runtimes deserves this section all to itself, because
    it can be one of the most confusing aspects to someone new to the world of containers.
    Even more confusing is that container runtimes fall into two different categories,
    low-level or high-level, depending on what features are implemented. And just
    to keep you on your toes, some overlap can occur in that feature set.
  prefs: []
  type: TYPE_NORMAL
- en: This is a good spot to present a visual on how container runtimes fit together
    with what you’ve learned earlier about the OCI and projects like containerd and
    runC. [Figure 3-3](#container_runtimes) illustrates the relationship between older
    and newer versions of Docker, high-level and low-level runtimes, and where Kubernetes
    fits in.
  prefs: []
  type: TYPE_NORMAL
- en: '![Container runtimes](Images/dtjd_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. Runtimes in the container ecosystem
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One of the best explanations I’ve come across that really gets into the details
    of container runtimes along with a historical perspective is a [blog series](https://oreil.ly/Y2Fow)
    composed by Ian Lewis, a developer advocate on the Google Cloud Platform Team.
  prefs: []
  type: TYPE_NORMAL
- en: Prior to version 1.11 (released in 2016), Docker could be described as a monolithic
    application that wrapped up the entire feature set required of a runtime, plus
    other management tools. Docker did quite a bit of reorganizing its codebase over
    the last several years, developing abstractions and pulling out discrete functionality.
    The runC project that was contributed by Docker to the OCI came out of this effort.
    This was the first and, for some time, the *only* implementation of a low-level
    container runtime that implemented the OCI Runtime Specification.
  prefs: []
  type: TYPE_NORMAL
- en: Other runtimes are out there, and as of this writing this is an active space,
    so be sure to reference [the current list maintained by the OCI](https://oreil.ly/Vro14)
    for the most up-to-date information. Notable low-level runtime projects include
    *crun*, an implementation in C led by Red Hat; and *railcar*, an implementation
    in Rust led by Oracle, although this project is now archived.
  prefs: []
  type: TYPE_NORMAL
- en: Developing a specification is a challenging feat, and collaboration on the OCI
    Runtime Specification wasn’t any less challenging. Figuring out the boundaries—what
    *should* and what *should not* be included in the specification—took time before
    the release of version 1.0\. It’s clear, however, that just implementing the OCI
    Runtime Specification isn’t enough to drive adoption of an implementation. Additional
    features are needed to make a low-level runtime usable for developers since we
    are concerned with much more than just the launching and running of a container.
  prefs: []
  type: TYPE_NORMAL
- en: This leads us to higher-level runtimes like *containerd* and *cri-o*, the two
    primary players as of this writing that include solutions for many of the concerns
    around container orchestration, including image management and distribution. Both
    of these runtimes implement the CRI (which eases the path to a Kubernetes deployment)
    and delegate low-level container activities to OCI-compliant low-level runtimes
    (for example, runC).
  prefs: []
  type: TYPE_NORMAL
- en: Docker on Your Machine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The second most important thing to understand about containers is that they
    are not magic. Containers utilize a combination of existing Linux features (as
    covered at the beginning of this chapter). Container implementations vary in the
    details, but a container image, in a sense, is simply a tarball of a complete
    filesystem, and a running container is a Linux process that is constrained to
    provide a level of isolation from other processes running on a host. The implementation
    of a Docker container, for example, primarily involves these three ingredients:'
  prefs: []
  type: TYPE_NORMAL
- en: Namespaces
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: cgroups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A union filesystem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But what does a container look like on your local filesystem? First, let’s figure
    out where Docker is storing things on our development machine. Then let’s take
    a look at a real Docker image pulled from Docker Hub.
  prefs: []
  type: TYPE_NORMAL
- en: 'After installing Docker Desktop, running the command `docker info` from a terminal
    will provide you with detailed information about your installation. This output
    includes information about where your images and containers are stored with the
    label `Docker Root Dir`. The following example output (truncated for brevity)
    indicates that the Docker root directory is */var/lib/docker*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This result is from an existing Docker Desktop (version 3.3.3) installation
    on macOS Big Sur. A quick listing of */var/lib/docker* shows the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: According to the previous output, 5 stopped containers and 62 images are on
    this system, so how is it that this directory doesn’t exist? Is the output incorrect?
    You can check another place for the image and container storage location, as shown
    in [Figure 3-4](#docker_desktop_preferences), a screenshot of the Preferences
    section available in the Mac version of the Docker Desktop UI.
  prefs: []
  type: TYPE_NORMAL
- en: However, this location is completely different. A reasonable explanation for
    this exists, and note that depending on your operating system, your installation
    may be slightly different. The reason this matters at all is that Docker Desktop
    for Mac requires a Linux environment to run Linux containers, and to that end,
    a minimal Linux virtual machine is instantiated during installation. This means
    that the Docker root directory referred to in the earlier output is actually referencing
    a directory within this Linux VM.
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker Desktop Preferences](Images/dtjd_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Docker Desktop Preferences
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: But wait…what if you’re on Windows? Because containers are sharing the host’s
    operating system, Windows-based containers require a Windows environment to run,
    and Linux-based containers require a Linux environment. Docker Desktop (version
    3.3.3) is a marked improvement from earlier versions (a.k.a Docker Toolbox) in
    that no additional supporting software is required to run Linux-based containers.
    In the old days, to run Docker on a Mac, you would need to install something like
    VirtualBox and boot2docker to get everything up and functioning as expected. Today,
    Docker Desktop handles the necessary virtualization behind the scenes. Docker
    Desktop also supports Windows containers via Hyper-V on Windows 10 and Linux containers
    on Windows 10 via Windows Subsystem for Linux 2 (WSL 2). To run Windows containers
    on macOS, however, VirtualBox is still required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you know we need to access the Linux virtual machine to get to this
    Docker root directory, let’s pull a Docker image by using the command **`docker
    pull *IMAGE NAME*`** and see what it looks like on the filesystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The command **`docker images`** lists all of the images stored locally. You
    can see from its output that two versions of the *openjdk* image are stored. The
    one we pulled in the previous command brought in the image with the tag `latest`.
    This is the default behavior, but we could have specified a specific *openjdk*
    image version like this: **`docker pull openjdk:11-jre`**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You can learn more details about the latest *openjdk* image by running the
    `**docker inspect**` command using the *image ID*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `docker inspect` command spits out a ton of interesting information. But
    what I want to highlight here is the `GraphDriver` section, which contains the
    paths to the directories where all the layers that belong to this image live.
  prefs: []
  type: TYPE_NORMAL
- en: Docker images are composed of layers that correspond to instructions in the
    Dockerfile that was used to build the image originally. These layers translate
    into directories and can be shared across images in order to save space.
  prefs: []
  type: TYPE_NORMAL
- en: Note the `LowerDir`, `MergedDir`, and `UpperDir` sections. The `LowerDir` section
    contains all the directories, or layers, that were used to build the original
    image. These are *read-only*. The *UpperDir* directory contains all the content
    that has been modified while the container is running. If modifications are needed
    for a read-only layer in *LowerDir*, then that layer is copied into the *UpperDir*
    where it can be written to. This is called a *copy-on-write* operation.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to remember that the data in *UpperDir* is ephemeral data that
    lives only as long as the container lives. In fact, if you have data that you
    intend to keep, you should utilize the volume features of Docker and mount a location
    that will stick around even after the container dies. For example, a database-driven
    application running in a container will likely utilize a volume mounted to the
    container for the database data.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the `MergedDir` section is kind of like a virtual directory that combines
    everything from *LowerDir* and *Upper Dir*. The way the Union File System works
    is that any edited layers that were copied into *UpperDir* will overlay layers
    in *LowerDir*.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice all the references to directories within */var/lib/docker*, the Docker
    root directory. If you monitor the size of this directory, you will notice that
    the more images and containers you create and run, the storage space required
    for this directory will increase substantially over time. Consider mounting a
    dedicated drive, and make sure that you are cleaning up unused images and containers
    regularly. Also, make sure that containerized apps aren’t continuously producing
    unmanaged data files or other artifacts. For example, utilize log shipping and
    or log rotation to manage logs generated by your container and its running processes.
  prefs: []
  type: TYPE_NORMAL
- en: Any number of containers can be launched using the same image. Each container
    will be created with the image blueprint and will run independently. In the context
    of Java, think of a container image as a Java class, and a container as a Java
    object instantiated from that class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Containers can be stopped and later restarted without being re-created. To
    list containers on your system, use the `docker ps -a` command. Note that the
    `-a` flag will display both stopped containers as well as containers currently
    running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If you navigate to the Docker root directory, you will see a subdirectory named
    *containers*. Within this directory, you will find additional subdirectories named
    after the *container ID* of each container on your system. Stopped containers
    will retain their state and data in these directories so that they can be restarted
    if needed. When a container is removed using the `docker rm *CONTAINER NAME*`,
    its correlated directory will be deleted.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Remember to regularly remove unused containers from your systems (*remove*,
    not just stop). I personally witnessed a scenario with this part of the deployment
    process missing. Every time new images were released, the old containers were
    stopped and new containers were launched based on the new images. This was an
    oversight that quickly used up hard drive space and eventually prevented new deployments.
    The following Docker command is useful to clean up unused containers in bulk:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker container prune`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you are using Mac for development, remember that your containers are running
    in a tiny VM that you will need to first access before you can see the Docker
    root directory contents. For example, on a Mac, you can access and navigate this
    directory by interactively running a container in privileged mode that has *nsenter*
    installed (you may need to run this with `sudo`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Later versions of Windows (10+) now have the capability of running Linux containers
    natively using Windows Subsystem for Linux (WSL). The default Docker root directory
    for Windows 11 Home can be found here in File Explorer:'
  prefs: []
  type: TYPE_NORMAL
- en: '*\\wsl.localhost\docker-desktop-data\version-pack-data\* *community\docker\*'
  prefs: []
  type: TYPE_NORMAL
- en: Basic Tagging and Image Version Management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After working with images awhile, you will see that identifying them and versioning
    them are a bit different from the way you version your Java software. Working
    with build tools like Maven has gotten most Java developers accustomed to standard
    semantic versioning and always specifying dependency versions (or at least accepting
    of the version Maven chooses to pull within a particular dependency tree). These
    guardrails are a little more relaxed in other package managers like npm, where
    a dependency version can be specified as a range in order to allow for ease and
    flexibility in updating dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Image versioning can become a stumbling block if not well understood. No guardrails
    exist (at least not the kind that Java developers are used to). Flexibility in
    tagging an image is preferred over any enforcement of good practices. However,
    just because you *can*, doesn’t mean you *should*, and just as with proper versioning
    of Java libraries and packages, it is best to start out of the gate with a naming
    and versioning scheme that makes sense and follows an accepted pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Container image names and versions follow a specific format, including multiple
    components that you rarely see in complete form in examples and tutorials. Most
    example code and Dockerfiles you find when scouring the internet identify images
    in an abbreviated format.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is easiest to visualize image management as a directory structure, where
    the name of an image (such as *openjdk*) is a directory containing all the versions
    available for this image. Images are usually identified by a *name* and a version,
    known as a *tag*. But these two components are composed of subcomponents that
    have an assumed default value if not specified, and often, even a tag is omitted
    in commands. For example, the simplest command for pulling the *openjdk* Docker
    image might take the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: What is this command actually giving us? Aren’t there several versions of the
    *openjdk* image that you could use? Indeed, yes, and if you are concerned with
    having repeatable builds, you will immediately spot this ambiguity as a potential
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to include the image tag in this command, which represents
    a version. The following command implies that I would be pulling version 11 of
    the *openjdk* image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: So what was I pulling previously, if not 11? A special tag called `latest` is
    implied by default if a tag is not specified. This tag is intended to point to
    the latest version of the image available, but that might not always be the case.
    At any point, a tag can be updated to point to a different version of an image,
    and in some cases, you might find that the tag `latest` has not been set to point
    to anything at all.
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to stumble over the nomenclature as well, notably *tag*, which can
    mean something different in different contexts. The term *tag* can mean a specific
    version, or it can also mean the full *image tag*, which includes all the components
    of identification together, including the image *name*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the complete format of a Docker image tag with all possible components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The only required component is the image *name*, also known as the *image repository*.
    If *tag* is not specified, then *latest* is assumed. If the registry is not specified,
    Docker Hub is the default registry. The following command is an example of how
    to reference an image on a registry other than Docker Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Image and Container Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To build efficient containers, having a thorough understanding of layers is
    essential. The details behind how you build the source of your containers—your
    container *images*—greatly impact their size and performance, and some approaches
    have security implications, making this concept even more important to master.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, Docker images are built by establishing a base layer and then subsequently
    making small changes until you arrive at your desired final state. Each layer
    represents a set of changes including, but not limited to, the creation of users
    and related permissions, modifications to configuration or application settings,
    and updates to existing packages or adding/removing packages. These changes all
    amount to additions, modifications, or the removal of sets of files in the resulting
    filesystem. Layers are stacked on top of each other, each one being a delta of
    the changes from the previous layer, and each one identified by a SHA-256 hash
    digest of its contents. As discussed in [“Docker on Your Machine”](#docker_on_your_machine),
    these layers are stored within the root Docker directory.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One good way to really visualize layers is to use the command-line tool `dive`,
    available on [GitHub](https://oreil.ly/M2ZBZ). [Figure 3-5](#dive-openjdk-screenshot)
    shows a screenshot of the tool in action using the official latest *openjdk* image
    pulled from Docker Hub. The left pane displays details about the three layers
    that compose the *openjdk* image. The right pane highlights the changes each layer
    applies to the filesystem of the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Dive OpenJDK](Images/dtjd_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. `dive` with openjdk
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `dive` tool is useful in showing you what the filesystem would look like
    if you were to launch a container based on the *openjdk* image. As you move through
    each subsequent layer, you can see the changes made to the initial filesystem.
    The most important part to convey here is that subsequent layers may obfuscate
    parts of the filesystem of the previous layer (in the case of any moves or deletions
    of files), but the original layer still exists in its original form.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging layer cache
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Utilizing image layers speeds up image requests, builds, and pushes. It’s a
    clever way to decrease the amount of storage required for images. This strategy
    allows for identical image layers to be shared across multiple images, and reduces
    the amount of time and bandwidth needed for pulling or pushing images that are
    already cached locally or stored in the registry.
  prefs: []
  type: TYPE_NORMAL
- en: If you’re using Docker, your system will keep an internal cache of all the images
    you’ve either requested from external registries or built yourself. When new images
    are pushed and pulled, comparisons of each of the image layers are made between
    your local cache and the registry, and decisions are made about whether to push
    or pull individual layers, increasing efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Anyone who has ever struggled with their internal Maven repository (haven’t
    we all at some point?), or with any caching mechanism for that matter, is very
    aware that the efficiency and performance improvements internal cache provides
    also come with caveats. Sometimes what you have stored in cache is *not* what
    you intended to use. Using stale cache can easily happen in active development
    and local testing if you aren’t mindful of how and when your local image cache
    is used.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the commands `docker run openjdk` and `docker pull openjdk` behave
    differently where cache is concerned. The former searches for the specified image
    in your local cache with the tag `latest`. If the image exists, the search will
    be considered satisfied, and a new container based on the cached image will be
    launched. The latter command will go a step further and update the *openjdk* image
    on your system if an update exists in the remote registry it was retrieved from.
  prefs: []
  type: TYPE_NORMAL
- en: Another common mistake is assuming a command in a Dockerfile will run again
    when an image is rebuilt. This is common with `RUN` commands such as `RUN apt-get
    update`. If this line in the Dockerfile doesn’t change at all, as it would if
    you were to specify package names along with specific versions, then the initial
    layer built with this command will live in your cache. It will not get built again.
    This is not a bug, but a feature of the cache to speed up build processes. If
    a layer is determined to be already built, the layer will not be built again.
  prefs: []
  type: TYPE_NORMAL
- en: In an attempt to avoid stale cache, you might be tempted to combine commands
    on one line (producing one layer) in a Dockerfile in order for changes to be more
    easily recognized and acted on more frequently. The problem with this approach
    is that by squashing too much into a single layer, you lose the benefit of the
    cache altogether.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As a developer, be conscientious regarding your local cache. And beyond local
    development, consider how your continuous integration, build servers, and automated
    integration testing is using cache. Ensuring that all systems are consistent in
    this way will help keep you from chasing unexplained and intermittent failures.
  prefs: []
  type: TYPE_NORMAL
- en: Best Image Build Practices and Container Gotchas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After some time spent building and playing with images, you’re going to discover
    that you can shoot yourself in the foot in a lot of places in even the most basic
    build process. The following is a set of practices to keep in mind as you start
    on your image-building journey. You will discover more, but these are the most
    important.
  prefs: []
  type: TYPE_NORMAL
- en: Respect the Docker Context and .dockerignore File
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You don’t want to have certain things in your production Docker image—things
    like your development environment configuration, keys, your *.git* directory,
    or other sensitive hidden directories. When you run the command to build a Docker
    image, you provide the *context*, or the location of files you want to make available
    to the build process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a contrived Dockerfile example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: See the `COPY` instruction? Depending on what you sent in as the context, this
    could be problematic. It could be copying *everything* from your working directory
    into the Docker image you build, which will end up in any container launched from
    this image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure to use a *.dockerignore* file to exclude files from the context that
    you don’t want showing up unintentionally. You can use it to avoid accidentally
    adding any user-specific files or secrets that you might have stored locally.
    In fact, you can greatly reduce the size of the context (and the time it takes
    to build) by excluding anything the build doesn’t require access to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The *.dockerignore* matching format follows [Go’s `filepath.Match` rules](https://oreil.ly/sCjIv).
  prefs: []
  type: TYPE_NORMAL
- en: Use Trusted Base Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whether you choose to use images that include OpenJDK, Oracle JDK, GraalVM,
    or other images that include a web server or a database, make sure you are using
    trusted images as parent images, or creating your own from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Docker Hub proclaims to be the world’s largest library for publicly available
    container images, with over 100,000 images from software vendors, open source
    projects, and the community. *Not all of these images should be trusted to use
    as base images.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker Hub includes a set of curated images labeled “Docker Official Images”
    that are suitable for use as base images (note that distribution of these requires
    an agreement with Docker). These details are from the [online Docker docs on official
    images](https://oreil.ly/TO8Po):'
  prefs: []
  type: TYPE_NORMAL
- en: Docker, Inc. sponsors a dedicated team that is responsible for reviewing and
    publishing all content in the Docker Official Images. This team works in collaboration
    with upstream software maintainers, security experts, and the broader Docker community.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As important as understanding what Java dependencies are brought into your project
    and the depth of your dependency tree, so is understanding what your base image
    is bringing in under that one little `FROM` line at the top of your Dockerfile.
    The inheritance structure of Dockerfiles can easily obfuscate how much your base
    image is dragging along with it in the form of additional libraries and packages
    you don’t need or possibly even malevolent content.
  prefs: []
  type: TYPE_NORMAL
- en: Specify Package Versions and Keep Up with Updates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given the caveats discussed earlier about caching on top of the desire to maintain
    repeatable builds, specify versions in your Dockerfile just as you would in your
    Java project. Avoid broken builds and unexpected behavior from new versions or
    unexpected updates.
  prefs: []
  type: TYPE_NORMAL
- en: That said, it’s easy to get complacent with updating versions if they never
    force you to look at them because of a failed build or test. Regularly audit your
    project for needed updates and make these updates intentional. This should be
    part of your regular project planning. I advise that this activity be separate
    from any other feature development or bug fixing in order to eliminate unrelated
    moving parts in your development lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Keep Your Images Small
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is easy for images to become very large, very fast. Monitor size increases
    in your automated builds and set up notifications for unusual size changes. Gluttonous
    disk storage packages can easily sneak in via updates to a base image or be unintentionally
    included in a `COPY` statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Utilize multistage builds to keep your images small. A multistage build can
    be set up by creating a Dockerfile that uses multiple `FROM` statements, which
    begin a build stage with a different base image. By using multistage builds, you
    can avoid including things like build tools or package managers that are not needed
    (and really should not be included) in a production image. For example, the following
    Dockerfile shows a two-stage build. The first uses a base image that includes
    Maven. After the Maven build is complete, the required JAR file is copied to the
    second stage, which uses an image that does *not* include Maven:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This is also a good way to implement the use of a custom *distroless* image,
    an image that has been stripped of everything (including a shell) but the absolute
    essentials for running your application.
  prefs: []
  type: TYPE_NORMAL
- en: Beware of External Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I have often seen requests to external resources within Dockerfiles in the form
    of `wget` commands for installation of proprietary software or even external requests
    for shell scripts that perform a custom installation. These terrify me. More than
    general suspicion and paranoia are involved here. Even if the external resource
    is trusted, the more you relinquish control to parts of your build to external
    parties, the more likely you are to suffer build failures that are out of your
    control to fix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first response I often get when making this observation is this: “There’s
    nothing to worry about because once you’ve built your image, it’s cached or stored
    within a base image, and you won’t have to ever make the request again.”'
  prefs: []
  type: TYPE_NORMAL
- en: This is absolutely true. Once you have your base image stored, or your image
    layer cached, you’re good to go. But the first time a new build node (with zero
    cache) is put into play, or even when a new developer joins your team, building
    that image might fail. When you need to build a new version of the base image,
    your build might fail. Why? Because time and time again, external managers of
    resources will move them, restrict access to them, or simply *dispose of them*.
  prefs: []
  type: TYPE_NORMAL
- en: Protect Your Secrets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I include this because in addition to not moving secrets into your image in
    the first place, don’t think that using a command in a Dockerfile to remove them
    from a base image or any other previous layer is good enough. I’ve seen this before
    as a hack to “fix” a base layer that couldn’t be rebuilt right away.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how layering works, you know that a subsequent layer deleting
    items does not actually remove them from the underlying layer. You can’t see them
    if you were to `exec` into a running container based on that image, but they are
    still there. They exist on the system the image is stored on, they exist anywhere
    a container based on that image is launched, and they also exist in the image
    registry you’ve chosen for long-term storage. This is close to the equivalent
    of checking your passwords into source control. Do not put secrets into images
    to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: Know Your Outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Numerous factors can cause a container to continuously grow while it’s running.
    One of the most common is not dealing with log files appropriately. Ensure that
    your application is logging to a volume where you can implement a log-rotating
    solution. Given the ephemeral nature of containers, it doesn’t make sense to keep
    logs that you would use for troubleshooting or for compliance stored within the
    container (on the Docker host).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Much of this chapter was about exploring Docker. And this is an excellent place
    to start. Once you are comfortable with images and containers, you can branch
    out to other tools available in the ecosystem. Depending on the operating system
    and build utilities you’ve chosen for your project, tools such as [Buildah](https://buildah.io),
    [Podman](https://podman.io), or [Bazel](https://bazel.build) might work well for
    you. You might also choose to use a Maven plug-in such as [Jib](https://oreil.ly/pwGsw)
    to build your container image.
  prefs: []
  type: TYPE_NORMAL
- en: 'One word of caution: whichever tool you choose, understand how your images
    and containers are built so you don’t suffer the consequences of bulky and/or
    insecure images and containers when you are ready to deploy.'
  prefs: []
  type: TYPE_NORMAL
