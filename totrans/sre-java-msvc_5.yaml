- en: Chapter 5\. Safe, Multicloud Continuous Delivery
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。安全的多云持续交付
- en: The placement of this chapter in the second half of this book should signal
    the importance of telemetry in achieving safe and effective delivery practices.
    This may come as a surprise, because every organization ships software with an
    emphasis on testing as a means of guaranteeing safety, but not every organization
    measures it actively in a way that is directly related to end-user experience.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书后半部分中本章的安排应表明在实现安全和有效的交付实践中，遥测的重要性。这可能会让人感到意外，因为每个组织都强调测试作为确保安全性的手段，但并非每个组织都以直接关联到最终用户体验的方式积极地进行测量。
- en: The concepts in this chapter will be introduced with a continuous delivery tool
    called Spinnaker, but as with earlier chapters, a different tool could achieve
    similar ends. I’d like to establish a minimum base for what you should expect
    from a worthy CD tool.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍的概念将以一个名为Spinnaker的持续交付工具为例，但与早期章节一样，不同的工具也可以达到类似的目的。我想建立一个你应该从一个值得信赖的CD工具中期望的最低基础。
- en: Spinnaker is an open source continuous delivery solution that started at Netflix
    in 2014 to help manage its microservices in AWS. It was preceded at Netflix by
    a tool called Asgard, which was really just an alternative AWS console organized
    with application developers in mind, and built for Netflix’s unusual scale of
    AWS consumption. At one point, I was interacting with an AWS console form that
    required me to select a security group. The UI element in the console was a plain
    HTML select (list box) with four visible elements. The available security groups
    were unsorted in the list box, and there were thousands of them (again, because
    of Netflix’s broad scale in this account)! Usability issues like this led to Asgard,
    which in turn led to Spinnaker. Asgard was really just an application inventory
    with some actions (like the AWS console). Spinnaker was conceived as an inventory
    *plus* pipelines.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Spinnaker是一个开源的持续交付解决方案，起源于2014年Netflix为了帮助管理其AWS上的微服务而开发。在Netflix之前，有一个称为Asgard的工具，它实际上只是一个专为应用程序开发人员设计的替代AWS控制台，并且专为Netflix在AWS上的异常规模构建而成。曾经有一次，我在与AWS控制台交互时需要选择安全组。控制台中的UI元素是一个普通的HTML选择框，显示四个可见元素。由于Netflix在此帐户中的广泛规模，可用的安全组在列表框中未排序，且有数千个！像这样的可用性问题导致了Asgard，进而演变为Spinnaker。Asgard实际上只是一个带有一些操作（类似AWS控制台）的应用程序清单。Spinnaker则构想为清单*加*流水线。
- en: In 2015 Spinnaker was open sourced and other, initially IaaS implementations
    were added to it. At various points, Spinnaker has seen significant contributions
    from Google, Pivotal, Amazon, and Microsoft, as well as end users like Target.
    Many of these contributors worked together to write a [separate book](https://oreil.ly/fuhPb)
    on the topic of Spinnaker.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，Spinnaker 开源并添加了其他初始的IaaS实现。在不同阶段，Spinnaker 受到了Google、Pivotal、Amazon 和Microsoft等公司以及像Target这样的终端用户的重要贡献。许多这些贡献者共同撰写了一本[单独的书籍](https://oreil.ly/fuhPb)，内容是关于Spinnaker的。
- en: The practices described in this chapter are applicable to a wide variety of
    platforms.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述的实践适用于各种平台。
- en: Types of Platforms
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平台类型
- en: 'Different types of platforms have a surprising level of commonality in the
    high-level concepts that make up a running application. The concepts introduced
    in this chapter will be mostly platform neutral. Platforms fall into one of the
    following categories:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 不同类型的平台在组成运行应用程序的高级概念方面具有惊人的共性。本章介绍的概念将大部分是平台中立的。平台可分为以下几类：
- en: Infrastructure as a service (IaaS)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施即服务（IaaS）
- en: An infrastructure as a service provides virtualized computing resources as a
    service. An IaaS provider traditionally is responsible for servers, storage, networking
    hardware, the hypervisor layer, and APIs and other forms of user interface to
    manage these resources. Originally, to the extent you were using an IaaS, it was
    as an alternative to having physical hardware. Provisioning resources on an IaaS
    involves building virtual machine (VM) images. Deploying to an IaaS requires building
    VM images at some point in the delivery pipeline.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 基础设施即服务提供虚拟化计算资源作为服务。传统上，IaaS提供商负责服务器、存储、网络硬件、虚拟化层以及用于管理这些资源的API和其他形式的用户界面。最初，使用IaaS是作为不使用物理硬件的替代方法。在IaaS上配置资源涉及构建虚拟机（VM）映像。在IaaS上部署需要在交付流水线的某个时刻构建VM映像。
- en: Container as a service (CaaS)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 容器即服务（CaaS）
- en: A container as a service is a kind of specialization of an IaaS for container-based
    workloads rather than VMs. It provides a higher level of abstraction for the deployment
    of apps as containers. Kubernetes has of course become the de facto standard CaaS
    offered by public cloud vendors and on-prem. It provides a lot of other services
    that are outside the scope of this book. Deploying to a CaaS requires the extra
    step of building containers somewhere in the delivery pipeline (often at build
    time).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 容器即服务（CaaS）是对基于容器而不是虚拟机的工作负载的IaaS的一种专门化。它为应用程序作为容器部署提供了更高级别的抽象。Kubernetes当然已成为公有云供应商和本地环境提供的事实标准CaaS。它提供了许多本书范围之外的其他服务。部署到CaaS需要在交付管道中的某个地方（通常在构建时）构建容器的额外步骤。
- en: Platform as a service (PaaS)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 平台即服务（PaaS）
- en: A platform as a service further abstracts away the details of the underlying
    infrastructure, generally by allowing you to upload an application binary like
    a JAR or WAR directly to the PaaS API, which is then responsible for building
    an image and provisioning it. Contrary to the as-a-service implication of PaaS,
    sometimes PaaS offerings like Cloud Foundry are layered on top of virtualized
    infrastructure in customer datacenters. They also can be layered on top of IaaS
    offerings to provide a further abstraction away from the IaaS resource model,
    which may serve the purpose of preserving some degree of public cloud provider
    vendor neutrality or permitting similar delivery and management workflows in a
    hybrid private/public cloud environment.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 平台即服务（PaaS）进一步将底层基础架构的细节抽象化，通常允许您直接将应用程序二进制文件（如JAR或WAR）上传到PaaS API，然后由PaaS负责构建图像并进行配置。与PaaS的即服务含义相反，有时像Cloud
    Foundry这样的PaaS提供是在客户数据中心的虚拟化基础设施上的。它们也可以在IaaS提供的基础上进行层叠，以进一步抽象出IaaS资源模型，这可能达到保留某种程度的公有云提供商供应商中立性或允许在混合私有/公共云环境中提供类似的交付和管理工作流程的目的。
- en: These abstractions can be provided to you by another company, or you can build
    this cloud native infrastructure yourself (as a number of large companies have).
    The key requirement is an elastic, self-serve, and API-driven platform upon which
    to build.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些抽象可以由另一家公司提供给您，或者您可以自己构建这个云原生基础设施（就像一些大公司所做的那样）。关键要求是一个弹性的、自助服务的、基于API的平台，用于构建。
- en: A key assumption we will make throughout this chapter is that you are building
    immutable infrastructure. While nothing about an IaaS, for example, prevents you
    from building a VM image, launching an instance of it, and dropping an application
    onto it after launch, we are going to assume that the VM image is “baked” along
    with the application and any supporting software in such a way that when a new
    instance is provisioned, the application should start and run.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中将做出的一个关键假设是，您正在构建不可变基础设施。虽然IaaS的任何内容都不会阻止您构建VM映像、启动其实例并在启动后将应用程序放置到其中，但我们假设VM映像与应用程序和任何支持软件一起“烘焙”，以便在新实例被配置时应用程序应该能够启动和运行。
- en: A further assumption is that applications deployed in this manner are approximately
    cloud native. The definition of cloud native varies from source to source, but
    at a minimum the applications that are amenable to the deployment strategies discussed
    throughout this chapter are stateless. Other elements of [12-Factor apps](https://12factor.net/)
    aren’t as crucial.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的假设是以这种方式部署的应用程序大致上是云原生的。云原生的定义因来源而异，但至少可以适用于本章讨论的部署策略的应用程序是无状态的。[12因素应用](https://12factor.net/)的其他元素并不是那么关键。
- en: 'For example, I managed a service at Netflix that routinely took over 40 minutes
    to start, which doesn’t look good against the disposability criteria, but was
    otherwise unavoidable. The same service used exceedingly high memory footprint
    instance types in AWS, of which we had a small reserved pool. These put constraints
    on my choices: I couldn’t have more than about four instances of this service
    running at any given time, so I wasn’t going to be doing blue/green deployments
    with several disabled clusters (described in [“Blue/Green Deployment”](part0010_split_010.html#9H5RH-2d714b853a094e9a910510217e0e3d73)).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我在Netflix管理的一个服务通常需要超过40分钟才能启动，这与可处理性标准不符，但在其他方面是不可避免的。同一服务在AWS中使用了占用内存极高的实例类型，我们只有一个小型的保留池。这对我的选择施加了限制：我不能同时运行超过四个此服务的实例，所以我不会使用几个禁用的集群进行蓝/绿部署（在[“蓝/绿部署”](part0010_split_010.html#9H5RH-2d714b853a094e9a910510217e0e3d73)中描述）。
- en: To further center the discussion around a common language, let’s discuss the
    resource building blocks common to all of these platforms.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步围绕一个共同语言进行讨论，让我们讨论一下所有这些平台共有的资源构建块。
- en: Resource Types
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源类型
- en: 'To keep our discussion of delivery concepts platform-neutral, we will adopt
    the abstractions as defined by Spinnaker, which are surprisingly portable across
    different types of platforms:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持我们对交付概念的讨论与平台无关，我们将采用由Spinnaker定义的抽象概念，这些概念在不同类型的平台上都非常易于移植：
- en: Instance
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 实例
- en: An instance is a running copy of some microservice (that isn’t on a local developer
    machine, because I sincerely hope production traffic isn’t finding its way there).
    The AWS EC2 and Cloud Foundry platforms both call this “instance,” conveniently.
    In Kubernetes, an instance is a pod.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实例是某些微服务的运行副本（不是在本地开发者机器上，因为我真诚地希望生产流量不会找到那里）。AWS EC2和Cloud Foundry平台都称之为“实例”，非常方便。在Kubernetes中，实例是一个Pod。
- en: Server group
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器组
- en: A server group represents a collection of instances, managed together. Platforms
    have different ways of managing a collection of instances, but they tend to have
    a responsibility for ensuring that a certain number of instances are running.
    We generally assume that all of the instances of a server group have the same
    code and configuration, because they are immutable (except when they aren’t).
    Server groups can logically be devoid of any instances at all but simply have
    the potential to scale into a nonzero set of instances. In AWS EC2 a server group
    is an Auto Scaling Group. In Kubernetes a server group is roughly the combination
    of a Deployment and ReplicaSet (where a deployment manages rollout of ReplicaSets)
    or a StatefulSet. In Cloud Foundry, a server group is an Application (not to be
    confused with Application as defined in this list and as we are going to use the
    term throughout this chapter).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器组表示一组一起管理的实例。不同平台管理实例集合的方式各不相同，但它们通常负责确保运行一定数量的实例。我们通常假设服务器组的所有实例具有相同的代码和配置，因为它们是不可变的（除非不是这样）。服务器组可以在逻辑上完全没有任何实例，但只需潜在地扩展到非零数量的实例。在AWS
    EC2中，服务器组是一个自动扩展组。在Kubernetes中，服务器组大致是部署（Deployment）和副本集（ReplicaSet）的组合（其中部署管理副本集的推出），或者是StatefulSet。在Cloud
    Foundry中，服务器组是一个应用（不要与本列表中定义的应用混淆，我们在本章中将使用该术语）。
- en: Cluster
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 集群
- en: A cluster is a set of server groups that may span multiple regions. Within a
    single region, multiple server groups may represent different versions of the
    microservice. Clusters do *not* span cloud providers. You could be running two
    very similar clusters in different cloud providers, but for our discussion they
    would be considered distinct. A cluster is a logical concept that doesn’t actually
    have a correlated resource type in any cloud provider. Even more precisely, it
    doesn’t span multiple installations of a particular platform. So a cluster does
    not span multiple Cloud Foundry foundations or Kubernetes clusters. There is no
    higher-level abstraction in AWS EC2 that represents a collection of Auto Scaling
    Groups or in Kubernetes that represents a collection of Deployments. Spinnaker
    manages cluster membership by naming conventions or additional metadata that it
    places on resources it creates, depending on the platform’s implementation in
    Spinnaker.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 集群是跨多个区域可能存在的服务器组集合。在单个区域内，多个服务器组可能表示微服务的不同版本。集群不跨云服务提供商。您可以在不同的云服务提供商中运行两个非常相似的集群，但对于我们的讨论，它们将被视为不同的。集群是一个逻辑概念，在任何云服务提供商中实际上并没有对应的资源类型。更准确地说，它不涵盖特定平台的多个安装。因此，集群不跨多个Cloud
    Foundry基础设施或Kubernetes集群。在AWS EC2中没有更高级别的抽象来表示一组自动扩展组，在Kubernetes中也没有表示部署集合的抽象。Spinnaker通过命名约定或者附加到它创建的资源的元数据来管理集群成员资格，具体取决于Spinnaker在平台实现中的方式。
- en: Application
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 应用
- en: An application is a logical business function and not one particular resource.
    All of the running instances of an application are included. They may span multiple
    clusters in multiple regions. They may exist on multiple cloud providers, either
    because you are transitioning from one provider to another, or because you have
    some concrete business case for not being locked into one provider, or for any
    other reason. Wherever there is a running process that represents an instance
    of this business function, it is part of the grander concept called application.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 应用是一个逻辑业务功能，而不是一个特定的资源。所有运行中的应用实例都包括在内。它们可能跨越多个集群和多个地区。它们可能存在于多个云提供商，要么是因为您正在从一个提供商过渡到另一个提供商，要么是因为您有某些具体的业务情况，不希望锁定在一个提供商上，或者出于任何其他原因。只要存在代表这个业务功能实例的运行进程，它就是所谓的应用的一部分。
- en: Load balancer
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器
- en: A load balancer is a component that allocates individual requests to instances
    in one or more server groups. Most load balancers have a set of strategies or
    algorithms they can use to allocate traffic. Also, they generally have a health-checking
    feature that allows the load balancer to determine if a candidate microservice
    instance is healthy enough to receive traffic. In AWS EC2 a load balancer is an
    Application Load Balancer (or a legacy Elastic Load Balancer). In Kubernetes,
    the Service resource is a load balancer. The Cloud Foundry Router is a load balancer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器是一个组件，它将单独的请求分配给一个或多个服务器组中的实例。大多数负载均衡器有一组策略或算法，可以用来分配流量。此外，它们通常具有健康检查功能，允许负载均衡器确定候选微服务实例是否健康到足以接收流量。在AWS
    EC2中，负载均衡器是应用负载均衡器（或传统的弹性负载均衡器）。在Kubernetes中，Service资源就是负载均衡器。Cloud Foundry路由器也是一个负载均衡器。
- en: Firewall
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 防火墙
- en: A firewall is a set of rules that govern ingress and egress to a set of server
    groups. In AWS EC2 these are called Security Groups.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 防火墙是一组管理对一组服务器组的入站和出站流量的规则。在AWS EC2中，这些称为安全组。
- en: Spinnaker’s Kubernetes implementation is a little unique among the providers.
    Spinnaker can really deploy any Kubernetes resource because it internally uses
    `kubectl apply` and passes the manifest to the Kubernetes cluster. Furthermore,
    Spinnaker allows you to treat manifests as templates and provide variable substitution.
    It then maps some Kubernetes objects like ReplicaSets/Deployments/StatefulSets
    to server groups and Services to load balancers.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Spinnaker的Kubernetes实现在提供商中有些独特。Spinnaker实际上可以部署任何Kubernetes资源，因为它内部使用`kubectl
    apply`并将清单传递给Kubernetes集群。此外，Spinnaker允许您将清单视为模板，并提供变量替换。然后，它将某些Kubernetes对象如ReplicaSets/Deployments/StatefulSets映射到服务器组，将Services映射到负载均衡器。
- en: '[Figure 5-1](part0010_split_002.html#spinnaker_k8s_actions) shows a Spinnaker
    view of a series of Kubernetes ReplicaSets. Note how this infrastructure view
    also contains actions like Edit, Scale, Disable, and Delete for the selected resource.
    In this view, `replicaSet helloworldapp-frontend` is the “Cluster” resource (an
    amalgamation in this case of the Kubernetes resource type and name), representing
    a set of ReplicaSets in one or more Kubernetes namespaces. `HELLOWORLDWEBAPP-STAGING`
    is the “Region” corresponding to a Kubernetes namespace of the same name. `helloworldapp-frontend-v004`
    is a server group (a ReplicaSet). The individual blocks are the “Instances” corresponding
    to Kubernetes pods.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-1](part0010_split_002.html#spinnaker_k8s_actions)显示了一个Spinnaker视图，展示了一系列Kubernetes
    ReplicaSets。请注意，此基础设施视图还包含对所选资源的编辑、缩放、禁用和删除等操作。在此视图中，`replicaSet helloworldapp-frontend`是“Cluster”资源（在本例中是Kubernetes资源类型和名称的结合体），代表一个或多个Kubernetes命名空间中的一组ReplicaSets。`HELLOWORLDWEBAPP-STAGING`是对应于同名Kubernetes命名空间的“Region”。`helloworldapp-frontend-v004`是一个服务器组（一个ReplicaSet）。各个块是对应于Kubernetes
    pods的“Instances”。'
- en: '![srej 0501](../images/00035.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0501](../images/00035.png)'
- en: Figure 5-1\. Spinnaker view of three Kubernetes ReplicaSets with actions highlighted
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-1\. Spinnaker视图展示了三个Kubernetes ReplicaSets，并突出显示了操作
- en: Delivery Pipelines
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 交付管道
- en: 'Spinnaker pipelines are just one of many delivery-focused pipeline solutions
    on the market both commercially and in OSS. They range from the low-level and
    heavily opinionated Spring Cloud Pipelines to continuous integration pipelining
    extended to include delivery building blocks like JenkinsX. For the sake of this
    chapter, we will stick to Spinnaker pipelines, but if you substitute another pipelining
    solution, look for some key capabilities:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Spinnaker 流水线只是市场上商业和开源软件中众多以交付为重点的流水线解决方案之一。它们从低级别且具有强烈观点的 Spring Cloud Pipelines
    到包括 JenkinsX 等交付构建块的持续集成流水线，涵盖了各种解决方案。在本章中，我们将专注于 Spinnaker 流水线，但如果您替换其他流水线解决方案，请寻找一些关键能力：
- en: Platform neutrality
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 平台中立性
- en: A delivery solution doesn’t have to support every possible vendor to qualify
    as a platform-neutral solution, but delivery solutions based on, for example,
    Kubernetes custom resource definitions are guaranteed to be locked into a given
    platform. With this kind of lock-in, any heterogeny in your deployed environment
    means you are going to be building to multiple tools. Mixed platform use is exceedingly
    common in enterprises of sufficient scale (as it should be).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个交付解决方案不必支持每个可能的供应商才能被视为平台中立的解决方案，但基于 Kubernetes 自定义资源定义的交付解决方案保证会锁定到特定平台。有了这种锁定，您在部署环境中的任何异构性都意味着您将要使用多种工具。在足够规模的企业中，混合平台使用非常普遍（这也是应该的）。
- en: Automated triggers
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化触发
- en: Pipelines should be able to be automatically triggered by events, especially
    by changes in artifact inputs. We will discuss more about how artifact triggers
    help you repave your infrastructure in a safe and controlled way in [“Packaging
    for the Cloud”](part0010_split_004.html#9H5O7-2d714b853a094e9a910510217e0e3d73).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线应该能够根据事件自动触发，尤其是根据工件输入的变化。我们将更多地讨论工件触发如何帮助您以安全和可控的方式重新布置基础设施，详见[《云端打包》](part0010_split_004.html#9H5O7-2d714b853a094e9a910510217e0e3d73)。
- en: Scalability
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性
- en: A good pipelining solution accounts for the radically different computational
    nature of different pipeline stages. “Deploy” stages that are exercising platform
    API endpoints to provision new resources have very low computational needs, even
    if the stage may run for several minutes. A single instance of a pipeline execution
    service can easily run thousands of these in parallel. “Execute script” stages
    that execute something like a Gradle task are arbitrarily resource-intensive,
    so the execution of them is best delegated to something like a container scheduler
    such that the resource utilization of the stage execution doesn’t affect the performance
    of the pipeline execution service.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一个良好的流水线解决方案考虑到不同流水线阶段的根本不同的计算特性。“部署”阶段通过调用平台 API 端点来提供新资源，其计算需求非常低，即使该阶段可能运行几分钟。一个流水线执行服务的单个实例可以轻松并行运行数千个这样的阶段。“执行脚本”阶段执行类似
    Gradle 任务的操作，其资源需求任意复杂，因此最好将其委托给像容器调度器这样的东西，以确保阶段执行的资源利用不会影响流水线执行服务的性能。
- en: When continuous integration products are used to perform deployment operations,
    they generally inefficiently use resources in a significant way. For one financial
    institution I once visited, performing delivery operations with the CI system
    Concourse was costing several million dollars per year. For this organization,
    running 30 `m4.large` reserved instances in EC2 to support a Spinnaker installation
    would have cost a little over $15,000 per year. The resource inefficiency can
    easily swing the other direction though. Stages of arbitrary computational complexity
    should not be run on host or in process with Spinnaker’s Orca (i.e., pipeline)
    service.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用持续集成产品执行部署操作时，它们通常会以显着的方式浪费资源。我曾经参观过的一家金融机构使用 CI 系统 Concourse 进行交付操作，每年的成本达到数百万美元。对于这样的组织来说，运行
    30 个 `m4.large` 预留实例在 EC2 上支持一个 Spinnaker 安装每年只需花费超过 15,000 美元。资源的低效性很容易朝另一个方向转变。任意计算复杂度的阶段不应该在主机上或者在
    Spinnaker 的 Orca（即流水线）服务中运行。
- en: The various cloud providers *feel* very different. The deployable resource is
    a different level of abstraction for each type.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 各种云提供商的*感觉*非常不同。可部署资源对于每种类型的抽象级别也是不同的。
- en: Spinnaker pipelines consist of stages that are approximately cloud neutral.
    That is, the same basic building blocks will be available for every cloud provider
    implementation, but the configuration of a stage like “Deploy” will vary from
    platform to platform.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Spinnaker 流水线由大致与云中性相关的阶段组成。也就是说，每个云提供商实现将有相同的基本构建块可用，但是像 “部署” 这样的阶段的配置会因平台而异。
- en: '[Figure 5-2](part0010_split_003.html#spinnaker_k8s_pipeline_definition) shows
    the definition of a Spinnaker pipeline that is going to deploy to Kubernetes.
    Pipelines can be arbitrarily complex, containing parallel stages and multiple
    triggers defined in the special configuration stage up front.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-2](part0010_split_003.html#spinnaker_k8s_pipeline_definition) 展示了一个将部署到
    Kubernetes 的 Spinnaker 流水线的定义。流水线可以非常复杂，包含并行阶段和在特殊配置阶段上定义的多个触发器。'
- en: '![srej 0502](../images/00001.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0502](../images/00001.png)'
- en: Figure 5-2\. A detailed view of a Spinnaker pipeline showing multiple stages
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-2\. 展示了一个详细视图的 Spinnaker 流水线，显示了多个阶段
- en: Spinnaker defines several different trigger types. This pipeline is triggered
    by the publication of a new container image in a Docker registry, as shown in
    [Figure 5-3](part0010_split_003.html#spinnaker_k8s_expected_artifact).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Spinnaker 定义了几种不同的触发器类型。这个流水线是通过发布到 Docker 注册表的新容器镜像来触发的，如 [图 5-3](part0010_split_003.html#spinnaker_k8s_expected_artifact)
    所示。
- en: '![srej 0503](../images/00060.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0503](../images/00060.png)'
- en: Figure 5-3\. Spinnaker expected artifact definition
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-3\. Spinnaker 期望的构件定义
- en: '[Figure 5-4](part0010_split_003.html#spinnaker_k8s_pipelines) shows the execution
    history of two Spinnaker pipelines, including the one whose configuration we just
    saw. The staging pipeline was last executed by a Docker registry trigger (a new
    container published to a Docker registry). In the other circumstances, the pipeline
    was manually triggered.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-4](part0010_split_003.html#spinnaker_k8s_pipelines) 展示了两个 Spinnaker 流水线的执行历史，包括我们刚刚看到的配置。阶段流水线是通过
    Docker 注册触发器（发布到 Docker 注册表的新容器）最后执行的。在其他情况下，流水线是手动触发的。'
- en: '![srej 0504](../images/00101.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0504](../images/00101.png)'
- en: Figure 5-4\. Spinnaker view of two different delivery pipelines
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-4\. Spinnaker 查看两个不同交付流水线的视图
- en: The first task of any delivery pipeline is to package the application in an
    immutable unit of deployment that can be stamped out in instances across a server
    group.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 任何交付流水线的第一个任务是将应用程序打包成一个不可变的部署单元，可以在服务器组的实例中复制。
- en: Packaging for the Cloud
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云端的打包
- en: The different abstractions given by the various types of cloud platforms have
    tradeoffs in terms of startup time, resource efficiency, and cost. But as we’ll
    see, there shouldn’t be a significant difference in terms of the effort each requires
    to package a microservice for deployment.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 不同云平台提供的各种抽象层面上，存在启动时间、资源效率和成本的权衡。但正如我们将看到的，从包装微服务到部署的各自工作量之间应该没有显著差异。
- en: Generating a new application from [start.spring.io](https://start.spring.io)
    includes the generation of a Gradle or Maven build that can generate a runnable
    JAR. For PaaS platforms like Cloud Foundry and Heroku, this runnable JAR *is*
    the input unit of deployment. It is the responsibility of the cloud provider to
    take this runnable JAR and containerize or otherwise package it and then provision
    some underlying resource to run this package on.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从 [start.spring.io](https://start.spring.io) 生成一个新的应用程序包括生成一个 Gradle 或 Maven
    构建，可以生成一个可运行的 JAR。对于像 Cloud Foundry 和 Heroku 这样的 PaaS 平台，这个可运行的 JAR *是* 部署的输入单元。由云提供商负责获取这个可运行的
    JAR 并将其容器化或以其他方式打包，然后为其运行提供一些基础资源。
- en: For cloud platforms other than PaaS, the effort required from the application
    team is surprisingly not much different. The examples included here are implemented
    with Gradle because open source tooling exists for both IaaS and CaaS uses. There
    is no reason similar tooling couldn’t be produced for Maven.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 对于除了 PaaS 外的云平台，应用团队所需的工作量惊人地并没有太大差异。这里的示例使用 Gradle 实现，因为开源工具同时适用于 IaaS 和 CaaS
    的用途。同样，类似的工具也可以为 Maven 生产。
- en: 'One of the typical value propositions of a PaaS is that you provide just the
    application binary as an input to deployment processes and let the PaaS manage
    the operating system and package patching on your behalf, even transparently.
    In practice, it doesn’t work out quite like this. In the case of Cloud Foundry,
    the platform is responsible for a certain level of patching that is achieved in
    a rolling manner, affecting one instance at a time in any server group (a.k.a.
    “application” in Cloud Foundry parlance). But such patching comes with a certain
    degree of risk: an update to any part of the operating system could adversely
    affect an application running on it. So there is this risk/reward trade-off that
    carefully circumscribes the types of changes the platform is willing to automate
    on behalf of users. All other patches/updates are applied to the “type” of image
    that the platform will layer your application on. Cloud Foundry calls these buildpacks.
    For example, Java version upgrades involve an update to the buildpack. The platform
    does not automatically update buildpack versions for every running application
    that used the Java buildpack. It really is up to the organization then to redeploy
    every application using the Java buildpack to pick up the update.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: PaaS 的典型价值主张之一是，您只需将应用程序二进制文件作为部署过程的输入，并让 PaaS 代表您管理操作系统和软件包补丁，甚至是透明地。但实际操作并非完全如此。在
    Cloud Foundry 的情况下，该平台负责以滚动方式进行一定程度的补丁操作，影响服务器组中的一个实例（在 Cloud Foundry 术语中称为“应用程序”）。但这样的补丁操作会带来一定程度的风险：操作系统的任何部分更新都可能对正在其上运行的应用程序产生不利影响。因此，这里存在着风险与回报的权衡，该权衡会仔细界定平台愿意代表用户自动化的更改类型。所有其他的补丁/更新都将应用于平台将应用程序放置在其上的“类型”镜像。Cloud
    Foundry 称这些为构建包（buildpacks）。例如，Java 版本升级涉及对构建包的更新。平台不会自动更新每个正在运行使用 Java 构建包的应用程序的构建包版本。这确实取决于组织是否重新部署每个使用
    Java 构建包的应用程序来获取更新。
- en: 'For non-PaaS environments, with the extra amount of effort involved in generating
    another type of artifact (other than a JAR) from your build or having an additional
    stage in your deployment pipeline comes a dramatically greater degree of control
    and flexibility over how infrastructure can be patched across your organization.
    While the type of base image is different between IaaS and CaasS (virtual machine
    and container image, respectively), the principle of baking your application on
    top of a base image allows you to define your application binary and the base
    image it is layered upon as separate inputs to each microservice delivery pipeline.
    [Figure 5-5](part0010_split_004.html#base_image_trigger) shows a hypothetical
    delivery pipeline for a microservice that first deploys to a test environment,
    runs tests, and goes through an audit check before finally being deployed to production.
    Note how Spinnaker supports multiple trigger types in this case: one for a new
    application binaries and one for a new base image.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非 PaaS 环境来说，通过从构建生成除 JAR 之外的另一种类型的工件（或在部署流水线中增加额外阶段），可以在整个组织中更大程度地控制和灵活性地处理基础设施的补丁。虽然
    IaaS 和 CaaS 之间的基础镜像类型不同（分别是虚拟机和容器镜像），但在基础镜像上构建您的应用程序的原则允许您将应用程序二进制文件和其上层叠的基础镜像作为每个微服务交付流水线的独立输入。[图 5-5](part0010_split_004.html#base_image_trigger)
    展示了一个假想的微服务交付流水线，首先部署到测试环境，运行测试，并经过审计检查，最终部署到生产环境。请注意，在此示例中 Spinnaker 支持多种触发类型：一种用于新的应用程序二进制文件，另一种用于新的基础镜像。
- en: '![srej 0505](../images/00115.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0505](../images/00115.png)'
- en: Figure 5-5\. Changes to the base image trigger pipelines
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-5\. 基础镜像的更改会触发流水线
- en: In the same organization, a different microservice may have more or fewer stages
    to verify the fitness of the combination of application artifact and base image
    before promoting to production. Having changes to the base image *trigger* delivery
    pipelines is the ideal balance between safety and speed. Microservices whose delivery
    pipelines contain all fully automated stages may adopt the new base within minutes,
    where another service with more stringent manual verification and approval stages
    takes days. Both types of services adopt the change in a way that is best in line
    with the responsible team’s unique culture and requirements.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一组织中，不同的微服务可能需要更多或更少的阶段来验证应用程序工件和基础镜像的组合是否适合推广到生产环境。使基础镜像的变更*触发*交付流水线是安全和速度的理想平衡点。交付流水线包含所有完全自动化阶段的微服务可能在几分钟内采用新的基础镜像，而具有更严格手动验证和批准阶段的服务可能需要几天时间。这两种服务都以最符合负责团队独特文化和要求的方式采用变更。
- en: Packaging for IaaS Platforms
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适用于 IaaS 平台的打包
- en: For an IaaS platform, the immutable unit of deployment is a virtual machine
    image. In AWS EC2, this image is called an Amazon Machine Image. Creating one
    is a matter of provisioning an instance of the base image (which contains common
    opinions for all of your microservices like the Java version, common system dependencies,
    and monitoring and debugging agents.), installing a system dependency on it containing
    your application binary, snapshotting the resultant image, and configuring new
    server groups to use this image as the template when provisioning instances of
    the microservice.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 IaaS 平台，部署的不可变单元是虚拟机镜像。在 AWS EC2 中，此镜像称为 Amazon Machine Image。创建镜像只需实例化基础镜像（其中包含所有微服务的公共偏好设置，如
    Java 版本、常见系统依赖项以及监控和调试代理），在其上安装包含应用程序二进制文件的系统依赖项，然后对结果镜像进行快照，并在配置新服务器组时将此镜像作为模板使用。
- en: The process of provisioning the instance, installing the system dependency,
    and snapshotting it is collectively called *baking*. It isn’t always necessary
    to even launch a live copy of the base image to bake. HashiCorp’s battle-tested
    [Packer](https://www.packer.io) provides an open source bakery solution that works
    for a variety of different IaaS providers.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 将实例供应、安装系统依赖项并创建快照的过程称为*烘焙*。甚至不必启动基础镜像的实时副本也是可能的。HashiCorp 的经过考验的开源烘焙解决方案 [Packer](https://www.packer.io)
    适用于各种不同的 IaaS 提供商。
- en: '[Figure 5-6](part0010_split_005.html#iaas_packaging_participants) shows where
    the boundaries of responsibility are for the build tool, the bakery, and the server
    group managed by the cloud provider. A Spinnaker pipeline stage is responsible
    for starting the baking process and for creating the server group with the image
    resulting from the bake stage. It shows that there is one additional requirement
    of each microservice’s build, the production of a system dependency, meaning the
    production of a Debian package on an Ubuntu or Debian base image, an RPM on a
    Red Hat base image, etc. Ultimately, the bakery will in some way be invoking the
    operating-system-level package installer to layer your application binary onto
    the base image (e.g., `apt-get install <system-package>`).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-6](part0010_split_005.html#iaas_packaging_participants) 显示了构建工具、烘焙工厂和由云提供商管理的服务器组的责任边界。Spinnaker
    流水线阶段负责启动烘焙过程，并使用烘焙阶段产生的镜像创建服务器组。它显示每个微服务构建的额外要求，即生产系统依赖项，意味着在 Ubuntu 或 Debian
    基础镜像上生产 Debian 包，在 Red Hat 基础镜像上生产 RPM 等。最终，烘培工厂将以某种方式调用操作系统级别的包安装程序，将应用程序二进制文件叠加到基础镜像上（例如，`apt-get
    install <system-package>`）。'
- en: '![srej 0506](../images/00076.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0506](../images/00076.png)'
- en: Figure 5-6\. Participants in IaaS packaging
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-6\. IaaS 打包参与者
- en: Producing a Debian or RPM system dependency is fortunately straightforward with
    the application of a Gradle plug-in from Netflix’s Nebula suite of Gradle plug-ins,
    as shown in [Example 5-1](part0010_split_005.html#nebula_ospackage). This adds
    a Gradle task called `buildDeb` that does all the work necessary to output a Debian
    package for a Spring Boot application. It is a one-line change to the build file!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 利用 Netflix 的 Nebula Gradle 插件套件中的 Gradle 插件，如 [示例 5-1](part0010_split_005.html#nebula_ospackage)
    所示，生成 Debian 或 RPM 系统依赖项非常简单。这将在构建文件中添加一个名为 `buildDeb` 的 Gradle 任务，该任务完成所有生成 Spring
    Boot 应用程序的 Debian 包所需的工作。这只需要对构建文件进行一行更改！
- en: Example 5-1\. Using a Nebula Gradle plug-in to produce a Debian package
  id: totrans-74
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-1\. 使用 Nebula Gradle 插件生成 Debian 包
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](../images/00112.png)](part0010_split_005.html#co_safe__multicloud_continuous_delivery_CO1-1)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](../images/00112.png)](part0010_split_005.html#co_safe__multicloud_continuous_delivery_CO1-1)'
- en: Replace `LATEST` with whatever the latest version is on the [Gradle plug-in
    portal](https://oreil.ly/xPGaq), because `LATEST` isn’t actually valid for Gradle
    plug-in version specifications.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 用 [Gradle 插件门户网站](https://oreil.ly/xPGaq) 上的最新版本替换 `LATEST`，因为 `LATEST` 实际上对
    Gradle 插件版本规范无效。
- en: The ospackage plug-in contains a variety of options for adding start scripts,
    configuring output locations for configuration files and runnable artifacts, etc.
    Ultimately though, wherever and whatever happens with these files, there should
    be enough commonality between microservices in an organization to encapsulate
    these opinions in a similar way to what Netflix has done with `nebula.ospackage-application-spring-boot`
    and distribute them as a build tool plug-in that makes adoption trivial.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ospackage 插件包含各种选项，用于添加启动脚本，配置配置文件和可运行的文件的输出位置等。不过，无论这些文件发生在何处和发生了什么，组织中的微服务之间应该有足够的共性，以类似于
    Netflix 对 `nebula.ospackage-application-spring-boot` 所做的方式封装这些观点，并将它们作为一个构建工具插件进行分发，以便采用变得微不足道。
- en: Packaging for Container Schedulers
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为容器调度器打包
- en: Preparing a microservice for deployment to a container scheduler like Kubernetes
    can be similar. Opinionated tooling is again available in open source to package
    for common frameworks like Spring Boot, as shown in [Example 5-2](part0010_split_006.html#muschko_docker_spring_boot).
    This plug-in also understands how to publish to Docker registries, given a little
    more configuration (which can easily be encapsulated and shipped as a common build
    tool plug-in across an organization).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 准备微服务以部署到像 Kubernetes 这样的容器调度器可能会类似。有可供选择的工具可以为常见框架（如 Spring Boot）打包，如 [Example 5-2](part0010_split_006.html#muschko_docker_spring_boot)
    所示。这个插件还了解如何发布到 Docker 注册表，只需进行一些配置（可以很容易地封装并作为组织内的常见构建工具插件进行发布）。
- en: Example 5-2\. Using a Nebula Gradle plug-in to produce and publish a Docker
    image
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-2\. 使用 Nebula Gradle 插件生成和发布 Docker 镜像
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Rely on Open Source Build Tooling, but Be Careful About Consuming Base Images
    Without Validation
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 依赖开源构建工具，但小心使用没有经过验证的基础镜像
- en: It’s great to have tools like Ben Muschko’s Gradle Docker plug-in for producing
    an image containing an application built on top of some base. But you should expect
    that somebody in your organization is validating and creating approved images,
    known to perform well and be free of known defects and security vulnerabilities.
    This is applicable to both VM and container images.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有像本 Muschko 的 Gradle Docker 插件这样的工具很好，可以生成一个包含在某个基础之上构建的应用程序的镜像。但你应该期望你组织中有人正在验证和创建经过批准的镜像，已知性能良好且没有已知缺陷和安全漏洞。这适用于
    VM 和容器镜像。
- en: This approach does have the disadvantage that operating system and other system
    package updates are part of the base Docker image used to produce the application
    container image. Propagating a base container image change across the whole organization
    then requires us to rebuild the application binary, which can be inconvenient.
    After all, to effect a change to *only* the base image and not the application
    code (which may have a set of further source code changes since the last time
    it was built), we have to check out the application code at the hash of the version
    in production and rebuild with the new image. This process, since it involves
    the build again, is fraught with the potential for nonreproducibility in the application
    binary when all we want to do is update the base image.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是操作系统和其他系统软件包更新是基础 Docker 镜像的一部分，用于生成应用容器镜像。然后需要将基础容器镜像的更改传播到整个组织，这要求我们重新构建应用程序二进制文件，这可能会很不方便。毕竟，要想*仅仅*改变基础镜像而不改变应用程序代码（可能自上次构建以来已经有一系列的源代码更改），我们必须将应用程序代码签出到生产版本的哈希值，并使用新的镜像重新构建。这个过程，因为涉及再次构建，可能会导致应用程序二进制文件无法复现，而我们只是想要更新基础镜像。
- en: Adding a bake stage to containerized workloads simplifies the build by removing
    the need to publish a container image at all (just publish the JAR to a Maven
    artifact repository) and allows for mass updating of the base image, again with
    the same process and safety guarantees that we received by making the base image
    an artifact trigger for IaaS-based workloads above. Spinnaker supports baking
    container images with [Kaniko](https://oreil.ly/JpW3V), removing the need for
    container image building/publishing to be part of the build workflow. One of the
    advantages of doing so is that you can rebake the same application binary on a
    more recent base (say, when you fix a security vulnerability in the base), operating
    effectively with an immutable copy of the application code.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在容器化工作负载中添加一个烘烤阶段可以通过删除完全消除需要发布容器镜像的必要性（只需将 JAR 发布到 Maven 构件库），并允许大规模更新基础镜像，再次采用与
    IaaS 基础工作负载触发器相同的流程和安全保证。Spinnaker 支持使用 [Kaniko](https://oreil.ly/JpW3V) 烘烤容器镜像，从而无需将容器镜像构建/发布作为构建工作流的一部分。这样做的一个优点是，您可以在更新的基础上重新烘烤相同的应用程序二进制文件（例如，在基础中修复安全漏洞时），有效地运行应用程序代码的不可变副本。
- en: Surprisingly then, the desire for safe base updates across all three cloud abstractions
    (IaaS, CaaS, and PaaS) leads to a remarkably similar workflow for all three (and
    similar application developer experience). In effect, ease of deployment is no
    longer a decision criterion between these level of abstractions, and we are left
    with considering other differentiators like startup time, vendor lock-in, cost,
    and security.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，跨所有三种云抽象（IaaS、CaaS 和 PaaS）实现安全的基本更新的愿望导致了所有三者都采用了非常类似的工作流程（以及类似的应用程序开发者体验）。实际上，部署的便捷性不再是这些抽象层次之间的决策标准，我们必须考虑其他差异化因素，比如启动时间、厂商锁定、成本和安全性。
- en: Now that we’ve discussed packaging, let’s turn our attention to deployment strategies
    that can be used to stand up these packages on your platform.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了打包问题，让我们转向可以用来在您的平台上部署这些包的部署策略。
- en: The Delete + None Deployment
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 删除 + 无部署
- en: If the name *delete + none* sounds ugly, that’s because I’m about to describe
    a hack that may only be useful in certain narrow situations but helps set the
    framework for other deployment strategies to follow.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果“删除 + 无部署”听起来很丑陋，那是因为我即将描述的这种小技巧可能仅在某些狭窄情况下有用，但它有助于为随后的其他部署策略奠定框架。
- en: The basic idea is to simply delete the existing deployment and deploy a new
    one. The obvious ramification of this is downtime, however short it may be. The
    existence of downtime suggests that API compatibility across versions isn’t strictly
    required, provided you coordinate deployments to all callers of a service with
    a changing API at the same time.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 基本思想就是简单地删除现有的部署并部署新的。显然，这样做会导致停机时间，无论多么短暂。停机时间的存在表明，版本间的 API 兼容性并不严格要求，只要您在服务的所有调用方在同一时间进行版本更改的部署协调即可。
- en: Every deployment strategy that follows will be zero-downtime.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 后续的每种部署策略都将实现零停机时间。
- en: To tie this concept to deployment practices you may be familiar with that are
    *not immutable*, a delete + none deployment strategy is in use when, on an always-running
    virtual machine, a new application version is installed and started (replacing
    the previously running version). Again, this chapter focuses strictly on immutable
    deployments, and no other deployment strategy that follows has an obvious mutable
    counterpart.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要将这个概念与您可能熟悉的*非不可变*部署实践联系起来，当在始终运行的虚拟机上安装并启动新的应用程序版本（替换之前运行的版本）时，就会使用删除 + 无部署部署策略。再次强调，本章仅专注于不可变部署，随后的任何其他部署策略都没有明显的可变对应策略。
- en: The strategy is also in use when performing a basic `cf push` on Cloud Foundry
    and an operation on AWS EC2 that reconfigures an Auto Scaling Group to use a different
    Amazon Machine Image. The point is, often basic CLI or console-based deployment
    options do accept downtime and operate more or less with this strategy.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略在执行基本的 `cf push`（Cloud Foundry 的命令）时也会使用，在 AWS EC2 上操作重新配置 Auto Scaling
    Group 以使用不同的 Amazon Machine Image 时同样适用。关键在于，通常基本的 CLI 或控制台部署选项确实接受停机时间，并且更多或少地按照这种策略操作。
- en: The next strategy is similar, but with zero downtime.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个策略类似，但是没有停机时间。
- en: The Highlander
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高地人
- en: 'Despite the odd name, the Highlander strategy is the most common zero-downtime
    strategy in practice today. The name comes from a slogan from the *Highlander*
    movie: “there can be only one.” In other words, when you deploy a new version
    of a service, you replace the old version. There can be only one. Only the new
    version is running at the end of the deployment.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管名称奇特，但Highlander策略实际上是当今实践中最常见的零停机策略。名称源自《炫目之剑》电影中的一句口号：“只能有一个。”换句话说，当你部署服务的新版本时，你会替换旧版本。只能有一个。部署结束时，只有新版本在运行。
- en: The Highlander strategy is zero-downtime. In practice it involves deploying
    a new version of the application and adding it to the load balancer, which causes
    traffic to be served to both for a short period of time while the old version
    is destroyed automatically. So maybe the more accurate slogan for this deployment
    strategy is “there is usually only one.” Required API compatibility across versions
    follows from the existence of this brief overlap.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Highlander策略是零停机时间的。在实践中，它涉及部署应用程序的新版本并将其添加到负载均衡器，这会导致在销毁旧版本时短时间内同时为两个版本提供服务。因此，这种部署策略的更准确的口号可能是“通常只有一个”。跨版本所需的API兼容性源于这种短暂重叠的存在。
- en: The Highlander model is simple, and its simplicity can make it an attractive
    option for many services. Since there is only one server group at any given time,
    there is no need to worry about coordinating to prevent interference from “other”
    running versions that are not supposed to be in service.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Highlander模型简单，其简单性使其成为许多服务的有吸引力的选择。由于任何给定时间只有一个服务器组，所以无需担心协调以防止来自“其他”不应处于服务状态的运行版本的干扰。
- en: Going back to a previous version of code under a Highlander strategy involves
    reinstalling an old version of the microservice (which receives a new server group
    version number). Therefore, the time to completion for this pseudorollback action
    is the amount of time it takes to install and initialize the application process.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在Highlander策略下返回到先前版本的代码涉及重新安装旧版本的微服务（该微服务接收一个新的服务器组版本号）。因此，此伪回滚操作完成所需的时间是安装和初始化应用程序进程所需的时间。
- en: The next strategy offers faster rollback at the expense of some coordination
    and complexity.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个策略提供更快的回滚速度，但需要一些协调和复杂性。
- en: Blue/Green Deployment
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝/绿部署
- en: The blue/green deployment strategy involves having at least two copies of the
    microservice provisioned (whether in an enabled or disabled state), involving
    server groups for old and new versions. At any given time, production traffic
    is being served from one of these versions. Rolling back is a matter of switching
    which copy is considered live. Rolling forward to the newer version has the same
    experience. How this switching logic is achieved is cloud-platform-dependent (but
    orchestrated by Spinnaker), but at a high level it involves influencing the cloud
    platform’s load balancer abstraction to send traffic to one version or the other.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝/绿部署策略涉及至少两个微服务副本（无论是启用还是禁用状态），涉及到旧版本和新版本的服务器组。在任何给定时间，生产流量都是从这些版本中的一个版本提供的。回滚只是切换哪个副本被视为活动副本。向前滚动到更新版本具有相同的体验。如何实现这种切换逻辑取决于云平台（但由Spinnaker编排），但在高层次上涉及影响云平台的负载平衡器抽象以将流量发送到一个版本或另一个版本。
- en: kubectl apply Is a Specific Kind of Blue/Green by Default
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: kubectl apply默认是一种特定类型的蓝/绿部署。
- en: '`kubectl apply` that updates a Kubernetes Deployment (from the CLI and not
    using Spinnaker) is by default a rolling blue/green deployment, allowing you to
    roll back to a ReplicaSet representing a previous version. Because it is a container
    deployment type, the rollback operation involves pulling back the image. The Kubernetes
    Deployment resource is implemented as a controller on top of ReplicaSet that manages
    rolling blue/green deployment and rollback. Spinnaker offers more control for
    Kubernetes ReplicaSets, enabling blue/green functionality with N disabled versions,
    canary deployments, etc. So think of a Kubernetes Deployment as a limited, opinionated
    blue/green deployment strategy.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl apply`更新Kubernetes部署（从CLI而不是使用Spinnaker）默认是滚动蓝/绿部署，允许您回滚到表示先前版本的ReplicaSet。因为它是一种容器部署类型，所以回滚操作涉及将镜像拉回来。Kubernetes部署资源在管理滚动蓝/绿部署和回滚的ReplicaSet之上实现为控制器。Spinnaker为Kubernetes
    ReplicaSets提供了更多控制，可以启用蓝/绿功能，包括禁用版本，金丝雀部署等。因此，将Kubernetes部署视为一种有限的，持有的蓝/绿部署策略。'
- en: Load balancer switching can have implications for the structure of deployed
    assets. For example, on Kubernetes, blue/green basically requires that you use
    the ReplicaSet abstraction. The blue/green strategy requires that *running* resources
    are somehow edited to influence traffic. For Kubernetes, we can do this with label
    manipulation, and this is what the Spinnaker Kubernetes implementation does to
    achieve blue/green. If we instead tried to edit the Kubernetes Deployment object,
    it would trigger a rollout. Spinnaker automatically adds special labels to ReplicaSets
    that indirectly cause them to be considered enabled or disabled and a label selector
    to the service to only route traffic to enabled ReplicaSets. If you aren’t using
    Spinnaker, you’ll need to create some sort of similar process that mutates labels
    in place on ReplicaSets and configure your services to be aware of these labels.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器切换可能会对部署资产的结构产生影响。例如，在 Kubernetes 上，蓝绿部署基本上要求您使用 ReplicaSet 抽象。蓝绿策略要求 *运行*
    资源通过某种方式进行编辑以影响流量。对于 Kubernetes，我们可以通过标签操作来实现这一点，这就是 Spinnaker Kubernetes 实现用来实现蓝绿部署的方法。如果我们尝试编辑
    Kubernetes 的 Deployment 对象，将触发一次滚动更新。Spinnaker 会自动向 ReplicaSet 添加特殊标签，间接导致它们被视为启用或禁用，并在服务上添加标签选择器以仅将流量路由到启用的
    ReplicaSet。如果您不使用 Spinnaker，则需要创建一些类似的过程，在 ReplicaSet 上原地修改标签，并配置服务以识别这些标签。
- en: The colors blue/green imply that there are two server groups and either the
    blue or the green server group is serving traffic. Blue/green strategies aren’t
    always binary either, and the coloring should not suggest that these server groups
    need to be long-lived, mutating with new service versions as they become available.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝/绿色暗示有两个服务器组，其中蓝色或绿色服务器组中的一个正在提供流量服务。蓝绿策略并不总是二元的，颜色也不应暗示这些服务器组需要长期存在，随着新服务版本的可用性而变化。
- en: A blue/green deployment is more generally a 1:N relationship in any given cluster,
    where one server group is live and N server groups are not live. A visual representation
    of such a 1:N blue/green cluster is shown in [Figure 5-7](part0010_split_011.html#spinnaker_blue_green_cluster).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝绿部署通常在任何给定集群中是一个 1:N 的关系，其中一个服务器组是活跃的，而 N 个服务器组是非活跃的。这种 1:N 蓝绿集群的可视化表示如图 [5-7](part0010_split_011.html#spinnaker_blue_green_cluster)
    所示。
- en: '![srej 0507](../images/00116.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0507](../images/00116.png)'
- en: Figure 5-7\. Spinnaker blue/green cluster
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-7\. Spinnaker 蓝绿集群
- en: The rollback server group action, shown in [Figure 5-8](part0010_split_011.html#spinnaker_server_group_actions),
    allows for the selection of any one of these disabled server group versions (V023–V026).
    At the completion of the rollback, the current live version (V027) will still
    be around, but disabled.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 回滚服务器组操作，如图 [5-8](part0010_split_011.html#spinnaker_server_group_actions) 所示，允许选择这些禁用的服务器组版本之一（V023–V026）。回滚完成后，当前的活跃版本（V027）仍将存在，但被禁用。
- en: '![srej 0508](../images/00032.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0508](../images/00032.png)'
- en: Figure 5-8\. Spinnaker rollback server group action
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-8\. Spinnaker 回滚服务器组操作
- en: Depending on what the underlying cloud platform can support, a disabled cluster
    can retain live running instances that aren’t receiving any traffic, or it may
    be reduced to zero instances, ready at rollback to be scaled back up. To achieve
    the fastest form of rollbacks, disabled clusters should be left with active instances.
    This of course increases the expense of running the service, as you now pay not
    only for the cost of the set of instances serving live production traffic, but
    also the set of instances remaining from prior service versions that may potentially
    be rolled back to.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 根据底层云平台的支持情况，禁用的集群可以保留不接收任何流量的运行实例，或者可以减少到零实例，准备回滚以进行扩展。为了实现最快的回滚形式，禁用的集群应该保留活跃实例。当然，这会增加服务的费用，因为现在你不仅需要支付用于提供实时生产流量的实例集合的成本，还需要支付先前服务版本中剩余实例的成本，这些实例有可能会回滚到。
- en: Ultimately, you need to evaluate the trade-off between rollback speed and cost,
    the spectrum of which is shown in [Figure 5-9](part0010_split_011.html#cost_rollback_speed_tradeoff).
    This should be done on a microservice by microservice basis rather than across
    the organization. The extra operational cost of maintaining fully scaled disabled
    server groups in a blue/green deployment of a microservice that needs to run hundreds
    of live instances is not equivalent to the extra cost for such a service that
    only needs a handful of instances.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，您需要评估回滚速度与成本之间的权衡，其光谱显示在[图 5-9](part0010_split_011.html#cost_rollback_speed_tradeoff)中。这应该基于每个微服务而不是整个组织来完成。在蓝/绿部署中，对需要运行数百个实例的微服务维护完全缩放禁用的服务器组所产生的额外运营成本，与对只需要少数实例的服务所产生的额外成本并不相等。
- en: '![srej 0509](../images/00079.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0509](../images/00079.png)'
- en: Figure 5-9\. The trade-off between operational cost and rollback speed by deployment
    strategy
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-9。按部署策略权衡操作成本与回滚速度
- en: When a microservice is not purely RESTful, the blue/green deployment strategy
    that doesn’t completely scale to zero disabled clusters has implications for the
    application code itself.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个微服务不是纯粹的RESTful时，不完全缩放到零禁用集群的蓝/绿部署策略对应用代码本身有影响。
- en: Consider for example an (at least partially) event-driven microservice that
    reacts to messages on a Kafka topic or RabbitMQ queue. Shifting the load balancer
    from one server group to another has no effect on such a service’s connection
    to their topic/queue. In some way, the application code needs to respond to being
    placed out of service by some external process, in this case a blue/green deployment.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 例如考虑一个（至少部分地）事件驱动的微服务，它对Kafka主题或RabbitMQ队列上的消息做出反应。将负载均衡器从一个服务器组转移到另一个服务器组对这类服务连接到它们的主题/队列没有影响。在某种程度上，应用代码需要响应由外部进程将其置于服务外部的情况，本例中为蓝/绿部署。
- en: Similarly, an application process running on an instance that is part of a disabled
    server group needs to respond to being placed back *in service* by an external
    process such as a Rollback server group action in Spinnaker, in this case reconnecting
    to queues and beginning to process work again. The Spinnaker AWS implementation
    of the blue/green deployment strategy is aware of this problem, and when [Eureka](https://oreil.ly/ODfkK)
    service discovery is also in use, it influences service availability using Eureka’s
    API endpoints for taking instances in and out of service, as shown in [Table 5-1](part0010_split_011.html#eureka_api_availability).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，运行在禁用服务器组的实例上的应用程序进程需要响应由外部进程（例如Spinnaker中的回滚服务器组操作）将其重新置于*服务中*，在这种情况下重新连接到队列并开始处理工作。Spinnaker的AWS实现蓝/绿部署策略意识到了这个问题，当也在使用[Eureka](https://oreil.ly/ODfkK)服务发现时，使用Eureka的API端点影响服务的可用性，如[表 5-1](part0010_split_011.html#eureka_api_availability)所示。
- en: Notice how this is done on a per-instance basis. Spinnaker’s awareness of what
    instances exist (through polling the state of deployed environment regularly)
    helps it build this kind of automation.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这是基于每个实例进行的。Spinnaker通过定期轮询部署环境的状态来意识到存在哪些实例，从而帮助构建这种自动化。
- en: Table 5-1\. Eureka API endpoints that externally affect service availability
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-1。影响服务可用性的Eureka API端点
- en: '| Action | API | Notes |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 操作 | API | 注释 |'
- en: '| --- | --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Take instance out of service | `PUT /eureka/v2/apps/appID/instanceID/status?value=OUT_OF_SERVICE`
    |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 将实例置于服务外部 | `PUT /eureka/v2/apps/appID/instanceID/status?value=OUT_OF_SERVICE`
    |  |'
- en: '| Move instance back into service (remove override) | `DELETE /eureka/v2/apps/appID/instanceID/status?value=UP`
    | The value=UP is optional; it is used as a suggestion for the fallback status
    due to removal of the override |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 将实例重新置于服务中（移除覆盖） | `DELETE /eureka/v2/apps/appID/instanceID/status?value=UP`
    | value=UP是可选的；它被用作由于移除覆盖而建议的回退状态 |'
- en: This supposes that your application is using the Eureka service discovery client
    to register with Eureka. But doing so means that you can add a Eureka status-changed
    event listener, as shown in [Example 5-3](part0010_split_011.html#eureka_event_listener).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这假定您的应用程序正在使用Eureka服务发现客户端注册到Eureka。但这样做意味着您可以添加一个Eureka状态变更事件监听器，如[示例 5-3](part0010_split_011.html#eureka_event_listener)所示。
- en: Example 5-3\.
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-3。
- en: '[PRE2]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Naturally the same sort of workflow could be achieved with [Consul](https://www.consul.io),
    a dynamic configuration server that allows for tagging (i.e., tag by server group
    name, or cluster), or any other central source of data that has two characteristics:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 自然而然，可以使用[Consul](https://www.consul.io)来实现相同类型的工作流程，它是一个动态配置服务器，允许进行标记（例如，按服务器组名称或集群标记），或者任何其他具有以下两个特征的中心数据源：
- en: Application code can respond to change events in near real time via some sort
    of event listener.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序代码可以通过某种事件监听器几乎实时地响应变更事件。
- en: The data is groupable by at least server group, cluster, and application, and
    your application code is able to determine which server group, cluster, and application
    it belongs to.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可以按服务器组、集群和应用程序至少分组，并且您的应用程序代码能够确定它属于哪个服务器组、集群和应用程序。
- en: The same requirement to respond to external modifications of a service’s availability
    is applicable also to microservices using persistent RPC connections like [RSocket](https://rsocket.io)
    or streaming/bidirectional [GRPC](https://oreil.ly/tNORN), where a disabled server
    group needs to terminate any persistent RPC connections either outbound or inbound.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 相应地响应服务可用性的外部修改的要求也适用于使用持久化RPC连接（如[RSocket](https://rsocket.io)或流/双向[GRPC](https://oreil.ly/tNORN)）的微服务，其中禁用的服务器组需要终止任何持久化的RPC连接，无论是出站还是入站。
- en: 'There’s a hidden and significant point embedded in having to listen to discovery
    status events (or any other external indicator service availability): the application
    is aware of its participation in service discovery. A goal of service mesh (see
    [“Implementation in Service Mesh”](part0012_split_023.html#BE895-2d714b853a094e9a910510217e0e3d73))
    is to take this kind of responsibility away from the application and externalize
    it to a sidecar process or container, generally for the sake of achieving polyglot
    support for these patterns quickly. We’ll talk about other problems with this
    model later, but blue/green deployment of message-driven applications where you’d
    like to preserve live instances in disabled server groups is an example of where
    a language-specific binding (in this case for service discovery) is necessary.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 必须监听发现状态事件（或任何其他外部指示器服务可用性）中隐藏且重要的一点是：应用程序意识到其在服务发现中的参与。服务网格（参见[“服务网格中的实现”](part0012_split_023.html#BE895-2d714b853a094e9a910510217e0e3d73)）的目标是将这种责任从应用程序中移出，并将其外部化到旁路进程或容器中，通常是为了快速实现这些模式的多语言支持。稍后我们将讨论该模型的其他问题，但是在消息驱动应用程序的蓝/绿部署中，您希望保留处于禁用状态的服务器组中的活动实例，这是语言特定绑定（在这种情况下是服务发现）必要的一个例子。
- en: What’s in a Name?
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 名称中有什么？
- en: A blue/green deployment is the same thing as a red/black deployment. They are
    just different sets of colors, but the techniques have precisely the same meaning.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝/绿部署与红/黑部署是相同的事物。它们只是不同的颜色组合，但是这些技术确实具有完全相同的意义。
- en: Blue/green deployments are something every team should practice before considering
    a more complex strategy, such as automated canary analysis.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑更复杂的策略（例如自动金丝雀分析）之前，每个团队都应该在实施蓝/绿部署之前进行练习。
- en: Automated Canary Analysis
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动金丝雀分析
- en: Blue/green deployments achieve a great deal of reliability at a reasonably low
    cost most of the time. Not every service needs to go any further than this. Yet,
    there is an additional level of safety we can pursue.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝/绿部署通常在大多数情况下以相对较低的成本实现了很高的可靠性。并非每个服务都需要进一步发展。然而，我们可以追求额外的安全级别。
- en: While blue/green deployments allow you to quickly roll back a code or configuration
    change that causes unanticipated issues, canary releases provide an additional
    level of risk reduction by exposing a small subset of users to a new version of
    the service that runs alongside the existing version.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然蓝/绿部署允许您快速回滚导致意外问题的代码或配置更改，但金丝雀发布通过向现有版本旁边运行的新版本服务的小子集暴露，提供了额外的风险降低级别。
- en: Canaries aren’t appropriate for every service. Services with low throughput
    make it difficult, but not impossible, to send just a small percentage of traffic
    to a canary server group without prolonging the determination of the canary’s
    fitness to a long period of time. There isn’t a right amount of time that a canary
    fitness determination needs to take. It may very well be acceptable for you to
    run a canary test for days on a relatively low throughput service to make a determination.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每个服务都适合金丝雀部署。低吞吐量的服务使得将流量的一小部分发送到金丝雀服务器组变得困难，但并非不可能，而且不会延长金丝雀适应性的决定时间。金丝雀适应性决策需要花费的时间没有一个正确的数量。对于您来说，在相对低吞吐量的服务上运行几天的金丝雀测试以做出决定可能是完全可以接受的。
- en: There is a noticeable bias in many engineering teams to understate how much
    traffic their service actually receives and thus assume that techniques like canary
    analysis couldn’t possibly work for them. Recall the real-world team mentioned
    in [“Learning to Expect Failure”](part0005_split_005.html#monitoring_dont_expect_perfection),
    whose business application was receiving over 1,000 requests per minute. This
    throughput is significantly higher than most engineers on this team would guess.
    This is yet another reason that real production telemetry should be the first
    priority. Building up even a short history of what is happening in production
    helps you make better decisions about which kinds of techniques, in this case
    deployment strategies, are appropriate later.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 许多工程团队存在明显的偏差，低估其服务实际接收的流量，因此认为金丝雀分析等技术无法适用于他们。回想一下[“学会预期失败”](part0005_split_005.html#monitoring_dont_expect_perfection)提到的现实团队，他们的业务应用每分钟接收超过1,000个请求。这个吞吐量比大多数该团队工程师的猜测要高得多。这也是实际生产遥测数据应该是首要任务的另一个原因。建立起甚至是短期内在生产中发生的情况的历史，可以帮助你更好地决定哪种技术，例如部署策略，在后续情况下是适当的。
- en: My Service Can Never Fail Because It Is Too Important
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我的服务永远不能失败，因为它太重要了
- en: Be cautious of a line of reasoning that eschews strategies like automated canary
    analysis strictly on the basis that a particular microservice is too important
    to fail. Rather, adopt the mindset that failure is not only possible, but will
    happen on every service regardless of its importance to the business, and act
    accordingly.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 要谨慎对待那种避开自动化金丝雀分析策略的推理，仅仅因为某个微服务对于不失败太重要。相反，应采取一种观念，即失败不仅可能发生，而且无论其对业务的重要性如何，每个服务都会发生故障，据此行事。
- en: The fitness of a canary deployment is determined by comparing service level
    indicators of the old and new versions. When there is a significant enough worsening
    in one or more of these SLIs, all traffic is routed to the stable version, and
    the canary is aborted.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀部署的适应性通过比较旧版本和新版本的服务水平指标来确定。当一个或多个这些SLI出现显著恶化时，所有流量都会路由到稳定版本，并且金丝雀测试将被中止。
- en: Ideally, canary deployments consist of three server groups, as shown in [Figure 5-10](part0010_split_014.html#aca_participants).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，金丝雀部署应包括三个服务器组，如图[5-10](part0010_split_014.html#aca_participants)所示。
- en: '![srej 0510](../images/00006.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0510](../images/00006.png)'
- en: Figure 5-10\. Canary release participants
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5-10\. 金丝雀发布参与者
- en: 'These canary deployments can be described as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这些金丝雀部署可以描述如下：
- en: Production
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 生产环境
- en: This is the existing server group prior to the canary deployment, containing
    one or more instances.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这是金丝雀部署之前的现有服务器组，包含一个或多个实例。
- en: Baseline
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 基线
- en: This server group runs the same version of code and configuration as the production
    server group. While it may seem counterintuitive to run another copy of the old
    code at first, we need a baseline that is launched at roughly the same time as
    the canary because a production server group, by virtue of the fact that it has
    been running for some amount of time, may have different characteristics like
    heap consumption or cache contents. It is important to be able to make an accurate
    comparison between the old code and new, and the best way to do that is to launch
    copies of each at roughly the same time.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个服务器组运行与生产服务器组相同版本的代码和配置。虽然一开始运行另一个旧代码副本似乎有些违反直觉，但我们需要一个基线，它大致在金丝雀发布时启动，因为生产服务器组由于运行了一段时间，可能具有不同的特征，比如堆消耗或缓存内容。能够准确比较旧代码和新代码之间的差异非常重要，而最佳方法是大致同时启动每个副本。
- en: Canary
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀
- en: This server group consists of the new code or configuration.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这个服务器组包含了新的代码或配置。
- en: The fitness of the canary is entirely determined by comparing a set of metrics
    indicators relative only to the baseline (not the production cluster). This implies
    that applications undergoing canaries are publishing metrics with a `cluster`
    common tag so that the canary analysis system can aggregate over indicators coming
    from instances belonging to the canary and baseline clusters and compare the two
    aggregates relative to each other.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀的适应性完全由将一组指标相对于基准线（而不是生产集群）进行比较来确定。这意味着正在进行金丝雀测试的应用正在发布带有`cluster`公共标签的度量，以便金丝雀分析系统可以聚合来自属于金丝雀和基准集群的实例的指标，并相互比较这两个聚合指标。
- en: This relative comparison is much preferable to testing a canary against a set
    of fixed thresholds because fixed thresholds tend to make certain assumptions
    about the amount of throughput going through the system at test time. [Figure 5-11](part0010_split_014.html#canary_test_against_fixed_threshold)
    shows the problem. The application exhibits a higher response time during peak
    business hours when most traffic is flowing through the system (which is typical).
    So for the fixed threshold, perhaps we try to set a value that would fail a canary
    if response time was more than 10% worse than the normative case. During peak
    business hours, the canary would fail because it has a worse than 10% degradataion
    from the baseline. But if we ran the canary test after hours, it could be significantly
    worse than 10% from the baseline and still be under the fixed threshold, which
    was set to 10% worse than what we expect under *different operating conditions*.
    There is a greater chance that a relative comparison would be able to catch a
    performance degradation whether the test runs during or after business hours.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 相对比较远比测试一个金丝雀针对一组固定阈值要可取得多，因为固定阈值往往对测试时系统的吞吐量做出某些假设。[图 5-11](part0010_split_014.html#canary_test_against_fixed_threshold)展示了这个问题。在高峰业务时间，应用展示出更高的响应时间，这时系统流量最大（这是典型情况）。因此，对于固定阈值，也许我们试图设置一个数值，如果响应时间比正常情况差10%以上，金丝雀测试就会失败。在高峰业务时间，金丝雀测试可能会失败，因为相对于基准线，其性能比差了超过10%。但是如果我们在非高峰业务时间运行金丝雀测试，它可能会比基准线差得多于10%，但仍然在设定的固定阈值内，因为这个固定阈值是相对于*不同操作条件*设定的。相对比较的方法更有可能在测试运行时无论是在或者非业务高峰期，都能捕捉到性能下降。
- en: '![srej 0511](../images/00045.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0511](../images/00045.png)'
- en: Figure 5-11\. Canary test against fixed threshold
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-11\. 对固定阈值的金丝雀测试
- en: On several occassions, I’ve met with organizations about automated delivery
    practices where the conversation came about because someone heard about canary
    deployments and the idea sounds so compelling that it stimulates an interest in
    the topic. Commonly, these organizations didn’t have dimensional metrics instrumentation
    in place or an automated release process resembling a blue/green deployment. Maybe
    because of the allure of the safety of a canary deployment, platforms sometimes
    include canary deployment features. Often they lack baselining and/or comparative
    measurement, so evaluate platform-provided canary features from this perspective
    and decide whether giving up one or both still makes sense. I would suggest it
    doesn’t in many cases.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个场合，我会和组织讨论自动化交付实践，谈论到金丝雀部署，这个想法听起来如此吸引人，以至于激发了对这个主题的兴趣。通常，这些组织没有维度度量仪器，也没有类似蓝/绿部署的自动发布流程。也许是因为金丝雀部署的安全性吸引力，平台有时会包含金丝雀部署功能。通常情况下，它们缺少基线和/或比较测量，因此从这个角度评估平台提供的金丝雀功能，并决定是否放弃其中一个或两个功能，这在许多情况下并不明智。我建议在许多情况下都不应该。
- en: In the three-cluster setup (production, baseline, canary), most traffic will
    go to the production cluster, with a small amount going to the baseline and canary.
    The canary deployment uses load balancer configuration, service mesh configuration,
    or whatever other platform feature is available to distribute traffic proportionally.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在三集群设置（生产、基准、金丝雀）中，大部分流量将流向生产集群，少量流向基准和金丝雀。金丝雀部署使用负载均衡器配置、服务网格配置或任何其他平台功能来按比例分发流量。
- en: '[Figure 5-12](part0010_split_014.html#spinnaker_aca_infra_when_canary) shows
    a Spinnaker infrastructure view of the three clusters participating in a canary
    test. In this case, they are running on a single Kubernetes cluster that has been
    named “PROD-CLUSTER” in Spinnaker (“cluster” referring to Kubernetes cluster,
    not as we’ve defined the word in the delivery definitions at the beginning of
    this chapter).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-12](part0010_split_014.html#spinnaker_aca_infra_when_canary) 展示了参与 canary
    测试的三个集群的 Spinnaker 基础设施视图。在这个案例中，它们在一个名为“PROD-CLUSTER”的单个 Kubernetes 集群上运行（“cluster”
    指的是 Kubernetes 集群，不是我们在本章开头定义的交付定义中的含义）。'
- en: Spinnaker integrates with an open source automated canary analysis service,
    which encapsulates the evaluation of metrics from baseline and canary clusters.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: Spinnaker 与一个开源自动化 canary 分析服务集成，该服务封装了来自 baseline 和 canary 集群的度量评估。
- en: '![srej 0512](../images/00042.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0512](../images/00042.png)'
- en: Figure 5-12\. Three clusters of an application undergoing a canary
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-12\. 应用程序 undergoing a canary 的三个集群
- en: Spinnaker with Kayenta
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spinnaker 配合 Kayenta 使用
- en: '[Kayenta](https://oreil.ly/f4KZW) is a stand-alone open source automated canary
    analysis service that is also deeply integrated into Spinnaker by way of pipeline
    stages and configuration.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '[Kayenta](https://oreil.ly/f4KZW) 是一个独立的开源自动化 canary 分析服务，也通过管道阶段和配置深度集成到 Spinnaker
    中。'
- en: Kayenta determines whether there is a significant difference between the canary
    and baseline for each metric, yielding a *pass*, *high*, or *low* classification.
    *High* and *low* are both failing conditions. Kayenta makes this difference comparatively
    between the two clusters using a [Mann-Whitney *U* test](https://oreil.ly/qLYOS).
    The implementation of this statistical test is called a judge, and Kayenta could
    be configured with alternative judges, but they typically involve code that goes
    beyond what you could achieve through a single query against a metrics system.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Kayenta 确定每个指标的 canary 和 baseline 之间是否存在显著差异，得出 *pass*、*high* 或 *low* 的分类结果。*High*
    和 *low* 都属于失败条件。Kayenta 使用 [Mann-Whitney *U* test](https://oreil.ly/qLYOS) 在两个集群之间进行统计上的比较。这个统计测试的实现称为
    judge，Kayenta 可以配置使用其他的 judge，但它们通常涉及超出单一查询度量系统所能达到的代码。
- en: '[Figure 5-13](part0010_split_015.html#aca_metrics) presents an example of Kayenta’s
    classification decisions for several metrics. This screenshot is from the original
    Netflix [blog](https://oreil.ly/ik79d) on Kayenta. In this case, latency has failed
    the test.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-13](part0010_split_015.html#aca_metrics) 展示了 Kayenta 对多个指标进行分类决策的示例。这张截图来自原始的
    Netflix [博客](https://oreil.ly/ik79d) 关于 Kayenta 的内容。在这个案例中，延迟未通过测试。'
- en: '![srej 0513](../images/00028.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0513](../images/00028.png)'
- en: Figure 5-13\. Canary metrics
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-13\. Canary 指标
- en: In Spinnaker, the canary metrics for an application can be defined in the “Canary
    Configs” tab of the application infrastructure view. In the configuration, as
    shown in [Figure 5-14](part0010_split_015.html#spinnaker_aca_canary_config), you
    can define one or more service level indicators. If enough of these indicators
    fail, the canary will fail.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spinnaker 中，一个应用程序的 canary 指标可以在应用程序基础设施视图的“Canary Configs”选项卡中定义。在配置中，如 [图
    5-14](part0010_split_015.html#spinnaker_aca_canary_config) 所示，您可以定义一个或多个服务水平指标。如果足够多的这些指标失败，canary
    将失败。
- en: '![srej 0514](../images/00118.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0514](../images/00118.png)'
- en: Figure 5-14\. Canary configuration for an application in Spinnaker
  id: totrans-174
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-14\. Spinnaker 中一个应用程序的 canary 配置
- en: '[Figure 5-15](part0010_split_015.html#spinnaker_aca_cpu_configuration) shows
    the configuration of a single indicator, in this case processor utilization. Notice
    how the configuration contains a metrics query that is specific to a monitoring
    system that you have configured Kayenta to poll from (in this case, Prometheus).
    You then indicate very broadly that an increase or decrease (or a deviation either
    way) is considered bad. We would not like to see significantly higher processor
    utilization in this case, although a decrease would be welcome.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5-15](part0010_split_015.html#spinnaker_aca_cpu_configuration) 展示了单个指标的配置，即处理器利用率。请注意，配置包含一个针对监控系统的特定度量查询，您已经配置
    Kayenta 来从中轮询（在本案例中是 Prometheus）。然后，您广泛指示增加或减少（或任何方向的偏差）被认为是不良的。在这种情况下，我们不希望看到处理器利用率显著增加，尽管减少则是受欢迎的。'
- en: On the other hand, a *decrease* in servicing throughput for an application that
    should process rather continuously at a certain rate would be a bad sign. The
    indicator can be marked as critical enough that failure alone should fail the
    canary.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，对于应该以某一速率持续处理的应用程序来说，服务吞吐量的*减少*将是一个不良信号。该指标可以标记为足够严重，仅仅失败就应该导致金丝雀失败。
- en: '![srej 0515](../images/00071.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0515](../images/00071.png)'
- en: Figure 5-15\. Processor utilization canary configuration
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-15\. 处理器利用率金丝雀配置
- en: Once the canary configuration is built, it can be used in pipelines. A characteristic
    canary deployment pipeline is shown in [Figure 5-16](part0010_split_015.html#spinnaker_aca_pipeline).
    The “Configuration” stage defines the triggers that begin the process of evaluating
    the canary. “Set Cluster Name to Canary” sets a variable that is used in the subsequent
    stage “Deploy Canary” by Spinnaker to name the cluster canary. It is this variable
    that eventually yields the named canary cluster shown in [Figure 5-12](part0010_split_014.html#spinnaker_aca_infra_when_canary).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀配置一旦建立，就可以在流水线中使用。如图 [5-16](part0010_split_015.html#spinnaker_aca_pipeline)
    所示，一个典型的金丝雀部署流水线。在“配置”阶段定义了触发器，开始评估金丝雀的流程。“将集群名称设置为金丝雀”设置了一个变量，Spinnaker 在随后的“部署金丝雀”阶段中使用该变量来命名金丝雀集群。正是这个变量最终产生了如图
    [5-12](part0010_split_014.html#spinnaker_aca_infra_when_canary) 所示的命名金丝雀集群。
- en: '![srej 0516](../images/00022.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0516](../images/00022.png)'
- en: Figure 5-16\. A canary deployment pipeline in Spinnaker
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-16\. Spinnaker 中的金丝雀部署流水线
- en: In parallel, Spinnaker is retrieving the artifacts that the current production
    version is based off of and creating a baseline cluster with those artifacts as
    well. The “Canary Analysis” stage could run for hours or even days, depending
    on how it is configured. If it passes, we will deploy a new prod cluster (with
    the same artifact used to created the canary, which may no longer be the latest
    version available in the artifact repository). In parallel, we can tear down the
    baseline and canary clusters that are no longer needed. This whole pipeline can
    be configured in Spinnaker to be run serially so that only one canary is being
    evaluated at any given time.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，Spinnaker 正在检索当前生产版本所基于的工件，并使用这些工件创建基线集群。 “金丝雀分析”阶段的运行时间可能长达数小时甚至数天，具体取决于其配置。如果测试通过，我们将部署一个新的生产集群（使用用于创建金丝雀的相同工件，这些工件可能不再是存储库中最新的版本）。同时，可以拆除不再需要的基线和金丝雀集群。整个流水线可以在
    Spinnaker 中配置为串行运行，以便每次只评估一个金丝雀。
- en: The outcome of a canary run is viewable in a few different ways. Spinnaker presents
    a “Canary Reports” tab that shows the outcome of the judgment for a canary stage,
    breaking down each service level indicator that goes into the decision individually.
    Each indicator can be viewed as a time series graph over the interval in which
    the canary ran, as in [Figure 5-17](part0010_split_015.html#spinnaker_aca_cpu_comparison).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀运行的结果可以通过几种不同的方式查看。Spinnaker 提供了一个“金丝雀报告”选项卡，显示了每个服务水平指标的判断结果，以及单独评估每个进入决策的指标。每个指标可以作为时间序列图在金丝雀运行期间查看，就像
    [图 5-17](part0010_split_015.html#spinnaker_aca_cpu_comparison) 中显示的那样。
- en: '![srej 0517](../images/00108.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0517](../images/00108.png)'
- en: Figure 5-17\. Time series visualization of CPU utilization in baseline and canary
  id: totrans-185
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-17\. 基线和金丝雀中 CPU 利用率的时间序列可视化
- en: Current Production Version Isn’t Always Latest
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 当前生产版本并不总是最新版本
- en: Note that the current production version from which a baseline will be created
    is *not* always the latest version of the application binary (e.g., JAR or WAR)
    available in an artifact repository. It may actually be several versions older,
    in cases where we have attempted to release new versions but they have failed
    canaries or were otherwise rolled back. One of the values of a stateful continuous
    delivery solution like Spinnaker is its ability to poll the environment for what
    is there and to then act upon this information.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，从中创建基线的当前生产版本并不总是应用程序二进制（例如 JAR 或 WAR）存储库中最新的版本。在某些情况下，它实际上可能是几个版本较旧的版本，这是因为我们曾试图发布新版本，但它们在试验或其他情况下被回退了。像
    Spinnaker 这样的有状态持续交付解决方案的一个价值在于其能力，即轮询环境以获取当前状态，并基于此信息采取行动。
- en: Alternatively, the comparison for an indicator can be viewed as a bar chart
    (or histogram), as in [Figure 5-18](part0010_split_016.html#spinnaker_aca_latency_histogram_view).
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以将指标的比较视为条形图（或直方图），如 [图 5-18](part0010_split_016.html#spinnaker_aca_latency_histogram_view)
    中所示。
- en: '![srej 0518](../images/00062.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0518](../images/00062.png)'
- en: Figure 5-18\. Histogram visualization of 99th-percentile latency
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-18\. 99百分位延迟的直方图可视化
- en: Lastly, and perhaps most usefully, the comparison between canary and baseline
    can be visualized as a beeswarm plot, shown in [Figure 5-19](part0010_split_016.html#spinnaker_aca_latency_beeswarm_view).
    The canary is judged over time, with Kayenta polling the monitoring system for
    values for the canary and baseline at a prescribed interval. The individual samples
    here are shown on the beeswarm plot along with a box-and-whisker plot showing
    the basic quartiles (min, 25th percentile, median, 75th percentile, and max of
    all samples). The median has certainly increased, but as discussed in [Chapter 2](part0006_split_000.html#5N3C4-2d714b853a094e9a910510217e0e3d73),
    measures of centrality like mean and median aren’t really that useful for judging
    the fitness of a service. This plot really highlights this fact. The max and even
    75% latency haven’t changed much at all between versions. So there’s a little
    more variation in the median, but it probably doesn’t indicate a performance regression
    at all.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，也许最有用的是，可以将金丝雀和基准之间的比较可视化为蜂群图，如[图 5-19](part0010_split_016.html#spinnaker_aca_latency_beeswarm_view)所示。金丝雀随时间而判断，Kayenta定期轮询监控系统以获取金丝雀和基准的值。这里的单个样本显示在蜂群图上，以及显示所有样本的基本四分位数（最小值、25th百分位数、中位数、75th百分位数和最大值）的箱形图。中位数肯定增加了，但正如在[第2章](part0006_split_000.html#5N3C4-2d714b853a094e9a910510217e0e3d73)中讨论的那样，像均值和中位数这样的中心度量并不真正有用于判断服务的适用性。这个图表确实突显了这一事实。最大值甚至75%的延迟在版本之间几乎没有变化。因此，中位数的变化略有增加，但这可能根本不表示性能退化。
- en: '![srej 0519](../images/00021.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0519](../images/00021.png)'
- en: Figure 5-19\. Beeswarm visualization of 99th-percentile latency
  id: totrans-193
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5-19\. 99百分位延迟的蜂群图可视化
- en: The key indicators for canary analysis will be different sometimes than the
    indicators we use to alert, because they are designed for comparative analysis
    between two clusters rather than against an absolute measure. Even if a new application
    version remains underneath a service level objective boundary that we’ve set as
    the test for an alert, it’s still probably best that the general trajectory of
    the code doesn’t continue to regress closer and closer to that service level objective.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀分析的关键指标有时会与我们用于警报的指标不同，因为它们是为了在两个集群之间进行比较分析而设计的，而不是针对绝对测量。即使新的应用程序版本仍然在我们设置为警报测试的服务水平目标边界之下，最好的代码的一般轨迹仍然不要继续向该服务水平目标逐渐退化。
- en: General-Purpose Canary Metrics for Every Microservice
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为每个微服务提供通用的金丝雀指标
- en: Think of the L-USE acronym when considering canary metrics that are useful to
    start with. In fact, many of the same SLIs that you should alert on for most microservices
    are also good canary metrics, with a twist.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑起始时有用的金丝雀指标时，请考虑L-USE首字母缩略词。事实上，对于大多数微服务应该发出警报的相同的服务级别指标也是很好的金丝雀指标，只是稍有不同。
- en: Let’s consider a few good canary metrics, beginning with latency. Really any
    of the signals described in [Chapter 4](part0009_split_000.html#8IL24-2d714b853a094e9a910510217e0e3d73)
    are good candidates for canary analysis.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些好的金丝雀指标，首先是延迟。实际上，[第4章](part0009_split_000.html#8IL24-2d714b853a094e9a910510217e0e3d73)中描述的任何信号都是金丝雀分析的好候选者。
- en: Latency
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 延迟
- en: Latency for some bellwether API endpoint is a good starting point. Constrain
    the metric to successful outcomes only, since successful outcomes tend to have
    different latency characteristics than unsuccessful outcomes. Imagine fixing a
    bug that caused failures for a critical API endpoint in production, only to have
    the canary fail because the bug caused the API endpoint to fail fast and the canary
    decides that latency has degraded too much by fixing it!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一些指示性API端点的延迟是一个很好的起点。将指标限制在成功的结果上，因为成功的结果往往具有与不成功的结果不同的延迟特性。想象一下，在生产中修复了导致关键API端点失败的错误后，由于该错误导致API端点快速失败，而金丝雀却认为修复该错误导致了延迟过高而失败！
- en: In [Chapter 4](part0009_split_000.html#8IL24-2d714b853a094e9a910510217e0e3d73),
    the idea was to measure a timed operation’s decaying *maximum* latency against
    a fixed service level objective, that service level objective being a conservative
    boundary inside engineering’s service level agreement determined with business
    partners. But maximum latency has a way of being spiky. For example, hypervisor
    and garbage collection pauses or full connection pools are mostly temporary conditions
    (and outside of your control) that naturally affect instances at different times.
    For the purposes of measuring an application’s fitness relative to a service level
    objective, we want to make sure that performance is still acceptable even under
    these conditions. But because of the staggered nature in which they occur on different
    instances, these effects lead to bad *comparative* measures.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第四章](part0009_split_000.html#8IL24-2d714b853a094e9a910510217e0e3d73) 中，这个想法是测量一个定时操作的衰减
    *最大* 延迟相对于固定的服务水平目标，这个服务水平目标是工程服务级别协议中的保守边界，与业务伙伴确定。但最大延迟往往是波动的。例如，Hypervisor
    和垃圾收集暂停或完全连接池大多是暂时的条件（并且超出你的控制），自然会在不同时间影响实例。为了衡量应用程序相对于服务水平目标的适应性，我们希望确保即使在这些条件下，性能仍然是可接受的。但由于它们在不同实例上发生的交错性质，这些效果导致了不良的
    *比较* 措施。
- en: For canaries, it is best to look at a distribution statistic like 99th-percentile
    latency, which shaves off the top 1% where these temporary conditions exhibit.
    99th percentile (or some other high percentile) is generally a better measure
    of a piece of code’s performance *potential* minus temporary environmental factors.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对于金丝雀，最好查看像第99百分位延迟这样的分布统计，它剔除了这些临时条件表现出来的顶部1%。99th 百分位数（或其他高百分位数）通常是代码性能 *潜力*
    的更好度量，减去临时环境因素。
- en: Recall from [“Histograms”](part0007_split_007.html#6LKA8-2d714b853a094e9a910510217e0e3d73)
    that in order to compute a high-percentile latency across a cluster (and constrained
    to successful outcomes for a particular endpoint), we need to use a method like
    percentile approximation based off of histogram data that can be summed across
    the instances in this cluster and any other tag variation that exists for successful
    outcomes for this critical API endpoint. Only a handful of monitoring systems
    at this point support aggregable percentile approximation. If your monitoring
    system can’t do percentile approximation, do not attempt to aggregate individual
    percentiles from instances (we showed why the math for this doesn’t work in [“Percentiles/Quantiles”](part0007_split_003.html#6LK44-2d714b853a094e9a910510217e0e3d73)).
    Also, avoid the temptation to use another measure like average. Look at the beeswarm
    plot in [Figure 5-18](part0010_split_016.html#spinnaker_aca_latency_histogram_view)
    to see how measures of centrality like median and mean can vary widely between
    versions (and realistically even over time with the same version!) without any
    real change in performance.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 从 [“直方图”](part0007_split_007.html#6LKA8-2d714b853a094e9a910510217e0e3d73) 中回忆，为了在群集中计算高百分位延迟（并且限制为特定端点的成功结果），我们需要使用像基于直方图数据的百分位近似这样的方法，可以在此群集中和任何其他标记变化之间进行累加，以此关键
    API 端点的成功结果。目前只有少数监控系统支持可聚合的百分位近似。如果您的监控系统不能进行百分位近似，请勿尝试从实例中聚合单个百分位数（我们展示了为什么这样的数学不适用于
    [“百分位/分位数”](part0007_split_003.html#6LK44-2d714b853a094e9a910510217e0e3d73)）。此外，避免使用其他像平均值这样的测量方法。查看
    [图 5-18](part0010_split_016.html#spinnaker_aca_latency_histogram_view) 中的蜂群图，了解像中位数和均值这样的中心性度量如何在版本之间（实际上甚至在相同版本的时间内！）有很大的变化，而没有任何真正的性能变化。
- en: 'Average: a random number that falls somewhere between the maximum and 1/2 the
    median. Most often used to ignore reality.'
  id: totrans-203
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 'Average: 介于最大值和中位数的一半之间的随机数。通常用于忽视现实。'
- en: ''
  id: totrans-204
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Gil Tene
  id: totrans-205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 吉尔·特纳
- en: To compute an aggregable percentile approximation for Atlas, use the `:percentiles`
    function, as in [Example 5-4](part0010_split_017.html#atlas_percentile_latency).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算 Atlas 的可聚合百分位近似，使用 `:percentiles` 函数，如 [示例 5-4](part0010_split_017.html#atlas_percentile_latency)
    所示。
- en: Example 5-4\. Atlas percentile latency for canaries
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 5-4\. Atlas 百分位延迟对金丝雀
- en: '[PRE3]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For Prometheus, use the `histogram_quantile` function, as in [Example 5-5](part0010_split_017.html#prometheus_percentile_latency).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Prometheus，使用 `histogram_quantile` 函数，如 [示例 5-5](part0010_split_017.html#prometheus_percentile_latency)
    所示。
- en: Example 5-5\. Prometheus percentile latency for canaries
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 5-5\. 金丝雀的 Prometheus 百分位延迟
- en: '[PRE4]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In a similar way, you should include latency metrics for interactions with critical
    downstream resources like a database. Consider relational database interactions.
    It is not uncommon for new code to inadvertently cause an existing database index
    to go unused (increasing latency substantially, as well as load on the database),
    or for a new index to not perform as well as expected once it gets to production.
    Regardless of how well we try to replicate and test these new interactions in
    lower-level environments, there is nothing quite like production.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，你应该包括与关键下游资源的交互的延迟指标，如数据库。考虑关系数据库的交互。新代码可能会意外地导致现有数据库索引未被使用（显著增加延迟和数据库负载），或者新索引在投产后表现不如预期。无论我们如何努力在低级环境中复制和测试这些新的交互，实际生产环境永远不会如此。
- en: Error ratio
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 错误比率
- en: Error ratio ([Example 5-6](part0010_split_017.html#atlas_error_ratio) on Atlas
    and [Example 5-7](part0010_split_017.html#prometheus_error_ratio) for Prometheus)
    on some bellwether API endpoint (or all of them) can be incredibly useful as well,
    as this will determine whether you have introduced semantic regressions into your
    code that may have not been caught by tests but are causing issues in production.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 错误比率（在 Atlas 的 [示例 5-6](part0010_split_017.html#atlas_error_ratio) 和 Prometheus
    的 [示例 5-7](part0010_split_017.html#prometheus_error_ratio) 上）对于某些基准 API 端点（或全部端点）同样非常有用，因为这将确定您是否引入了语义回归问题，这些问题可能未被测试捕获，但却在生产中造成问题。
- en: Example 5-6\. Error ratio of HTTP server requests in Atlas
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-6\. Atlas 中 HTTP 服务器请求的错误比率
- en: '[PRE5]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Example 5-7\. Error ratio of HTTP server requests in Prometheus
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-7\. Prometheus 中 HTTP 服务器请求的错误比率
- en: '[PRE6]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Carefully consider whether to include more than one API endpoint in a single
    canary signal. Suppose you had two separate API endpoints that receive significantly
    different throughput, one receiving 1,000 requests/second and the other receiving
    10 requests/second. Because our service isn’t perfect (what is?), the old code
    is failing the high throughput endpoint at a fixed rate of 3 requests/second,
    but all requests to the low throughput endpoint succeed. Now imagine we make a
    code change that causes 3 out of 10 requests to the low throughput endpoint to
    fail but doesn’t change the error ratio of the other endpoint. If these endpoints
    are considered together by the canary judge, the judge will probably pass the
    regression because it only resulted in a small uptick in error ratio (0.3% to
    0.6%). Considered separately, however, the judge would certainly fail the error
    ratio on the low throughput endpoint (0% to 33%).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细考虑是否在单个金丝雀信号中包含多个 API 端点。假设您有两个单独的 API 端点，它们接收的吞吐量显著不同，一个接收每秒 1,000 次请求，另一个接收每秒
    10 次请求。由于我们的服务并非完美（什么是完美？），旧代码在高吞吐量端点上以固定速率失败，每秒 3 次请求，但所有低吞吐量端点的请求都成功。现在想象我们进行代码更改，导致低吞吐量端点的每
    10 次请求中有 3 次失败，但不会改变另一个端点的错误比率。如果这些端点被金丝雀判断一起考虑，判断可能会通过回归，因为错误比率略有上升（从 0.3% 到
    0.6%）。然而，如果分开考虑，判断肯定会在低吞吐量端点的错误比率上失败（从 0% 到 33%）。
- en: Heap saturation
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 堆饱和度
- en: 'Heap utilization can be compared in two ways: for total consumption relative
    to the maximum heap and allocation performance.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 堆利用率可以通过两种方式进行比较：对于总消耗相对于最大堆和分配性能。
- en: Total consumption is determined by dividing used by max, shown in [Example 5-8](part0010_split_017.html#atlas_heap_canary)
    and [Example 5-9](part0010_split_017.html#prometheus_heap_canary).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 总消耗由使用量除以最大值确定，如 [示例 5-8](part0010_split_017.html#atlas_heap_canary) 和 [示例 5-9](part0010_split_017.html#prometheus_heap_canary)
    所示。
- en: Example 5-8\. Atlas heap consumption canary metric
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-8\. Atlas 堆消耗的金丝雀指标
- en: '[PRE7]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Example 5-9\. Prometheus heap consumption canary metric
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-9\. Prometheus 堆消耗的金丝雀指标
- en: '[PRE8]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Allocation performance can be measured by dividing allocations by promotions,
    shown in Examples [5-10](part0010_split_017.html#atlas_allocation_canary) and
    [5-11](part0010_split_017.html#prometheus_allocation_canary).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 分配性能可以通过分配量除以提升量来衡量，如示例 [5-10](part0010_split_017.html#atlas_allocation_canary)
    和 [5-11](part0010_split_017.html#prometheus_allocation_canary) 所示。
- en: Example 5-10\. Atlas allocation performance canary metric
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-10\. Atlas 分配性能的金丝雀指标
- en: '[PRE9]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Example 5-11\. Prometheus allocation performance canary metric
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-11\. Prometheus 分配性能的金丝雀指标
- en: '[PRE10]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: CPU utilization
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CPU 利用率
- en: Process CPU utilization can be compared rather simply, shown in [Example 5-12](part0010_split_017.html#atlas_cpu_canary)
    and [Example 5-13](part0010_split_017.html#prometheus_cpu_canary).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 处理器 CPU 利用率可以相对简单地进行比较，如 [示例 5-12](part0010_split_017.html#atlas_cpu_canary)
    和 [示例 5-13](part0010_split_017.html#prometheus_cpu_canary) 所示。
- en: Example 5-12\. Atlas CPU utilization canary metric
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-12\. Atlas CPU 利用率金丝雀指标
- en: '[PRE11]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Example 5-13\. Prometheus CPU utilization canary metric
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 5-13\. Prometheus CPU 利用率金丝雀指标
- en: '[PRE12]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Add canary metrics incrementally, since failed canary tests block the production
    path, potentially slowing down the delivery of features and bug fixes unnecessarily.
    Canary failures should be tuned to block dangerous regressions.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步增加金丝雀指标，因为失败的金丝雀测试会阻塞生产路径，可能不必要地减慢功能和错误修复的交付速度。金丝雀失败应调整为阻止危险的回归。
- en: Summary
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'This chapter introduced continuous delivery concepts at a high level with Spinnaker
    as its example system. You don’t need to rush to adopt Spinnaker in order to find
    a way to gain some of the benefits. For many enterprises, I believe clearing two
    hurdles would go a long way in improving release success:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了连续交付概念的高层次，以 Spinnaker 作为其示例系统。你不需要急于采用 Spinnaker 以获取一些好处。对于许多企业来说，我认为清除两个障碍将极大地提高发布成功率：
- en: Blue/green capability
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝/绿能力
- en: Have a blue/green deployment strategy that supports *N* active disabled clusters
    for fast rollback and takes into account the unique needs of event-driven applications
    (as switching a load balancer isn’t enough to effectively take an event-driven
    application out of service).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 必须有一个支持 *N* 个活动禁用集群以便快速回滚并考虑到事件驱动应用程序独特需求的蓝/绿部署策略（因为仅切换负载均衡器不足以有效将事件驱动应用程序停止服务）。
- en: Deployed asset inventory
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 已部署资产清单
- en: Have some means of querying the live state of your deployed assets. It’s easier
    (and likely more accurate) to accomplish this by actually polling the state of
    your deployed environments periodically than by trying to make every possible
    mutating action pass through some central system like a CI server and trying to
    rebuild the state of the system from all the individual mutations that have occurred.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 必须有一些手段来查询部署资产的实时状态。通过定期轮询部署环境的状态，实际上比试图使每个可能的变异动作通过某些像 CI 服务器这样的中央系统并尝试从发生的所有个别变异中重建系统状态更容易（也可能更准确）。
- en: A further goal would be to have sufficient access and quality controls in place
    in your delivery system (again, whether that is Spinnaker or something else) to
    allow for some variation in deployments between teams. Some deployments, especially
    for static assets or internal tools, aren’t going to benefit from blue/green deployment
    significantly. Others may release frequently enough that multiple disabled server
    groups in a blue/green deployment strategy are needed. Some start up fast enough
    that having active instances in disabled clusters represents a cost inefficiency.
    A platform engineering team thinking in terms of “guardrails not gates” will favor
    allowing this pipeline diversity over organizational consistency, maximizing the
    safety/cost trade-off uniquely for each team.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的目标是在交付系统中确保足够的访问和质量控制（再次强调，无论是 Spinnaker 还是其他系统），以允许团队之间的一些部署变化。对于某些部署，特别是静态资产或内部工具，蓝/绿部署可能不会带来显著好处。其他可能会频繁发布，因此需要蓝/绿部署策略中的多个禁用服务器组。有些启动速度快到在禁用的集群中拥有活跃实例会导致成本效率低下。一个以“护栏而非门栓”思维的平台工程团队将更倾向于允许这种管道多样性，而不是组织一致性，从而最大化每个团队独特的安全/成本权衡。
- en: In the next chapter, we’ll use the presence of a deployed asset inventory as
    an assumption for building an artifact provenance chain for your deployed assets
    that reaches down to the source code running in each environment.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将假设已部署资产清单，用于构建一个到每个环境中运行的源代码的工件溯源链。
