<html><head></head><body>
<div id="sbo-rt-content"><section data-pdf-bookmark="Chapter 2. Performance Testing Methodology" data-type="chapter" epub:type="chapter"><div class="chapter" id="pracjavaperf-CHP-2">
<h1><span class="label">Chapter 2. </span>Performance Testing Methodology</h1>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id95">
<h1>A Note for Early Release Readers</h1>
<p>With Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.</p>
<p>This will be the 2nd chapter of the final book.</p>
<p>If you have comments about how we might improve the content and/or examples in this book, or if you notice missing material within this chapter, please reach out to the editor at <em>rfernando@oreilly.com</em>.</p>
</div></aside>
<p>Performance testing is undertaken for a variety of reasons.
In this chapter we will introduce the different types of performance test that a team may wish to execute, and discuss some best practices for each subtype of testing.</p>
<p>Later in the chapter we will discuss statistics, and some very important human factors that, are often neglected when  considering performance problems.</p>
<section data-pdf-bookmark="Types of Performance Test" data-type="sect1"><div class="sect1" id="pracjavaperf-CHP-2-SECT-1">
<h1>Types of Performance Test</h1>
<p>Performance tests are frequently conducted for the wrong reasons, or conducted
badly. The reasons for this vary widely, but are often rooted in a failure to
understand the nature of performance analysis and a belief that “doing something is
better than doing nothing.”
As we will see several times throughout the book, this belief is often a dangerous half-truth at best.</p>
<p>One of the more common mistakes is to speak generally of “performance testing” without
engaging with the specifics. In fact, there are many different
types of large-scale performance tests that can be conducted on a system.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Good performance tests are quantitative. They ask questions that produce a numeric
answer that can be handled as an experimental output and subjected to statistical
analysis.</p>
</div>
<p>The types of performance tests we will discuss in this book usually have independent (but somewhat overlapping) goals.
It is therefore important to understand the quantitative questions you are trying to answer before deciding what type of testing should be carried out.</p>
<p>This doesn’t have to be that complex—​simply writing down the questions the test is intended to answer can be enough.
However, it is usual to consider why these tests are important for the application and confirming the reason with the application owner (or key customers).</p>
<p>Some of the most common test types, and an example question for each, are as follows:</p>
<dl>
<dt>Latency test</dt>
<dd>
<p>What is the end-to-end transaction time?</p>
</dd>
<dt>Throughput test</dt>
<dd>
<p>How many concurrent transactions can the current system capacity
deal with?</p>
</dd>
<dt>Load test</dt>
<dd>
<p>Can the system handle a specific load?</p>
</dd>
<dt>Stress test</dt>
<dd>
<p>What is the breaking point of the system?</p>
</dd>
<dt>Endurance test</dt>
<dd>
<p>What performance anomalies are discovered when the system is run for an
extended period?</p>
</dd>
<dt>Capacity planning test</dt>
<dd>
<p>Does the system scale as expected when additional resources are added?</p>
</dd>
<dt>Degradation</dt>
<dd>
<p>What happens when the system is partially failed?</p>
</dd>
</dl>
<p>Let’s look in more detail at each of these test types in turn.</p>
<section data-pdf-bookmark="Latency Test" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-1.1">
<h2>Latency Test</h2>
<p>Latency is one of the most common types of performance test, because it is often a system observable of keen interest to management (and users): how long are our customers waiting for a transaction (or a page load)?</p>
<p>This can a double-edged sword, because the simplicity of the question (that a latency test seeks to answer) can cause teams to focus too much on latency.
This, in turn, can cause the team to ignore the necessity of identifying quantitative questions for other types of performance tests.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The goal of a latency tuning exercise is usually to directly improve the user experience, or to meet a service-level agreement.</p>
</div>
<p>However, even in the simplest of cases, a latency test has some subtleties that must be treated carefully.
One of the most noticeable is that a simple mean (average) is not very useful as a measure of how well an application is reacting to requests.
We will discuss this subject more fully in <a data-type="xref" href="#pracjavaperf-CHP-2-SECT-4">“Statistics for JVM Performance”</a> and explore additional measures.</p>
</div></section>
<section data-pdf-bookmark="Throughput Test" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-1.2">
<h2>Throughput Test</h2>
<p>Throughput is probably the second most common quantity to be performance-tested.
It can even be thought of as dual to latency, in some senses.</p>
<p>For example, when we are conducting a latency test, it is important to state (and control) the concurrent transactions count when producing a distribution of latency results.
Similarly, when we are conducting a throughput test, we must make sure to keep an eye on latency and check that it is not blowing up to unacceptable values as we ramp up.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The observed latency of a system should be stated at known and controlled throughput levels, and vice versa.</p>
</div>
<p>We determine the “maximum throughput” by noticing when the latency distribution suddenly changes—effectively a “breaking point” (also called an <em>inflection point</em>) of the system.
The point of a stress test, as we will see in an upcoming section, is to locate such points and the load levels at which they occur.</p>
<p>A throughput test, on the other hand, is about measuring the observed maximum throughput before the system starts to degrade.
Once again, these test types are discussed separately, but are rarely truly independent in practice.</p>
</div></section>
<section data-pdf-bookmark="Stress Test" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-1.3">
<h2>Stress Test</h2>
<p>One way to think about a stress test is as a way to determine how much spare
headroom the system has. The test typically proceeds by placing the system into a
steady state of transactions—that is, a specified throughput level (often the current
peak). The test then ramps up the concurrent transactions slowly, until the system
observables start to degrade.</p>
<p>The value just before the observables started to degrade determines the maximum throughput achieved in a stress test.</p>
</div></section>
<section data-pdf-bookmark="Load Test" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-1.4">
<h2>Load Test</h2>
<p>A load test differs from a throughput test (or a stress test) in that it is usually
framed as a binary test: “Can the system handle this projected load or not?” Load tests are sometimes conducted in advance of expected business events—for example, the onboarding of a new customer or market that is expected to drive greatly increased
traffic to the application.</p>
<p>Other examples of possible events that could warrant performing this type of test include advertising campaigns, social media events, and “viral content.”</p>
</div></section>
<section data-pdf-bookmark="Endurance Test" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-1.5">
<h2>Endurance Test</h2>
<p>Some problems manifest only over much longer periods of time (often measured in days). These include slow memory leaks, cache pollution, and memory fragmentation (especially for applications that may eventually suffer a GC concurrent mode failure; see Chapter 5 for more details).</p>
<p>To detect these types of issues, an endurance test (also known as a soak test) is the usual approach. These are run at average (or high) utilization, but within observed realistic loads for the system.
During the test, resource levels are closely monitored to spot any breakdowns or exhaustions of resources.</p>
<p>This type of test is more common in low-latency systems, as it is very common that those systems will not be able to tolerate the length of a stop-the-world event caused by a full GC cycle (see Chapter 4 and subsequent chapters for more on stop-the-world events and related GC concepts).</p>
<p>Endurance tests are not performed as often as they perhaps should be, for the simple reason that they take a long time to run and can be very expensive—​but there are no shortcuts.
There is also the inherent difficulty of testing with realistic data or usage patterns over a long period.
This can be one of the major reasons why teams end up “testing in production”.</p>
<p>This type of test is also not always applicable to microservice or other architectures where there may be a lot of code changes deployed in a short time.</p>
</div></section>
<section data-pdf-bookmark="Capacity Planning Test" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-1.6">
<h2>Capacity Planning Test</h2>
<p>Capacity planning tests bear many similarities to stress tests, but they are a distinct type of test.
The role of a stress test is to find out what the current system will cope with, whereas a capacity planning test is more forward-looking and seeks to find out what load an upgraded system could handle.</p>
<p>For this reason, capacity planning tests are often carried out as part of a scheduled planning exercise, rather than in response to a specific event or threat.</p>
</div></section>
<section data-pdf-bookmark="Degradation Test" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-1.7">
<h2>Degradation Test</h2>
<p>Once upon a time, rigorous failover and recovery testing was really only practiced in the most highly regulated and scrutinized environments (including banks and financial institutions).
However, as applications have migrated to the cloud, clustered deployments (e.g. based on Kubernetes) have become more and more common.
One primary consequence of this is that more and more developers now need to be aware of the possible failure modes of clustered applications.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>A full discussion of all aspects of resilience and fail-over testing is outside the scope of this book.
In Chapter 15, we will discuss some of the simpler effects that can be seen in cloud systems when a cluster partially fails, or needs to recover.</p>
</div>
<p>In this section, the only type of resilience test we will discuss is the degradation test—​this type of test is also known as a <em>partial failure</em> test.</p>
<p>The basic approach to this test is to see how the system behaves when a component or entire subsystem suddenly loses capacity while the system is running at simulated loads equivalent to usual production volumes.
Examples could be application server clusters that suddenly lose members, or network bandwidth that suddenly drops.</p>
<p>Key observables during a degradation test include the transaction latency distribution and throughput.</p>
<p>One particularly interesting subtype of partial failure test is known as the <a href="https://github.com/Netflix/chaosmonkey"><em>Chaos Monkey</em></a>. This is named after a project at Netflix that was undertaken to verify the robustness of its infrastructure.</p>
<p>The idea behind Chaos Monkey is that in a truly resilient architecture, the failure of a single component should not be able to cause a cascading failure or have a meaningful impact on the overall system.</p>
<p>Chaos Monkey forces system operators to confront this possibility by randomly killing off live processes in the production environment.</p>
<p>In order to successfully implement Chaos Monkey–type systems, an organization must have very high levels of system hygiene, service design, and operational excellence.
Nevertheless, it is an area of interest and aspiration for an increasing number of companies and teams.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Best Practices Primer" data-type="sect1"><div class="sect1" id="pracjavaperf-CHP-2-SECT-2">
<h1>Best Practices Primer</h1>
<p>When deciding where to focus your effort in a performance tuning exercise, there are
three golden rules that can provide useful guidance:</p>
<ul>
<li>
<p>Identify what you care about and figure out how to measure it.</p>
</li>
<li>
<p>Optimize what matters, not what is easy to optimize.</p>
</li>
<li>
<p>Play the big points first.</p>
</li>
</ul>
<p>The second point has a converse, which is to remind yourself not to fall into the trap of attaching too much significance to whatever quantity you can easily measure.
Not every observable is significant to a business, but it is sometimes tempting to report on an easy measure, rather than the right measure.</p>
<p>To the third point, it is also easy to fall into the trap of optimizing small things simply for the sake of optimizing.</p>
<section data-pdf-bookmark="Top-Down Performance" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-2.1">
<h2>Top-Down Performance</h2>
<p>One of the aspects of Java performance that many engineers miss at first sight is that large-scale benchmarking of Java applications is usually much easier than trying to get accurate numbers for small sections of code.</p>
<p>This is such a widely misunderstood point, that to deliberately deemphasize it, we do not discuss <em>microbenchmarking</em> in the main book text at all.
Instead, it is discussed in Appendix A --a placement that more accurately reflects the utility of the technique for the majority of applications.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The approach of starting with the performance behavior of an entire application is usually called <em>top-down</em> performance.</p>
</div>
<p>To make the most of the top-down approach, a testing team needs a test environment, a clear understanding of what it needs to measure and optimize, and an understanding of how the performance exercise will fit into the overall software development lifecycle.</p>
</div></section>
<section data-pdf-bookmark="Creating a Test Environment" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-2.2">
<h2>Creating a Test Environment</h2>
<p>Setting up a test environment is one of the first tasks most performance testing teams will need to undertake. Wherever possible, this should be an exact duplicate of the production environment, in all aspects.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Some teams may be in a position where they are forced to forgo testing environments and simply measure in production using modern deployment and Observability techniques. This is the subject of Chapter 10, but it is not recommended as an approach unless it’s necessary.</p>
</div>
<p>This includes not only application servers (which servers should have the same number of CPUs, same version of the OS and Java runtime, etc.), but web servers, databases, message queues, and so on.
Any services (e.g., third-party network services that are not easy to replicate, or do not have sufficient QA capacity to handle a production-equivalent load) will need to be mocked for a representative performance testing environment.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Performance testing environments that are significantly different from the production deployments that they purport to represent are usually ineffective—​they fail to produce results that have any usefulness or predictive power in the live environment.</p>
</div>
<p>For traditional (i.e., non-cloud-based) environments, a production-like performance testing environment is relatively straightforward to achieve in theory—​the team simply buys as many machines as are in use in the production environment and then configures them in exactly the same way as production is configured.</p>
<p>Management is sometimes resistant to the additional infrastructure cost that this represents.
This is almost always a false economy, but sadly many organizations fail to account correctly for the cost of outages.
This can lead to a belief that the savings from not having an accurate performance testing environment are meaningful, as it fails to properly account for the risks introduced by having a QA environment that does not mirror production.</p>
<p>The advent of cloud technologies, has changed this picture.
More dynamic approaches to infrastructure management are now widespread.
This includes on-demand and autoscaling infrastructure, as well as approaches such as <em>immutable infrastructure</em>, also referred to as treating server infrastructure as “livestock, not pets”.</p>
<p>In theory, these trends make the construction of a performance testing environment that looks like production easier.
However, there are subtleties here.
For example:</p>
<ul>
<li>
<p>Having a process that allows changes to be made in a test environment first and then migrated to production</p>
</li>
<li>
<p>Making sure that a test environment does not have some overlooked dependencies that depend upon production</p>
</li>
<li>
<p>Ensuring that test environments have realistic authentication and authorization systems, not dummy components</p>
</li>
</ul>
<p>Despite these concerns, the possibility of setting up a testing environment that can be turned off when not in use is a key advantage of cloud-based deployments.
This can bring significant cost savings to the project, but it requires a proper process for starting up and shutting down the environment as scheduled.</p>
</div></section>
<section data-pdf-bookmark="Identifying Performance Requirements" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-2.3">
<h2>Identifying Performance Requirements</h2>
<p>The overall performance of a system is not solely determined by your application code.
As we will discover throughout the rest of this book, the container, operating system, and hardware all have a role to play.</p>
<p>Therefore, the metrics that we will use to evaluate performance should not be thought about solely in terms of the code.
Instead, we must consider systems as a whole and the observable quantities that are important to customers and management.
These are usually referred to as performance <em>nonfunctional requirements</em> (NFRs), and are the key indicators that we want to optimize.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In Chapter 7, we will meet a simple system model that describes in more detail how the interaction between OS, hardware, JVM and code impacts performance.</p>
</div>
<p>Some performance goals are obvious:</p>
<ul>
<li>
<p>Reduce 95% percentile transaction time by 100 ms.</p>
</li>
<li>
<p>Improve system so that 5x throughput on existing hardware is possible.</p>
</li>
<li>
<p>Improve average response time by 30%.</p>
</li>
</ul>
<p>Others may be less apparent:</p>
<ul>
<li>
<p>Reduce resource cost to serve the average customer by 50%.</p>
</li>
<li>
<p>Ensure system is still within 25% of response targets, even when application clusters are degraded by 50%.</p>
</li>
<li>
<p>Reduce customer “drop-off” rate by 25% by removing 10 ms of latency.</p>
</li>
</ul>
<p>An open discussion with the stakeholders as to exactly what should be measured and what goals are to be achieved is essential.
Ideally, this discussion should form part of the first kick-off meeting for any performance exercise.</p>
</div></section>
<section data-pdf-bookmark="Performance Testing as Part of the SDLC" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-2.4">
<h2>Performance Testing as Part of the SDLC</h2>
<p>Some companies and teams prefer to think of performance testing as an occasional, one-off activity.
However, more sophisticated teams tend to make ongoing performance tests, and in particular performance regression testing, an integral part of their software development lifecycle (SDLC).</p>
<p>This requires collaboration between developers and infrastructure teams to control which versions of code are present in the performance testing environment at any given time.
It is also virtually impossible to implement without a dedicated testing environment.</p>
</div></section>
<section data-pdf-bookmark="Java-Specific Issues" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-2.5">
<h2>Java-Specific Issues</h2>
<p>Much of the science of performance analysis is applicable to any modern software system.
However, the nature of the JVM is such that there are certain additional complications that the performance engineer should be aware of and consider carefully.
These largely stem from the dynamic self-management capabilities of the JVM, such as the dynamic tuning of memory areas and JIT compilation.</p>
<p>For example, modern JVMs analyze which methods are being run to identify candidates for JIT compilation to optimized machine code.
This means that if a method is not being JIT-compiled, then one of two things is true about the method:</p>
<ul>
<li>
<p>It is not being run frequently enough to warrant being compiled.</p>
</li>
<li>
<p>The method is too large or complex to be analyzed for compilation.</p>
</li>
</ul>
<p>The second condition is, by the way, much rarer than the first.
In Chapter 6 we will discuss JIT compilation in detail, and show some simple techniques for ensuring that the important methods of applications are targeted for JIT compilation by the JVM.</p>
<p>Having discussed some of the most common best practices for performance, let’s now turn our attention to the pitfalls and antipatterns that teams can fall prey to.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Causes of performance antipatterns" data-type="sect1"><div class="sect1" id="pracjavaperf-CHP-2-SECT-3">
<h1>Causes of performance antipatterns</h1>
<p>An antipattern is an undesired behavior of a software project or team that is observed across a large number of projects.<sup><a data-type="noteref" href="ch02.xhtml#id96" id="id96-marker">1</a></sup>
The frequency of occurrence leads to the conclusion (or suspicion) that some underlying factor is responsible for creating the unwanted behavior.
Some antipatterns may at first sight seem to be justified, with their non-ideal aspects not immediately obvious. Others are the result of negative project practices slowly accreting over time.</p>
<p>A partial catalogue of antipatterns can be found in Appendix B—​where an example of the first kind would be something like <em>Distracted By Shiny</em>, whereas <em>Tuning By Folklore</em> is an example of the second kind.</p>
<p>In some cases the behavior may be driven by social or team constraints, or by
common misapplied management techniques, or by simple human (and developer)
nature. By classifying and categorizing these unwanted features, we develop
a <em>pattern language</em> for discussing them, and hopefully eliminating them from our
projects.</p>
<p>Performance tuning should always be treated as a very objective process, with precise goals set early in the planning phase.
This is easier said than done: when a team is under pressure or not operating under reasonable circumstances, this can simply fall by the wayside.</p>
<p>Many readers will have seen the situation where a new client is going live or
a new feature is being launched, and an unexpected outage occurs—in
user acceptance testing (UAT) if you are lucky, but often in production. The team is then left scrambling
to find and fix what has caused the bottleneck. This usually means performance testing
has not been carried out, or the team “ninja” made an assumption and has now
disappeared (ninjas are good at this).</p>
<p>A team that works in this way will likely fall victim to antipatterns more often than a team that follows good performance testing practices and has open and reasoned conversations.
As with many development issues, it is often the human elements, such as communication problems, rather than any technical aspect that leads to an application having problems.</p>
<p>One interesting possibility for classification was provided in a blog post by Carey Flichel called <a href="http://www.carfey.com/blog/why-developers-keep-making-bad-technology-choices/">“Why Developers Keep Making Bad Technology Choices”</a>. The post specifically calls out five main reasons that cause developers to make bad choices. Let’s look at each in turn.</p>
<section data-pdf-bookmark="Boredom" data-type="sect2"><div class="sect2" id="id60">
<h2>Boredom</h2>
<p>Most developers have experienced boredom in a role, and for some this doesn’t have to
last very long before they are seeking a new challenge or role either in the company or elsewhere. However, other opportunities may not be present in the organization, and moving somewhere else may not be possible.</p>
<p>It is likely many readers have come across a developer who is simply riding it out, perhaps even actively seeking an easier life. However, bored developers can harm a project in a number of ways.</p>
<p>For example, they might introduce code complexity that is not required, such as writing a sorting algorithm directly
in code when a simple <code>Collections.sort()</code> would be sufficient. They might also express their boredom by looking to build components with technologies that are unknown or perhaps don’t fit the
use case just as an opportunity to use them—which leads us to the next section.</p>
</div></section>
<section data-pdf-bookmark="Résumé Padding" data-type="sect2"><div class="sect2" id="id61">
<h2>Résumé Padding</h2>
<p>Occasionally the overuse of technology is not tied to boredom, but rather represents the developer exploiting an opportunity
to boost their experience with a particular technology on their résumé (or CV).</p>
<p>In this scenario, the developer is making an active attempt to increase their potential salary and marketability
as they’re about to re-enter the job market. It’s unlikely that many people would get away with this
inside a well-functioning team, but it can still be the root of a choice that takes a project
down an unnecessary path.</p>
<p>The consequences of an unnecessary technology being added due to a developer’s boredom
or résumé padding can be far-reaching and very long-lived, lasting for many years
after the original developer has left.</p>
</div></section>
<section data-pdf-bookmark="Social Pressure" data-type="sect2"><div class="sect2" id="id62">
<h2>Social Pressure</h2>
<p>Technical decisions are often at their worst when concerns are not voiced or discussed at the time choices are being made.
This can manifest in a few ways; for example, perhaps a junior developer doesn’t want to make a mistake in front of more senior members of their team, or perhaps a developer fears appearing to their peers as uninformed on a particular topic.</p>
<p>Another particularly toxic type of social pressure is for competitive teams, wanting to be seen as having high development velocity, to rush key decisions without fully exploring all the consequences.</p>
</div></section>
<section data-pdf-bookmark="Lack of Understanding" data-type="sect2"><div class="sect2" id="id63">
<h2>Lack of Understanding</h2>
<p>Developers may look to introduce new tools to help solve a problem
because they are not aware of the full capability of their current tools. It is often
tempting to turn to a new and exciting technology component because it is great at performing
one specific task. However, introducing more technical complexity must be taken on
balance with what the current tools can actually do.</p>
<p>For example, Hibernate is sometimes seen as the answer to simplifying translation
between domain objects and databases. If there is only limited understanding of
Hibernate on the team, developers can make assumptions about its suitability based on
having seen it used in another project.</p>
<p>This lack of understanding can cause overcomplicated usage of Hibernate and unrecoverable
production outages. By contrast, rewriting the entire data layer using simple JDBC
calls allows the developer to stay on familiar territory.</p>
<p>One of the authors taught a Hibernate course that contained an attendee in exactly this position; they were trying to learn enough Hibernate to see if the application could be recovered, but ended up having to rip out Hibernate over the course of a weekend—​definitely not an enviable position.</p>
</div></section>
<section data-pdf-bookmark="Misunderstood/Nonexistent Problem" data-type="sect2"><div class="sect2" id="id64">
<h2>Misunderstood/Nonexistent Problem</h2>
<p>Developers may often use a technology to solve a particular issue where the problem space
itself has not been adequately investigated. Without having measured performance values, it
is almost impossible to understand the success of a particular solution. Often collating
these performance metrics enables a better understanding of the problem.</p>
<p>To avoid antipatterns it is important to ensure that communication about
technical issues is open to all participants in the team, and actively encouraged. Where
things are unclear, gathering factual evidence and working on prototypes can help to steer
team decisions. A technology may look attractive; however, if the prototype does not
measure up then the team can make a more informed decision.</p>
<p>To see how these underlying causes can lead to a variety of performance antipatterns, interested readers should consult Appendix B.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Statistics for JVM Performance" data-type="sect1"><div class="sect1" id="pracjavaperf-CHP-2-SECT-4">
<h1>Statistics for JVM Performance</h1>
<p>If performance analysis is truly an experimental science, then we will inevitably find ourselves dealing with distributions of results data. Statisticians and scientists know that results that stem from the real world are virtually never represented by clean, stand-out signals. We must deal with the world as we find it, rather than the overidealized state in which we would like to find it.</p>
<blockquote>
<p>In God we trust; all others must use data.<sup><a data-type="noteref" href="ch02.xhtml#id97" id="id97-marker">2</a></sup></p>
<p data-type="attribution">W. Edwards Deming (attr)</p>
</blockquote>
<p>All measurements contain some amount of error. In the next section we’ll describe the two main types of error that a Java developer may expect to encounter when doing performance analysis.</p>
<section data-pdf-bookmark="Types of Error" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-4.1">
<h2>Types of Error</h2>
<p>There are two main sources of error that an engineer may encounter. These are:</p>
<dl>
<dt>Random error</dt>
<dd>
<p>A measurement error or an unconnected factor affects results in an uncorrelated manner</p>
</dd>
<dt>Systematic error</dt>
<dd>
<p>An unaccounted factor affects measurement of the observable in a correlated way</p>
</dd>
</dl>
<p>There are specific words associated with each type of error. For example, <em>accuracy</em> is used to describe the level of systematic error in a measurement; high accuracy corresponds to low systematic error. Similarly, <em>precision</em> is the term corresponding to random error; high precision is low random error.</p>
<p>The graphics in <a data-type="xref" href="#pracjavaperf-CHP-2-FIG-1">Figure 2-1</a> show the effect of these two types of error on a measurement.
The extreme left image shows a clustering of shots (which represent our measurements) around the true result (the “center of the target”).
These measurements have both high precision and high accuracy.</p>
<p>The second image has a systematic effect (miscalibrated sights perhaps?) that is causing all the shots to be off-target, so these measurements have high precision, but low accuracy.
The third image shows shots basically on target but loosely clustered around the center, so low precision but high accuracy.
The final image shows no clear pattern, as a result of having both low precision and low accuracy.</p>
<figure><div class="figure" id="pracjavaperf-CHP-2-FIG-1">
<img alt="opjv 0501" height="342" src="assets/opjv_0501.png" width="1440"/>
<h6><span class="label">Figure 2-1. </span>Different types of error</h6>
</div></figure>
<p>Let’s move on to explore these types of error in more detail, starting with random error.</p>
<section data-pdf-bookmark="Random error" data-type="sect3"><div class="sect3" id="pracjavaperf-CHP-2-SECT-4.1.1">
<h3>Random error</h3>
<p>Random errors are hopefully familiar to most people—​they are a very well-trodden path.
However, they still deserve a mention here, as any handling of observed or experimental data needs to contend with them to some level.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The discussion assumes readers are familiar with basic statistical handling of normally distributed measurements (mean, mode, standard deviation, etc.); readers who aren’t should consult a basic textbook, such as <a href="http://biostathandbook.com/"><em>The Handbook of Biological Statistics</em></a>.<sup><a data-type="noteref" href="ch02.xhtml#id98" id="id98-marker">3</a></sup></p>
</div>
<p>Random errors are caused by unknown or unpredictable changes in the environment. In general scientific usage, these changes may occur in either the measuring instrument or the environment, but for software we assume that our measuring harness is reliable, and so the source of random error can only be the operating environment.</p>
<p>Random error is usually considered to obey a Gaussian (aka normal) distribution.
A couple of typical examples of Gaussian distributions are shown in <a data-type="xref" href="#pracjavaperf-CHP-2-FIG-3">Figure 2-2</a>.</p>
<figure><div class="figure" id="pracjavaperf-CHP-2-FIG-3">
<img alt="opjv 0503" height="750" src="assets/opjv_0503.png" width="1440"/>
<h6><span class="label">Figure 2-2. </span>A Gaussian distribution (aka normal distribution or bell curve)</h6>
</div></figure>
<p>The distribution is a good model for the case where an error is equally likely to make a positive or negative contribution to an observable.
However, as we will see in the section on non-normal statistics, the situation for JVM measurements is a little more complicated.</p>
</div></section>
<section data-pdf-bookmark="Systematic error" data-type="sect3"><div class="sect3" id="pracjavaperf-CHP-2-SECT-4.1.2">
<h3>Systematic error</h3>
<p>As an example of systematic error, consider a performance test running against a group of backend Java web services that send and receive JSON.
This type of test is very common when it is problematic to directly use the application frontend for load testing.</p>
<p><a data-type="xref" href="#pracjavaperf-CHP-2-FIG-2">Figure 2-3</a> was generated from the Apache JMeter load-generation tool. In it, there are actually two systematic effects at work.
The first is the linear pattern observed in the topmost line (the outlier service), which represents slow exhaustion of some limited server resource.</p>
<figure><div class="figure" id="pracjavaperf-CHP-2-FIG-2">
<img alt="opjv 0502" height="600" src="assets/opjv_0502.png" width="800"/>
<h6><span class="label">Figure 2-3. </span>Systematic error</h6>
</div></figure>
<p>This type of pattern is often associated with a memory leak, or some other resource being used and not released by a thread during request handling, and represents a candidate for investigation—​it looks like it could be a genuine problem.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Further analysis would be needed to confirm the type of resource that was being affected; we can’t just conclude that it’s a memory leak.</p>
</div>
<p>The second effect that should be noticed is the consistency of the majority of the other services at around the 180 ms level.
This is suspicious, as the services are doing very different amounts of work in response to a request. So why are the results so consistent?</p>
<p>The answer is that while the services under test are located in London, this load test was conducted from Mumbai, India.
The observed response time includes the irreducible round-trip network latency from Mumbai to London.
This is in the range 120–150 ms, and so accounts for the vast majority of the observed time for the services other than the outlier.</p>
<p>This large, systematic effect is drowning out the differences in the actual response time (as the services are actually responding in much less than 120 ms).
This is an example of a systematic error that does not represent a problem with our application.</p>
<p>Instead, this error stems from a problem in our test setup, and so the good news is that this artifact completely disappeared (as expected) when the test was rerun from London.</p>
<p>To finish off this section, let’s take a quick look at a notorious problem that frequently accompanies systematic error—​the spurious correlation.</p>
</div></section>
<section data-pdf-bookmark="Spurious correlation" data-type="sect3"><div class="sect3" id="pracjavaperf-CHP-2-SECT-4.1.3">
<h3>Spurious correlation</h3>
<p>One of the most famous aphorisms about statistics is “correlation does not imply causation” —-that is, just because two variables appear to behave similarly does not imply that there is an underlying connection between them.</p>
<p>In the most extreme examples, if a practitioner looks hard enough, then a correlation can be found between <a href="http://tylervigen.com/spurious-correlations">entirely unrelated measurements</a>.
For example, in <a data-type="xref" href="#pracjavaperf-CHP-2-FIG-4">Figure 2-4</a> we can see that consumption of chicken in the US is well correlated with total import of crude oil.<sup><a data-type="noteref" href="ch02.xhtml#id99" id="id99-marker">4</a></sup></p>
<figure><div class="figure" id="pracjavaperf-CHP-2-FIG-4">
<img alt="opjv 0504" height="568" src="assets/opjv_0504.png" width="1440"/>
<h6><span class="label">Figure 2-4. </span>A completely spurious correlation (Vigen)</h6>
</div></figure>
<p>These numbers are clearly not causally related; there is no factor that drives both the import of crude oil and the eating of chicken.
However, it isn’t the absurd and ridiculous correlations that the practitioner needs to be wary of.</p>
<p>In <a data-type="xref" href="#pracjavaperf-CHP-2-FIG-5">Figure 2-5</a>, we see the revenue generated by video arcades correlated to the number of computer science PhDs awarded.
It isn’t too much of a stretch to imagine a sociological study that claimed a link between these observables, perhaps arguing that “stressed doctoral students were finding relaxation with a few hours of video games.”
These types of claim are depressingly common, despite no such common factor actually existing.</p>
<figure><div class="figure" id="pracjavaperf-CHP-2-FIG-5">
<img alt="opjv 0505" height="568" src="assets/opjv_0505.png" width="1440"/>
<h6><span class="label">Figure 2-5. </span>A less spurious correlation? (Vigen)</h6>
</div></figure>
<p>In the realm of the JVM and performance analysis, we need to be especially careful not to attribute a causal relationship between measurements based solely on correlation and that the connection “seems plausible.”</p>
<blockquote>
<p>The first principle is that you must not fool yourself—​and you are the easiest person to fool.<sup><a data-type="noteref" href="ch02.xhtml#id100" id="id100-marker">5</a></sup></p>
<p data-type="attribution">Richard Feynman</p>
</blockquote>
<p>We’ve met some examples of sources of error and mentioned the notorious bear traps of spurious correlation and fooling oneself, so let’s move on to discuss an aspect of JVM performance measurement that requires some special care and attention to detail.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Non-Normal Statistics" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-4.2">
<h2>Non-Normal Statistics</h2>
<p>Statistics based on the normal distribution do not require much mathematical sophistication.
For this reason, the standard approach to statistics that is typically taught at pre-college or undergraduate level focuses heavily on the analysis of normally distributed data.</p>
<p>Students are taught to calculate the mean and the standard deviation (or variance), and sometimes higher moments, such as skew and kurtosis.
However, these techniques have a serious drawback, in that the results can easily become distorted if the distribution has even relatively few far-flung outlying points.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In Java performance, the outliers represent slow transactions and unhappy customers. We need to pay special attention to these points, and avoid techniques that dilute the importance of outliers.</p>
</div>
<p>To consider it from another viewpoint: unless a large number of customers are already complaining, it is unlikely that improving the average response time is a useful performance goal.
For sure, doing so will improve the experience for everyone, but it is far more
usual for a few disgruntled customers to be the cause of a latency tuning exercise.
This implies that the outlier events are likely to be of more interest than the
experience of the majority who are receiving satisfactory service.</p>
<p>In <a data-type="xref" href="#pracjavaperf-CHP-2-FIG-6">Figure 2-6</a> we can see a more realistic curve for the
likely distribution of method (or transaction) times. It is clearly not a normal distribution.</p>
<figure class="width_set_50"><div class="figure" id="pracjavaperf-CHP-2-FIG-6">
<img alt="opjv 0506" height="948" src="assets/opjv_0506.png" width="1440"/>
<h6><span class="label">Figure 2-6. </span>A more realistic view of the distribution of transaction times</h6>
</div></figure>
<p>The shape of the distribution in <a data-type="xref" href="#pracjavaperf-CHP-2-FIG-6">Figure 2-6</a> shows something
that we know intuitively about the JVM: it has “hot paths” where all the relevant code
is already JIT-compiled, there are no GC cycles, and so on. These represent a best-case scenario
(albeit a common one); there simply are no calls that are “a bit faster” due to random effects.</p>
<p>This violates a fundamental assumption of Gaussian statistics and forces us to consider distributions that are non-normal.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>For distributions that are non-normal, many “basic rules” of normally distributed statistics are violated.
In particular, standard deviation/variance and other higher moments are basically useless.</p>
</div>
<p>One technique that is very useful for handling the non-normal, “long-tail” distributions that the JVM produces is to use a modified scheme of percentiles.
Remember that a distribution is a whole collection of points—​a shape of data, and is not well-represented by a single number.</p>
<p>Instead of computing just the mean, which tries to express the whole distribution in a single result, we can use a sampling of the distribution at intervals.
When used for normally distributed data, the samples are usually taken at regular intervals.
However, a small adaptation allows the technique to be used more effectively for JVM statistics.</p>
<p>The modification is to use a sampling that takes into account the long-tail distribution by starting from the mean, then the 90th percentile, and then moving out logarithmically, as shown in the following method timing results. This means that we’re sampling according to a pattern that better corresponds to the shape of the data:</p>
<pre data-type="programlisting">50.0% level was 23 ns
90.0% level was 30 ns
99.0% level was 43 ns
99.9% level was 164 ns
99.99% level was 248 ns
99.999% level was 3,458 ns
99.9999% level was 17,463 ns</pre>
<p>The samples show us that while the average time was 23 ns to execute a getter method,
for 1 request in 1,000 the time was an order of magnitude worse, and for 1 request in
100,000 it was <em>two</em> orders of magnitude worse than average.</p>
<p>Long-tail distributions can also be referred to as <em>high dynamic range</em>
distributions. The dynamic range of an observable is usually defined as the maximum
recorded value divided by the minimum (assuming it’s nonzero).</p>
<p>Logarithmic percentiles are a useful simple tool for understanding the long tail.
However, for more sophisticated analysis, we can use a public domain library for handling datasets with high dynamic range. The library is called HdrHistogram and is <a href="https://github.com/HdrHistogram/HdrHistogram">available from GitHub</a>.
It was originally created by Gil Tene (Azul Systems), with additional work by Mike Barker and other contributors.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>A histogram is a way of summarizing data by using a finite set of ranges (called <em>buckets</em>) and displaying how often data falls into each bucket.</p>
</div>
<p>HdrHistogram is also available on Maven Central. At the time of writing, the current version is 2.1.12, and you can add it to your projects by adding this dependency stanza to <em>pom.xml</em>:</p>
<pre data-code-language="xml" data-type="programlisting"><code class="nt">&lt;dependency&gt;</code><code class="w"/>
<code class="w">    </code><code class="nt">&lt;groupId&gt;</code>org.hdrhistogram<code class="nt">&lt;/groupId&gt;</code><code class="w"/>
<code class="w">    </code><code class="nt">&lt;artifactId&gt;</code>HdrHistogram<code class="nt">&lt;/artifactId&gt;</code><code class="w"/>
<code class="w">    </code><code class="nt">&lt;version&gt;</code>2.1.12<code class="nt">&lt;/version&gt;</code><code class="w"/>
<code class="nt">&lt;/dependency&gt;</code><code class="w"/></pre>
<p>Let’s look at a simple example using HdrHistogram. This example takes in a file of
numbers and computes the HdrHistogram for the difference between successive results:</p>
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code><code class="w"> </code><code class="kd">class</code> <code class="nc">BenchmarkWithHdrHistogram</code><code class="w"> </code><code class="p">{</code><code class="w"/>
<code class="w">    </code><code class="kd">private</code><code class="w"> </code><code class="kd">static</code><code class="w"> </code><code class="kd">final</code><code class="w"> </code><code class="kt">long</code><code class="w"> </code><code class="n">NORMALIZER</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="mi">1_000_000</code><code class="p">;</code><code class="w"/>
<code class="w">    </code>
<code class="w">    </code><code class="kd">private</code><code class="w"> </code><code class="kd">static</code><code class="w"> </code><code class="kd">final</code><code class="w"> </code><code class="n">Histogram</code><code class="w"> </code><code class="n">HISTOGRAM</code><code class="w"/>
<code class="w">            </code><code class="o">=</code><code class="w"> </code><code class="k">new</code><code class="w"> </code><code class="n">Histogram</code><code class="p">(</code><code class="n">TimeUnit</code><code class="p">.</code><code class="na">MINUTES</code><code class="p">.</code><code class="na">toMicros</code><code class="p">(</code><code class="mi">1</code><code class="p">),</code><code class="w"> </code><code class="mi">2</code><code class="p">);</code><code class="w"/>
<code class="w">    </code>
<code class="w">    </code><code class="kd">public</code><code class="w"> </code><code class="kd">static</code><code class="w"> </code><code class="kt">void</code><code class="w"> </code><code class="nf">main</code><code class="p">(</code><code class="n">String</code><code class="o">[]</code><code class="w"> </code><code class="n">args</code><code class="p">)</code><code class="w"> </code><code class="kd">throws</code><code class="w"> </code><code class="n">Exception</code><code class="w"> </code><code class="p">{</code><code class="w"/>
<code class="w">        </code><code class="kd">final</code><code class="w"> </code><code class="n">List</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code><code class="w"> </code><code class="n">values</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="n">Files</code><code class="p">.</code><code class="na">readAllLines</code><code class="p">(</code><code class="n">Paths</code><code class="p">.</code><code class="na">get</code><code class="p">(</code><code class="n">args</code><code class="o">[</code><code class="mi">0</code><code class="o">]</code><code class="p">));</code><code class="w"/>
<code class="w">        </code><code class="kt">double</code><code class="w"> </code><code class="n">last</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="mi">0</code><code class="p">;</code><code class="w"/>
<code class="w">        </code><code class="k">for</code><code class="w"> </code><code class="p">(</code><code class="kd">final</code><code class="w"> </code><code class="n">String</code><code class="w"> </code><code class="n">tVal</code><code class="w"> </code><code class="p">:</code><code class="w"> </code><code class="n">values</code><code class="p">)</code><code class="w"> </code><code class="p">{</code><code class="w"/>
<code class="w">            </code><code class="kt">double</code><code class="w"> </code><code class="n">parsed</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="n">Double</code><code class="p">.</code><code class="na">parseDouble</code><code class="p">(</code><code class="n">tVal</code><code class="p">);</code><code class="w"/>
<code class="w">            </code><code class="kt">double</code><code class="w"> </code><code class="n">gcInterval</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="n">parsed</code><code class="w"> </code><code class="o">-</code><code class="w"> </code><code class="n">last</code><code class="p">;</code><code class="w"/>
<code class="w">            </code><code class="n">last</code><code class="w"> </code><code class="o">=</code><code class="w"> </code><code class="n">parsed</code><code class="p">;</code><code class="w"/>
<code class="w">            </code><code class="n">HISTOGRAM</code><code class="p">.</code><code class="na">recordValue</code><code class="p">((</code><code class="kt">long</code><code class="p">)(</code><code class="n">gcInterval</code><code class="w"> </code><code class="o">*</code><code class="w"> </code><code class="n">NORMALIZER</code><code class="p">));</code><code class="w"/>
<code class="w">        </code><code class="p">}</code><code class="w"/>
<code class="w">        </code><code class="n">HISTOGRAM</code><code class="p">.</code><code class="na">outputPercentileDistribution</code><code class="p">(</code><code class="n">System</code><code class="p">.</code><code class="na">out</code><code class="p">,</code><code class="w"> </code><code class="mf">1000.0</code><code class="p">);</code><code class="w"/>
<code class="w">    </code><code class="p">}</code><code class="w"/>
<code class="p">}</code><code class="w"/></pre>
<p>The output shows the times between successive garbage collections. As we’ll see in Chapters 4 and 5, GC does do not occur at regular intervals, and understanding the distribution of how frequently it occurs could be useful. Here’s what the histogram plotter produces for a sample GC log:</p>
<pre data-type="programlisting">       Value     Percentile TotalCount 1/(1-Percentile)

       14.02 0.000000000000          1           1.00
     1245.18 0.100000000000         37           1.11
     1949.70 0.200000000000         82           1.25
     1966.08 0.300000000000        126           1.43
     1982.46 0.400000000000        157           1.67

...

    28180.48 0.996484375000        368         284.44
    28180.48 0.996875000000        368         320.00
    28180.48 0.997265625000        368         365.71
    36438.02 0.997656250000        369         426.67
    36438.02 1.000000000000        369
#[Mean    =      2715.12, StdDeviation   =      2875.87]
#[Max     =     36438.02, Total count    =          369]
#[Buckets =           19, SubBuckets     =          256]</pre>
<p>The raw output of the formatter is rather hard to analyze, but fortunately, the HdrHistogram project includes an <a href="http://hdrhistogram.github.io/HdrHistogram/plotFiles.xhtml">online formatter</a> that can be used to generate visual histograms from the raw output.</p>
<p>For this example, it produces output like that shown in <a data-type="xref" href="#pracjavaperf-CHP-2-FIG-7">Figure 2-7</a>.</p>
<figure><div class="figure" id="pracjavaperf-CHP-2-FIG-7">
<img alt="opjv 0507" height="613" src="assets/opjv_0507.png" width="1440"/>
<h6><span class="label">Figure 2-7. </span>Example HdrHistogram visualization</h6>
</div></figure>
<p>For many observables that we wish to measure in Java performance tuning, the statistics are often highly non-normal, and HdrHistogram can be a very useful tool in helping to understand and visualize the shape of the data.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Interpretation of Statistics" data-type="sect1"><div class="sect1" id="pracjavaperf-CHP-2-SECT-5">
<h1>Interpretation of Statistics</h1>
<p>Empirical data and observed results do not exist in a vacuum, and it is quite common that one of the hardest jobs lies in interpreting the results that we obtain from measuring our applications.</p>
<blockquote>
<p>No matter what the problem is, it’s always a people problem.</p>
<p data-type="attribution">Gerald Weinberg (attr)</p>
</blockquote>
<p>In <a data-type="xref" href="#pracjavaperf-CHP-2-FIG-8">Figure 2-8</a> we show an example memory allocation rate for a real Java application. This example is for a reasonably well-performing application.</p>
<figure><div class="figure" id="pracjavaperf-CHP-2-FIG-8">
<img alt="opjv 0508" height="758" src="assets/opjv_0508.png" width="1133"/>
<h6><span class="label">Figure 2-8. </span>Example allocation rate</h6>
</div></figure>
<p>The interpretation of the allocation data is relatively straightforward, as there is a clear signal present. Over the time period covered (almost a day), allocation rates were basically stable between 350 and 700 MB per second. There is a downward trend starting approximately 5 hours after the JVM started up, and a clear minimum between 9 and 10 hours, after which the allocation rate starts to rise again.</p>
<p>These types of trends in observables are very common, as the allocation rate will usually reflect the amount of work an application is actually doing, and this will vary widely depending on the time of day. However, when we are interpreting real observables, the picture can rapidly become more complicated.</p>
<p>This can lead to what is sometimes called the “Hat/Elephant” problem, after a passage in <em>The Little Prince</em> by Antoine de Saint-Exupéry.
In the book, the narrator describes drawing, at age six, a picture of a boa constrictor that has eaten an elephant.
However, as the view is external, the picture just resembles (at least to the ignorant eyes of the adults in the story) a slightly shapeless hat.</p>
<p>The metaphor stands as an admonition to the reader to have some imagination and to think more deeply about what you are really seeing, rather than just accepting a shallow explanation at face value.</p>
<p>The problem, as applied to software, is illustrated by <a data-type="xref" href="#pracjavaperf-CHP-2-FIG-9">Figure 2-9</a>. All we can initially see is a complex histogram of HTTP request-response times.
However, just like the narrator of the book, if we can imagine or analyze a bit more, we can see that the complex picture is actually made up of several fairly simple pieces.</p>
<figure class="width_set_60"><div class="figure" id="pracjavaperf-CHP-2-FIG-9">
<img alt="opjv 0509" height="894" src="assets/opjv_0509.png" width="1440"/>
<h6><span class="label">Figure 2-9. </span>Hat, or elephant eaten by a boa?</h6>
</div></figure>
<p>The key to decoding the response histogram is to realize that “web application responses” is a very general category, including successful requests (so-called 2xx responses), client errors (4xx, including the infamous 404 error), and server errors (5xx, especially 500 Internal Server Error).</p>
<p>Each type of response has a different characteristic distribution for response times. If a client makes a request for a URL that has no mapping (a 404), then the web server can immediately reply with a response.
This means that the histogram for only client error responses looks more like <a data-type="xref" href="#pracjavaperf-CHP-2-FIG-10">Figure 2-10</a>.</p>
<figure class="width_set_60"><div class="figure" id="pracjavaperf-CHP-2-FIG-10">
<img alt="opjv 0510" height="945" src="assets/opjv_0510.png" width="1440"/>
<h6><span class="label">Figure 2-10. </span>Client errors</h6>
</div></figure>
<p>By contrast, server errors often occur after a large amount of processing time has been expended (for example, due to backend resources being under stress or timing out).
So, the histogram for server error responses might look like <a data-type="xref" href="#pracjavaperf-CHP-2-FIG-11">Figure 2-11</a>.</p>
<figure class="width_set_60"><div class="figure" id="pracjavaperf-CHP-2-FIG-11">
<img alt="opjv 0512" height="945" src="assets/opjv_0512.png" width="1440"/>
<h6><span class="label">Figure 2-11. </span>Server errors</h6>
</div></figure>
<p>The successful requests will have a long-tail distribution, but in reality we may expect the response distribution to be “multimodal” and have several local maxima. An example is shown in <a data-type="xref" href="#pracjavaperf-CHP-2-FIG-12">Figure 2-12</a>, and represents the possibility that there could be two common execution paths through the application with quite different response times.</p>
<figure class="width_set_60"><div class="figure" id="pracjavaperf-CHP-2-FIG-12">
<img alt="opjv 0511" height="945" src="assets/opjv_0511.png" width="1440"/>
<h6><span class="label">Figure 2-12. </span>Successful requests</h6>
</div></figure>
<p>Combining these different types of responses into a single graph results in the structure shown in <a data-type="xref" href="#pracjavaperf-CHP-2-FIG-13">Figure 2-13</a>. We have rederived our original “hat” shape from the separate histograms.</p>
<figure class="width_set_60"><div class="figure" id="pracjavaperf-CHP-2-FIG-13">
<img alt="opjv 0513" height="945" src="assets/opjv_0513.png" width="1440"/>
<h6><span class="label">Figure 2-13. </span>Hat or elephant revisited</h6>
</div></figure>
<p>The concept of breaking down a general observable into more meaningful sub-populations is a very useful one.
It shows that we need to make sure that we understand our data and domain well enough before trying to infer conclusions from our results.
We may well want to further break down our data into smaller sets; for example, the successful requests may have very different distributions for requests that are predominantly read, as opposed to requests that are updates or uploads.</p>
<p>The engineering team at PayPal have written extensively about their use of statistics and analysis; <a href="https://www.paypal-engineering.com/">they have a blog</a> that contains excellent resources.
In particular, the piece <a href="https://medium.com/paypal-tech/statistics-for-software-e395ca08005d/">“Statistics for Software”</a> by Mahmoud Hashemi is a great introduction to their methodologies, and includes a version of the Hat/Elephant problem discussed earlier.</p>
<p>Also worth mentioning is the “Datasaurus Dozen” --a collection of datasets that have the same basic statistics but wildly different appearances.<sup><a data-type="noteref" href="ch02.xhtml#id101" id="id101-marker">6</a></sup></p>
</div></section>
<section data-pdf-bookmark="Cognitive Biases and Performance Testing" data-type="sect1"><div class="sect1" id="pracjavaperf-CHP-2-SECT-6">
<h1>Cognitive Biases and Performance Testing</h1>
<p>Humans can be bad at forming accurate opinions quickly—​even when faced with a problem where they can draw upon past experiences and similar situations.</p>
<p>A cognitive bias is a psychological effect that causes the human brain to draw incorrect conclusions.
It is especially problematic because the person exhibiting the bias is usually unaware of it and may believe they are being rational.</p>
<p>Many of the antipatterns we observe in performance analysis (such as those in Appendix B, which you might want to read in conjunction with this section) are caused, in whole or in part, by one or more cognitive biases that are in turn based on an unconscious assumptions.</p>
<p>For example, with the <em>Blame Donkey</em> antipattern, if a component has caused several recent outages the team may be biased to expect that same component to be the cause of any new performance problem.
Any data that’s analyzed may be more likely to be considered credible if it confirms the idea that the Blame Donkey component is responsible.</p>
<p>The antipattern combines aspects of the biases known as confirmation bias and recency bias (a tendency to assume that whatever has been happening recently will keep happening).</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>A single component in Java can behave differently from application to application depending on how it is optimized at runtime. In order to remove any pre-existing bias, it is important to look at the application as a whole.</p>
</div>
<p>Biases can be complementary or dual to each other. For example, some developers may be biased to assume that the problem is not software-related at all, and the cause must be the infrastructure the software is running on; this is common in the <em>Works for Me</em> antipattern, characterized by statements like “This worked fine in UAT, so there must be a problem with the production kit.” The converse is to assume that every problem must be caused by software, because that’s the part of the system the developer knows about and can directly affect.</p>
<p>Let’s meet some of the most common biases that every performance engineer should look out for.</p>
<blockquote>
<p>Knowing where the trap is—​that’s the first step in evading it.<sup><a data-type="noteref" href="ch02.xhtml#id102" id="id102-marker">7</a></sup></p>
<p data-type="attribution">Duke Leto Atreides I</p>
</blockquote>
<p>By recognizing these biases in ourselves, and others, we increase the chance of being able to do sound performance analysis and solve the problems in our systems.</p>
<section data-pdf-bookmark="Reductionist Thinking" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-6.1">
<h2>Reductionist Thinking</h2>
<p>The reductionist thinking cognitive bias is based on an analytical approach that presupposes that if you break a system into small enough pieces, you can understand it by understanding its constituent parts.
Understanding each part means reducing the chance of incorrect assumptions being made.</p>
<p>The major problem with this view is simple to explain—​in complex systems it just isn’t true.
Nontrivial software (or physical) systems almost always display emergent behavior, where the whole is greater than a simple summation of its parts would indicate.</p>
</div></section>
<section data-pdf-bookmark="Confirmation Bias" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-6.2">
<h2>Confirmation Bias</h2>
<p>Confirmation bias can lead to significant problems when it comes to performance testing
or attempting to look at an application subjectively. A confirmation bias is introduced, usually
not intentionally, when a poor test set is selected or results from the test are not analyzed
in a statistically sound way. Confirmation bias is quite hard to counter, because there are often strong motivational or emotional factors at play (such as someone in the team trying to prove a point).</p>
<p>Consider an antipattern such as <em>Distracted by Shiny</em>, where a team member is looking to bring in the latest and greatest NoSQL database.
They run some tests against data that isn’t like production data, because representing the full schema is too complicated for evaluation purposes.</p>
<p>They quickly prove that on a test set the NoSQL database produces superior access times on their local machine.
The developer has already told everyone this would be the case, and on seeing the results they proceed with a full implementation.
There are several antipatterns at work here, all leading to new unproved assumptions in the new library stack.</p>
</div></section>
<section data-pdf-bookmark="Fog of War (Action Bias)" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-6.3">
<h2>Fog of War (Action Bias)</h2>
<p>The fog of war bias usually manifests itself during outages or situations where the system is not performing as expected and the team are under pressure.
Some common causes include:</p>
<ul>
<li>
<p>Changes to infrastructure that the system runs on, perhaps without notification or realizing there would be an impact</p>
</li>
<li>
<p>Changes to libraries that the system is dependent on</p>
</li>
<li>
<p>A strange bug or race condition the manifests itself, but only on busy days</p>
</li>
</ul>
<p>In a well-maintained application with sufficient logging and monitoring, these should generate clear error messages that will lead the support team to the cause of the problem.</p>
<p>However, too many applications have not tested failure scenarios and lack appropriate logging.
Under these circumstances even experienced engineers can fall into the trap of needing to feel that they’re doing something to resolve the outage and mistaking motion for velocity—​the “fog of war” descends.</p>
<p>At this time, many of the human elements discussed in this chapter can come into play if participants are not systematic about their approach to the problem.</p>
<p>For example, an antipattern such as <em>Blame Donkey</em> may shortcut a full investigation and lead the production team down a particular path of investigation—​often missing the bigger picture.
Similarly, the team may be tempted to break the system down into its constituent parts and look through the code at a low level without first establishing in which subsystem the problem truly resides.</p>
</div></section>
<section data-pdf-bookmark="Risk Bias" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-2-SECT-6.4">
<h2>Risk Bias</h2>
<p>Humans are naturally risk averse and resistant to change.
Mostly this is because people have seen examples of how change can cause things to go wrong—​this leads them to attempt to avoid that risk.
This can be incredibly frustrating when taking small, calculated risks could move the product forward.
Much of this risk aversion arises from teams that are reluctant to make changes that might modify the performance profile of the application.</p>
<p>We can reduce this risk bias significantly by having a robust set of unit tests and production regression tests.
The performance regression tests are a great place to link in the system’s non-functional requirements and ensure that the concerns the NFRs represent are reflected in the regression tests.</p>
<p>However, if either of these is not sufficiently trusted by the team, change becomes extremely difficult and the risk factor is not controlled.
This bias often manifests in a failure to learn from application problems (including service outages) and implement appropriate mitigation.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="pracjavaperf-CHP-2-SECT-7">
<h1>Summary</h1>
<p>When you are evaluating performance results, it is essential to handle the data in an appropriate manner and avoid falling into unscientific and subjective thinking.
This includes avoiding the statistical pitfalls of relying upon Gaussian models when they are not appropriate.</p>
<p>In this chapter, we have met some different types of performance tests, testing best practices, and human problems that are native to performance analysis.</p>
<p>In the next chapter, we’re going to move on to an overview of the JVM, introducing the basic subsystems, the lifecycle of a “classic” Java application and a first look at monitoring and tooling.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id96"><sup><a href="ch02.xhtml#id96-marker">1</a></sup> The term was popularized by the book <em>AntiPatterns: Refactoring Software, Architectures, and Projects in Crisis</em>, by William J. Brown, Raphael C. Malvo, Hays W. McCormick III, and Thomas J. Malbray (New York: Wiley, 1998).</p><p data-type="footnote" id="id97"><sup><a href="ch02.xhtml#id97-marker">2</a></sup> M. Walton, <em>The Deming Management Method</em> (Mercury Books, 1989)</p><p data-type="footnote" id="id98"><sup><a href="ch02.xhtml#id98-marker">3</a></sup> John H. McDonald, <em>Handbook of Biological Statistics</em>, 3rd ed. (Baltimore, MD: Sparky House Publishing, 2014).</p><p data-type="footnote" id="id99"><sup><a href="ch02.xhtml#id99-marker">4</a></sup> The spurious correlations in this section come from Tyler Vigen’s site and are reused here with permission under CC BY 4.0. If you enjoy them, there is a book with many more amusing examples available from his website.</p><p data-type="footnote" id="id100"><sup><a href="ch02.xhtml#id100-marker">5</a></sup> R. Feynman and R. Leighton, “Surely You’re Joking Mr Feynman” (W.W. Norton, 1985)</p><p data-type="footnote" id="id101"><sup><a href="ch02.xhtml#id101-marker">6</a></sup> J. Matejka and G. Fitzmaurice, “Same stats, different graphs: Generating datasets with varied appearance and identical statistics through simulated annealing,” CHI 2017, Denver USA (2017)</p><p data-type="footnote" id="id102"><sup><a href="ch02.xhtml#id102-marker">7</a></sup> F. Herbert, <em>Dune</em>, (Chilton Books 1965)</p></div></div></section></div></body></html>