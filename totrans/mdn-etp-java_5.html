<html><head></head><body><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Beyond Lift and Shift: Working with Legacy"><div class="chapter" id="beyond_lift_and_shift">
<h1><span class="label">Chapter 5. </span>Beyond Lift and Shift: Working with Legacy</h1>

<blockquote>
<p>Legacy is not what I did for myself. It’s what I’m doing for the next generation.</p>
<p data-type="attribution">Vitor Belfort</p>
</blockquote>

<p>Many organizations are faced with the challenge of keeping their existing business operations running while also trying to innovate. There are typically increased expectations to deliver new functionality faster and to reduce cost, something that seems challenging when looking at the existing application landscape and prevalence of legacy systems.</p>

<p>We often use the term “legacy system” to describe an old methodology, or technology, or application <a data-type="indexterm" data-primary="legacy systems" id="idm45261444678304"/>that is not written according to the latest methods or uses an outdated technology stack. Admittedly, many of the systems we created early on in our career belong to this category. We do know that most of them are still in use. Some of them even paved the way for new approaches or even standards that followed them. We usually also imply that those systems would need a replacement, which ultimately contributes to the perceived negative connotation. Thankfully, this isn’t always true. Legacy also is a beautiful word to describe achievements and heritage. Calling something “legacy” doesn’t automatically make it outdated and unusable. There are plenty of reasons to keep the legacy systems in place, including:</p>

<ul>
<li>
<p>The system works as designed, and there is no need to change.</p>
</li>
<li>
<p>The business processes implemented are no longer known or documented, and replacing them is expensive.</p>
</li>
<li>
<p>The cost for replacing a system is higher than the benefit of keeping it unchanged.</p>
</li>
</ul>

<p>The book <a href="https://oreil.ly/iogGC"><em>Working Effectively with Legacy Code</em></a> by <a data-type="indexterm" data-primary="Feathers, Michael" id="idm45261444544176"/>Michael Feathers (O’Reilly) provides programmers with techniques to cost-effectively handle common legacy code problems without having to go through the hugely expensive task of rewriting all existing code.</p>

<p>Feathers said, “To me, legacy code is simply code without tests.” If we read the term “legacy” today, it primarily refers <a data-type="indexterm" data-primary="monolith architectures" data-secondary="legacy systems" id="idm45261444542624"/>to monolithic applications. There are various approaches to handling legacy applications in a modern enterprise landscape, and picking the right one is the first and most crucial part of the modernization journey.</p>

<p>We’ve only talked about individual systems so far. And developers usually only care about this specific system scope. Modernization plans should follow overarching company goals and should also take the company-wide IT strategy into account. A particularly exciting approach for cloud migration is presented in Gregor Hohpe’s book <a href="https://oreil.ly/EuW9J"><em>Cloud Strategy: A Decision-Based Approach to Successful Cloud Migration</em></a>. It is a must-read if you want to know more about building the abstraction above individual migration efforts.</p>






<section data-type="sect1" data-pdf-bookmark="Managing Legacy"><div class="sect1" id="idm45261444522960">
<h1>Managing Legacy</h1>

<p>Every successful journey begins with a first step. The first step for an application migration journey is the assessment of the existing applications. We assume that you know the company-wide goals and directives. We can map them into assessment categories now. Another source for assessment categories is technical requirements—for example, existing blueprints or recommended master solutions or framework versions. Building and updating this list of assessment categories should be a recurring task that becomes part of your governance process. Ultimately, you can derive migration criteria from these assessment criteria and use them as decision-making cornerstones for your modernization journey.</p>








<section data-type="sect2" data-pdf-bookmark="Assessing Applications for Migration"><div class="sect2" id="idm45261444520976">
<h2>Assessing Applications for Migration</h2>

<p>When assessing a migration or <a data-type="indexterm" data-primary="migration" data-secondary="application assessment" id="migAppAss"/><a data-type="indexterm" data-primary="modernization" data-secondary="application assessment" id="modAppAss"/><a data-type="indexterm" data-primary="applications" data-secondary="assessment for modernization" id="appAssess"/>modernization effort, it is essential to consider the specific challenges that motivate or influence your organization. Some examples of challenges that organizations might face include:</p>
<dl>
<dt>Limited budgets for development</dt>
<dd>
<p>Development teams <a data-type="indexterm" data-primary="modernization" data-secondary="budget considerations" id="idm45261444513504"/>need to become more efficient, and their velocity has to increase. Instead of working with complex specifications, they aim to switch to lightweight frameworks and prebuilt functionalities. Modernizations should be usually scheduled as part of an ongoing development or maintanance project.</p>
</dd>
<dt>Lack of in-house skills</dt>
<dd>
<p>The team <a data-type="indexterm" data-primary="modernization" data-secondary="in-house skills" id="idm45261444510784"/>skills for existing in-house technologies are decreasing. Examples of this are host programming or even earlier versions of Enterprise Java specifications that are no longer taught or state-of-the-art. Changing existing systems that use older technologies might mean needing to add specific skills for the development project.</p>
</dd>
<dt>Perceived risks</dt>
<dd>
<p>Following a famous proverb <a data-type="indexterm" data-primary="modernization" data-secondary="risk assessment" id="idm45261444508032"/><a data-type="indexterm" data-primary="risk assessment" id="idm45261444507056"/>popularized around 1977, “If it ain’t broken, don’t fix it,” we do see a lot of perceived risks to changing well-established and running software. The reasons for this can be numerous and range from knowledge issues about the system to fear of stopped production in factories. These risks need to be addressed individually and mitigated through suitable actions in the migration plan.</p>
</dd>
<dt>No known predictable process</dt>
<dd>
<p>This book helps you <a data-type="indexterm" data-primary="progress predictions" id="idm45261444504544"/><a data-type="indexterm" data-primary="modernization" data-secondary="progress prediction" id="idm45261444503808"/>with this particular point. Navigating the unknown can be a considerable challenge. Having a proven and repeatable process for modernization efforts in place that all parties respect and follow is critical for success.</p>
</dd>
<dt>Real effort estimation</dt>
<dd>
<p>Estimating migration efforts <a data-type="indexterm" data-primary="modernization" data-secondary="effort estimation" id="idm45261444501168"/>should not be magic. Unfortunately, many companies have a minimal idea about the genuine efforts to modernize Enterprise Java applications. Following a predictable and optimized approach will remove this challenge.</p>
</dd>
</dl>

<p>Turning these challenges into actionable items for your assessment can look like this:</p>

<ul>
<li>
<p>Predicting the level of effort and cost</p>
</li>
<li>
<p>Scheduling application migrations and handling conflicts</p>
</li>
<li>
<p>Identifying all potential risks at a code, infrastructure, process, or knowledge level</p>
</li>
<li>
<p>Predicting the return on investment to make the business case</p>
</li>
<li>
<p>Identifying and mitigating risks to the business</p>
</li>
<li>
<p>Minimizing disruption to existing business operations</p>
</li>
</ul>

<p>It is sufficient to do this in a spreadsheet or document if you are only looking at a single application. However, every mid- to large-scale effort needs a better solution. Large-scale efforts need automated routines and rules to assess an install base and link applications to business services to plan the next steps reliably. An open source and straightforward way of gathering and managing all relevant information comes from the <a href="https://www.konveyor.io">Konveyor project</a>. It combines a set of tools that aim at helping with modernization and migration onto Kubernetes.</p>

<p>The Konveyor subproject Forklift provides the ability to migrate virtual machines to KubeVirt with minimal downtime. The subproject Crane concentrates on migrating applications between Kubernetes clusters. Also part of the suite is Move2Kube to help accelerate the replatforming of Swarm and Cloud Foundry-based applications to Kubernetes.</p>

<p>For application modernization in <a data-type="indexterm" data-primary="migration" data-secondary="application assessment" data-startref="migAppAss" id="idm45261444490688"/><a data-type="indexterm" data-primary="modernization" data-secondary="application assessment" data-startref="modAppAss" id="idm45261444489440"/><a data-type="indexterm" data-primary="applications" data-secondary="assessment for modernization" data-startref="appAssess" id="idm45261444488224"/>particular, Konveyor offers the <a href="https://oreil.ly/u99Gf">Tackle</a> project. It assesses and analyzes applications for refactoring into containers and provides a standard inventory.</p>










<section data-type="sect3" data-pdf-bookmark="Tackle Application Inventory"><div class="sect3" id="idm45261444485952">
<h3>Tackle Application Inventory</h3>

<p>This allows users to maintain their <a data-type="indexterm" data-primary="modernization" data-secondary="application assessment" data-tertiary="inventory" id="idm45261444484560"/><a data-type="indexterm" data-primary="applications" data-secondary="assessment for modernization" data-tertiary="inventory" id="idm45261444483312"/>portfolio of applications, link them to the business services they support, and define their interdependencies. The Application Inventory uses an extensible tagging model to add metadata, which is a great way to link migration categories, as discussed earlier. The Application Inventory is used to select an application for an assessment by Pathfinder.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Tackle Pathfinder"><div class="sect3" id="idm45261444481280">
<h3>Tackle Pathfinder</h3>

<p>This is an interactive, questionnaire-based tool <a data-type="indexterm" data-primary="modernization" data-secondary="application assessment" data-tertiary="Pathfinder" id="idm45261444479920"/><a data-type="indexterm" data-primary="applications" data-secondary="assessment for modernization" data-tertiary="Pathfinder" id="idm45261444478672"/><a data-type="indexterm" data-primary="Pathfinder" id="idm45261444477440"/>that assesses the suitability of applications for modernization so they can be deployed in containers on an enterprise Kubernetes platform. <a href="https://oreil.ly/K4V4u">Pathfinder</a> generates reports about an application’s suitability for Kubernetes, including the associated risk, and creates an adoption plan. Pathfinder does this based on the information present in the application inventory and additional assessment questions. If an application depends on a direct host system connection, it might disqualify this particular application for a migration to Kubernetes because it would overload the host parts. Some examples of assessment questions are:</p>

<ul>
<li>
<p>Are third-party vendor components supported in containers?</p>
</li>
<li>
<p>Is the application under active development?</p>
</li>
<li>
<p>Does the application have any legal requirements (e.g., PCI, HIPAA)?</p>
</li>
<li>
<p>Does the application provide metrics?</p>
</li>
</ul>

<p>We strongly recommend looking at Pathfinder to manage large-scale modernization projects across complete landscapes. It will help you categorize and prioritize applications in your scope today and continuously track your migration assessment for future changes.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Tackle Controls"><div class="sect3" id="idm45261444470224">
<h3>Tackle Controls</h3>

<p>Controls are a collection of entities that <a data-type="indexterm" data-primary="modernization" data-secondary="application assessment" data-tertiary="controls" id="idm45261444468832"/><a data-type="indexterm" data-primary="applications" data-secondary="assessment for modernization" data-tertiary="controls" id="idm45261444467584"/><a data-type="indexterm" data-primary="controls" id="idm45261444466304"/>add different values to the Application Inventory and the Pathfinder assessment. They comprise business services, stakeholders, stakeholder groups, job functions, tag types, and tags. In addition, you can capture company- or project-specific attributes by implementing your own entities. This will filter your Application Inventory, for example, all applications used by a certain “job function” to identify all applications used by the human resources department.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Tackle DiVA"><div class="sect3" id="idm45261444464768">
<h3>Tackle DiVA</h3>

<p>Finally, <a href="https://oreil.ly/UGzn2">DiVA</a> is a data-centric application analysis tool. As a <a data-type="indexterm" data-primary="modernization" data-secondary="application assessment" data-tertiary="DiVA" id="idm45261444462688"/><a data-type="indexterm" data-primary="applications" data-secondary="assessment for modernization" data-tertiary="DiVA" id="idm45261444461440"/><a data-type="indexterm" data-primary="DiVA" id="idm45261444460208"/>successor to the project <a href="https://oreil.ly/sjiNq">Windup</a>, it is the most exciting project to look at if you want to assess individual applications. It focuses on the traditional monolithic application and currently supports Servlets and Spring Boot applications. You can import a set of application source files (Java/XML), and DiVA then provides the following:</p>

<ul>
<li>
<p>Service entry (exported API) inventory</p>
</li>
<li>
<p>Database inventory</p>
</li>
<li>
<p>Transaction inventory</p>
</li>
<li>
<p>Code-to-Database dependencies (call graphs)</p>
</li>
<li>
<p>Database-to-Database dependencies</p>
</li>
<li>
<p>Transaction-to-Transaction dependencies</p>
</li>
<li>
<p>Transaction refactoring recommendations</p>
</li>
</ul>

<p>DiVA is currently under active development, and the incorporation of the original Windup project isn’t finished yet. However, it still gives you a solid foundation for your modernization efforts. Additionally, it presents an excellent opportunity to contribute your findings and become part of a larger community dedicated to automating migrations.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Migration Toolkit for Applications"><div class="sect3" id="idm45261444450560">
<h3>Migration Toolkit for Applications</h3>

<p>While we wait for Windup to be fully <a data-type="indexterm" data-primary="modernization" data-secondary="application assessment" data-tertiary="MTA (Migration Toolkit for Applications)" id="modAppMTA"/><a data-type="indexterm" data-primary="applications" data-secondary="assessment for modernization" data-tertiary="MTA (Migration Toolkit for Applications)" id="appAssMTA"/><a data-type="indexterm" data-primary="MTA (Migration Toolkit for Applications)" id="MTAKit"/><a data-type="indexterm" data-primary="Migration Toolkit for Applications (MTA)" id="MigTool"/>integrated into DiVA, you can still use an automated migration assessment for Enterprise Java-based applications by using the <a href="https://oreil.ly/SIOSR">Migration Toolkit for Applications (MTA)</a>.</p>

<p>MTA assembles tools that support large-scale Enterprise Java application modernization and migration projects across many transformations and use cases. You can import your application binary or archives into it, and it automatically performs code analysis, including the application portfolio, application dependencies, migration challenges, and migration effort estimation in the form of story points. Initially it was designed to support Java EE server migrations (e.g., WebSphere or WebLogic to JBoss EAP). Still, it has a highly extensible rule set mechanism that allows developers to create their own set of rules or even adapt existing ones to their needs. Today it also covers Spring Boot to Quarkus migrations.</p>

<p class="pagebreak-before less_space">An excerpt from an example rule in Java looks like this:</p>

<pre data-type="programlisting" data-code-language="java"><code class="c1">//...</code>
<code class="n">JavaClass</code><code class="o">.</code><code class="na">references</code><code class="o">(</code><code class="s">"weblogic.servlet.annotation.WLServlet"</code><code class="o">)</code>
    <code class="o">.</code><code class="na">at</code><code class="o">(</code><code class="n">TypeReferenceLocation</code><code class="o">.</code><code class="na">ANNOTATION</code><code class="o">)</code>
        <code class="o">)</code>
        <code class="o">.</code><code class="na">perform</code><code class="o">(</code>
            <code class="n">Classification</code><code class="o">.</code><code class="na">as</code><code class="o">(</code><code class="s">"WebLogic @WLServlet"</code><code class="o">)</code>
               <code class="o">.</code><code class="na">with</code><code class="o">(</code><code class="n">Link</code><code class="o">.</code><code class="na">to</code><code class="o">(</code><code class="s">"Java EE 6 @WebServlet"</code><code class="o">,</code>
                             <code class="s">"https://some.url/index.html"</code><code class="o">))</code>
               <code class="o">.</code><code class="na">withEffort</code><code class="o">(</code><code class="mi">0</code><code class="o">)</code>
               <code class="o">.</code><code class="na">and</code><code class="o">(</code><code class="n">Hint</code><code class="o">.</code><code class="na">withText</code><code class="o">(</code><code class="s">"Migrate to Java EE 6 @WebServlet."</code><code class="o">)</code>
               <code class="o">.</code><code class="na">withEffort</code><code class="o">(</code><code class="mi">8</code><code class="o">))</code>
        <code class="o">);</code>
<code class="c1">//...</code></pre>

<p>This rule scans Java classes for <code>@WLServlet</code> annotations and adds an effort (story points) to this finding. You can learn more about rules and how to develop them in the <a href="https://oreil.ly/FbQKL">Windup documentation</a>.</p>

<p>Beyond that, it can also support nonmigration use cases as part of a build process (via a <a href="https://oreil.ly/T8mom">Maven plug-in</a> or a <a href="https://oreil.ly/U7Dsk">Command Line Interface</a>), either validating code regularly against organizational standards or ensuring application portability.</p>

<p>Some of the patterns MTA can detect include the following:</p>

<ul>
<li>
<p>Proprietary libraries</p>
</li>
<li>
<p>Proprietary configurations</p>
</li>
<li>
<p>Service locators</p>
</li>
<li>
<p>Web services</p>
</li>
<li>
<p>EJB descriptors</p>
</li>
<li>
<p>Deprecated Java code</p>
</li>
<li>
<p>Transaction managers</p>
</li>
<li>
<p>Injection frameworks</p>
</li>
<li>
<p>Thread pooling mechanisms</p>
</li>
<li>
<p>Timer services</p>
</li>
<li>
<p>WAR/EAR descriptors</p>
</li>
<li>
<p>Static IP addresses</p>
</li>
</ul>

<p>MTA and DiVA are two potent tools that help us identify overall technical debt, resulting in a classification of migration needs and risks. However, they do not allow us to identify the functionality that should be migrated or modernized first. For this, we need to <a data-type="indexterm" data-primary="modernization" data-secondary="application assessment" data-tertiary="MTA (Migration Toolkit for Applications)" data-startref="modAppMTA" id="idm45261444335920"/><a data-type="indexterm" data-primary="applications" data-secondary="assessment for modernization" data-tertiary="MTA (Migration Toolkit for Applications)" data-startref="appAssMTA" id="idm45261444334272"/><a data-type="indexterm" data-primary="MTA (Migration Toolkit for Applications)" data-startref="MTAKit" id="idm45261444332736"/><a data-type="indexterm" data-primary="Migration Toolkit for Applications (MTA)" data-startref="MigTool" id="idm45261444331760"/>take a deeper look into the application design and functionality.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Assessing Functionality for Migration"><div class="sect2" id="idm45261444520512">
<h2>Assessing Functionality for Migration</h2>

<p>Traditional monoliths come in various <a data-type="indexterm" data-primary="modernization" data-secondary="functionality assessment" id="modfunass"/><a data-type="indexterm" data-primary="functionality assessment" id="functassess"/><a data-type="indexterm" data-primary="monolith architectures" data-secondary="modernization, functionality assessment" id="monomodfunct"/>shapes, forms, and sizes. When someone uses the term “monolith,” they are usually referring to the deployment artifact itself. In Enterprise Java, this has <a data-type="indexterm" data-primary="monolith architectures" data-secondary="types" id="idm45261444325648"/>traditionally been Enterprise Archives (EAR) or Web Archives (WAR). You can also look at them as single-process applications. They can be designed following modularity recommendations like OSGi (Open Services Gateway Initiative) or following more technical approaches like the three-tier design without significant business modules. The overall direction of your modernization efforts heavily depends on the type of monolith you are dealing with. As a rule of thumb, the more modular an existing application already is, the easier it is to modernize it. In a perfect world, modules directly translate into service boundaries. But this rarely happens.</p>

<p>If the monolith seems like a <a data-type="indexterm" data-primary="monolith architectures" data-secondary="logical model" id="idm45261444323520"/>giant box, we have to apply a logical model to it. And we realize that inside this box are organized business and technical components, for example, order management, PDF rendering, client notifications, etc. While the code is probably not organized around these concepts, they exist in the codebase from a business-domain-model perspective.  These business <a data-type="indexterm" data-primary="DDD (domain-driven design)" id="idm45261444322048"/>domain boundaries, often called “bounded contexts” in Domain-Driven-Design (DDD), become the new services.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>If you are interested in learning more, many consider Eric Evans’s book <a href="https://oreil.ly/kgLPl"><em>Domain-Driven Design: Tackling Complexity in the Heart of Software</em> (O’Reilly)</a> the de facto standard introduction to DDD.</p>
</div>

<p>Once you have identified the modules and functionality, you can start to think about your modernizing order. But first, make sure to look at the cost versus benefit tradeoffs for each module and start with the best candidate. <a data-type="xref" href="#fig5-1">Figure 5-1</a> gives a very high-level overview of how this could look for a sample application with six modules. Let’s assume we are talking about a fictitious online shop in this case. For modules that are heavily interdependent, for example, Order and Customer, it will be complex to extract them individually. If you also consider the necessity for scalability and with that the benefit of removing them from a monolith, it might not be very high. Those two modules reside on the lower left side of the graph. On the opposite side, we might find the Catalog service. It lists the available products and is a read-only service with very little interdependencies. During high demand on the website, this is the number-one requested module, and it benefits heavily from being extracted, as shown in Figure 5-1, indicated by the green module in the upper right of the graph. Do a similar exercise for all the modules in your application to assess cost versus 
<span class="keep-together">benefit</span>.</p>

<figure><div id="fig5-1" class="figure">
<img src="Images/moej_0501.png" alt="Cost vs. Benefit" width="600" height="533"/>
<h6><span class="label">Figure 5-1. </span>Cost versus benefit</h6>
</div></figure>

<p>You’ve now reached the last checkpoint to validate your earlier strategic application assessment. Does the estimated <a data-type="indexterm" data-primary="modernization" data-secondary="cost versus benefits" id="idm45261444313328"/>modernization benefit outweigh the estimated modernization cost? Unfortunately, there is no generally applicable recommendation, as it heavily depends on the application itself, the business requirements, and the overarching company goals and challenges. Document your decisions and conclusions because now is the time to decide about the future direction of your modernization effort. Remember the 6 Rs from <a data-type="xref" href="ch03.xhtml#travel_light_on_your_pathway">Chapter 3</a>? Retain (change nothing), Retire (turn off), Repurchase (a new version), Rehost (put into containers), Replatform (some slight adjustments), or Refactor (build something new).</p>

<p>We’ve now assessed the application for migration, and we’ve evaluated the functionality for migration. We know which aspects of the application we’re ready to modernize. You’ve concluded that you do not want to build a new application but rather gently modernize the existing legacy.  In the next section, we are going to take a deeper <a data-type="indexterm" data-primary="modernization" data-secondary="functionality assessment" data-startref="modfunass" id="idm45261444310032"/><a data-type="indexterm" data-primary="functionality assessment" data-startref="functassess" id="idm45261444308768"/><a data-type="indexterm" data-primary="monolith architectures" data-secondary="modernization, functionality assessment" data-startref="monomodfunct" id="idm45261444307808"/>look at some approaches to migration.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Migration Approaches"><div class="sect1" id="idm45261444306432">
<h1>Migration Approaches</h1>

<p>The aforementioned tools and assessments will help you on your journey to identify the most suitable applications and services. Now it’s time to dig deeper into the strategies and challenges of a single application.</p>








<section data-type="sect2" data-pdf-bookmark="Protecting Legacy (Replatform)"><div class="sect2" id="idm45261444304928">
<h2>Protecting Legacy (Replatform)</h2>

<p>With only one or two modules needing a <a data-type="indexterm" data-primary="migration" data-secondary="legacy protection" id="idm45261444303552"/><a data-type="indexterm" data-primary="migration" data-secondary="replatforming" id="idm45261444302544"/><a data-type="indexterm" data-primary="replatforming" id="idm45261444301600"/><a data-type="indexterm" data-primary="platform" data-secondary="replatforming" id="idm45261444300928"/><a data-type="indexterm" data-primary="legacy systems" data-secondary="migrations" data-tertiary="protecting" id="idm45261444299984"/>business refresh or added functionality, the most straightforward way is to focus on the two modules and keep as much as possible of the existing application, making it runnable on modern infrastructure. Besides changes of the relevant modules, this also involves a reevaluation of the runtime, libraries, or even target infrastructure while touching as little code as possible.</p>

<p>This can be achieved by simply containerizing the application and databases and modifying relevant modules of a well-architected monolith or extracting certain functionality completely and reintegrating it to a partly distributed system, as <a data-type="xref" href="#fig5-2">Figure 5-2</a> shows.</p>

<figure><div id="fig5-2" class="figure">
<img src="Images/moej_0502.png" alt="Putting the pieces back together" width="600" height="426"/>
<h6><span class="label">Figure 5-2. </span>Putting the pieces back together</h6>
</div></figure>

<p>What is easily said isn’t quickly done. There are plenty of nonfunctional requirements that need to be reallocated from the application server platform to the <a href="https://oreil.ly/rrcZG">outer architecture</a>. We will focus on the more critical pieces in the next chapter. In this chapter, we want to focus on the migration of the application and database itself.</p>










<section data-type="sect3" data-pdf-bookmark="Service to application"><div class="sect3" id="idm45261444293056">
<h3>Service to application</h3>

<p>Once you’ve extracted certain functionality, the most <a data-type="indexterm" data-primary="migration" data-secondary="legacy protection" data-tertiary="service to monolith interactions" id="idm45261444291552"/><a data-type="indexterm" data-primary="migration" data-secondary="replatforming" data-tertiary="service to monolith interactions" id="idm45261444290160"/><a data-type="indexterm" data-primary="replatforming" data-secondary="service to monolith interactions" id="idm45261444288928"/><a data-type="indexterm" data-primary="platform" data-secondary="replatforming" data-tertiary="service to monolith interactions" id="idm45261444287968"/><a data-type="indexterm" data-primary="microservices" data-secondary="monolith interaction" id="idm45261444286736"/><a data-type="indexterm" data-primary="monolith architectures" data-secondary="services" id="idm45261444285792"/>pressing question is how to integrate the remaining monolith with the newly extracted service. Assuming that you switch to a container runtime, you should use an API Gateway to load balance and switch traffic on a URL basis. We’ll cover this in more detail in <a data-type="xref" href="ch06.xhtml#microservices_architecture">Chapter 6</a>.</p>

<p>Another approach is to use <a data-type="indexterm" data-primary="HTTP proxy" id="idm45261444283216"/>an HTTP proxy. It is essential to have the proxy up in production before you even try to extract parts of the monolith. Ensure it does not break the existing monolith and take some time to push the new service into production regularly, even without it being used by end users. Gradually switch over by redirecting traffic if everything looks good.</p>

<p>For more simple service to monolith interactions, you can even think about implementing a <a data-type="indexterm" data-primary="JAX-RS direct communiction" id="idm45261444281520"/>simple JAX-RS direct communication. This approach is only suitable when you work with very few services, though. Make sure to treat the extracted service as an integration system from the perspective of the monolith.</p>

<p>All three approaches (API, gateway, HTTP proxy, and JAX-RS interface) are a pathway to your first successful microservice. They all implement the strangler pattern (refer to <a data-type="xref" href="ch03.xhtml#travel_light_on_your_pathway">Chapter 3</a>) and help to refactor the monolith into separate systems as a first step.</p>

<p>Interception is a potentially dangerous path: if you start building a custom protocol translation <a data-type="indexterm" data-primary="interception" id="idm45261444278464"/>layer that is shared by multiple services, you risk adding too much intelligence to the shared proxy. This design approach leads away from independent microservices and becomes a more service-oriented architecture with too much intelligence in the routing layer. A better alternative is the so-called Sidecar pattern, which basically describes an additional container in the Pod. Rather than placing custom proxy logic in a shared layer, it becomes part of the new service. As a Kubernetes sidecar, it becomes a runtime binding and can serve legacy clients and new clients.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>A <em>sidecar</em> is just a <a data-type="indexterm" data-primary="sidecars" id="idm45261444275424"/><a data-type="indexterm" data-primary="containers" data-secondary="sidecars" id="idm45261444274688"/>container that runs on the same Pod as the application container. It shares the same volume and network as the application container and can “help” or enhance application behavior with this. Typical examples are logging, or more generally agent functionality.</p>
</div>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Database to databases"><div class="sect3" id="idm45261444273088">
<h3>Database to databases</h3>

<p>Once we have identified the functional <a data-type="indexterm" data-primary="migration" data-secondary="legacy protection" data-tertiary="database separation" id="miglegdbsep"/><a data-type="indexterm" data-primary="migration" data-secondary="replatforming" data-tertiary="database separation" id="migredatsep"/><a data-type="indexterm" data-primary="replatforming" data-secondary="database separation" id="replatdatsep"/><a data-type="indexterm" data-primary="platform" data-secondary="replatforming" data-tertiary="database separation" id="platdatsep"/><a data-type="indexterm" data-primary="legacy systems" data-secondary="migrations" data-tertiary="database separation" id="legmigdatsep"/><a data-type="indexterm" data-primary="monolith architectures" data-secondary="database separation" id="monodatsep"/><a data-type="indexterm" data-primary="database separation" id="datsep"/>boundary and the integration method, we need to decide how to approach database separation. While monolith applications typically rely on a single large database, each extracted service should operate on its own data. The correct way to solve this puzzle again depends on the existing data layout and transactions.</p>

<p>A relatively easy first step is to separate the tables necessary for the service into a read-only view and a write table and adjust the flow of the monolith to use an interface for both read and write operations. These interfaces can more easily be abstracted in a later step into a service access. This option requires changes to the monolith application only and should have minimal impact on the existing codebase. We can move the table into a separate database and adjust the dependent queries in the next step.</p>

<p>All this solely happens in the old monolith as preparation. Evolving existing code into a more modularized structure as preparation can be risky. In particular, the risk increases as the data model complexity does. In the last step, we can separate the extracted tables into a new database and adjust the monolith to use the newly created service for interactions with the business object. This is relatively easy with pen and paper and quickly reaches the end of practicality if the data access requires many joins across tables. Simple candidates are master data objects, like “User.” More complex ones could be combined objects, like an “Order.” What was said about the modularization of the application code is even more true for the database. The better the design and modularization already are, the easier it will be to extract functionality and data into a separate service.
There will be cases where you won’t find an excellent solution to extract objects from the data model. Or you may see different approaches not delivering suitable performance anymore. This is the time to revisit your chosen modernization path.</p>

<p>Continuing on the happy path, you now have two separate databases and two very unequal “services” composing a system. Now it’s time to think about data synchronization strategies between your services.
Most databases implement some functionality to execute behavior on data changes. Simple cases support trigger functionality on changed rows to add copies to other tables or even call higher-level features (e.g., WebServices) on change. It is often proprietary functionality and heavily depends on the database being used. This could be an option if you have a company-wide directive to use certain features or you’re confident enough in further altering the original legacy database.</p>

<p>If this isn’t possible, there’s <a data-type="indexterm" data-primary="batch job-based synchronization" id="idm45261444257872"/><a data-type="indexterm" data-primary="synchronization, batch job based" id="idm45261444257088"/>the batch job-based synchronization. Changed timestamps, versions, or status columns indicate a needed replication. You can rely on this as a very mature and well-known version of data synchronization, which you can find in many legacy systems. The major drawback is that you’ll always end up with a discrepancy in data accuracy in the target system no matter the implementation. Higher replication intervals might also lead to additional costs for transactions or additional load on the source system. This approach is only suitable for infrequent updates that ideally have a non-time-sensitive process step in between. It is unsuitable for real- or near-time update requirements.</p>

<p>The modern approach to solving data synchronization challenges relies on log readers. As third-party libraries, they identify changes by scanning the database transaction log files. These log files exist for backup and recovery operations and provide a reliable way to capture all changes, including deletes. This concept is also <a data-type="indexterm" data-primary="change-data-capture" id="idm45261444254608"/>known as change-data-capture. One of the most notable projects here is <a href="https://debezium.io">Debezium</a>. Using log readers is the least disruptive option for synchronizing changes between databases because they require no modification to the source database, and they don’t have a query load on the source systems. Change <a data-type="indexterm" data-primary="migration" data-secondary="legacy protection" data-tertiary="database separation" data-startref="miglegdbsep" id="idm45261444252800"/><a data-type="indexterm" data-primary="migration" data-secondary="replatforming" data-tertiary="database separation" data-startref="migredatsep" id="idm45261444251280"/><a data-type="indexterm" data-primary="replatforming" data-secondary="database separation" data-startref="replatdatsep" id="idm45261444249792"/><a data-type="indexterm" data-primary="platform" data-secondary="replatforming" data-tertiary="database separation" data-startref="platdatsep" id="idm45261444248576"/><a data-type="indexterm" data-primary="legacy systems" data-secondary="migrations" data-tertiary="database separation" data-startref="legmigdatsep" id="idm45261444247088"/><a data-type="indexterm" data-primary="monolith architectures" data-secondary="database separation" data-startref="monodatsep" id="idm45261444245600"/><a data-type="indexterm" data-primary="database separation" data-startref="datsep" id="idm45261444244384"/>data events generate notifications for other systems with the help of the Outbox pattern.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Build Something New (Refactor)"><div class="sect2" id="idm45261444243200">
<h2>Build Something New (Refactor)</h2>

<p>If, for whatever reasons, you’ve reached a <a data-type="indexterm" data-primary="migration" data-secondary="refactoring" id="idm45261444241728"/><a data-type="indexterm" data-primary="refactoring" id="idm45261444240752"/>fork in the road where you decide to re-implement and refactor your complete system into a new distributed architecture, you are most likely thinking about synergies and ways to keep effort small and 
<span class="keep-together">predictable</span>. Given the complexity of a full microservices stack, this isn’t an easy task. One critical factor with this approach is team knowledge. After many years of development on an Enterprise Java application server, a team should profit from 
<span class="keep-together">continuous</span> API and standards knowledge. There are various ways to implement services on the JVM that all help teams with reusing the most critical functionalities we all already know from Enterprise Java/Jakarta EE standards. Let’s discuss some of these methods for implementing services on the JVM.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><a href="https://jakarta.ee/about">Jakarta EE</a> is a set of specifications that enables Java developers to work on Java Enterprise applications. The specifications are developed by well-known industry leaders that instill confidence in technology developers and consumers. It is the open source version of the Java Enterprise Edition.</p>
</div>










<section data-type="sect3" data-pdf-bookmark="MicroProfile"><div class="sect3" id="idm45261444235632">
<h3>MicroProfile</h3>

<p><a href="https://microprofile.io">MicroProfile</a> was created in 2016 and quickly joined the Eclipse foundation. The <a data-type="indexterm" data-primary="migration" data-secondary="refactoring" data-tertiary="MicroProfile" id="idm45261444233744"/><a data-type="indexterm" data-primary="refactoring" data-secondary="MicroProfile" id="idm45261444232416"/><a data-type="indexterm" data-primary="MicroProfile" id="idm45261444231472"/>primary purpose of MicroProfile is to create a Java Enterprise framework for implementing portable microservices in a vendor-neutral way.  MicroProfile is composed of a vendor-agnostic programming model and configuration and services such as tracing, fault tolerance, health, and metrics. MicroProfile API components are built upon the model of Jakarta EE, making a transition to microservices more natural for Java developers. You can reuse the existing knowledge of Jakarta EE you’ve already accumulated in your career. MicroProfile defines 12 specifications as shown in <a data-type="xref" href="#fig5-3">Figure 5-3</a>, and the component model underneath uses a subset of the existing Jakarta EE standards. Compared to the full Jakarta EE specification, the more heavyweight specifications are missing. Most relevant for larger monolithic applications are Enterprise JavaBeans (EJB) and Jakarta XML Web Services.</p>

<figure><div id="fig5-3" class="figure">
<img src="Images/moej_0503.png" alt="MicroProfile technologies overview" width="600" height="273"/>
<h6><span class="label">Figure 5-3. </span>MicroProfile technologies overview</h6>
</div></figure>

<p>There are various implementations of <a data-type="indexterm" data-primary="Open Liberty" id="idm45261444226608"/><a data-type="indexterm" data-primary="Thorntail" id="idm45261444225904"/><a data-type="indexterm" data-primary="Paraya Server" id="idm45261444225232"/><a data-type="indexterm" data-primary="TomEE" id="idm45261444224560"/><a data-type="indexterm" data-primary="SmallRye" id="idm45261444223888"/>the MicroProfile specifications available: Open Liberty, Thorntail, Paraya Server, TomEE, SmallRye, etc. As the MicroProfile relies on principles and components close to the Jakarta EE Web Profile, it is comparably easy to migrate existing applications.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Quarkus"><div class="sect3" id="idm45261444222560">
<h3>Quarkus</h3>

<p><a href="http://quarkus.io">Quarkus</a> is a relatively new member of the so-called microservices frameworks. It <a data-type="indexterm" data-primary="migration" data-secondary="refactoring" data-tertiary="Quarkus" id="idm45261444220704"/><a data-type="indexterm" data-primary="refactoring" data-secondary="Quarkus" id="idm45261444219456"/><a data-type="indexterm" data-primary="Quarkus" id="idm45261444218512"/>is a full stack, Kubernetes-native Java framework for JVMs and native compilation. It is optimized specifically for containers and constrained runtime environments. Its primary purpose is to be an ideal runtime for serverless, cloud, and Kubernetes environments.</p>

<p>It works with popular Java standards, frameworks, and libraries like Eclipse MicroProfile, Spring Boot, Apache Kafka, RESTEasy (JAX-RS), Hibernate ORM (JPA), Infinispan, Camel, and many more.</p>

<p>The dependency injection <a data-type="indexterm" data-primary="dependency injection solution" id="idm45261444216384"/><a data-type="indexterm" data-primary="CDI (Contexts and Dependency Injection)" id="idm45261444215616"/>solution is based on CDI (Contexts and Dependency Injection) coming from Jakarta EE, making it compatible with established component models. An interesting part is the extension framework, which helps expand functionality to configure, boot, and integrate company-specific libraries into your application. It runs on JVMs and supports GraalVM (a general-purpose virtual machine for many languages).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Component models to services"><div class="sect3" id="idm45261444214128">
<h3>Component models to services</h3>

<p>One of the most common questions <a data-type="indexterm" data-primary="migration" data-secondary="refactoring" data-tertiary="component models" id="idm45261444212816"/><a data-type="indexterm" data-primary="refactoring" data-secondary="component models" id="idm45261444211568"/><a data-type="indexterm" data-primary="component models" id="idm45261444210624"/>among developers is how to migrate existing component models of Enterprise Java applications into microservices. Commonly, this question refers to Enterprise Java Beans or CDI Beans, especially the container-managed persistence beans (before EJB3), which need to be re-created on a Java Persistence API (JPA) basis. We strongly recommend checking if the underlying data/object mapping is still accurate and suitable for the new requirements and re-creating it entirely. This is not the most time- and cost-consuming part of modernization. Typically, the more challenging parts are the coded business requirements. While CDI Beans are technically part of MicroProfile-compatible implementations, the decision of whether a simple code migration is appropriate depends on the new business requirements. It is essential to look for existing code transaction boundaries to ensure no downstream resource needs to be involved. A general recommendation is to reuse as little source code as possible. The reason here is mainly the different approaches in system design between the two technologies. While we got away with a halfway modularized monolith, this isn’t possible with microservices anymore. Taking extra care to define the bounded contexts will pay off for the performance and design of the final solution.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Spring applications to services"><div class="sect3" id="idm45261444208240">
<h3>Spring applications to services</h3>

<p>We can take a similar approach with <a data-type="indexterm" data-primary="migration" data-secondary="refactoring" data-tertiary="Spring" id="idm45261444206880"/><a data-type="indexterm" data-primary="refactoring" data-secondary="Spring" id="idm45261444205632"/><a data-type="indexterm" data-primary="Spring" id="idm45261444204688"/>applications following a different programming framework like Spring. While it will technically be easy to update and copy existing implementations, the drawbacks stay the same. In particular, it might be helpful for Spring-based development teams to use compatibility APIs in different frameworks like Quarkus.</p>

<p>Quarkus’s Spring API compatibility includes Spring DI, Spring Web, and Spring Data JPA. Additional Spring APIs are partially supported like Spring Security, Spring Cache, Spring Scheduled, and Spring Cloud Config. The Spring API compatibility in Quarkus is not intended to be a complete Spring platform to rehost existing Spring applications. The intent is to offer enough Spring API compatibility to develop new applications with Quarkus.</p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Challenges"><div class="sect1" id="idm45261444202336">
<h1>Challenges</h1>

<p>With assessment, planning, and care, you can decompose and modernize existing monolithic applications. It is not an automated process most of the time and will require a decent amount of work. There are some specific challenges to watch out for.</p>








<section data-type="sect2" data-pdf-bookmark="Avoiding Dual-Writes"><div class="sect2" id="idm45261444200448">
<h2>Avoiding Dual-Writes</h2>

<p>Once you build a few microservices, you <a data-type="indexterm" data-primary="migration" data-secondary="challenges" id="migchalla"/>quickly realize that the most challenging part about them is data. As part of their business logic, microservices often have to update their local data store. At the same time, they also need to notify other services about the changes that happened. This challenge is not so evident in the world of monolithic applications, nor on legacy-distributed transactions operating on one data model. This situation isn’t easy to resolve. With a switch to distributed applications, you most likely lose consistency. This is described in the CAP theorem.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The <a href="https://oreil.ly/TVwYw">CAP theorem</a>, or the “two out of three” concept, states that we can only simultaneously provide two of the following three guarantees: consistency, availability, and partitition tolerance.</p>
</div>

<p>Modern distributed applications <a data-type="indexterm" data-primary="distributed systems" data-secondary="event bus" id="idm45261444194592"/>use an event bus, <a data-type="indexterm" data-primary="Apache Kafka" id="idm45261444193488"/>like Apache Kafka, to transport data between services. Migrating your transactions from two-phase commit (2PC) in your monolith to a distributed world will significantly change the way your application behaves and reacts to failures. You need a way to control long-running and distributed transactions.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Long-Running Transactions"><div class="sect2" id="idm45261444192080">
<h2>Long-Running Transactions</h2>

<p>The Saga pattern offers a <a data-type="indexterm" data-primary="distributed systems" data-secondary="Saga pattern" id="idm45261444190720"/><a data-type="indexterm" data-primary="Saga pattern" id="idm45261444189744"/>solution to dual writes and long-running transactions. While the Outbox pattern solves the more straightforward interservice communication problem, it is insufficient to solve the more complex, long-running, distributed business transactions use case. The latter requires executing multiple operations across multiple services with a consistent all-or-nothing semantic. Every multistep business process can be an example of this when split out across multiple services. The shopping cart application needs to generate confirmation emails and print a shipping label in the inventory. All actions must be carried out together or not at all. In the legacy world, or with a monolithic architecture, you might not be aware of this problem as the coordination between the modules is done in a single process and a single transactional context. The distributed world requires a different approach.</p>

<p>The Saga pattern offers a solution to this problem by splitting up an overarching business transaction into multiple local database transactions, which are executed by the participating services. Generally, there are two ways to implement distributed sagas:</p>

<ul>
<li>
<p>Choreography: In this approach, one participating service sends a message to the next one after it has executed its local transaction.</p>
</li>
<li>
<p>Orchestration: In this approach, one central coordinating service coordinates and invokes the participating services. Communication between the participating services might be either synchronous, via HTTP or gRPC, or asynchronous, via messaging such as Apache Kafka.</p>
</li>
</ul>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Removing Old Code Too Quickly"><div class="sect2" id="idm45261444184512">
<h2>Removing Old Code Too Quickly</h2>

<p>As soon as we extract a service, we want to get rid of the old source code, maintenance costs, and duplicate development. But be careful. You can look at the old code as a reference and test changes in behavior against both code bases. It might also be helpful from time to time to check the timing of the newly created service. A recommendation is to run them side by side for a defined period and compare the results. After this, <a data-type="indexterm" data-primary="migration" data-secondary="challenges" data-startref="migchalla" id="idm45261444182720"/>you can remove the old implementation. That is early enough.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Integration Aspects"><div class="sect1" id="idm45261444181088">
<h1>Integration Aspects</h1>

<p>Traditional monoliths have a solid <a data-type="indexterm" data-primary="monolith architectures" data-secondary="integration logic" id="idm45261444179696"/><a data-type="indexterm" data-primary="integration logic" id="idm45261444178720"/>relationship with complex integration logic. This is mainly proxied behind session facades or integrated with data synchronization logic. Every single system integrated into the overarching business process needs to be treated as a separate service. You can apply the same principles when extracting parts of the data from the existing data model and do this step by step. Another approach is to treat your integration logic as a service from the very beginning. A method that was primarily designed to support microservices is <a href="https://oreil.ly/JiOwc">Camel K</a>. It builds on the foundation of the well-known Apache Camel integration library and wraps integration routes into containers or better individual services. This way, you can separate the complete integration logic of your monolithic application and your services.</p>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm45261444176176">
<h1>Summary</h1>

<p>Modern enterprise Java systems are like generations of families: they evolve on top of legacy systems. Using proven patterns, standardized tools, and open source resources will help you create long-lasting systems that can grow and change with your needs. Fundamentally, your migration approach is directly related to what problems you’re trying to solve today and tomorrow. What are you trying to achieve that your current architecture doesn’t scale up to? Maybe microservices are the answer, or perhaps something else is. You must understand what you’re trying to achieve because it will be challenging to establish how to migrate the existing systems without that comprehension. Understanding your end goal will change how you decompose a system and how you prioritize that work.</p>
</div></section>







</div></section></div></body></html>