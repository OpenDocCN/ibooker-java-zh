<html><head></head><body><section data-pdf-bookmark="Chapter 11. The Event Bus: The Backbone" data-type="chapter" epub:type="chapter"><div class="chapter" id="event-bus">&#13;
<h1><span class="label">Chapter 11. </span>The Event Bus: The Backbone</h1>&#13;
&#13;
&#13;
<p><a data-primary="event bus" data-type="indexterm" id="ix_event-bus-adoc0"/>In <a data-type="xref" href="ch10.html#messaging">Chapter 10</a>, we discussed Reactive Messaging and utilizing its annotations&#13;
to produce, consume, and process messages, as well as to bridge imperative and reactive programming.&#13;
This chapter dives deeper into the backbone of a reactive system built with Reactive Messaging,&#13;
focusing on Apache Kafka and Advanced Message Queuing Protocol (AMQP).<sup><a data-type="noteref" href="ch11.html#idm45358818317616" id="idm45358818317616-marker">1</a></sup></p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kafka or AMQP: Picking the Right Tool" data-type="sect1"><div class="sect1" id="idm45358818316960">&#13;
<h1>Kafka or AMQP: Picking the Right Tool</h1>&#13;
&#13;
<p><a data-primary="AMQP (Advanced Message Queuing Protocol)" data-secondary="as event bus tool" data-secondary-sortas="event bus" data-type="indexterm" id="idm45358818315584"/><a data-primary="event bus" data-secondary="Kafka versus AMQP" data-type="indexterm" id="idm45358818298144"/><a data-primary="Kafka" data-secondary="as event bus tool" data-secondary-sortas="event bus" data-type="indexterm" id="idm45358818297296"/>Plenty of messaging solutions let you implement event-driven architecture, event streaming, and reactive systems in general.&#13;
Recently, Apache Kafka became a prominent player in this space.&#13;
AMQP is another approach for messaging that should not be immediately ruled out.&#13;
Both have pros and cons.&#13;
Your choice depends entirely on your use cases,&#13;
and to a lesser extent the existing skills and experience of a team.</p>&#13;
&#13;
<p>Rather than favoring one event bus over another,&#13;
this section details the characteristics and behaviors of each,&#13;
along with their relative strengths and weaknesses.&#13;
We want to provide you sufficient information about each system,&#13;
enabling you to determine how they may fit into the use cases for a particular system.</p>&#13;
&#13;
<p>At a high level, Kafka can be described as smart consumers with a dumb broker,&#13;
while AMQP has a smart broker but dumb consumers.&#13;
Sometimes the choice comes down to the amount of flexibility needed when implementing a solution.</p>&#13;
&#13;
<p>Sure it’s cliché, but there really is no one-size-fits-all event bus.&#13;
Each situation has specific requirements and use cases to be fulfilled,&#13;
requiring careful evaluation of the pros and cons of each. Note that other messaging solutions might be better for your use case, such as the <a href="https://solace.com">Solace PubSub+ Platform</a>, <a href="https://oreil.ly/eXz0E">Microsoft Azure Event Hubs</a>, <a href="https://rabbitmq.com">RabbitMQ</a>, or <a href="https://nats.io">NATS</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Building Reactive Systems with Kafka" data-type="sect1"><div class="sect1" id="idm45358818290592">&#13;
<h1>Building Reactive Systems with Kafka</h1>&#13;
&#13;
<p><a data-primary="event bus" data-secondary="building reactive systems with Kafka" data-type="indexterm" id="ix_event-bus-adoc1"/><a data-primary="Kafka" data-secondary="building reactive systems with" data-type="indexterm" id="ix_event-bus-adoc2"/><a data-primary="reactive systems" data-secondary="building with Kafka" data-type="indexterm" id="ix_event-bus-adoc3"/>Since 2011, when Apache Kafka was open sourced by LinkedIn,&#13;
it has skyrocketed to becoming one of the most prominent actors in the event-driven space.&#13;
Fueled by the rise of microservices, serverless architecture, and distributed systems in general,&#13;
Kafka is a popular choice for developers needing a messaging backbone.</p>&#13;
&#13;
<p>While <a data-type="xref" href="ch10.html#messaging">Chapter 10</a> already gave examples of using Kafka,&#13;
we didn’t explain the important details of how it works underneath, which are necessary to understand it well.&#13;
This chapter dives a bit deeper and explains how to use Kafka as the connective tissue in a reactive system.&#13;
We don’t intend to cover Kafka in full detail,&#13;
but enough to appreciate how Kafka operates to effectively develop a reactive system. First off, we need to cover the basics of Kafka.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Apache Kafka" data-type="sect2"><div class="sect2" id="_kafka_101">&#13;
<h2>Apache Kafka</h2>&#13;
&#13;
<p><a data-primary="Kafka" data-secondary="about" data-type="indexterm" id="ix_event-bus-adoc4"/>Kafka is a powerful distributed commit log,&#13;
and if you’re a developer, you’re probably familiar with another distributed commit log, Git! <a data-primary="record (Kafka)" data-type="indexterm" id="idm45358818279104"/>When communicating with Kafka, we use a <em>record</em>, or event,&#13;
as the piece of information we want written to the log,&#13;
and which we then read from the log later.&#13;
<a data-primary="topic (Kafka)" data-type="indexterm" id="idm45358818277792"/>The log contains a <em>topic</em> for each record grouping we want to track (<a data-type="xref" href="#image:kafka-topic">Figure 11-1</a>).</p>&#13;
&#13;
<figure><div class="figure" id="image:kafka-topic">&#13;
<img alt="Kafka topic" src="assets/rsij_1101.png"/>&#13;
<h6><span class="label">Figure 11-1. </span>Kafka topic</h6>&#13;
</div></figure>&#13;
&#13;
<p>A record can hold only four pieces of information:</p>&#13;
<dl>&#13;
<dt>Key</dt>&#13;
<dd>&#13;
<p>Assigned by Kafka when writing a record into the log,&#13;
but can also be used in partitioning, which we cover later</p>&#13;
</dd>&#13;
<dt>Value</dt>&#13;
<dd>&#13;
<p>The actual value, or payload, we want to be stored in the log for retrieval by consumers</p>&#13;
</dd>&#13;
<dt>Timestamp</dt>&#13;
<dd>&#13;
<p>Optionally set when we create the record,&#13;
or set by Kafka when the record is written to the log</p>&#13;
</dd>&#13;
<dt>Headers</dt>&#13;
<dd>&#13;
<p>Optional metadata about the record to provide extra information to Kafka,&#13;
or for downstream consumers to utilize</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p><a data-type="xref" href="#image:log-produce-consume">Figure 11-2</a> outlines the process of interacting with the log.</p>&#13;
&#13;
<figure><div class="figure" id="image:log-produce-consume">&#13;
<img alt="Producing and consuming records" src="assets/rsij_1102.png"/>&#13;
<h6><span class="label">Figure 11-2. </span>Producing and consuming records</h6>&#13;
</div></figure>&#13;
&#13;
<p>With a record created, we write it to the log in Kafka with a producer.&#13;
We can have one or many producer instances writing the same type of record to the log,&#13;
because the way we write records to the log is decoupled from the way they’re consumed.&#13;
Once a record is written to the log,&#13;
we use a consumer to read the record from the log and perform whatever processing is needed with it.</p>&#13;
&#13;
<p>When writing records to the log,&#13;
a producer always appends.&#13;
A producer cannot insert or delete records.&#13;
An append-only approach means Kafka can offer high scalability for writes. Because there is no contention or locking for existing records,&#13;
every write is a new record.</p>&#13;
&#13;
<p>Separation of producers and consumers is a key concept with Kafka.&#13;
This decoupling of time between when records are written to the log and consumed from it&#13;
is key for reactive systems.&#13;
Granted, this is 100% achievable only when the log retention policies are sufficiently long enough&#13;
to prevent any produced records from being removed before they’re consumed!&#13;
We don’t want to log all the records and have them deleted by Kafka before we consume them years later.</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch10.html#messaging">Chapter 10</a>, you saw how to produce messages with <code>@Outgoing</code>.&#13;
Let’s modify that example slightly to also set a key for the record, as shown in <a data-type="xref" href="#data::config-outgoing-meta">Example 11-1</a>.</p>&#13;
<div data-type="example" id="data::config-outgoing-meta">&#13;
<h5><span class="label">Example 11-1. </span>Configure the Kafka outgoing metadata</h5>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="nd">@Outgoing</code><code class="o">(</code><code class="s">"my-channel"</code><code class="o">)</code>&#13;
<code class="n">Multi</code><code class="o">&lt;</code><code class="n">Message</code><code class="o">&lt;</code><code class="n">Person</code><code class="o">&gt;&gt;</code> <code class="nf">produceAStreamOfMessagesOfPersons</code><code class="o">()</code> <code class="o">{</code>&#13;
    <code class="k">return</code> <code class="n">Multi</code><code class="o">.</code><code class="na">createFrom</code><code class="o">().</code><code class="na">items</code><code class="o">(</code>&#13;
            <code class="n">Message</code><code class="o">.</code><code class="na">of</code><code class="o">(</code><code class="k">new</code> <code class="n">Person</code><code class="o">(</code><code class="s">"Luke"</code><code class="o">))</code>&#13;
                <code class="o">.</code><code class="na">addMetadata</code><code class="o">(</code><code class="n">OutgoingKafkaRecordMetadata</code><code class="o">.</code><code class="na">builder</code><code class="o">()</code>&#13;
                        <code class="o">.</code><code class="na">withKey</code><code class="o">(</code><code class="s">"light"</code><code class="o">).</code><code class="na">build</code><code class="o">()),</code>&#13;
            <code class="n">Message</code><code class="o">.</code><code class="na">of</code><code class="o">(</code><code class="k">new</code> <code class="n">Person</code><code class="o">(</code><code class="s">"Leia"</code><code class="o">))</code>&#13;
                <code class="o">.</code><code class="na">addMetadata</code><code class="o">(</code><code class="n">OutgoingKafkaRecordMetadata</code><code class="o">.</code><code class="na">builder</code><code class="o">()</code>&#13;
                        <code class="o">.</code><code class="na">withKey</code><code class="o">(</code><code class="s">"light"</code><code class="o">).</code><code class="na">build</code><code class="o">()),</code>&#13;
            <code class="n">Message</code><code class="o">.</code><code class="na">of</code><code class="o">(</code><code class="k">new</code> <code class="n">Person</code><code class="o">(</code><code class="s">"Obiwan"</code><code class="o">))</code>&#13;
                <code class="o">.</code><code class="na">addMetadata</code><code class="o">(</code><code class="n">OutgoingKafkaRecordMetadata</code><code class="o">.</code><code class="na">builder</code><code class="o">()</code>&#13;
                        <code class="o">.</code><code class="na">withKey</code><code class="o">(</code><code class="s">"light"</code><code class="o">).</code><code class="na">build</code><code class="o">()),</code>&#13;
            <code class="n">Message</code><code class="o">.</code><code class="na">of</code><code class="o">(</code><code class="k">new</code> <code class="n">Person</code><code class="o">(</code><code class="s">"Palpatine"</code><code class="o">))</code>&#13;
                <code class="o">.</code><code class="na">addMetadata</code><code class="o">(</code><code class="n">OutgoingKafkaRecordMetadata</code><code class="o">.</code><code class="na">builder</code><code class="o">()</code>&#13;
                        <code class="o">.</code><code class="na">withKey</code><code class="o">(</code><code class="s">"dark"</code><code class="o">).</code><code class="na">build</code><code class="o">())</code>&#13;
    <code class="o">);</code>&#13;
<code class="o">}</code></pre></div>&#13;
&#13;
<p>Here we have a key to indicate whether the person is part of the <em>light</em> or <em>dark</em> side of the force.&#13;
To switch to producing the message to Kafka, we need to make two changes.&#13;
First, modify <em>pom.xml</em> to include the SmallRye Reactive Messaging for Kafka dependency (<a data-type="xref" href="#data:connector-dependency">Example 11-2</a>).</p>&#13;
<div data-type="example" id="data:connector-dependency">&#13;
<h5><span class="label">Example 11-2. </span>Kafka connector dependency (<em>/Users/clement/Documents/book/code-repository/chapter-11/processor/pom.xml</em>)</h5>&#13;
&#13;
<pre data-code-language="xml" data-type="programlisting"><code class="nt">&lt;dependency&gt;</code>&#13;
    <code class="nt">&lt;groupId&gt;</code>io.quarkus<code class="nt">&lt;/groupId&gt;</code>&#13;
    <code class="nt">&lt;artifactId&gt;</code>quarkus-smallrye-reactive-messaging-kafka<code class="nt">&lt;/artifactId&gt;</code>&#13;
<code class="nt">&lt;/dependency&gt;</code></pre></div>&#13;
&#13;
<p>Lastly, configure the dependency, as shown in <a data-type="xref" href="#data::config-dependency">Example 11-3</a>.</p>&#13;
<div data-type="example" id="data::config-dependency">&#13;
<h5><span class="label">Example 11-3. </span>Configure the Kafka connector to write records (<em>chapter-11/processor/src/main/resources/application.properties</em>)</h5>&#13;
&#13;
<pre data-code-language="properties" data-type="programlisting"><code class="na">mp.messaging.outgoing.my-channel.connector</code><code class="o">=</code><code class="s">smallrye-kafka</code>&#13;
<code class="na">mp.messaging.outgoing.my-channel.topic</code><code class="o">=</code><code class="s">starwars</code>&#13;
<code class="na">mp.messaging.outgoing.my-channel.value.serializer</code><code class="o">=</code><code class="s">\</code>&#13;
<code class="s">    org.apache.kafka.common.serialization.StringSerializer</code></pre></div>&#13;
&#13;
<p class="pagebreak-before less_space">The configuration indicates the connector we’re using, <code>smallrye-kafka</code>,&#13;
the name of the topic the channel should be writing to,&#13;
and the serializer for converting the &#13;
<span class="keep-together">payload</span> content.&#13;
If the topic being written to matches the name of the channel,&#13;
we would not need the <code>topic</code> configuration, as the channel name is the default.</p>&#13;
&#13;
<p>On the consuming side,&#13;
we can read the key with <a data-type="xref" href="#data::extract-metadata">Example 11-4</a>.</p>&#13;
<div data-type="example" id="data::extract-metadata">&#13;
<h5><span class="label">Example 11-4. </span>Extract the incoming Kafka metadata</h5>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="nd">@Incoming</code><code class="o">(</code><code class="s">"my-channel"</code><code class="o">)</code>&#13;
<code class="n">CompletionStage</code><code class="o">&lt;</code><code class="n">Void</code><code class="o">&gt;</code> <code class="nf">consume</code><code class="o">(</code><code class="n">Message</code><code class="o">&lt;</code><code class="n">Person</code><code class="o">&gt;</code> <code class="n">person</code><code class="o">)</code> <code class="o">{</code>&#13;
    <code class="n">String</code> <code class="n">msgKey</code> <code class="o">=</code> <code class="o">(</code><code class="n">String</code><code class="o">)</code> <code class="n">person</code>&#13;
            <code class="o">.</code><code class="na">getMetadata</code><code class="o">(</code><code class="n">IncomingKafkaRecordMetadata</code><code class="o">.</code><code class="na">class</code><code class="o">).</code><code class="na">get</code><code class="o">()</code>&#13;
            <code class="o">.</code><code class="na">getKey</code><code class="o">();</code>&#13;
    <code class="c1">// ...</code>&#13;
    <code class="k">return</code> <code class="n">person</code><code class="o">.</code><code class="na">ack</code><code class="o">();</code>&#13;
<code class="o">}</code></pre></div>&#13;
&#13;
<p>We also need similar configuration as outgoing for us to use Kafka; see <a data-type="xref" href="#data::config-poll-records">Example 11-5</a>.</p>&#13;
<div data-type="example" id="data::config-poll-records">&#13;
<h5><span class="label">Example 11-5. </span>Configure the Kafka connector to poll records (<em>chapter-11/processor/src/main/resources/application.properties</em>)</h5>&#13;
&#13;
<pre data-code-language="properties" data-type="programlisting"><code class="na">mp.messaging.incoming.my-channel.connector</code><code class="o">=</code><code class="s">smallrye-kafka</code>&#13;
<code class="na">mp.messaging.incoming.my-channel.topic</code><code class="o">=</code><code class="s">starwars</code>&#13;
<code class="na">mp.messaging.incoming.my-channel.value.deserializer</code><code class="o">=</code><code class="s">\</code>&#13;
<code class="s">    org.apache.kafka.common.serialization.StringDeserializer</code></pre></div>&#13;
&#13;
<p>So far, we’ve been generically referring to write records to and consume records from a log.&#13;
<a data-primary="topic (Kafka)" data-type="indexterm" id="idm45358817928560"/>As you have seen in <a data-type="xref" href="#image:kafka-topic">Figure 11-1</a>, a <em>topic</em> is a log,&#13;
a means of organizing and durably storing records.&#13;
This log is for a specific type of record, or group of records,&#13;
enabling us to customize the behavior specifically to the needs of those records.&#13;
For instance, if the records are of extremely high volume and not meaningful to the application for longer than a week, we could change the retention policy for one topic to retain records for only that period of time, even if they haven’t been consumed.&#13;
We could also have another topic that has records retained for six months or even &#13;
<span class="keep-together">indefinitely</span>.</p>&#13;
&#13;
<p><a data-primary="offset (Kafka)" data-type="indexterm" id="idm45358817942400"/>Also in <a data-type="xref" href="#image:kafka-topic">Figure 11-1</a> you can see each record as a box with a number;&#13;
this represents the <em>offset</em>, or index, of where a record is written in a topic.&#13;
In this instance, six records have already been written, and a producer is about to write the seventh,&#13;
which is offset 6.&#13;
We also see a consumer reading the record at offset 0, the first record in the topic.&#13;
Though the default is for a new consumer to begin reading records from the first offset,&#13;
we could decide to start at any offset we wanted.</p>&#13;
&#13;
<p>Another way to consider a topic is as a virtual address representing an external destination.&#13;
When a producer writes a record to a topic,&#13;
it has no knowledge of when, if, or even where the record will be read by a consumer.&#13;
Use of a virtual address, or topic,&#13;
provides the means of decoupling our reactive system components from one another in space and time.</p>&#13;
&#13;
<p><a data-primary="consumer groups (Kafka)" data-type="indexterm" id="idm45358817938864"/>A consumer can be combined with others to form a <em>consumer group</em>.&#13;
Any consumer created with the same consumer group name, or identifier,&#13;
will be placed in the same consumer group.&#13;
When creating a consumer without setting a consumer group identifier, we end up with a consumer group containing a single consumer by default.</p>&#13;
&#13;
<p>So far, what we’ve described implies a topic with a single log of records.&#13;
<a data-primary="partition (Kafka)" data-type="indexterm" id="idm45358817936896"/>A <em>partition</em> is how we improve the problems associated with a single-log approach.&#13;
Partitions are useful for improving the read and write performance of a topic,&#13;
as we split a single topic into multiple partitions.</p>&#13;
&#13;
<p>Instead of a single partition with a single consumer,&#13;
we could have three partitions with a separate consumer reading records from each of them.&#13;
Looking at this situation unscientifically,&#13;
we could expect there to be three times the amount of throughput with three partitions and three consumers,&#13;
as opposed to a single partition and consumer.</p>&#13;
&#13;
<p>Though we mentioned three consumers for three partitions,&#13;
in <a data-type="xref" href="#image:kafka-topic-partitions">Figure 11-3</a> we have two consumers within a single consumer group.&#13;
One consumer is assigned to read records from two partitions, to ensure that all partitions have consumers.&#13;
We’ve now improved our throughput by partitioning the topic,&#13;
enabling multiple consumers to consume records.</p>&#13;
&#13;
<figure><div class="figure" id="image:kafka-topic-partitions">&#13;
<img alt="Topic partitions" src="assets/rsij_1103.png"/>&#13;
<h6><span class="label">Figure 11-3. </span>Topic partitions</h6>&#13;
</div></figure>&#13;
&#13;
<p>In the situation shown in <a data-type="xref" href="#image:kafka-topic-partitions">Figure 11-3</a>,&#13;
a producer can write a record to the topic,&#13;
leaving the broker to decide which partition the record is actually written to.&#13;
Alternatively, the producer can explicitly define the partition that a record should be written to.&#13;
If records have a key unique to the record’s contents, such as username for a &#13;
<span class="keep-together"><code>Person</code></span> record,&#13;
it can be efficient to determine the appropriate partition with the Kafka key-hashing algorithm.&#13;
It will ensure that all records with an identical key are written to the same partition.&#13;
We need to be careful, though,&#13;
to ensure that any key is reasonably distributed. <a data-primary="hot partition (Kafka)" data-type="indexterm" id="idm45358817902384"/>Otherwise, we risk creating a <em>hot partition</em> (for example, partitioning by country may see trillions of records placed in a USA partition, but only a few thousand records in the Andorra partition).</p>&#13;
&#13;
<p>Right now we have an issue with resiliency because all our partitions are on the same broker instance. In <a data-type="xref" href="#image:kafka-topic-partition-replication">Figure 11-4</a>, we’ve replicated the topic across three partitions.</p>&#13;
&#13;
<figure><div class="figure" id="image:kafka-topic-partition-replication">&#13;
<img alt="Topic partition replication" src="assets/rsij_1104.png"/>&#13;
<h6><span class="label">Figure 11-4. </span>Topic partition replication</h6>&#13;
</div></figure>&#13;
&#13;
<p>To support resiliency&#13;
and ensure that consumers don’t read the same record in a partition from different brokers,&#13;
<a data-primary="leader partition (Kafka)" data-type="indexterm" id="idm45358817896992"/>a <em>leader partition</em> is elected for consumers to read from.&#13;
Partition 0 in the first broker,&#13;
Partition 1 in the second,&#13;
and Partition 2 in the third broker are the leader partitions in this example.</p>&#13;
&#13;
<p>With the design we have in <a data-type="xref" href="#image:kafka-topic-partition-replication">Figure 11-4</a>,&#13;
Kafka ensures that our consumers cannot read a record from a partition before it has been successfully replicated.&#13;
<a data-primary="high watermark offset" data-type="indexterm" id="idm45358817894016"/>It does this by tracking the <em>high watermark offset</em>,&#13;
the offset of the last message successfully replicated across all partitions.&#13;
The broker prevents consumers from reading beyond the high watermark offset,&#13;
stopping unreplicated records from being read.<a data-startref="ix_event-bus-adoc4" data-type="indexterm" id="idm45358817892560"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Point-to-Point Communication" data-type="sect2"><div class="sect2" id="idm45358818282064">&#13;
<h2>Point-to-Point Communication</h2>&#13;
&#13;
<p><a data-primary="Kafka" data-secondary="point-to-point communication model" data-type="indexterm" id="idm45358817890528"/><a data-primary="point-to-point communication" data-secondary="Kafka and" data-type="indexterm" id="idm45358817889488"/>Kafka is not a traditional messaging system.&#13;
It can be confusing to implement the standard delivery patterns with it.</p>&#13;
&#13;
<p>With <em>point-to-point communication</em>, we want the same message to be consumed once,&#13;
by the same consumer or by any other consumer within the same consumer group.&#13;
Note that when facing network failures, you cannot guarantee that records are consumed only once per consumer group.&#13;
You need to be prepared to see duplicated messages.</p>&#13;
&#13;
<p><a data-primary="consumer groups (Kafka)" data-type="indexterm" id="idm45358817886880"/>We use <em>consumer groups</em> to scale a consumer in Kafka to perform identical processing with greater throughput.&#13;
In <a data-type="xref" href="#image:kafka-topic-consumer-groups">Figure 11-5</a>,&#13;
only one consumer within the group is able to read records from a single topic partition,&#13;
conforming to the needs of point-to-point communication.&#13;
Here we see Consumer 2 unable to read records because it’s part of the same consumer group as Consumer 1,&#13;
effectively making Consumer 2 idle,&#13;
as we have only one partition in this situation.</p>&#13;
&#13;
<figure><div class="figure" id="image:kafka-topic-consumer-groups">&#13;
<img alt="Consumer groups" src="assets/rsij_1105.png"/>&#13;
<h6><span class="label">Figure 11-5. </span>Consumer groups</h6>&#13;
</div></figure>&#13;
&#13;
<p>Why can’t we have two consumers in the same group reading from the same partition?&#13;
Kafka tracks the last committed offset per partition for a given consumer group&#13;
and uses the offset for restarting processing.&#13;
However, consumers don’t commit the offset until they’ve completely finished processing a record.&#13;
This creates a window where multiple consumers in the same group could read the same record,&#13;
thus duplicating the processing of a message.</p>&#13;
&#13;
<p>When a new consumer subscribes to a consumer group,&#13;
and Kafka does not know the last committed offset for the partition,&#13;
there are two strategies.&#13;
The strategies are <em>Earliest</em>, where the consumer starts reading events from the first offset of the partition,&#13;
and <em>Latest</em>, which consumes only events received after the consumer subscribed.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Publish/Subscribe" data-type="sect2"><div class="sect2" id="idm45358817879856">&#13;
<h2>Publish/Subscribe</h2>&#13;
&#13;
<p><a data-primary="Kafka" data-secondary="publish/subscribe model" data-type="indexterm" id="idm45358817878656"/><a data-primary="publish/subscribe model" data-secondary="Kafka and" data-type="indexterm" id="idm45358817877680"/>Point-to-point ensures that messages are consumed once.&#13;
Another popular pattern dispatches a message to multiple consumers.&#13;
With a <em>publish/subscribe</em> model,&#13;
we can have many subscribers, or consumers,&#13;
reading the same message, usually for different purposes.</p>&#13;
&#13;
<p><a data-type="xref" href="#image:kafka-topic-multiple-consumer-groups">Figure 11-6</a> has two consumer groups consuming messages from the same topic.&#13;
One consumer group has three consumers, while the other has two consumers.&#13;
We see each partition being read by only a single consumer from the same consumer group,&#13;
but multiple consumers across consumer groups.&#13;
Though the two consumer groups are connected to the same topic&#13;
and its partitions,&#13;
there is no requirement for each consumer to be at the same offset.&#13;
Such a requirement would remove the benefits of being able to consume records with different groups.</p>&#13;
&#13;
<figure><div class="figure" id="image:kafka-topic-multiple-consumer-groups">&#13;
<img alt="Multiple consumer groups" src="assets/rsij_1106.png"/>&#13;
<h6><span class="label">Figure 11-6. </span>Multiple consumer groups</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Elasticity Patterns" data-type="sect2"><div class="sect2" id="idm45358817872128">&#13;
<h2>Elasticity Patterns</h2>&#13;
&#13;
<p><a data-primary="elasticity" data-secondary="Kafka partition mechanism and" data-type="indexterm" id="idm45358817870928"/><a data-primary="Kafka" data-secondary="elasticity patterns" data-type="indexterm" id="idm45358817869888"/><em>Elasticity</em> is one of the pillars of reactive systems.&#13;
The partition mechanism offered by Kafka lets us implement elasticity patterns.&#13;
<a data-type="xref" href="#image:kafka-topic-multiple-consumer-groups">Figure 11-6</a> also highlights the elasticity patterns of consumer groups in Kafka.&#13;
Consumer group 1 has three consumers,&#13;
each consuming from a different partition.&#13;
If a consumer fails for any reason,&#13;
another consumer takes up the load of reading from the partition that is now without a consumer.&#13;
Consumer elasticity ensures that all partitions are being consumed as long as at least one consumer is present.&#13;
Granted, such an occurrence does reduce the throughput,&#13;
but it is preferable over no records being consumed.&#13;
Consumer group 2 could represent such an occurrence.</p>&#13;
&#13;
<p>Consumer group elasticity is limited, though.&#13;
As we mentioned earlier,&#13;
it is not possible for multiple consumers within the same group to read from the same partition.&#13;
In <a data-type="xref" href="#image:kafka-topic-multiple-consumer-groups">Figure 11-6</a>, with three partitions&#13;
we’re limited to three consumers within a single consumer group.&#13;
Any additional consumers in the same group would be idle,&#13;
as we cannot have multiple consumers in the same group connected to the same partition.</p>&#13;
&#13;
<p>Elasticity is a key factor to consider when determining the number of partitions we want for a topic.&#13;
With too few, we limit the throughput for processing records,&#13;
while too many can lead to idle consumers if the records are not distributed across the partitions&#13;
sufficiently evenly.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dealing with Failures" data-type="sect2"><div class="sect2" id="idm45358817864336">&#13;
<h2>Dealing with Failures</h2>&#13;
&#13;
<p><a data-primary="failure handling" data-secondary="Kafka and" data-type="indexterm" id="ix_event-bus-adoc5"/><a data-primary="Kafka" data-secondary="failure handling" data-type="indexterm" id="ix_event-bus-adoc6"/>Failures happen!&#13;
It’s the nature of distributed systems,&#13;
and not one we can avoid even when developing reactive systems.&#13;
However, Kafka provides us with mechanisms for appropriately dealing with failure.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Commit strategies" data-type="sect3"><div class="sect3" id="idm45358817860016">&#13;
<h3>Commit strategies</h3>&#13;
&#13;
<p><a data-primary="commit strategies" data-type="indexterm" id="idm45358817858672"/><a data-primary="failure handling" data-secondary="commit strategies" data-type="indexterm" id="idm45358817857968"/><a data-primary="offset commit" data-type="indexterm" id="idm45358817857024"/>Each consumer periodically informs the broker of its latest <em>offset commit</em>.&#13;
The number represents the last message that was successfully processed from a topic partition by the consumer.&#13;
The offset commit then becomes the starting point for the new consumer of the partition when the current&#13;
consumer fails or crashes.</p>&#13;
&#13;
<p>Committing an offset is not a cheap operation.&#13;
For performance reasons, we recommend not committing the offset after every record processed.&#13;
Quarkus provides a few options for commit strategies to use with Kafka:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Throttled</p>&#13;
</li>&#13;
<li>&#13;
<p>Ignore</p>&#13;
</li>&#13;
<li>&#13;
<p>Latest</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p><a data-primary="Throttled commit strategy" data-type="indexterm" id="idm45358817851520"/>The <em>Throttled</em> strategy, the default option, tracks the received records for a consumer and monitors their acknowledgment.&#13;
When all records before a position are successfully processed,&#13;
that position is committed to the broker as the new offset for that consumer group.&#13;
If any record is neither acked nor nacked,&#13;
it’s no longer possible to commit a new offset position, and records will be continually enqueued.&#13;
Without the ability to bail out, it would lead to out-of-memory errors eventually.&#13;
The Throttled strategy can detect this problem by reporting a failure to the connector, enabling the application to be marked as unhealthy.&#13;
Note that this situation is often an application bug causing a message to be “forgotten.”</p>&#13;
&#13;
<p><a data-primary="Ignore commit strategy" data-type="indexterm" id="idm45358817849232"/>The <em>Ignore</em> strategy utilizes the default offset commit of the Kafka consumer, which occurs periodically when polling for new records.&#13;
This strategy ignores message acknowledgment and relies on record processing to be synchronous.&#13;
This strategy is the default when <code>enabled.auto.commit=true</code> is used.&#13;
Any asynchronous processing that fails will be unknown to the process that is polling for new records to consume.</p>&#13;
&#13;
<p>If we’ve set <code>commit-strategy</code> to <code>ignore</code> and <code>enable.auto.commit</code> to <code>false</code>, as shown in <a data-type="xref" href="#ebus::config-commit-strat">Example 11-6</a>,&#13;
no offset is ever committed.&#13;
Every time a new consumer starts reading messages from a topic,&#13;
it will always start from offset 0.&#13;
In some situations, this approach is desired,&#13;
but it needs to be a conscious choice.</p>&#13;
<div data-type="example" id="ebus::config-commit-strat">&#13;
<h5><span class="label">Example 11-6. </span>Configure the commit strategy</h5>&#13;
&#13;
<pre data-code-language="properties" data-type="programlisting"><code class="na">mp.messaging.incoming.my-channel.connector</code><code class="o">=</code><code class="s">smallrye-kafka</code>&#13;
<code class="na">mp.messaging.incoming.my-channel.enable.auto.commit</code><code class="o">=</code><code class="s">false</code>&#13;
<code class="na">mp.messaging.incoming.my-channel.commit-strategy</code><code class="o">=</code><code class="s">ignore</code></pre></div>&#13;
&#13;
<p><a data-primary="Latest commit strategy" data-type="indexterm" id="idm45358817840800"/><em>Latest</em> will commit the offset after every message is acknowledged,&#13;
which as we described earlier will impact performance of the consumer.&#13;
In lower-throughput scenarios, this strategy may be preferable to have a higher confidence that the offset is accurate.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Acknowledgment strategies" data-type="sect3"><div class="sect3" id="idm45358817839408">&#13;
<h3>Acknowledgment strategies</h3>&#13;
&#13;
<p><a data-primary="acknowledgment strategies" data-type="indexterm" id="idm45358817834528"/><a data-primary="failure handling" data-secondary="acknowledgment strategies" data-type="indexterm" id="idm45358817833856"/>In <a data-type="xref" href="ch10.html#acknowledgements_aspect">“Acknowledgments”</a>,&#13;
you learned how Reactive Messaging utilizes <code>ack</code> and <code>nack</code> to inform the upstream reactive streams of the record-processing status.&#13;
These acknowledgment methods are part of the failure-handling strategies we have available for Kafka.&#13;
The application configures the Kafka connector with one of these strategies.</p>&#13;
&#13;
<p><a data-primary="Fail Fast acknowledgment strategy" data-type="indexterm" id="idm45358817830512"/>The simplest, and default, strategy is <em>Fail Fast</em>.&#13;
When an application rejects a message, the connector is notified of the failure, and the application is stopped.&#13;
If the failure is transient in origin, such as network issues,&#13;
restarting the application should allow processing to continue without an issue.&#13;
However, if a particular record causes a consumer failure,&#13;
the application will be in a perpetual loop of failure → stop → restart,&#13;
as it will be continually trying to process the record causing a failure.</p>&#13;
&#13;
<p><a data-primary="Ignore acknowledgment strategy" data-type="indexterm" id="idm45358817828512"/>Another simple strategy is <em>Ignore</em>.&#13;
Any nacked message is logged and then ignored as the consumer continues processing new records.&#13;
The Ignore strategy is beneficial when our application handles any failure internally,&#13;
and we thus don’t need to inform the message producer of a failure,&#13;
or when an ignored message occasionally is acceptable because of the type of messages being processed.&#13;
If, on the other hand, large numbers of messages are being ignored,&#13;
it is worth investigating the root cause as it’s likely not an intended consequence.</p>&#13;
&#13;
<p>The last strategy for failure handling is <em>Dead-Letter Queue</em>.&#13;
It sends the failing records to a specific topic to be handled later either automatically or manually.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dead-letter queue" data-type="sect3"><div class="sect3" id="idm45358817825552">&#13;
<h3>Dead-letter queue</h3>&#13;
&#13;
<p><a data-primary="Dead-Letter Queue strategy" data-type="indexterm" id="idm45358817824112"/><a data-primary="failure handling" data-secondary="dead-letter queue" data-type="indexterm" id="idm45358817823392"/>This strategy has been a part of messaging systems for as long as messaging systems have existed!&#13;
Instead of failing straight away, or ignoring any failures,&#13;
this strategy stores the messages that fail to a separate destination, or topic.&#13;
Storing the failed messages enables an administration process,&#13;
human or automated, to determine the correct cause of action to resolve the failed handling.</p>&#13;
&#13;
<p>It’s important to note that the use of the Dead-Letter Queue strategy will work only when ordering of all messages is unnecessary,&#13;
as we don’t stop processing new messages waiting for a message failure to be resolved off the dead-letter queue (DLQ).</p>&#13;
&#13;
<p>When choosing this strategy, the default topic is named <code>dead-letter-topic-<em>[topic-name]</em></code>.&#13;
For our previous examples, it would be <code>dead-letter-topic-my-channel</code>.&#13;
It is possible to configure the topic name as shown in <a data-type="xref" href="#ebus::config-fail-strat-dlq">Example 11-7</a>.</p>&#13;
<div data-type="example" id="ebus::config-fail-strat-dlq">&#13;
<h5><span class="label">Example 11-7. </span>Configure the failure strategy to use a DLQ</h5>&#13;
&#13;
<pre data-code-language="properties" data-type="programlisting"><code class="na">mp.messaging.incoming.my-channel.failure-strategy</code><code class="o">=</code><code class="s">dead-letter-queue</code>&#13;
<code class="na">mp.messaging.incoming.my-channel.dead-letter-queue.topic</code><code class="o">=</code><code class="s">my-dlq</code></pre></div>&#13;
&#13;
<p>We can even retrieve the failure reason associated with the message from the <code>dead-letter-reason</code> header (<a data-type="xref" href="#ebus::retr-fail-reason">Example 11-8</a>).</p>&#13;
<div data-type="example" id="ebus::retr-fail-reason">&#13;
<h5><span class="label">Example 11-8. </span>Retrieve the failure reason</h5>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="nd">@Incoming</code><code class="o">(</code><code class="s">"my-dlq"</code><code class="o">)</code>&#13;
<code class="kd">public</code> <code class="n">CompletionStage</code><code class="o">&lt;</code><code class="n">Void</code><code class="o">&gt;</code> <code class="nf">dlq</code><code class="o">(</code><code class="n">Message</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">rejected</code><code class="o">)</code> <code class="o">{</code>&#13;
  <code class="n">IncomingKafkaRecordMetadata</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code> <code class="n">String</code><code class="o">&gt;</code> <code class="n">metadata</code> <code class="o">=</code>&#13;
      <code class="n">rejected</code><code class="o">.</code><code class="na">getMetadata</code><code class="o">(</code><code class="n">IncomingKafkaRecordMetadata</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>&#13;
  <code class="n">String</code> <code class="n">reason</code> <code class="o">=</code> <code class="k">new</code> <code class="n">String</code><code class="o">(</code><code class="n">metadata</code><code class="o">.</code><code class="na">getHeaders</code><code class="o">()</code>&#13;
    <code class="o">.</code><code class="na">lastHeader</code><code class="o">(</code><code class="s">"dead-letter-reason"</code><code class="o">).</code><code class="na">value</code><code class="o">());</code>&#13;
<code class="o">}</code></pre></div>&#13;
&#13;
<p>Don’t forget that using a DLQ requires having another application or a human operator to process the records sent to the DLQ.&#13;
The records may be reintroduced in the initial topic (but the order is lost) or dropped, or a mitigation logic would need to happen.<a data-startref="ix_event-bus-adoc6" data-type="indexterm" id="idm45358817784688"/><a data-startref="ix_event-bus-adoc5" data-type="indexterm" id="idm45358817678096"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Backpressure and Performance Considerations" data-type="sect2"><div class="sect2" id="idm45358817677360">&#13;
<h2>Backpressure and Performance Considerations</h2>&#13;
&#13;
<p><a data-primary="Kafka" data-secondary="backpressure/performance considerations" data-type="indexterm" id="idm45358817676016"/>There is no way to have a truly reactive system without appropriate backpressure to avoid overloading components.&#13;
So how do we handle backpressure for Kafka?</p>&#13;
&#13;
<p>The outbound connector for Kafka, used with <code>@Outgoing</code> or <code>Emitter</code>,&#13;
uses the number of in-flight messages waiting for acknowledgment from the broker.&#13;
<a data-primary="in-flight messages" data-type="indexterm" id="idm45358817673456"/><em>In-flight messages</em> are those the connector has sent to a Kafka broker for writing to a topic,&#13;
but for which the connector has not received acknowledgment that the record was successfully stored.</p>&#13;
&#13;
<p>We tweak the number of in-flight messages to adjust the backpressure of the outbound Kafka connector.&#13;
The default number of in-flight messages is 1,024.&#13;
Too high a number can lead to higher memory use, potentially out-of-memory errors depending on the payload size,&#13;
while too few causes a reduction in throughput.&#13;
We can customize the number of in-flight messages in the connector with the property <code>max-inflight-messages</code>.</p>&#13;
&#13;
<p>On the side of the consumer,&#13;
Kafka will pause the consumer and then resume it,&#13;
according to the Reactive Streams requests. We’ve talked a lot about Kafka, so in the next section we explore it in Kubernetes!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Kafka on Kubernetes" data-type="sect2"><div class="sect2" id="bus::install-kafka">&#13;
<h2>Kafka on Kubernetes</h2>&#13;
&#13;
<p><a data-primary="Kafka" data-secondary="on Kubernetes" data-secondary-sortas="Kubernetes" data-type="indexterm" id="ix_event-bus-adoc7"/><a data-primary="Kubernetes" data-secondary="Kafka on" data-type="indexterm" id="ix_event-bus-adoc8"/>To <a data-primary="Kafka" data-secondary="installing" data-type="indexterm" id="idm45358817665088"/><a data-primary="Strimzi project" data-type="indexterm" id="idm45358817664112"/>use Kafka on Kubernetes, we need Kafka installed.&#13;
We will use the <a href="https://strimzi.io">Strimzi</a> project for installing Kafka.&#13;
This project has an operator for managing Kafka deployments in Kubernetes.</p>&#13;
&#13;
<p>Before setting up Kafka in Kubernetes, we need a Kubernetes environment.&#13;
If you already have one, great! If you don’t, we recommend you use minikube, as covered in <a data-type="xref" href="ch03.html#distributed-system::cloud-native-kube">“The New Kids on the Block: Cloud Native and Kubernetes Native Applications”</a>.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Running Kafka in minikube can require more memory than usual deployments,&#13;
so we recommend starting it with at least 4 GB of RAM:</p>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">minikube start --memory<code class="o">=</code>4096</pre>&#13;
</div>&#13;
&#13;
<p>With a Kubernetes environment running,&#13;
we need to install Strimzi, as shown in <a data-type="xref" href="#ebus::install-strimzi">Example 11-9</a>.&#13;
Be sure to have <a href="https://helm.sh">Helm</a> installed,&#13;
as we will use it to install Strimzi.</p>&#13;
<div data-type="example" id="ebus::install-strimzi">&#13;
<h5><span class="label">Example 11-9. </span>Install Strimzi</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code>kubectl</code><code> </code><code>create</code><code> </code><code>ns</code><code> </code><code>strimzi</code><code>             </code><a class="co" href="#callout_the_event_bus__the_backbone_CO1-1" id="co_the_event_bus__the_backbone_CO1-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>kubectl</code><code> </code><code>create</code><code> </code><code>ns</code><code> </code><code>kafka</code><code>               </code><a class="co" href="#callout_the_event_bus__the_backbone_CO1-2" id="co_the_event_bus__the_backbone_CO1-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
&#13;
</code><code>helm</code><code> </code><code>repo</code><code> </code><code>add</code><code> </code><code>strimzi</code><code> </code><code>https://strimzi.io/charts</code><code>            </code><a class="co" href="#callout_the_event_bus__the_backbone_CO1-3" id="co_the_event_bus__the_backbone_CO1-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
</code><code>helm</code><code> </code><code>install</code><code> </code><code>strimzi</code><code> </code><code>strimzi/strimzi-kafka-operator</code><code> </code><code>-n</code><code> </code><code>strimzi</code><code> </code><code class="se">\&#13;
</code><code>    </code><code>--set</code><code> </code><code class="nv">watchNamespaces</code><code class="o">=</code><code class="o">{</code><code>kafka</code><code class="o">}</code><code> </code><code>--wait</code><code> </code><code>--timeout</code><code> </code><code>300s</code><code>      </code><a class="co" href="#callout_the_event_bus__the_backbone_CO1-4" id="co_the_event_bus__the_backbone_CO1-4"><img alt="4" src="assets/4.png"/></a></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_the_event_bus__the_backbone_CO1-1" id="callout_the_event_bus__the_backbone_CO1-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Create a <code>strimzi</code> namespace for the Kubernetes operator.</p></dd>&#13;
<dt><a class="co" href="#co_the_event_bus__the_backbone_CO1-2" id="callout_the_event_bus__the_backbone_CO1-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Namespace for the Kafka cluster.</p></dd>&#13;
<dt><a class="co" href="#co_the_event_bus__the_backbone_CO1-3" id="callout_the_event_bus__the_backbone_CO1-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>Add the Strimzi chart repository to Helm.</p></dd>&#13;
<dt><a class="co" href="#co_the_event_bus__the_backbone_CO1-4" id="callout_the_event_bus__the_backbone_CO1-4"><img alt="4" src="assets/4.png"/></a></dt>&#13;
<dd><p>Install the Strimzi operator into the <code>strimzi</code> namespace.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Once the installation has succeeded,&#13;
verify that the operator is running (as illustrated in <a data-type="xref" href="#ebus::strimzi-op-status">Example 11-10</a>).</p>&#13;
<div data-type="example" id="ebus::strimzi-op-status">&#13;
<h5><span class="label">Example 11-10. </span>Strimzi operator status</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">kubectl get pods -n strimzi&#13;
NAME                                        READY   STATUS    RESTARTS   AGE&#13;
strimzi-cluster-operator-58fcdbfc8f-mjdxg   1/1     Running   <code class="m">0</code>          46s</pre></div>&#13;
&#13;
<p>Now it’s time to create the Kafka cluster!&#13;
First we need to define the cluster we want to create, as shown in <a data-type="xref" href="#bus::kafka-cluster">Example 11-11</a>.</p>&#13;
<div data-type="example" id="bus::kafka-cluster">&#13;
<h5><span class="label">Example 11-11. </span>Kafka cluster definition</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting"><code>apiVersion:</code><code> </code><code>kafka.strimzi.io/v1beta2</code><code>&#13;
</code><code>kind:</code><code> </code><code>Kafka</code><code>&#13;
</code><code>metadata:</code><code>&#13;
  </code><code>name:</code><code> </code><code>my-cluster</code><code>                      </code><a class="co" href="#callout_the_event_bus__the_backbone_CO2-1" id="co_the_event_bus__the_backbone_CO2-1"><img alt="1" src="assets/1.png"/></a><code>&#13;
</code><code>spec:</code><code>&#13;
  </code><code>kafka:</code><code>&#13;
    </code><code>replicas:</code><code> </code><code class="m">1</code><code>                         </code><a class="co" href="#callout_the_event_bus__the_backbone_CO2-2" id="co_the_event_bus__the_backbone_CO2-2"><img alt="2" src="assets/2.png"/></a><code>&#13;
    </code><code>listeners:</code><code>&#13;
      </code><code>-</code><code> </code><code>name:</code><code> </code><code>plain</code><code>&#13;
        </code><code>port:</code><code> </code><code>9092</code><code>&#13;
        </code><code class="nb">type</code><code>:</code><code> </code><code>internal</code><code>&#13;
        </code><code>tls:</code><code> </code><code class="nb">false</code><code>&#13;
      </code><code>-</code><code> </code><code>name:</code><code> </code><code>tls</code><code>&#13;
        </code><code>port:</code><code> </code><code>9093</code><code>&#13;
        </code><code class="nb">type</code><code>:</code><code> </code><code>internal</code><code>&#13;
        </code><code>tls:</code><code> </code><code class="nb">true&#13;
    </code><code>config:</code><code>&#13;
      </code><code>offsets.topic.replication.factor:</code><code> </code><code>1</code><code>&#13;
      </code><code>transaction.state.log.replication.factor:</code><code> </code><code>1</code><code>&#13;
      </code><code>transaction.state.log.min.isr:</code><code> </code><code>1</code><code>&#13;
    </code><code>storage:</code><code>&#13;
      </code><code class="nb">type</code><code>:</code><code> </code><code>ephemeral</code><code>                  </code><a class="co" href="#callout_the_event_bus__the_backbone_CO2-3" id="co_the_event_bus__the_backbone_CO2-3"><img alt="3" src="assets/3.png"/></a><code>&#13;
  </code><code>zookeeper:</code><code>&#13;
    </code><code>replicas:</code><code> </code><code>1</code><code>&#13;
    </code><code>storage:</code><code>&#13;
      </code><code class="nb">type</code><code>:</code><code> </code><code>ephemeral</code><code>&#13;
  </code><code>entityOperator:</code><code>&#13;
    </code><code>topicOperator:</code><code> </code><code class="o">{</code><code class="o">}</code><code>&#13;
    </code><code>userOperator:</code><code> </code><code class="o">{</code><code class="o">}</code></pre></div>&#13;
<dl class="calloutlist">&#13;
<dt><a class="co" href="#co_the_event_bus__the_backbone_CO2-1" id="callout_the_event_bus__the_backbone_CO2-1"><img alt="1" src="assets/1.png"/></a></dt>&#13;
<dd><p>Name of the cluster.</p></dd>&#13;
<dt><a class="co" href="#co_the_event_bus__the_backbone_CO2-2" id="callout_the_event_bus__the_backbone_CO2-2"><img alt="2" src="assets/2.png"/></a></dt>&#13;
<dd><p>Number of Kafka replicas to create in the cluster.&#13;
In production, we would want more than one, but for testing this reduces the memory requirements.</p></dd>&#13;
<dt><a class="co" href="#co_the_event_bus__the_backbone_CO2-3" id="callout_the_event_bus__the_backbone_CO2-3"><img alt="3" src="assets/3.png"/></a></dt>&#13;
<dd><p>We choose ephemeral storage, again to reduce the requirements from a testing perspective.</p></dd>&#13;
</dl>&#13;
&#13;
<p>Now we use <a data-type="xref" href="#bus::kafka-cluster">Example 11-11</a> to create a Kafka cluster matching the requested definition, as shown in <a data-type="xref" href="#ebus::create-kafka-clust">Example 11-12</a>.</p>&#13;
<div data-type="example" id="ebus::create-kafka-clust">&#13;
<h5><span class="label">Example 11-12. </span>Create a Kafka cluster</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">kubectl apply -f deploy/kafka/kafka-cluster.yaml -n kafka</pre></div>&#13;
&#13;
<p>Verify that the cluster we wanted was created (<a data-type="xref" href="#ebus:kafka-cluster-stat">Example 11-13</a>).</p>&#13;
<div data-type="example" id="ebus:kafka-cluster-stat">&#13;
<h5><span class="label">Example 11-13. </span>Kafka cluster status</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">kubectl get pods -n kafka&#13;
NAME                                          READY   STATUS    RESTARTS   AGE&#13;
my-cluster-entity-operator-765f64f4fd-2t8mk   3/3     Running   <code class="m">0</code>          90s&#13;
my-cluster-kafka-0                            1/1     Running   <code class="m">0</code>          113s&#13;
my-cluster-zookeeper-0                        1/1     Running   <code class="m">0</code>          2m12s</pre></div>&#13;
&#13;
<p>With the cluster running,&#13;
we create the Kafka topics we need (<a data-type="xref" href="#ebus::create-kafka-topics">Example 11-14</a>).</p>&#13;
<div data-type="example" id="ebus::create-kafka-topics">&#13;
<h5><span class="label">Example 11-14. </span>Create Kafka topics</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">kubectl apply -f deploy/kafka/ticks.yaml&#13;
kubectl apply -f deploy/kafka/processed.yaml</pre></div>&#13;
&#13;
<p>To show you how Kafka works with Kubernetes, we will use an example that consists of three services:&#13;
produce a <em>tick</em> every two seconds,&#13;
receive the message and add details of the consumer processing it,&#13;
and expose all messages via SSE.&#13;
These three services will be used to showcase consumer handling with Kafka.&#13;
Follow the instructions in <em>/chapter-11/README.md</em> under <em>Application deployment</em> for&#13;
building the required Docker images and installing the services with Helm.</p>&#13;
&#13;
<p>Once the services are running, it’s time to test it! Open the SSE endpoint in a browser, and you will see data similar to <a data-type="xref" href="#ebus::sse-output-1">Example 11-15</a>.</p>&#13;
<div data-type="example" id="ebus::sse-output-1">&#13;
<h5><span class="label">Example 11-15. </span>SSE output: all the messages are consumed by the same pod</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">data:1 consumed in pod <code class="o">(</code>processor-d44564db5-48n97<code class="o">)</code>&#13;
data:2 consumed in pod <code class="o">(</code>processor-d44564db5-48n97<code class="o">)</code>&#13;
data:3 consumed in pod <code class="o">(</code>processor-d44564db5-48n97<code class="o">)</code>&#13;
data:4 consumed in pod <code class="o">(</code>processor-d44564db5-48n97<code class="o">)</code>&#13;
data:5 consumed in pod <code class="o">(</code>processor-d44564db5-48n97<code class="o">)</code></pre></div>&#13;
&#13;
<p>We can see all the ticks consumed by a single consumer,&#13;
even though we have three partitions for our topic.&#13;
Let’s scale up <code>processor</code> to add more consumers to the same group (<a data-type="xref" href="#ebus:inc-app-instances">Example 11-16</a>).</p>&#13;
<div data-type="example" id="ebus:inc-app-instances">&#13;
<h5><span class="label">Example 11-16. </span>Increase the number of application instances</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">kubectl scale deployment/processor -n event-bus --replicas<code class="o">=</code>3</pre></div>&#13;
&#13;
<p>In the browser, we now see the messages processed by three consumers of the same group,&#13;
increasing throughput and concurrency (<a data-type="xref" href="#ebus::sse-output-2">Example 11-17</a>).</p>&#13;
<div data-type="example" id="ebus::sse-output-2">&#13;
<h5><span class="label">Example 11-17. </span>SSE output: the messages are consumed by the three pods</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">data:11 consumed in pod <code class="o">(</code>processor-d44564db5-2cklg<code class="o">)</code>&#13;
data:12 consumed in pod <code class="o">(</code>processor-d44564db5-48n97<code class="o">)</code>&#13;
data:13 consumed in pod <code class="o">(</code>processor-d44564db5-s6rx9<code class="o">)</code>&#13;
data:14 consumed in pod <code class="o">(</code>processor-d44564db5-2cklg<code class="o">)</code>&#13;
data:15 consumed in pod <code class="o">(</code>processor-d44564db5-s6rx9<code class="o">)</code>&#13;
data:16 consumed in pod <code class="o">(</code>processor-d44564db5-48n97<code class="o">)</code>&#13;
data:17 consumed in pod <code class="o">(</code>processor-d44564db5-2cklg<code class="o">)</code></pre></div>&#13;
&#13;
<p>If we started another <code>processor</code> instance but without <code>mp.messaging.incoming.ticks.group. id=tick-consumer</code> set,&#13;
we would see the duplication of message numbers from the new consumer, as they have their own consumer group and offset position<a data-startref="ix_event-bus-adoc8" data-type="indexterm" id="idm45358817274464"/><a data-startref="ix_event-bus-adoc7" data-type="indexterm" id="idm45358817273824"/>.<a data-startref="ix_event-bus-adoc3" data-type="indexterm" id="idm45358817273024"/><a data-startref="ix_event-bus-adoc2" data-type="indexterm" id="idm45358817272320"/><a data-startref="ix_event-bus-adoc1" data-type="indexterm" id="idm45358817271648"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Building Reactive Systems with AMQP" data-type="sect1"><div class="sect1" id="idm45358818289968">&#13;
<h1>Building Reactive Systems with AMQP</h1>&#13;
&#13;
<p><a data-primary="AMQP (Advanced Message Queuing Protocol)" data-secondary="building reactive systems with" data-type="indexterm" id="ix_event-bus-adoc9"/><a data-primary="event bus" data-secondary="building reactive systems with AMQP" data-type="indexterm" id="ix_event-bus-adoc10"/><a data-primary="reactive systems" data-secondary="building with AMQP" data-type="indexterm" id="ix_event-bus-adoc11"/><em>Advanced Message Queuing Protocol</em>, or <em>AMQP</em>,&#13;
is an application layer protocol for message-oriented middleware that has been around since 2002.&#13;
The AMQP Broker is a highly advanced message broker with a tremendous amount of flexibility&#13;
and customization dependent on application requirements.</p>&#13;
&#13;
<p>We don’t cover all the possible uses of the AMQP Broker here.&#13;
With a huge array of possible broker topologies to support many varied use cases,&#13;
there is simply too much information to even attempt to squeeze it all into this section!&#13;
<a href="https://oreil.ly/xC0ar">Robert Godfrey on InfoQ</a> presents the AMQP 1.0 core features and introduces some possibilities.</p>&#13;
&#13;
<p>Unlike Kafka, all the <em>smarts</em> are inside the AMQP Broker, which knows about topologies,&#13;
clients, message statuses, what is delivered, and what is yet to be delivered.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="AMQP 1.0" data-type="sect2"><div class="sect2" id="idm45358817244992">&#13;
<h2>AMQP 1.0</h2>&#13;
&#13;
<p><a data-primary="AMQP (Advanced Message Queuing Protocol)" data-secondary="AMQP 1.0" data-type="indexterm" id="idm45358817243824"/><em>AMQP 1.0</em> is an open standard for passing business messages among applications or organizations.&#13;
It consists of several layers, the lowest of which is a binary wire-level protocol for transferring&#13;
a message between two processes.&#13;
On top of the wire-level protocol is the messaging layer, which defines an abstract message format and encoding.&#13;
The wire-level protocol is what enables many clients of different types to be able to send and receive messages with the AMQP Broker,&#13;
as long as they support the same 1.0 version of the AMQP specification.</p>&#13;
&#13;
<p>Utilizing the AMQP 1.0 connector in Quarkus requires the dependency in <a data-type="xref" href="#bus::amqp-dep">Example 11-18</a>.</p>&#13;
<div data-type="example" id="bus::amqp-dep">&#13;
<h5><span class="label">Example 11-18. </span>Dependency for the AMQP connector</h5>&#13;
&#13;
<pre data-code-language="xml" data-type="programlisting"><code class="nt">&lt;dependency&gt;</code>&#13;
    <code class="nt">&lt;groupId&gt;</code>io.quarkus<code class="nt">&lt;/groupId&gt;</code>&#13;
    <code class="nt">&lt;artifactId&gt;</code>quarkus-smallrye-reactive-messaging-amqp<code class="nt">&lt;/artifactId&gt;</code>&#13;
<code class="nt">&lt;/dependency&gt;</code></pre></div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Point-to-Point Communication" data-type="sect2"><div class="sect2" id="amqp-ptp">&#13;
<h2>Point-to-Point Communication</h2>&#13;
&#13;
<p><a data-primary="AMQP (Advanced Message Queuing Protocol)" data-secondary="point-to-point communication" data-type="indexterm" id="ix_event-bus-adoc12"/><a data-primary="point-to-point communication" data-secondary="AMQP and" data-type="indexterm" id="ix_event-bus-adoc13"/>With AMQP, point-to-point communication is achieved with a <em>queue</em> and not a <em>topic</em>.&#13;
<a data-primary="anycast (queue)" data-type="indexterm" id="idm45358817229216"/>In AMQP-speak a queue is referred to as <em>anycast</em>,&#13;
meaning any consumer can read the message, but only one of them (<a data-type="xref" href="#image:amqp-queue">Figure 11-7</a>).&#13;
Messages we add to a queue can be durable, as with Kafka,&#13;
but they can also be nondurable.&#13;
When a message is nondurable, it will be lost if the broker restarts before the message is consumed.</p>&#13;
&#13;
<figure><div class="figure" id="image:amqp-queue">&#13;
<img alt="AMQP queue consumers" src="assets/rsij_1107.png"/>&#13;
<h6><span class="label">Figure 11-7. </span>AMQP queue consumers</h6>&#13;
</div></figure>&#13;
&#13;
<p>A key difference between Kafka and AMQP for point-to-point is that once a message is read by a consumer in AMQP,&#13;
the message is removed from the queue and not retained in any way.&#13;
AMQP temporarily stores messages until they’ve been consumed,&#13;
whereas Kafka retains all messages in the log, at least until the log-retention policy begins removing older ones.&#13;
This makes AMQP unsuitable for use cases that could require a replay of messages within the reactive system at some point.</p>&#13;
&#13;
<p>We can also have many consumers reading messages from the same queue,&#13;
but the broker ensures that only one of them ever reads a single message.&#13;
AMQP does not have the same throughput restrictions as Kafka with respect to scaling consumers.&#13;
We can have dozens of consumers reading from a single queue with AMQP, provided order is not important.</p>&#13;
&#13;
<p>Let’s send a message to AMQP!&#13;
After adding the dependency we mentioned earlier (<a data-type="xref" href="#bus::amqp-dep">Example 11-18</a>),&#13;
we need to configure the broker properties as shown in <a data-type="xref" href="#ebus::config-amqp-broker">Example 11-19</a>, so the connector knows the location of the AMQP Broker.</p>&#13;
<div data-type="example" id="ebus::config-amqp-broker">&#13;
<h5><span class="label">Example 11-19. </span>Configure the AMQP Broker location and credentials</h5>&#13;
&#13;
<pre data-code-language="properties" data-type="programlisting"><code class="na">amqp-host</code><code class="o">=</code><code class="s">amqp</code>&#13;
<code class="na">amqp-port</code><code class="o">=</code><code class="s">5672</code>&#13;
<code class="na">amqp-username</code><code class="o">=</code><code class="s">username</code>&#13;
<code class="na">amqp-password</code><code class="o">=</code><code class="s">password</code>&#13;
&#13;
<code class="na">mp.messaging.outgoing.data.connector</code><code class="o">=</code><code class="s">smallrye-amqp</code></pre></div>&#13;
&#13;
<p>We’ve set the AMQP Broker configuration for host, port, username, and password globally,&#13;
meaning any channel we define will use the identical AMQP Broker configuration.&#13;
If desired, the configuration can be set on a per-channel basis.&#13;
We’ve also indicated to use the <code>smallrye-amqp</code> connector for the <code>data</code> outgoing channel.</p>&#13;
&#13;
<p>By default, the channel uses durable messages for the queue,&#13;
or we make them nondurable with <code>mp.messaging.outgoing.data.durable=false</code>.&#13;
We can also override the message durability directly when sending the message, as shown in <a data-type="xref" href="#ebus::use-outgoing-meta">Example 11-20</a>.</p>&#13;
<div data-type="example" id="ebus::use-outgoing-meta">&#13;
<h5><span class="label">Example 11-20. </span>Use outgoing metadata to send durable messages</h5>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="nd">@Outgoing</code><code class="o">(</code><code class="s">"data"</code><code class="o">)</code>&#13;
<code class="n">Multi</code><code class="o">&lt;</code><code class="n">Message</code><code class="o">&lt;</code><code class="n">Person</code><code class="o">&gt;&gt;</code> <code class="nf">produceAStreamOfMessagesOfPersons</code><code class="o">()</code> <code class="o">{</code>&#13;
  <code class="k">return</code> <code class="n">Multi</code><code class="o">.</code><code class="na">createFrom</code><code class="o">().</code><code class="na">items</code><code class="o">(</code>&#13;
      <code class="n">Message</code><code class="o">.</code><code class="na">of</code><code class="o">(</code><code class="k">new</code> <code class="n">Person</code><code class="o">(</code><code class="s">"Luke"</code><code class="o">))</code>&#13;
          <code class="o">.</code><code class="na">addMetadata</code><code class="o">(</code><code class="n">OutgoingAmqpMetadata</code><code class="o">.</code><code class="na">builder</code><code class="o">().</code><code class="na">withDurable</code><code class="o">(</code><code class="kc">false</code><code class="o">).</code><code class="na">build</code><code class="o">()),</code>&#13;
      <code class="n">Message</code><code class="o">.</code><code class="na">of</code><code class="o">(</code><code class="k">new</code> <code class="n">Person</code><code class="o">(</code><code class="s">"Leia"</code><code class="o">))</code>&#13;
          <code class="o">.</code><code class="na">addMetadata</code><code class="o">(</code><code class="n">OutgoingAmqpMetadata</code><code class="o">.</code><code class="na">builder</code><code class="o">().</code><code class="na">withDurable</code><code class="o">(</code><code class="kc">false</code><code class="o">).</code><code class="na">build</code><code class="o">()),</code>&#13;
      <code class="n">Message</code><code class="o">.</code><code class="na">of</code><code class="o">(</code><code class="k">new</code> <code class="n">Person</code><code class="o">(</code><code class="s">"Obiwan"</code><code class="o">))</code>&#13;
          <code class="o">.</code><code class="na">addMetadata</code><code class="o">(</code><code class="n">OutgoingAmqpMetadata</code><code class="o">.</code><code class="na">builder</code><code class="o">().</code><code class="na">withDurable</code><code class="o">(</code><code class="kc">false</code><code class="o">).</code><code class="na">build</code><code class="o">()),</code>&#13;
      <code class="n">Message</code><code class="o">.</code><code class="na">of</code><code class="o">(</code><code class="k">new</code> <code class="n">Person</code><code class="o">(</code><code class="s">"Palpatine"</code><code class="o">))</code>&#13;
          <code class="o">.</code><code class="na">addMetadata</code><code class="o">(</code><code class="n">OutgoingAmqpMetadata</code><code class="o">.</code><code class="na">builder</code><code class="o">().</code><code class="na">withDurable</code><code class="o">(</code><code class="kc">false</code><code class="o">).</code><code class="na">build</code><code class="o">())</code>&#13;
  <code class="o">);</code>&#13;
<code class="o">}</code></pre></div>&#13;
&#13;
<p>We can then consume the message similarly to Kafka,&#13;
but using the AMQP metadata object to retrieve more detailed information about the message (<a data-type="xref" href="#ebus:extract-amqp-meta">Example 11-21</a>).</p>&#13;
<div data-type="example" id="ebus:extract-amqp-meta">&#13;
<h5><span class="label">Example 11-21. </span>Extract AMQP metadata from incoming messages</h5>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="nd">@Incoming</code><code class="o">(</code><code class="s">"data"</code><code class="o">)</code>&#13;
<code class="n">CompletionStage</code><code class="o">&lt;</code><code class="n">Void</code><code class="o">&gt;</code> <code class="nf">consume</code><code class="o">(</code><code class="n">Message</code><code class="o">&lt;</code><code class="n">Person</code><code class="o">&gt;</code> <code class="n">person</code><code class="o">)</code> <code class="o">{</code>&#13;
    <code class="n">Optional</code><code class="o">&lt;</code><code class="n">IncomingAmqpMetadata</code><code class="o">&gt;</code> <code class="n">metadata</code> <code class="o">=</code> <code class="n">person</code>&#13;
            <code class="o">.</code><code class="na">getMetadata</code><code class="o">(</code><code class="n">IncomingAmqpMetadata</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>&#13;
    <code class="n">metadata</code><code class="o">.</code><code class="na">ifPresent</code><code class="o">(</code><code class="n">meta</code> <code class="o">-&gt;</code> <code class="o">{</code>&#13;
        <code class="n">String</code> <code class="n">address</code> <code class="o">=</code> <code class="n">meta</code><code class="o">.</code><code class="na">getAddress</code><code class="o">();</code>&#13;
        <code class="n">String</code> <code class="n">subject</code> <code class="o">=</code> <code class="n">meta</code><code class="o">.</code><code class="na">getSubject</code><code class="o">();</code>&#13;
        <code class="o">});</code>&#13;
    <code class="c1">// ...</code>&#13;
    <code class="k">return</code> <code class="n">person</code><code class="o">.</code><code class="na">ack</code><code class="o">();</code>&#13;
<code class="o">}</code></pre></div>&#13;
&#13;
<p>Successful receipt and processing of a message results in the connector&#13;
notifying the broker with an <code>accepted</code> acknowledgment.&#13;
On receiving this acknowledgment, the broker will delete the message from the queue.<a data-startref="ix_event-bus-adoc13" data-type="indexterm" id="idm45358816994928"/><a data-startref="ix_event-bus-adoc12" data-type="indexterm" id="idm45358816902080"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Publish/Subscribe" data-type="sect2"><div class="sect2" id="idm45358817233984">&#13;
<h2>Publish/Subscribe</h2>&#13;
&#13;
<p><a data-primary="AMQP (Advanced Message Queuing Protocol)" data-secondary="publish/subscribe model" data-type="indexterm" id="idm45358816900224"/><a data-primary="publish/subscribe model" data-secondary="AMQP and" data-type="indexterm" id="idm45358816899152"/>AMQP also supports a publish/subscribe model, similarly to Kafka,&#13;
allowing many subscribers for a single queue to read messages.&#13;
<a data-primary="multicast (queue)" data-type="indexterm" id="idm45358816897936"/>In AMQP, a queue can be of type <em>multicast</em> (the opposite of <em>unicast</em>) to indicate that many consumers can receive the same message.</p>&#13;
&#13;
<p><a data-type="xref" href="#image:amqp-topics">Figure 11-8</a> has three consumers of a multicast queue reading messages,&#13;
and we see how far through the messages each of the consumers has gotten.&#13;
As with unicast queues, the messages are durable by default but can also be made nondurable if desired.</p>&#13;
&#13;
<figure><div class="figure" id="image:amqp-topics">&#13;
<img alt="AMQP multicast queue consumers" src="assets/rsij_1108.png"/>&#13;
<h6><span class="label">Figure 11-8. </span>AMQP multicast queue consumers</h6>&#13;
</div></figure>&#13;
&#13;
<p>The code for sending and receiving messages from a multicast queue is identical to the code we used for point-to-point in <a data-type="xref" href="#amqp-ptp">“Point-to-Point Communication”</a>.&#13;
The <em>address</em> defaults to the channel name; it can be customized in configuration of the channel or set directly on the metadata of the message.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Elasticity Patterns" data-type="sect2"><div class="sect2" id="idm45358816890848">&#13;
<h2>Elasticity Patterns</h2>&#13;
&#13;
<p><a data-primary="AMQP (Advanced Message Queuing Protocol)" data-secondary="elasticity patterns" data-type="indexterm" id="idm45358816889648"/><a data-primary="elasticity" data-secondary="AMQP and" data-type="indexterm" id="idm45358816888640"/>The elasticity patterns for AMQP with point-to-point communication are a little different.&#13;
With Kafka, we can have only one consumer reading from a single partition.&#13;
With AMQP, we can have as many consumers as we want reading from the same queue, given the order in which the messages are processed is not important.</p>&#13;
&#13;
<p>Granted,&#13;
we may not want a lot of consumers reading the queue from the same broker node,&#13;
but we are able to cluster the brokers to spread the load across them. With a cluster of brokers,&#13;
the broker is smart enough to shift messages from one broker to another if it notices the consumers of a queue on that broker are underutilized compared to other brokers.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Acknowledgment and Redelivery" data-type="sect2"><div class="sect2" id="idm45358816886224">&#13;
<h2>Acknowledgment and Redelivery</h2>&#13;
&#13;
<p><a data-primary="acknowledgment strategies" data-type="indexterm" id="idm45358816884880"/><a data-primary="AMQP (Advanced Message Queuing Protocol)" data-secondary="acknowledgment and redelivery" data-type="indexterm" id="idm45358816883936"/><a data-primary="redelivery, AMQP and" data-type="indexterm" id="idm45358816882944"/>When we send a message to an AMQP Broker,&#13;
it is acknowledged if the broker successfully committed the message.&#13;
However, this will not be the case when routers are utilized between the producer and a broker.&#13;
In this situation, it is recommended to set <code>auto-acknowledgement</code> to <code>true</code> to ensure that the producer&#13;
receives acknowledgment when the message is sent to the router.&#13;
Any response from the broker of &#13;
<span class="keep-together"><code>rejected</code></span>, <code>released</code>, or <code>modified</code> results in the message being nacked.</p>&#13;
&#13;
<p>The consumption side has a few more possibilities for acknowledgment.&#13;
We can <code>fail</code> the message, causing the application to enter a failed state and process no further messages.&#13;
The message being processed resulting in the failure is marked as <code>rejected</code> with the broker.&#13;
This is the default behavior for the AMQP connector.</p>&#13;
&#13;
<p>The Accept, Release, and Reject strategies all result in the failure being logged, and the application to continue processing additional messages.&#13;
The only difference between them is in the way the AMQP messages are designated on the broker.&#13;
The Accept strategy marks the message as <code>accepted</code>,&#13;
the Release one marks it as <code>released</code>,&#13;
and, finally, the Reject one marks it as <code>rejected</code>.&#13;
When we want to continue processing messages on failure,&#13;
which of the three options you set depends on how you want the AMQP Broker to handle the message.</p>&#13;
&#13;
<p>What about redelivery?&#13;
If we mark the AMQP message as <code>released</code>,&#13;
the broker can redeliver the message, to the same or different consumer, at a later time.&#13;
When setting a message as <code>modified</code>,&#13;
we have two available strategies.&#13;
Using the <code>modified-failed</code> strategy sets a <code>delivery-failed</code> attribute on the message,&#13;
enabling the broker to attempt redelivery of the message while processing continues with the next message.&#13;
However, using the <code>modified-failed-undeliverable-here</code> strategy also sets the <code>delivery-failed</code> attribute,&#13;
and while the broker can attempt redelivery of the message,&#13;
it won’t do so with this consumer.</p>&#13;
&#13;
<p>If at any point the consumer loses the session with a broker,&#13;
any in-progress work will be rolled back.&#13;
This allows for other consumers,&#13;
or restarting of the current consumer,&#13;
to accept redelivery of any messages that were in-flight at the time the session with the broker was severed.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Credit-Flow Backpressure Protocol" data-type="sect2"><div class="sect2" id="idm45358816871312">&#13;
<h2>Credit-Flow Backpressure Protocol</h2>&#13;
&#13;
<p><a data-primary="AMQP (Advanced Message Queuing Protocol)" data-secondary="credit-flow backpressure protocol" data-type="indexterm" id="idm45358816870144"/><a data-primary="backpressure" data-secondary="credit-flow backpressure protocol" data-type="indexterm" id="idm45358816869008"/><a data-primary="credit-flow backpressure protocol" data-type="indexterm" id="idm45358816868048"/>AMQP enables backpressure in producers with a credit system.&#13;
Producers are able to send messages to a broker only as long as they have credits available,&#13;
preventing producers from overloading the broker with too many messages in a small amount of time.&#13;
The credits represent the number of bytes a producer can send.&#13;
For example, if we had 1,000 credits, representing 1,000 bytes,&#13;
a producer would be able to send 1 message of 1,000 bytes or 10 messages of 100 bytes before the credits expired.</p>&#13;
&#13;
<p>When a producer has spent all its credits,&#13;
it waits in a nonblocking manner until additional credits are granted from the broker.&#13;
The default is to request additional credits every 2,000 ms,&#13;
but this setting can be configured with the <code>credit-retrieval-period</code> configuration property.</p>&#13;
&#13;
<p>When running out of credit, the connector marks the application as <code>not ready</code>.&#13;
This information is then reported to the application health check.&#13;
If you deploy the application to Kubernetes, the readiness health check will fail, and Kubernetes will stop sending traffic to the pod until it becomes ready again.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="AMQP on Kubernetes" data-type="sect2"><div class="sect2" id="idm45358816864320">&#13;
<h2>AMQP on Kubernetes</h2>&#13;
&#13;
<p><a data-primary="AMQP (Advanced Message Queuing Protocol)" data-secondary="on Kubernetes" data-secondary-sortas="Kubernetes" data-type="indexterm" id="idm45358816862752"/><a data-primary="Kubernetes" data-secondary="AMQP on" data-type="indexterm" id="idm45358816861472"/>Setting up a production-ready AMQP Broker on Kubernetes is not a straightforward task,&#13;
so we’re opting to use a single Docker image to keep it simple. With a Kubernetes environment running,&#13;
run an AMQP Broker container, as shown in <a data-type="xref" href="#bus::amqp-broker">Example 11-22</a>.</p>&#13;
<div data-type="example" id="bus::amqp-broker">&#13;
<h5><span class="label">Example 11-22. </span>Start AMQP Broker container</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">kubectl run amqp --image<code class="o">=</code>quay.io/artemiscloud/activemq-artemis-broker <code class="se">\</code>&#13;
    --port<code class="o">=</code><code class="m">5672</code> --env<code class="o">=</code><code class="s2">"AMQ_USER=admin"</code> --env<code class="o">=</code><code class="s2">"AMQ_PASSWORD=admin"</code> <code class="se">\</code>&#13;
    -n event-bus</pre></div>&#13;
&#13;
<p>Here, we start an AMQP Broker in a Kubernetes pod,&#13;
but we need to expose the broker as a service to make it accessible to the services, as shown in <a data-type="xref" href="#ebus::expose-broker">Example 11-23</a>.</p>&#13;
<div data-type="example" id="ebus::expose-broker">&#13;
<h5><span class="label">Example 11-23. </span>Expose AMQP Broker service port</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">kubectl expose pod amqp --port<code class="o">=</code><code class="m">5672</code> -n event-bus</pre></div>&#13;
&#13;
<p>To be able to use AMQP, we need to switch our code to utilize a different dependency and configuration,&#13;
but the bulk of the services remain unchanged.&#13;
For each service, comment out the <code>quarkus-smallrye-reactive-messaging-kafka</code> dependency and uncomment&#13;
the <code>quarkus-smallrye-reactive-messaging-amqp</code> dependency in each <em>pom.xml</em>.&#13;
In the <em>application.properties</em> file for each service,&#13;
comment out the <code>smallrye-kafka</code> connector configuration and uncomment the <code>smallrye-amqp</code> connector.&#13;
Don’t forget to change both connectors in the <code>processor</code> service!&#13;
Be sure to run <code>mvn clean package</code> on all the services after making these changes.</p>&#13;
&#13;
<p>All the AMQP Broker configuration is present in the Helm charts,&#13;
with the actual values in <em>values.yaml</em>. Follow the instructions in <em>/chapter-11/README.md</em> under <em>Application deployment</em> for&#13;
building the required Docker images and installing the services.&#13;
They are the same steps we used for Kafka earlier in the chapter. Once the services are running, it’s time to test it! Open the SSE endpoint in a browser to see data as we did with Kafka (<a data-type="xref" href="#ebus::sse-output-3">Example 11-24</a>).</p>&#13;
<div data-type="example" id="ebus::sse-output-3">&#13;
<h5><span class="label">Example 11-24. </span>SSE output: all the messages are consumed by a single pod</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">data:2 consumed in pod <code class="o">(</code>processor-7558d76994-mq624<code class="o">)</code>&#13;
data:3 consumed in pod <code class="o">(</code>processor-7558d76994-mq624<code class="o">)</code>&#13;
data:4 consumed in pod <code class="o">(</code>processor-7558d76994-mq624<code class="o">)</code></pre></div>&#13;
&#13;
<p>Let’s scale up <code>processor</code> to add more consumers, as shown in <a data-type="xref" href="#ebus::increase-pods">Example 11-25</a>.</p>&#13;
<div data-type="example" id="ebus::increase-pods">&#13;
<h5><span class="label">Example 11-25. </span>Increase the number of application instances (pods)</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">kubectl scale deployment/processor -n event-bus --replicas<code class="o">=</code>3</pre></div>&#13;
&#13;
<p>Scaling with AMQP has a different outcome from that of scaling with Kafka; see <a data-type="xref" href="#ebus::sse-output-4">Example 11-26</a>.</p>&#13;
<div data-type="example" id="ebus::sse-output-4">&#13;
<h5><span class="label">Example 11-26. </span>SSE output: the messages are consumed by the three pods</h5>&#13;
&#13;
<pre data-code-language="shell" data-type="programlisting">data:187 consumed in pod <code class="o">(</code>processor-7558d76994-mq624<code class="o">)</code>&#13;
data:187 consumed in pod <code class="o">(</code>processor-7558d76994-hbp6j<code class="o">)</code>&#13;
data:187 consumed in pod <code class="o">(</code>processor-7558d76994-q2vcc<code class="o">)</code>&#13;
data:188 consumed in pod <code class="o">(</code>processor-7558d76994-q2vcc<code class="o">)</code>&#13;
data:188 consumed in pod <code class="o">(</code>processor-7558d76994-hbp6j<code class="o">)</code>&#13;
data:188 consumed in pod <code class="o">(</code>processor-7558d76994-mq624<code class="o">)</code>&#13;
data:189 consumed in pod <code class="o">(</code>processor-7558d76994-mq624<code class="o">)</code>&#13;
data:189 consumed in pod <code class="o">(</code>processor-7558d76994-hbp6j<code class="o">)</code>&#13;
data:189 consumed in pod <code class="o">(</code>processor-7558d76994-q2vcc<code class="o">)</code></pre></div>&#13;
&#13;
<p>We’re now seeing the same message consumed by all three producers,&#13;
instead of a message consumed once!<a data-startref="ix_event-bus-adoc11" data-type="indexterm" id="idm45358816714656"/><a data-startref="ix_event-bus-adoc10" data-type="indexterm" id="idm45358816714160"/><a data-startref="ix_event-bus-adoc9" data-type="indexterm" id="idm45358816713552"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45358817270976">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>This chapter went deeper into understanding the event bus when we use AMQP or Kafka with Reactive Messaging.&#13;
If we don’t need metadata classes for specific Kafka or AMQP behavior,&#13;
we can easily switch between the two with a dependency change and modifying configuration.&#13;
We covered how each of the event bus options support point-to-point communication, publish/subscribe,&#13;
acknowledgments, failure handling, and backpressure.&#13;
These are all key concepts in understanding the totality of a reactive system and its components.</p>&#13;
&#13;
<p>Kafka is the current popular choice for many event-driven reactive systems.&#13;
Kafka can handle a massive number of messages and makes ordering an essential characteristic.&#13;
AMQP does have a lot more flexibility than Kafka in the way it can be configured and customized.&#13;
It also has higher elasticity in point-to-point scenarios, as the limit is not constrained by the number of partitions.</p>&#13;
&#13;
<p>In the next chapter, we discuss using an HTTP client with Java interfaces representing an external service,&#13;
as well as how to use the lower-level web client and why that’s still useful.<a data-startref="ix_event-bus-adoc0" data-type="indexterm" id="idm45358816709712"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45358818317616"><sup><a href="ch11.html#idm45358818317616-marker">1</a></sup> Throughout this chapter, we will be talking about AMQP 1.0.</p></div></div></section></body></html>