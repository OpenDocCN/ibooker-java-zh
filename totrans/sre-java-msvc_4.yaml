- en: Chapter 4\. Charting and Alerting
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章。绘图和告警
- en: Monitoring doesn’t have to be an all-in proposition. If you only add a measure
    of error ratio for end-user interactions where you have no monitoring (or only
    resource monitoring like CPU/memory utilization), you’ve already taken a huge
    step forward in terms of understanding your software. After all, CPU and memory
    can look good but a user-facing API is failing 5% of all requests, and failure
    rate is a much easier idea to communicate between engineering organizations and
    their business partners.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 监控并不一定要全面投入。如果你只在你没有监控（或只有CPU/内存利用率等资源监控）的终端用户交互中添加一个错误比率的度量，那么在理解你的软件方面，你已经迈出了一大步。毕竟，CPU和内存可能看起来不错，但用户接口的API在所有请求中失败了5%，失败率在工程组织和业务合作伙伴之间沟通起来要容易得多。
- en: While Chapters [2](part0006_split_000.html#5N3C4-2d714b853a094e9a910510217e0e3d73)
    and [3](part0008_split_000.html#7K4G4-2d714b853a094e9a910510217e0e3d73) covered
    different forms of monitoring instrumentation, here we present the ways we can
    *use* that data effectively to promote action via alerting and visualization.
    This chapter covers three main topics.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然第[2](part0006_split_000.html#5N3C4-2d714b853a094e9a910510217e0e3d73)和[3](part0008_split_000.html#7K4G4-2d714b853a094e9a910510217e0e3d73)章节涵盖了不同形式的监控仪器化，但在这里我们提出了如何有效地利用这些数据通过告警和可视化来促进行动。本章涵盖了三个主要主题。
- en: First, we should think about what makes for a good visualization of an SLI.
    We’re only going to show charts from the commonly used [Grafana](https://grafana.com)
    charting and alerting tool, because it is a freely available open source tool
    that has datasource plug-ins for many different monitoring systems (so learning
    a little Grafana is a largely transferable skill from one monitoring system to
    another). Many of the same suggestions apply to charting solutions integrated
    into vendor products.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该思考一个好的 SLI 可视化是什么样的。我们只会展示来自常用的[Grafana](https://grafana.com)绘图和告警工具的图表，因为它是一个免费提供的开源工具，支持许多不同监控系统的数据源插件（因此从一个监控系统到另一个监控系统学习一些
    Grafana 是一个非常可转移的技能）。许多相同的建议也适用于集成到供应商产品中的绘图解决方案。
- en: Next, we’ll discuss specifics about the measurements that generate the most
    value and how to visualize and alert on them. Treat these as a checklist of SLIs
    that you can add incrementally. Incrementalism may even be preferable to implementing
    them all at once, because by adding an indicator at a time, you can really study
    and understand what it means in the context of your business and shape it in little
    ways to generate the most value to you. If I walked into the network operation
    center of an insurance company, I’d be much more relieved to see only indicators
    on the error ratio of policy rating and submissions than I would be to see a hundred
    low-level signals and no measure of business performance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论生成最大价值的测量数据的具体内容，以及如何对它们进行可视化和告警。将其视为你可以逐步添加的 SLI（服务级别指标）清单。逐步增加可能甚至优于一次性实施它们，因为逐个添加指标，你可以真正研究并理解它在你的业务背景下的含义，并进行微小调整以为你带来最大的价值。如果我走进一个保险公司的网络操作中心，我会更加放心地看到只有关于保单评级和提交错误比率的指标，而不是看到一百个低级信号和没有业务表现度量。
- en: Taking an incremental approach to introducing alerts is also an important trust-building
    exercise. Introducing too many alerts too quickly risks overwhelming engineers
    and leading to “alert fatigue.” You want engineers to feel comfortable subscribing
    to more alerts, not mute them! This also gives you a bit of time, if you are not
    already accustomed, to working through the on-call process, and training engineers
    how to respond to one alert condition at a time helps the team build a reservoir
    of knowledge about how to address anomalies.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 引入告警的增量方法也是一个建立信任的重要过程。过快引入过多的告警会导致工程师不堪重负，产生“告警疲劳”。你希望工程师能舒适地订阅更多的告警，而不是将它们静音！如果你还不习惯于值班流程，逐一培训工程师如何应对一个告警条件，有助于团队建立对如何处理异常的知识储备。
- en: So the focus in this chapter will be providing advice about those SLIs that
    are as close to business performance (e.g., API failure rate and the response
    times users see) as possible without being tied to any particular business. To
    the extent we cover things like heap utilization or file descriptors, they will
    be a select group of indicators that are most likely to be the direct cause of
    business performance degradation.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本章的重点将是提供关于那些与业务绩效（例如 API 失败率和用户看到的响应时间）尽可能接近的 SLI 的建议，而不与任何特定业务联系起来。在我们涵盖像堆使用或文件描述符之类的内容时，它们将是最有可能直接导致业务绩效下降的一组选择性指标。
- en: Recreating NASA mission control ([Figure 4-1](part0009_split_000.html#nasa_mission_control))
    should not be the end result of a well-monitored distributed system. While arraying
    screens across a wall and filling them with dashboards may look visually impressive,
    screens are not actions. They require somebody to be looking at them to respond
    to a visual indicator of a problem. I think this makes sense when you’re monitoring
    a single instance of a rocket with exorbitant costs and human lives on the line.
    Your API requests, of course, don’t have the same per-occurrence importance.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 重新创建 NASA 的任务控制中心（[图 4-1](part0009_split_000.html#nasa_mission_control)）不应该是良好监控的分布式系统的最终结果。尽管在墙上排列屏幕并填充它们与仪表盘可能看起来很震撼，但屏幕不是行动。他们需要有人关注以响应问题的视觉指示器。当你监控一个火箭的单个实例，成本高昂，人命关天时，这是有道理的。当然，您的
    API 请求没有相同的重要性。
- en: '![srej 0401](../images/00065.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0401](../images/00065.png)'
- en: Figure 4-1\. This is not a good role model!
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 这不是一个好的榜样！
- en: Almost every metrics collector will collect more data than you will find useful
    at any given time. While every metric may have usefulness in some circumstance,
    plotting every one is not helpful. However, several indicators (e.g., max latency,
    error ratio, resource utilization) are strong reliability signals for practically
    every Java microservice (with some tweaks to the alert thresholds). These are
    the ones we’ll focus on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每个指标收集器都会在任何给定时间收集比您发现有用的更多的数据。虽然每个指标在某些情况下可能有用，但绘制每个指标并不有助于。然而，几个指标（例如最大延迟、错误比率、资源利用率）对于几乎每个
    Java 微服务都是强大的可靠性信号（通过调整警报阈值）。这些是我们将重点关注的内容。
- en: Lastly, the market is eager to apply artificial intelligence methods to monitoring
    data to automate the delivery of insights into your systems without requiring
    much understanding of alert criteria and key performance indicators. In this chapter,
    we’ll survey several traditional statistical methods and artificial intelligence
    methods in the context of application monitoring. You should have a solid understanding
    of the strength and weakness of each method so that you can cut through the marketing
    hype and apply the best methods for your needs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，市场渴望将人工智能方法应用于监控数据，以自动提供对系统的洞察，而无需过多理解警报标准和关键绩效指标。在本章中，我们将在应用监控的背景下调查几种传统统计方法和人工智能方法。您应该对每种方法的优势和劣势有扎实的了解，以便能够洞悉市场宣传，并为您的需求应用最佳方法。
- en: Before going any further, it’s worth considering the breadth of variation in
    monitoring systems on the market and the impact that has on your decisions for
    how to instrument code and get data to these systems.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在进一步之前，值得考虑市场上监控系统的广泛变化以及这对于如何仪器化代码并将数据传递给这些系统的决策的影响。
- en: Differences in Monitoring Systems
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控系统的差异
- en: The point of discussing differences in monitoring systems here is that we are
    about to see specifics about how to chart and alert with Prometheus. A product
    like Datadog has a very different query system than Prometheus. Both are useful.
    More products are going to emerge in the future with capabilities we aren’t yet
    imagining. Ideally, we want our monitoring instrumentation (what we will put in
    our applications) to be portable across these monitoring systems with no changes
    in application code required (other than a new binary dependency and some registry-wide
    configuration).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里讨论监控系统的差异的要点在于，我们将看到如何使用 Prometheus 进行图表绘制和警报的具体内容。像 Datadog 这样的产品与 Prometheus
    的查询系统非常不同。两者都很有用。未来将会出现更多具有我们尚未想象到的功能的产品。理想情况下，我们希望我们的监控工具（我们将放入应用程序中的内容）在这些监控系统中是可移植的，不需要更改应用程序代码（除了新的二进制依赖项和一些全局配置）。
- en: 'There tends to be quite a bit more consistency in the way distributed tracing
    backend systems receive data than the way metrics systems receive data. Distributed
    tracing instrumentation libraries may have different propagation formats, requiring
    a degree of uniformity in the selection of an instrumentation library across the
    stack, but the data itself is fundamentally similar from backend to backend. This
    intuitively makes sense because of what the data is: distributed tracing really
    consists of per-event timing information (contextually stitched together by trace
    ID).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式跟踪后端系统接收数据的方式比指标系统更具一致性。分布式跟踪仪表化库可能具有不同的传播格式，需要在整个堆栈中选择一致的仪表化库，但数据本身在后端之间基本相似。这在直观上是有道理的，因为数据本质上是分布式跟踪事件的时间信息（通过跟踪
    ID 在上下文中粘合在一起）。
- en: Metrics systems could potentially represent not only aggregated timing information,
    but also gauges, counters, histogram data, percentiles, etc. They don’t agree
    on the way in which this data should be aggregated. They don’t have the same capabilities
    for performing further aggregation or calculation at query time. There is an inverse
    relationship between the number of time series a metrics instrumentation library
    must publish and the query capabilities of a particular metrics backend, as shown
    in [Figure 4-2](part0009_split_001.html#query_capabilities_vs_time_series).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 指标系统不仅可能表示聚合的计时信息，还可能表示仪表、计数器、直方图数据、百分位数等。它们对于数据聚合的方式并不一致。它们在查询时执行进一步聚合或计算的能力也不同。仪表化库需要发布的时间序列数量与特定指标后端的查询能力之间存在反向关系，如[图 4-2](part0009_split_001.html#query_capabilities_vs_time_series)所示。
- en: '![srej 0402](../images/00044.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0402](../images/00044.png)'
- en: Figure 4-2\. Inverse relationship between published time series and query capabilities
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 发布时间序列与查询能力之间的反向关系
- en: So, for example, when Dropwizard Metrics was initially developed, the popular
    monitoring system was Graphite, which didn’t have rate-calculating functions available
    in modern monitoring systems like Prometheus. As a result, when publishing a counter,
    Dropwizard had to publish cumulative count, 1-minute rate, 5-minute rate, 15-minute
    rate, etc. And because this was inefficient if you never needed to look at a rate,
    the instrumentation library itself distinguished between `@Counted` and `@Metered`.
    The instrumentation API was designed with the capabilities of its contemporary
    monitoring systems in mind.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当初开发 Dropwizard Metrics 时，流行的监控系统是 Graphite，它不像 Prometheus 这样的现代监控系统具有速率计算功能。因此，当发布计数器时，Dropwizard
    必须发布累计计数、1 分钟速率、5 分钟速率、15 分钟速率等。因为如果你从不需要查看速率，这样做就显得效率低下，所以仪表化库本身区分了`@Counted`和`@Metered`。仪表化
    API 的设计考虑了当代监控系统的能力。
- en: Fast forward to today, and a metrics instrumentation library intending to publish
    to multiple destination metrics systems needs to be aware of these subtleties.
    A Micrometer `Counter` is going to be presented to Graphite in terms of a cumulative
    count and several moving rates, but to Prometheus only as a cumulative count,
    because these rates can be computed at query time with a PromQL `rate` function.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 快进到今天，一个意图发布到多个目标指标系统的指标仪表化库需要意识到这些微妙之处。Micrometer 的 `Counter` 将以累计计数和几个移动速率的形式呈现给
    Graphite，但对于 Prometheus，仅作为累计计数，因为这些速率可以在查询时使用 PromQL 的 `rate` 函数计算。
- en: It’s important to the design of the API of any instrumentation library today
    to not simply lift all concepts found in earlier implementations forward, but
    to consider the historical context behind why these constructs existed at that
    time. [Figure 4-3](part0009_split_001.html#metrics_instrumentation_capabilities)
    shows where Micrometer has overlap with Dropwizard and Prometheus simple client
    predecessors, and where it has extended capabilities beyond those of its predecessors.
    Significantly, some concepts have been left behind, recognizing the evolution
    in the monitoring space since. In some cases, this difference is subtle. Micrometer
    incorporates histograms as a feature of a plain `Timer` (or `DistributionSummary`).
    It is often unclear at the point of instrumentation deep in a library where an
    operation is being timed whether the application incorporating this functionality
    views this operation as critical enough to warrant the extra expense of shipping
    histogram data. (So the decision should be left up to the downstream application
    author rather than the library author.)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何仪表化库的 API 设计而言，今天并不简单地提升早期实现中找到的所有概念，而是要考虑这些结构在当时存在的历史背景。[图 4-3](part0009_split_001.html#metrics_instrumentation_capabilities)
    显示 Micrometer 在与 Dropwizard 和 Prometheus 简单客户端前身的重叠以及超出其前身能力的扩展能力之处。显著的是，某些概念已被舍弃，认识到监控空间的进化。在某些情况下，这种差异是微妙的。Micrometer
    将直方图作为普通 `Timer`（或 `DistributionSummary`）的特性整合进去。在一个库深处进行仪表化的时候，很难清楚地知道应用是否将这个操作视为足够关键，值得支付额外费用来传送直方图数据。（因此，这个决定应留给下游应用程序的作者，而不是库的作者。）
- en: '![srej 0403](../images/00073.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0403](../images/00073.png)'
- en: Figure 4-3\. Metrics instrumentation capability overlap
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. 指标仪表化能力重叠
- en: Similarly, in the Dropwizard Metrics era, monitoring systems didn’t include
    query functionality that helped to reason about timing data (no percentile approximations,
    no latency heatmaps, etc.). So this concept of “don’t gauge something you can
    count, don’t count something you can time” wasn’t applicable yet. It wasn’t uncommon
    to add `@Counted` to a method, where now `@Counted` is almost never the right
    choice for a method (which is inherently timeable, and timers always publish with
    a count as well).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在 Dropwizard Metrics 时代，监控系统并不包括可帮助推理计时数据的查询功能（无百分位近似，无延迟热力图等）。因此，“不要衡量可以计数的东西，不要计数可以计时的东西”这一概念尚不适用。将
    `@Counted` 添加到方法中并不罕见，而现在 `@Counted` 几乎从不是方法的正确选择（方法本质上是可以计时的，并且计时器始终以计数方式发布）。
- en: While at the time of this writing OpenTelemetry’s metrics API is still in beta,
    it hasn’t changed substantially in the last couple years, and it appears the meter
    primitives won’t do a sufficient job to build usable abstractions for timing and
    counting. [Example 4-1](part0009_split_001.html#micrometer_timer_vs_ot) shows
    a Micrometer `Timer` with varying tags, depending on the outcome of an operation
    (this is the most verbose a timer gets in Micrometer).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在撰写本文时 OpenTelemetry 的指标 API 仍处于 beta 阶段，但在过去几年里它并未发生实质性变化，而且看起来仪表基元无法足够有效地构建用于计时和计数的可用抽象。[示例 4-1](part0009_split_001.html#micrometer_timer_vs_ot)
    展示了一个带有不同标签的 Micrometer `Timer`，取决于操作的结果（这是 Micrometer 中计时器最详细的描述方式）。
- en: Example 4-1\. A Micrometer timer with a variable outcome tag
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-1\. 带有可变结果标签的 Micrometer 计时器
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Even trying to get close to this with the OpenTelemetry metrics API right now
    is difficult, as shown in [Example 4-2](part0009_split_001.html#8IL70-2d714b853a094e9a910510217e0e3d73).
    No attempt has been made to record something similar to percentile histograms
    or SLO boundary counts like in the Micrometer equivalent. That would of course
    substantially increase the verbosity of this implementation, which is already
    getting lengthy.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 即使尝试使用 OpenTelemetry 指标 API 接近此功能也很困难，如 [示例 4-2](part0009_split_001.html#8IL70-2d714b853a094e9a910510217e0e3d73)
    所示。尚未尝试记录类似百分位直方图或 Micrometer 等效中的 SLO 边界计数。这显然会大大增加此实现的冗长性，而实现已经变得相当冗长。
- en: Example 4-2\. OpenTelemetry timing with variable outcome tags
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-2\. 使用可变结果标签的 OpenTelemetry 定时
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I believe the problem for OpenTelemetry is an emphasis on polyglot support,
    which naturally puts pressure on the project to want to define a consistent data
    structure for meter primitives like the “double sum observer” or “double counter.”
    The impact on the resulting API forces the end user to compose from lower-level
    building blocks the constituent parts of a higher-level abstraction like a Micrometer
    `Timer`. This not only leads to exceedingly verbose instrumentation code, but
    also leads to instrumentation that is specific to a particular monitoring system.
    For example, if we attempt to publish a counter to an older monitoring system
    like Graphite while we gradually migrate to Prometheus, we need to explicitly
    calculate per-interval moving rates and ship those too. The “double counter” data
    structure doesn’t support this. The reverse problem exists as well, the need to
    include the union of all possibly usable statistics for a “double counter” in
    the OpenTelemetry data structure to satisfy the widest array of monitoring systems,
    even though shipping this extra data is pure waste to a modern metrics backend.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为 OpenTelemetry 的问题在于强调多语言支持，这自然会给项目带来压力，希望为“double sum observer”或“double
    counter”等计量原语定义一致的数据结构。这对最终用户的 API 造成的影响迫使他们从低级构建块组合成高级抽象的组成部分，比如 Micrometer 的`Timer`。这不仅导致仪器化代码异常冗长，还导致仪器化针对特定监控系统的问题。例如，如果我们试图将计数器发布到旧的监控系统（如
    Graphite），而逐步迁移到 Prometheus，则需要显式计算每个间隔的移动速率并进行传送。然而，“double counter”数据结构无法支持这一点。反向问题也存在，即为了满足最广泛的监控系统，需要在
    OpenTelemetry 数据结构中包含“double counter”的所有可能可用统计数据的联合，尽管将这些额外的数据发送到现代指标后端是纯粹的浪费。
- en: As you get into exploring charting and alerting, you may want to experiment
    with different backends. And making a selection today based on what you know,
    you may find yourself transitioning with more experience in a year. Make sure
    your metrics *instrumentation* permits you to move fluidly between monitoring
    systems (and even publish to both while you transition).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始探索图表和警报时，你可能想尝试不同的后端。根据你目前的知识做出选择时，也许一年后你会更有经验。确保你的指标*仪器化*允许你在不同的监控系统之间流畅切换（甚至在过渡期间同时发布到两者）。
- en: Before we get into any particular SLIs, let’s first go over what makes for an
    effective chart.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入讨论任何特定的 SLI 之前，让我们先来看看什么样的图表才能有效。
- en: Effective Visualizations of Service Level Indicators
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务水平指标的有效可视化
- en: The recommendations offered here are naturally subjective. I’m going to state
    a preference for bolder lines and less “ink” on the chart, both of which deviate
    from Grafana’s defaults. To be honest, I’m a little embarrassed to offer these
    suggestions, because I don’t want to presume that my aesthetic sense is in some
    way greater than that of the excellent design team at Grafana.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供的建议自然是主观的。我倾向于更加粗线和图表上较少的“墨水”，这两者都偏离了 Grafana 的默认设置。老实说，我有点尴尬提出这些建议，因为我不想假定我的审美感比
    Grafana 设计团队的优秀设计更高一些。
- en: 'The stylistic sensibility I will offer is derived from two significant influences
    over my last few years of work:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我将提供的风格感知源于我过去几年工作中的两个重要影响：
- en: Watching engineers stare and squint at charts
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 看工程师盯着图表凝视和皱眉
- en: I worry when an engineer looks at a chart and squints. I worry especially that
    the lesson they take from an overly complex visualization is that monitoring itself
    is complex, and maybe too complex for them. Most of these indicators are *really*
    simple when presented correctly. It should feel that way.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当工程师看着图表皱着眉头时，我会感到担忧。尤其担心的是，他们从过于复杂的可视化中得出的教训是监控本身很复杂，也许对他们来说太复杂了。当这些指标在正确呈现时，大多数其实是*非常简单*的。它们应该给人一种这样的感觉。
- en: '*The Visual Display of Quantitative Information*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*定量信息的视觉展示*'
- en: 'For a time, I asked the same question of every member of the rare population
    of user experience designers I met who focus on operations engineering and developer
    experience: which book(s) did they find were the greatest influence on them? *The
    Visual Display of Quantitative Information* by Edward Tufte (Graphics Press) was
    always among their answers. One of the ideas most relevant to time series visualization
    that comes from this book is “data-ink” ratio, specifically to *increase* it as
    much as possible. If “ink” (or pixels) on a chart isn’t conveying information,
    it is conveying complexity. Complexity leads to squinting. Squinting leads to
    me worrying.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 曾经有一段时间，我问我遇到的每一位专注于运维工程和开发者体验的用户体验设计师同行同一个问题：哪本书对他们影响最大？*《定量信息的视觉显示》*（Edward
    Tufte，Graphics Press）总是他们的答案之一。来自这本书对时间序列可视化最相关的想法之一是“数据墨比率”，特别是尽可能*增加*它。如果图表上的“墨水”（或像素）并未传达信息，则传达的是复杂性。复杂性导致眯眼看。眯眼看会让我担心。
- en: Let’s think then from this perspective that data-ink ratio needs to go *up*.
    The specific recommendations that follow change the default styling of Grafana
    to maximize this ratio.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从这个角度来考虑，数据-墨比率需要*增加*。接下来的具体建议改变了Grafana的默认样式，以最大化这一比率。
- en: Styles for Line Width and Shading
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线宽和阴影风格
- en: Grafana’s default chart contains a 1 px solid line, a 10% transparency fill
    under the line, and interpolation between time slices. For better readability,
    increase the solid line width to 2 px and remove the fill. The fill reduces the
    data-ink ratio of the chart, and the overlapping colors of fills get disorienting
    with more than a couple lines on a chart. Interpolation is a little misleading,
    since it implies to a casual observer that the value may have briefly existed
    at intermediate points along the diagonal between two time slices. The opposite
    of interpolation is called “step” in Grafana’s options. The chart on the top in
    [Figure 4-4](part0009_split_003.html#grafana_line_width) uses the default options,
    and the chart on the bottom is adjusted with these recommendations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 的默认图表包含 1 像素的实线、线下 10% 的透明填充以及时间片段之间的插值。为了提高可读性，将实线宽度增加到 2 像素并移除填充。填充降低了图表的数据-墨比率，多个图表线条的填充颜色重叠会使人迷失方向。插值有些误导，因为它向普通观察者暗示值可能在两个时间片段之间的对角线上短暂存在。在Grafana的选项中，插值的相反称为“步进”。[图
    4-4](part0009_split_003.html#grafana_line_width)顶部的图表使用默认选项，底部的图表根据这些建议进行了调整。
- en: '![srej 0404](../images/00018.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0404](../images/00018.png)'
- en: Figure 4-4\. Grafana chart style default versus recommended
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 第 4-4 图。Grafana 图表样式的默认与推荐
- en: Change the options in the “Visualization” tab of the chart editor, as shown
    in [Figure 4-5](part0009_split_003.html#grafana_line_width_options).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表编辑器的“可视化”选项卡中更改选项，如[图 4-5](part0009_split_003.html#grafana_line_width_options)所示。
- en: '![srej 0405](../images/00080.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0405](../images/00080.png)'
- en: Figure 4-5\. Grafana line width options
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 第 4-5 图。Grafana 线宽选项
- en: Errors Versus Successes
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误与成功
- en: 'Plotting a stacked representation of outcomes (success, error, etc.) is very
    common for timers, as we’ll see in [“Errors”](part0009_split_013.html#8ILNL-2d714b853a094e9a910510217e0e3d73),
    and shows up in other scenarios as well. When we think of successes and errors
    as colors, many of us will immediately think of green and red: stoplight colors.
    Unfortunately, a significant portion of the population has color vision impairments
    that affect their ability to perceive color differences. For the most common impairements,
    deuteranopia and protanopia, the difference between green and red is difficult
    or impossible to distinguish! Those affected by monochromacy cannot distinguish
    colors at all, only brightness. Because this book is printed monochromatically,
    we all get to experience this briefly for the stacked chart of errors and successes
    in [Figure 4-6](part0009_split_004.html#grafana_error_colorblindness).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计时器，绘制成功和错误等结果的堆叠表示非常常见，我们将在[“Errors”](part0009_split_013.html#8ILNL-2d714b853a094e9a910510217e0e3d73)中看到，并且在其他场景中也会出现。当我们将成功和错误视为颜色时，许多人会立即想到绿色和红色：交通灯颜色。不幸的是，大部分人口中存在色盲，影响他们区分颜色的能力。对于最常见的变色性视觉障碍，绿色和红色之间的差异很难或根本无法区分！那些受单色视觉障碍影响的人根本无法区分颜色，只能区分亮度。由于本书是单色印刷，我们所有人都可以在短暂的时间内体验一下堆叠的错误和成功图表，见[图
    4-6](part0009_split_004.html#grafana_error_colorblindness)。
- en: '![srej 0406](../images/00089.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0406](../images/00089.png)'
- en: Figure 4-6\. Display errors with a different line style for accessibility
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6\. 使用不同的线条样式显示辅助功能中的错误
- en: We need to provide some sort of visual indicator of errors versus successes
    other than strictly color. In this case, we’ve chosen to plot “successful” outcomes
    as stacked lines and the errors above these outcomes as thick points to make them
    stand out.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要提供某种错误与成功的视觉指示器，而不仅仅是严格的颜色。在这种情况下，我们选择将“成功”结果绘制为堆叠线，将错误绘制在这些结果上方作为粗点以使其突出显示。
- en: Additionally, Grafana doesn’t offer an option to specify the order of time series
    as they appear in a stacked representation (i.e., “success” on the bottom or top
    of the stack), even for a limited set of possible values. We can force an ordering
    of them by selecting each value in a separate query and ordering the queries themselves,
    as shown in [Figure 4-7](part0009_split_004.html#grafana_outcomes_separate_queries).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Grafana没有提供指定时间序列在堆叠表示中出现顺序（即“成功”在堆叠底部或顶部）的选项，即使是针对一组可能值的有限集合也是如此。我们可以通过选择每个值在单独的查询中，并对查询本身进行排序来强制对它们进行排序，如[图 4-7](part0009_split_004.html#grafana_outcomes_separate_queries)所示。
- en: '![srej 0407](../images/00088.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0407](../images/00088.png)'
- en: Figure 4-7\. Ordering outcomes in a Grafana stack representation
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-7\. 在 Grafana 堆叠表示中排序结果
- en: Lastly, we can override the styling of each individual query, as shown in [Figure 4-8](part0009_split_004.html#grafana_line_style_override).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以覆盖每个单独查询的样式，如[图 4-8](part0009_split_004.html#grafana_line_style_override)所示。
- en: '![srej 0408](../images/00092.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0408](../images/00092.png)'
- en: Figure 4-8\. Overriding line styles for each outcome
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-8\. 为每个结果覆盖线条样式
- en: “Top k” Visualizations
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: “Top k” 可视化
- en: In many cases, we want to display some indicator of the “worst” performers by
    some category. Many monitoring systems offer some sort of query function to select
    the “top k” time series for some criteria. Selecting “top 3” worst performers
    doesn’t mean that there will be a maximum of three lines on the chart, however,
    because this race to the bottom is perpetual, and the worst performers can change
    over the course of the time interval visualized by the chart. At worst, you are
    displaying *N* datapoints on a particular visualization, and there will be 3**N*
    distinct time series displayed! If you draw a vertical line down any part of [Figure 4-9](part0009_split_005.html#grafana_table_to_right)
    and count the number of unique colors it intersects, it will always be less than
    or equal to three because this chart was built with a “top 3” query. But there
    are six items in the legend.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，我们希望按某个类别显示一些“最差”表现的指示器。许多监控系统提供某种查询功能，以选择某些标准的“top k”时间序列。然而，选择“top
    3”最差表现者并不意味着图表上会有最多三条线，因为这场“到底”的竞赛是永无止境的，而且最差表现者在图表可视化的时间间隔内可能会发生变化。在最坏的情况下，您将在特定可视化中显示*N*个数据点，并且将显示3**N*个不同的时间序列！如果您在[图 4-9](part0009_split_005.html#grafana_table_to_right)的任何部分绘制垂直线，并计算它所相交的唯一颜色数量，它将始终小于或等于三，因为此图是使用“top
    3”查询构建的。但是图例中有六个项目。
- en: '![srej 0409](../images/00053.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0409](../images/00053.png)'
- en: Figure 4-9\. Top k visualization with more than k distinct time series
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-9\. 具有超过 k 个不同时间序列的 top k 可视化
- en: It can get far busier than this very easily. Consider [Figure 4-10](part0009_split_005.html#topk_many_lines),
    which shows the top five longest Gradle build task times over a period of time.
    Since the set of build tasks running changes rapidly over the time slices shown
    in this chart, the legend fills up with many more values than simply five.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以很容易地变得比这更加繁忙。考虑[图 4-10](part0009_split_005.html#topk_many_lines)，它显示了一段时间内前五个最长的
    Gradle 构建任务时间。由于显示的时间片段内运行的构建任务集合会快速变化，所以图例中填充的值会比简单的五个值多得多。
- en: '![srej 0410](../images/00029.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0410](../images/00029.png)'
- en: Figure 4-10\. Top k can still yield many more items in the legend than k
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-10\. Top k 仍然可以在图例中产生比 k 更多的项目
- en: In such cases, the legend is overwhelmed with labels, to the point where it
    is illegible. Use the Grafana options to shift the legend to a table on the right,
    and add a summary statistic like “maximum,” as shown in [Figure 4-11](part0009_split_005.html#grafana_table_to_right_options).
    You can then click the summary statistic in the table to sort the legend-as-table
    by this statistic. Now when we look at the chart, we can quickly see which performers
    are overall worst for the time range that we are viewing.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，图例被标签压倒，以至于无法辨认。使用Grafana选项将图例移到右侧的表格，并添加一个“最大值”之类的摘要统计数据，如[图 4-11](part0009_split_005.html#grafana_table_to_right_options)所示。然后，您可以点击表格中的摘要统计数据，按此统计数据对图例作为表格进行排序。现在，当我们查看图表时，我们可以快速看出在我们查看的时间范围内哪些表现最差。
- en: '![srej 0411](../images/00040.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0411](../images/00040.png)'
- en: Figure 4-11\. Overriding line styles for each outcome
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-11\. 为每个结果覆盖线条样式
- en: Prometheus Rate Interval Selection
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Prometheus速率间隔选择
- en: Throughout this chapter, we are going to see Prometheus queries that use [range
    vectors](https://oreil.ly/RnMk3). I highly suggest using range vectors that are
    at least twice as long as the scrape interval (by default one minute). Otherwise,
    you risk missing datapoints due to slight variations in scrape timing that may
    cause adjacent datapoints to be just slightly more than the scrape interval apart.
    Similarly, if a service is restarted and a datapoint is missing, the rate function
    will not be able to make a rate during the gap or next datapoint until the interval
    contains at least two points. Using a higher interval for the rate avoids these
    problems. Because application startup may be longer than a scrape interval, depending
    on your application, if it is important to you to totally avoid gaps, you may
    choose a range vector longer than twice the scrape interval (something in fact
    closer to whatever application startup plus two intervals would be).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将看到使用[范围向量](https://oreil.ly/RnMk3)的Prometheus查询。我强烈建议使用至少是抓取间隔的两倍长的范围向量（默认为一分钟）。否则，由于抓取时间的轻微变化可能导致相邻数据点间隔略大于抓取间隔，您可能会错过数据点。类似地，如果服务重新启动且数据点丢失，速率函数将无法在间隙或下一个数据点之间进行速率计算，直到间隔包含至少两个点。使用更长的间隔可避免这些问题。由于应用程序的启动可能比抓取间隔长，具体取决于您的应用程序，如果完全避免间隙对您很重要，您可以选择比两倍抓取间隔更长的范围向量（实际上更接近应用程序启动加两个间隔的任何内容）。
- en: Range vectors are a somewhat unique concept to Prometheus, but the same principle
    applies in other contexts in other monitoring systems. For example, you’d want
    to construct a “min over interval” type query to compensate for potential gaps
    during application restart if you are setting a minimum threshold on an alert.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 范围向量在Prometheus中是一个相对独特的概念，但在其他监控系统中的其他上下文中也适用相同的原理。例如，如果您在警报上设置了最小阈值，并且由于应用程序重新启动而可能出现间隙，则需要构建“间隔内的最小值”类型的查询以进行补偿。
- en: Gauges
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计量器
- en: A time series representation of a gauge presents more information about as compactly
    as an instantaneous gauge. It is just as obvious when a line crosses an alert
    threshold, and the historical information about the gauge’s prior values provides
    useful context. As a result, the bottom chart is preferable in [Figure 4-12](part0009_split_007.html#gauge_instantaneous_vs_line).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 计量器的时间序列表示比即时计量器提供更多关于信息的紧凑表示。当线路穿过警报阈值时，它同样明显，并且有关计量器先前值的历史信息提供了有用的上下文。因此，在[图 4-12](part0009_split_007.html#gauge_instantaneous_vs_line)中，底部图表更可取。
- en: '![srej 0412](../images/00046.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0412](../images/00046.png)'
- en: Figure 4-12\. Prefer a line chart over an instantaneous gauge
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-12\. 更倾向于使用线图而不是即时计量器
- en: Gauges have a tendency to be spiky. Thread pools can appear to be temporarily
    near exhaustion and then recover. Queues get full and then empty. Memory utilization
    in Java is especially tricky to alert on since short-term allocations can quickly
    appear to fill up a significant portion of allocated space only for garbage collection
    to sweep away much of the consumption.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 计量器往往呈现尖峰。线程池可能出现短时间接近枯竭的情况，然后恢复。队列会变满然后清空。在Java中，内存利用尤其棘手，因为短期分配可能迅速填满分配的大部分空间，但垃圾收集可能会清除大部分消耗。
- en: One of the most effective methods to limit alert chattiness is to use a rolling
    count function, the results of which are shown in [Figure 4-13](part0009_split_007.html#rolling_count).
    In this way we can define an alert that only fires if a threshold is exceeded
    more than three times in the last five intervals, or some other combination of
    frequency and number of lookback intervals. The longer the lookback, the more
    time will elapse before the alert first fires, so be careful to not look back
    too far for critical indicators.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 限制警报频繁性的最有效方法之一是使用滚动计数功能，其结果显示在图 4-13 中（part0009_split_007.html#rolling_count）。通过这种方式，我们可以定义一个只有在过去五个间隔中超过三次超过阈值时才触发警报的警报，或者某种频率和回顾间隔数量的组合。回顾的时间越长，警报首次触发前的时间就越长，因此在寻找关键指标时，不要回顾得太久。
- en: '![The alarm fires only when there is clearly a problem developing](../images/00055.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![只有当问题明显发展时，警报才会触发。](../images/00055.png)'
- en: Figure 4-13\. Rolling count to limit alert chattiness
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-13\. 滚动计数以限制警报频繁性
- en: Being instantaneous values, gauges are basically just graphed as is on each
    monitoring system. Counters are a little more nuanced.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 作为瞬时值，计量器基本上只是在每个监控系统上直接绘制成图形。计数器稍微复杂一些。
- en: Counters
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计数器
- en: Counters are often tested against a maximum (or less frequently, a minimum)
    threshold. The need to test against a threshold reinforces the idea that counters
    should be observed as rates rather than a cumulative statistic, regardless of
    how the statistic is stored in the monitoring system.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 计数器经常针对最大（或较少时是最小）阈值进行测试。对阈值进行测试的需要强化了计数器应被视为速率而不是累积统计数据的想法，无论统计数据如何在监控系统中存储。
- en: '[Figure 4-14](part0009_split_008.html#counter_cumulative_vs_rate) shows an
    HTTP endpoint’s request throughput as a rate (yellow solid line) and also the
    cumulative count (green dots) of all requests to this endpoint since the application
    process started. Also, the chart shows a fixed minimum threshold alert (red line
    and area) of 1,000 requests/second that has been set on this endpoint’s throughput.
    This threshold makes sense relative to throughput represented as a rate (which
    in this window varies between 1,500 and 2,000 requests/second). It makes little
    sense against the cumulative count though, since the cumulative count is effectively
    a measure of both the rate of throughput and the longevity of the process. The
    longevity of the process is irrelevant to this alert.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-14](part0009_split_008.html#counter_cumulative_vs_rate) 显示了一个 HTTP 端点的请求吞吐量作为速率（黄色实线），还显示了自应用进程启动以来对该端点的所有请求的累积计数（绿色点）。图表还显示了在该端点吞吐量上设置的固定最小阈值警报（红色线和区域），阈值设定为每秒
    1,000 次请求。这个阈值在相对于速率表示的吞吐量时是有意义的（在此窗口中速率在每秒 1,500 到 2,000 次请求之间变化）。但对累积计数来说意义不大，因为累积计数实际上是吞吐率和进程的长期性的度量。进程的长期性对这个警报来说是不相关的。'
- en: '![The alert threshold only makes sense against the rate.](../images/00102.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![警报阈值只在考虑速率时才有意义。](../images/00102.png)'
- en: Figure 4-14\. A counter with a minimum alert threshold on rate, with cumulative
    count displayed as well
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-14\. 具有速率最小阈值警报的计数器，并显示累积计数
- en: Sometimes a fixed threshold is difficult to determine a priori. Also, the rate
    at which an event is occurring may fluctuate periodically based on something like
    peak and off-peak business hours. This is especially common with a throughput
    measure like requests/second, as seen in [Figure 4-15](part0009_split_008.html#counter_dynamic_threshold).
    If we set a fixed threshold on this service to detect when traffic was suddenly
    not reaching the service (a minimum threshold), we would have to set it somewhere
    below 40 RPS, the minimum throughput this service sees. Suppose the minimum threshold
    is set at 30 RPS. This alert fires when traffic drops below 75% of the expected
    value during off-peak hours, but only when traffic drops below 10% of the expected
    value during peak hours! The alert threshold is not equally valuable during all
    periods.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有时固定阈值很难事先确定。此外，事件发生的速率可能会周期性地波动，例如根据高峰和低峰的业务时间。这在像每秒请求数这样的吞吐量测量中特别常见，如图 4-15
    所示（part0009_split_008.html#counter_dynamic_threshold）。如果我们在此服务上设置一个固定阈值，以便检测到流量突然未达到服务的情况（最小阈值），我们将不得不将其设置在此服务看到的最低吞吐量
    40 RPS 以下的某个位置。假设最小阈值设置为 30 RPS。这个警报将在业务低峰期流量低于预期值的 75% 时触发，但只有在业务高峰期流量低于预期值的
    10% 时才触发！在所有时期，警报阈值的价值并不相等。
- en: '![A fixed threshold would not quickly detect a sudden change in RPS during
    peak hours.](../images/00051.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![在高峰时段，固定阈值无法快速检测到请求每秒的突然变化。](../images/00051.png)'
- en: Figure 4-15\. A service with periodic increases in traffic based on the time
    of day
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-15\. 根据一天中的时间增加流量的服务
- en: In these cases, consider framing an alert in terms of finding sharp increases
    or decreases in rate. A good general approach to this, seen in [Figure 4-16](part0009_split_008.html#des),
    is to take the counter rate, apply a smoothing function to it, and multiply the
    smoothing function by some factor (85% in the example). Because the smoothing
    function naturally takes at least a little time to respond to a sudden change
    in the rate, a test to ensure that the counter rate doesn’t fall below the smoothed
    line detects sudden change without having to know what the expected rate is at
    all. A much more detailed explanation of statistical methods for smoothing for
    dynamic alerting is presented in [“Building Alerts Using Forecasting Methods”](part0009_split_026.html#8INDM-2d714b853a094e9a910510217e0e3d73).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，考虑以寻找速率的急剧增加或减少的方式设置警报。一个很好的一般方法是在 [图 4-16](part0009_split_008.html#des)
    中看到的，即取计数器速率，应用平滑函数，并将平滑函数乘以某个因子（例如例子中的 85%）。因为平滑函数自然需要一点时间来对速率的突然变化做出响应，所以检测计数器速率是否低于平滑线，可以在完全不知道预期速率的情况下检测到突然变化。关于动态警报使用平滑统计方法的更详细解释，请参见
    [“使用预测方法构建警报”](part0009_split_026.html#8INDM-2d714b853a094e9a910510217e0e3d73)。
- en: '![The alert fires when there is a sudden drop in throughput.](../images/00104.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![当吞吐量突然下降时触发警报。](../images/00104.png)'
- en: Figure 4-16\. A counter with a double-exponentially smoothed threshold, forming
    a dynamic alert threshold
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-16\. 具有双指数平滑阈值的计数器，形成动态警报阈值
- en: It is Micrometer’s responsibility to ship the data to your monitoring system
    of choice in such a way that you can draw a rate representation of a counter in
    your chart. In the case of Atlas, counters are already shipped in a rate-normalized
    way, so a query for a counter already returns a rate value that can be directly
    plotted, as shown in [Example 4-3](part0009_split_008.html#atlas_counter_is_a_rate).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Micrometer 中，将数据发送到您选择的监控系统是其责任，以便您可以在图表中绘制计数器的速率表示。在 Atlas 的情况下，计数器已经以速率标准化的方式发送，因此对计数器的查询已经返回可以直接绘制的速率值，如
    [示例 4-3](part0009_split_008.html#atlas_counter_is_a_rate) 所示。
- en: Example 4-3\. Atlas counters are already a rate, so selecting them charts a
    rate
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-3\. Atlas 计数器已经是速率，因此选择它们可以绘制速率
- en: '[PRE2]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Other monitoring systems expect cumulative values to be shipped to the monitoring
    system and include some sort of rate function for use at query time. [Example 4-4](part0009_split_008.html#prometheus_counter_is_cumulative)
    would display roughly the same rate line as the Atlas equivalent, depending on
    what you select as the range vector (the time period in the `[]`).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 其他监控系统期望将累积值发送到监控系统，并在查询时包含某种速率函数。 [示例 4-4](part0009_split_008.html#prometheus_counter_is_cumulative)
    将显示与 Atlas 相似的速率线，具体取决于您选择的范围向量（`[]` 中的时间段）。
- en: Example 4-4\. Prometheus counters are cumulative, so we need to explicitly convert
    them to a rate
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-4\. Prometheus 计数器是累积的，因此我们需要显式将其转换为速率
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'There is one problem with the Prometheus rate function: when new tag values
    are added rapidly inside a chart’s time domain, the Prometheus rate function can
    generate a NaN value as opposed to a zero. In [Figure 4-17](part0009_split_008.html#prometheus_rate_zero_fill),
    we are charting Gradle build task throughput over time. Since in this window,
    build tasks are uniquely described by project and task name, and once a task is
    complete it isn’t incremented again, several new time series are coming into existence
    inside of the time domain we’ve selected for the chart.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 速率函数存在一个问题：当图表时间域内快速添加新的标签值时，Prometheus 速率函数可能生成 NaN 值，而不是零。在 [图 4-17](part0009_split_008.html#prometheus_rate_zero_fill)
    中，我们绘制了随时间变化的 Gradle 构建任务吞吐量。由于在此窗口中，构建任务由项目和任务名称唯一描述，并且一旦任务完成，它就不会再递增，因此在图表选择的时间域内会产生几个新的时间序列。
- en: '![srej 0417](../images/00097.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0417](../images/00097.png)'
- en: Figure 4-17\. Zero-filling Prometheus counter rates when new tag values are
    coming into existence inside the time domain
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-17\. 当图表时间域内快速添加新的标签值时，填充 Prometheus 计数器速率为零
- en: The query in [Example 4-5](part0009_split_008.html#prometheus_rate_zero_fill_query)
    shows the method we can use to zero-fill the gaps.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 4-5](part0009_split_008.html#prometheus_rate_zero_fill_query) 中的查询显示了我们可以用来填补间隙的方法。
- en: Example 4-5\. The query to zero-filling Prometheus counter rates
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-5\. 将 Prometheus 计数器速率填零的查询
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How to chart counters varies a bit from monitoring system to monitoring system.
    Sometimes we have to explicitly create rates, and sometimes counters are stored
    as rates up front. Timers have even more options.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如何绘制计数器图表在不同的监控系统中略有不同。有时我们必须明确创建速率，有时计数器从一开始就存储为速率。计时器甚至有更多选项。
- en: Timers
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计时器
- en: A `Timer` Micrometer meter generates a variety of different time series with
    one operation. Wrapping a block of code with a timer (`timer.record(() -> { ...
    })`) is enough to collect data on throughput through this block, maximum latency
    (decaying over time), the total sum of latency, and optionally other distribution
    statistics like histograms, percentiles, and SLO boundaries.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`Timer` Micrometer 米生成各种不同的时间序列，只需一个操作即可。用计时器包装代码块（`timer.record(() -> { ...
    })`）就足以收集有关此块的吞吐量数据，最大延迟（随时间衰减），总延迟总和，以及可选的其他分布统计数据，如直方图、百分位数和 SLO 边界。'
- en: On dashboards, latency is the most important to view, because it is most directly
    tied to user experience. After all, users care mostly about the performance of
    *their* individual requests. They care little to nothing about the total throughput
    the system is capable of, except indirectly to the extent that at a certain throughput
    level their response time is affected.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在仪表板上，延迟是最重要的要查看的，因为它与用户体验直接相关。毕竟，用户主要关心*他们*的个别请求的性能。他们对系统能够达到的总吞吐量几乎没有关心，除非在某种程度上，某个吞吐量水平会影响其响应时间。
- en: Secondarily, throughput can be included if there is an expectation of a certain
    shape to traffic (which may be periodic based on business hours, customer time
    zones, etc.). For example, a sharp decline in throughput during an expected peak
    period can be a strong indicator of a systemic problem where traffic that should
    be reaching the system is not.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 次要地，如果预期流量有一定的形状（可能是基于业务时间、客户时区等的周期性流量），可以包括吞吐量。例如，在预期高峰期间吞吐量的急剧下降可能是系统问题的强有力指标，即本应到达系统的流量未到达系统。
- en: For many cases, it is best to set alerts on maximum latency (in this case meaning
    maximum observed for each interval) and use high-percentile approximations like
    the 99th percentile for comparative analysis (see [“Automated Canary Analysis”](part0010_split_013.html#9H5VC-2d714b853a094e9a910510217e0e3d73)).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多情况，最好将警报设置在最大延迟上（在这种情况下，意味着每个间隔的最大观察值），并使用高百分位数近似值进行比较分析（见[“自动金丝雀分析”](part0010_split_013.html#9H5VC-2d714b853a094e9a910510217e0e3d73)）。
- en: Set Timer Alerts on Maximum Latency
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在最大延迟上设置计时器警报
- en: It is exceedingly common in Java applications for maximum timings to be an order
    of magnitude worse than the 99th percentile. It is best to set your alerts on
    maximum latency.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Java 应用程序中，最大延迟常常比第 99 百分位差一个数量级。最好将警报设置在最大延迟上。
- en: I didn’t discover the importance of even measuring maximum latency until after
    I left Netflix and was introduced to a [compelling argument](https://oreil.ly/ErwIo)
    by Gil Tene for alerting on maximum latency. He makes a particularly visceral
    point about worst case, drawing an analogy to pacemaker performance and emphasizing
    that “‘your heart will keep beating 99.9% of the time’ is not reassuring.” Always
    a sucker for a well-reasoned argument, I added maximum latency as a key statistic
    shipped by Micrometer `Timer` and `DistributionSummary` implementations just in
    time for the SpringOne conference in 2017\. There I met a former colleague from
    Netflix and sheepishly suggested this new idea, conscious of the fact that Netflix
    wasn’t actually monitoring max latency. He immediately laughed off the idea and
    left for a talk, leaving me a bit deflated. A short while later, I got a message
    from him with the chart shown in [Figure 4-18](part0009_split_010.html#max_p99_logging_service),
    showing max latency an order of magnitude worse than P99 on a key internal Netflix
    service (which he had gone and added max to as a quick experiment to test this
    hypothesis).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 直到我离开 Netflix 并由 Gil Tene 引入了一个[有力的论点](https://oreil.ly/ErwIo)，我才发现甚至衡量最大延迟的重要性。他对最坏情况做了一个特别深刻的观点，类比起搏器的性能，并强调“‘你的心脏将在99.9%的时间内保持跳动’并不令人放心”。作为一个喜欢有理有据的论点的人，我及时在2017年的SpringOne会议上将最大延迟作为由Micrometer
    `Timer`和`DistributionSummary`实现的关键统计数据推出。在那里，我遇到了一个来自 Netflix 的前同事，并羞怯地提出了这个新想法，意识到
    Netflix 实际上并没有监控最大延迟。他立即笑掉，走了一场演讲，让我有些泄气。不久之后，我收到了他的消息，附上了图表，显示了一项关键内部 Netflix
    服务上最大延迟比 P99 延迟差一个数量级（他仅仅为了测试这一假设而将最大延迟添加到了这个服务中）。
- en: '![srej 0418](../images/00037.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0418](../images/00037.png)'
- en: Figure 4-18\. Max versus P99 in a Netflix logging service (in nanoseconds)
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-18\. Netflix 日志服务中的最大与 P99 延迟（单位：纳秒）
- en: 'Even more amazing, Netflix had recently undergone an architectural shift that
    made P99 a little bit better but made max substantially worse! It’s easy to argue
    it was actually worse off for having made the change. I cherish the memory of
    this interaction because it is such an acute illustration of how every organization
    has something it can learn from another: in this case a highly sophisticated monitoring
    culture at Netflix learned a trick from Domo which in turn learned it from Azul
    Systems.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 更令人惊讶的是，Netflix 最近经历了一次架构转变，使得 P99 稍微好了一点，但最大延迟显著恶化！很容易辩论说实际上这次变动使事情变得更糟。我珍视这段互动的记忆，因为它生动地说明了每个组织都有可以从其他地方学习的东西：在这种情况下，Netflix
    的高度复杂的监控文化从 Domo 那里学到了一个技巧，而 Domo 则是从 Azul Systems 那里学到的。
- en: In [Figure 4-19](part0009_split_010.html#max_vs_p99_latency), we see the order-of-magnitude
    difference between maximum and 99th percentile. Response latency tends to be tightly
    packed around the 99th percentile with at least one separate grouping near the
    maximum reflective of garbage collection, VM pauses, etc.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 4-19](part0009_split_010.html#max_vs_p99_latency) 中，我们看到了最大和第99百分位数之间数量级的差异。响应延迟通常紧密围绕第99百分位数，至少有一个独立的分组接近最大值，反映了垃圾收集、虚拟机暂停等。
- en: '![A comparison of maximum and P99 latency](../images/00114.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![最大延迟和 P99 延迟的比较](../images/00114.png)'
- en: Figure 4-19\. Maximum versus P99 latency
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-19\. 最大与 P99 延迟
- en: In [Figure 4-20](part0009_split_010.html#avg_vs_p99_latency), a real-world service
    is exhibiting the characteristic that the average floats above the 99th percentile
    because requests are so densely packed around the 99th.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 4-20](part0009_split_010.html#avg_vs_p99_latency) 中，一个真实的服务展示了一个特征，即平均值浮动在第99百分位数之上，因为请求在第99百分位数周围非常密集。
- en: '![srej 0420](../images/00110.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0420](../images/00110.png)'
- en: Figure 4-20\. Average versus P99 latency
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-20\. 平均延迟对比 P99 延迟
- en: As insignificant as this top 1% may seem, real users are affected by these latencies,
    so it is important to recognize where that boundary is and compensate for it where
    needed. One recognized approach to limiting the effect of the top 1% is a client-side
    load-balancing strategy called hedge requests (see [“Hedge Requests”](part0012_split_014.html#hedge_requests)).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个前1%看起来微不足道，但实际用户会受到这些延迟的影响，因此重要的是要认识到这个边界，并在需要时进行补偿。识别限制前1%效果的一种认可方法是一种称为“对冲请求”的客户端负载平衡策略（参见[“对冲请求”](part0012_split_014.html#hedge_requests)）。
- en: Setting an alert on max latency is key (we’ll talk more about why in [“Latency”](part0009_split_017.html#8ILV9-2d714b853a094e9a910510217e0e3d73)).
    But once an engineer has been alerted to a problem, the dashboard that they use
    to start understanding the problem doesn’t necessarily need to have this indicator
    on it. It would be far more useful to see the distribution of latencies as a heatmap
    (as shown in [Figure 4-21](part0009_split_010.html#timer_heatmap)), which would
    include a nonzero bucket where the max is that caused the alert, to see how significant
    the problem is relative to the normative request coming through the system at
    that time. In a heatmap visualization, each vertical column represents a histogram
    (refer to [“Histograms”](part0007_split_007.html#6LKA8-2d714b853a094e9a910510217e0e3d73)
    for a definition) at a particular time slice. The colored boxes represent the
    frequency of latencies that are in a range of times defined on the *y*-axis. So
    the normative latency an end user is experiencing should look “hot” and outliers
    look cooler.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对最大延迟设置警报至关重要（我们将在[“延迟”](part0009_split_017.html#8ILV9-2d714b853a094e9a910510217e0e3d73)中更详细地讨论原因）。但是，一旦工程师收到问题的警报，他们用于开始理解问题的仪表板不一定需要有这个指标。看到延迟分布作为热度图会更有用（如[图4-21](part0009_split_010.html#timer_heatmap)所示），其中包括导致警报的最大值所在的非零桶，以了解问题相对于该时间系统中传递的规范请求的重要程度。在热度图可视化中，每个垂直列代表一个特定时间切片上的直方图（参考[“直方图”](part0007_split_007.html#6LKA8-2d714b853a094e9a910510217e0e3d73)的定义）。彩色方框表示在*y*轴上定义的时间范围内的延迟频率。因此，终端用户正在经历的规范延迟应该看起来“热”，而异常值则看起来较“冷”。
- en: '![srej 0421](../images/00094.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0421](../images/00094.png)'
- en: Figure 4-21\. A timer heatmap
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-21\. 计时器热度图
- en: Are most requests failing close to the max value, or are there just one or a
    few stray outliers? The answer to this question likely affects how quickly an
    alerted engineer escalates the issue and brings others in to help. There’s no
    need to plot both the max and heatmap on a diagnostic dashboard, as shown in [Figure 4-22](part0009_split_010.html#max_vs_heatmap).
    Just include the heatmap.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数请求是否接近最大值而失败，还是只有一个或几个偏离值？这个问题的答案可能会影响警报工程师升级问题并寻求他人帮助的速度。不需要在诊断仪表板上绘制最大值和热度图，如[图4-22](part0009_split_010.html#max_vs_heatmap)所示。只需包含热度图即可。
- en: '![srej 0422](../images/00077.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0422](../images/00077.png)'
- en: Figure 4-22\. Max latency versus heatmap of latency distribution
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-22\. 最大延迟与延迟分布热度图
- en: The latency heatmap is also expensive to draw, since it involves retrieving
    potentially dozens or hundreds of buckets (which are individual time series in
    the monitoring system) for each time slice on the chart, for a total that often
    amounts to thousands of time series. This reinforces the idea that there’s no
    reason to have this chart auto-updating on a prominent display somewhere hanging
    on a wall. Allow the alerting system to do its job and view the dashboard as needed
    to limit the load on the monitoring system.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟热度图的绘制也很昂贵，因为它涉及检索每个时间切片上可能的几十个或几百个桶（这些桶是监控系统中的单独时间序列），总量通常达到成千上万个时间序列。这强调了在墙上醒目的显示屏上自动更新此图表没有理由的观点。允许警报系统完成其工作，并根据需要查看仪表板，以限制对监控系统的负载。
- en: The toolbox of useful representations has now grown to the point that a word
    of caution is necessary.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 有用表示工具箱现在已经发展到需要谨慎使用的程度。
- en: When to Stop Creating Dashboards
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时停止创建仪表板
- en: I visited with a former colleague of mine, now VP of operations at Datadog,
    in 2019\. He lamented that, ironically, a lack of healthy moderation in dashboards
    built by customers is one of the key capacity problems confronting Datadog. Imagine
    legions of computer screens and TV displays arrayed around the world, each automatically
    refreshing at prescribed intervals a series of charts that look nice. I found
    this to be such a fascinating business problem, because clearly lots of TV displays
    showing Datadog branding improve the visibility and stickiness of the product
    while simultaneously producing an operational nightmare for a SaaS.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我在2019年拜访了一位曾经的同事，他现在是Datadog的运营副总裁。他感叹道，具有讽刺意味的是，由客户构建的仪表板缺乏健康的调节是Datadog面临的关键容量问题之一。想象一下，世界各地布满了计算机屏幕和电视显示器，每个都自动以规定的间隔刷新一系列漂亮的图表。我发现这是一个非常迷人的业务问题，因为显然，大量显示Datadog品牌的电视显示器可以提高产品的可见性和吸引力，同时又给SaaS产品带来了运营噩梦。
- en: I’ve always found the “mission control” dashboard view a bit of a curiosity.
    After all, what is it about a chart that visually indicates to me a problem? If
    it’s a sharp spike, a deep trough, or simply a value that has crept above all
    reasonable expectation, then an alert threshold can be created to define where
    that point of unacceptability is, and the metric can be monitored automatically
    (and around the clock).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我总是觉得“任务控制”仪表板视图有点奇怪。毕竟，图表上的什么视觉指示我存在问题？如果是急剧的尖峰、深谷，或者简单地超出所有合理预期的数值，那么可以创建一个警报阈值来定义不可接受的点，可以自动监视指标（全天候）。
- en: As an on-call engineer, it’s nice to receive an alert with an instantaneous
    visualization of the indicator (or a link to one). Ultimately, when we open alerts,
    we want to dig for information to discover a root cause (or sometimes determine
    that the alert isn’t worth paying attention to). If the alert links to a dashboard,
    ideally that dashboard is configured in such a way as to allow immediate dimensional
    explosion or exploration. In other words, the TV display dashboard treats humans
    as a sort of low-attention span, notoriously unreliable alerting system.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 作为值班工程师，收到带有即时指标可视化的警报（或指向其的链接）是件好事。最终，当我们打开警报时，我们希望深入挖掘信息，找出根本原因（或有时确定警报不值得关注）。如果警报链接到一个仪表板，理想情况下，该仪表板应配置成允许立即展开或探索维度。换句话说，电视显示仪表板把人类视为一种低注意力跨度、众所周知不可靠的警报系统。
- en: 'The visualizations useful for alerting may not be useful to include on a dashboard
    at all, and not all charts on a dashboard are possible to build alerts on. For
    example, [Figure 4-22](part0009_split_010.html#max_vs_heatmap) shows two representations
    of the same timer: a decaying max and a heatmap. The alerting system is going
    to watch max, but when an engineer is alerted to the anomalous condition, it’s
    much more useful to see the distribution of latencies around that time to know
    how severe the impact was (and the max should be captured in a latency bucket
    that’s visible on the heatmap).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 用于警报的可视化可能在仪表板上一点用也没有，并非所有仪表板上的图表都可以构建警报。例如，[图4-22](part0009_split_010.html#max_vs_heatmap)展示了同一个计时器的两种表示方式：衰减的最大值和热图。警报系统将观察最大值，但当工程师被警报到异常情况时，看到该时间点周围延迟的分布更有用，以了解影响的严重程度（最大值应该被捕获在热图上可见的延迟桶中）。
- en: However, be careful about how you construct these queries! If you look closely
    you will see that there is no latency around 15 ms on the heatmap. The Prometheus
    range vector in this case was too close to the scrape interval, and the resulting
    momentary gap in the chart that is invisible hides the 15 ms latency! Since Micrometer
    decays max, we still see it on the max chart.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，要注意构建这些查询的方式！如果你仔细观察，你会发现热图周围没有15毫秒的延迟。在这种情况下，Prometheus的范围向量太接近抓取间隔，结果是图表中短暂的不可见间隙隐藏了15毫秒的延迟！由于Micrometer在最大图表上衰减，我们仍然可以在最大图表上看到它。
- en: Heatmaps are also much more computationally expensive to render than a simple
    max line. For one chart this is fine, but add up this cost across many individual
    displays across business units in a large organization and this can be taxing
    on the monitoring system itself.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 热图的计算成本要比简单的最大线条高得多。对于一个图表来说这没问题，但是在大型组织的各个业务单位中多次展示，这可能会对监控系统本身造成负担。
- en: Charts aren’t a substitute for alerts. Focus first on delivering them as alerts
    to the right people when they stray from acceptable levels rather than rushing
    to set up monitors.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图表不能替代警报。首先专注于在超出可接受水平时将其作为警报交付给合适的人员，而不是匆忙设置监视器。
- en: Tip
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: A human constantly watching a monitor is just an expensive alerting system polling
    visually for unacceptable levels.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 人类不断地盯着监视器，这只是一个昂贵的视觉警报系统，用来视觉检测不可接受的水平。
- en: Alerts should be delivered to on-call personnel in such a way that they can
    quickly jump to a dashboard and start drilling down on the failing metric dimensionally
    to reason about where the problem is.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 警报应该以一种方式传递给值班人员，使他们能够快速跳转到仪表板，并开始针对失败的指标维度进行深入分析，找出问题所在。
- en: Not every alert or violation of an SLO needs be treated as a stop-the-world
    emergency.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每一个 SLO 的警报或违反情况都需要被视为停机紧急情况。
- en: Service Level Indicators for Every Java Microservice
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 每个 Java 微服务的服务水平指标
- en: Now that we have a sense of how to visually present SLIs on charts, we will
    turn our focus to the indicators you can add. They are presented in approximately
    the order of importance. So if you are following the incrementalist approach to
    adding charts and alerts, implement these in sequence.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了如何在图表上直观地呈现SLI，我们将把注意力转向您可以添加的指标。它们以大致的重要性顺序呈现。因此，如果您正在遵循逐步添加图表和警报的方法，请按顺序实施这些操作。
- en: Errors
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 错误
- en: When timing a block of code it’s useful to differentiate between successful
    and unsuccessful operations for two reasons.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在计时一段代码块时，区分成功和不成功的操作是有两个原因的。
- en: First, we can directly use the ratio of unsuccessful to total timings as a measure
    of the frequency of errors occurring in the system.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以直接使用失败与总计时的比率作为系统中错误发生频率的衡量标准。
- en: Also, successful and unsuccessful outcomes can have radically different response
    times, depending on the failure mode. For example, a `NullPointerException` resulting
    from making a bad assumption about the presence of some data in request input
    can fail early in a request handler. It then doesn’t get far enough to call other
    downstream services, interact with the database, etc., where the majority of time
    is spent when a request is successful. In this case, unsuccessful requests that
    fail in this way will skew our perspective on the latency of the system. Latency
    will in fact appear better than it actually is! On the other hand, a request handler
    that makes a blocking downstream request to another microservice that is under
    duress and for which the response ultimately times out may exhibit a much higher-than-normal
    latency (something close to the timeout on the HTTP client making the call). By
    not segregating errors, we present an overly pessimistic view of the latency of
    our system.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，成功和失败的结果在响应时间上可能有根本不同的差异，这取决于失败模式。例如，由于对请求输入中某些数据的存在做出了错误假设而导致的`NullPointerException`可能会在请求处理程序中早期失败。然后，它并没有足够的进展来调用其他下游服务，与数据库交互等等，而当请求成功时，系统的大部分时间都会花在这些地方。在这种情况下，以这种方式失败的不成功请求将扭曲我们对系统延迟的看法。延迟实际上会显得比实际情况要好！另一方面，使对另一个微服务进行阻塞下游请求的请求处理程序最终超时的微服务可能表现出比正常情况高得多的延迟（接近进行调用的HTTP客户端的超时）。通过不区分错误，我们呈现了对系统延迟的过度悲观的看法。
- en: Status tags (recall [“Naming Metrics”](part0006_split_007.html#5N3ND-2d714b853a094e9a910510217e0e3d73))
    should be added to timing instrumentation in most cases on two levels.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 状态标签（参见[“命名指标”](part0006_split_007.html#5N3ND-2d714b853a094e9a910510217e0e3d73)）在大多数情况下应该分两个级别添加到计时器中。
- en: Status
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 状态
- en: A tag that provides a detailed error code, exception name, or some other specific
    indicator of the failure mode
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 提供失败模式的详细错误代码、异常名称或其他特定指标的标签
- en: Outcome
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 结果
- en: A tag that provides a more course-grained error category that separates success,
    user-caused error, and service-caused error
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 提供更粗粒度的错误类别的标签，将成功、用户引起的错误和服务引起的错误分开
- en: When writing alerts, rather than trying to select a tag by matching a status
    code pattern (e.g., using Prometheus’s not-regex tag selector for `status !~"2.."`),
    it is preferable to perform an exact match on the outcome tag (`outcome="SERVER_ERROR"`).
    By selecting “not 2xx,” we are grouping server errors, like the common HTTP 500
    Internal Server Error, with errors caused by the user, like HTTP 400 Bad Request
    or HTTP 403 Forbidden. A high rate of HTTP 400s may indicate that you recently
    released code that contained an accidental backward incompatibility in an API,
    or it could indicate that a new end user (e.g., some other upstream microservice)
    is trying to onboard onto using your service and hasn’t gotten the payload right
    yet.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写警报时，与其尝试通过匹配状态代码模式（例如，使用Prometheus的非正则表达式标签选择器进行`status !~"2.."`）来选择标签，不如在结果标签上执行精确匹配（`outcome="SERVER_ERROR"`）。通过选择“非2xx”，我们将服务器错误（如常见的HTTP
    500内部服务器错误）与用户引起的错误（如HTTP 400错误的请求或HTTP 403禁止）分组。HTTP 400的高速率可能表明您最近发布的代码包含API中的意外向后不兼容性，或者可能表明新的终端用户（例如，其他上游微服务）正在尝试开始使用您的服务并且尚未正确传递有效载荷。
- en: Panera Faced Chatty Alerts Not Distinguishing Client from Server Errors
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Panera面临的唠叨警报未能区分客户端和服务器错误
- en: Panera Bread, Inc., faced an overly chatty alert from an anomaly detector implemented
    by its monitoring system vendor for HTTP errors. It caused several email alerts
    in one day because a single user provided the wrong password five times. Engineers
    discovered that the anomaly detector didn’t differentiate between client and server
    error ratio! Alerts on client error ratio might be nice for intrusion detection,
    but the threshold would be much higher than server error ratio (and certainly
    higher than five errors in a short period of time).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Panera Bread, Inc. 面临其监控系统供应商实现的异常检测器的过度活跃警报，用于 HTTP 错误。因为单个用户五次提供错误密码，导致一天中收到多封电子邮件警报。工程师们发现异常检测器未区分客户端和服务器错误比率！客户端错误比率的警报可能对入侵检测很有用，但阈值会比服务器错误比率高得多（当然比短时间内的五次错误高得多）。
- en: An HTTP 500 basically always is your fault as a service owner and needs attention.
    At best, an HTTP 500 shines a spotlight on where more up-front validation could
    have instead yielded a useful HTTP 400 back to the end user. I think “HTTP 500—Internal
    Server Error” is too passive. Something like “HTTP 500—Sorry, It’s My Fault” feels
    better.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP 500 几乎总是服务所有者的责任，需要关注。在最好的情况下，HTTP 500 表明哪里可以进行更多的前端验证，而不是直接给终端用户一个有用的
    HTTP 400。我认为“HTTP 500—Internal Server Error”太被动了。类似“HTTP 500—对不起，这是我的错”听起来更好。
- en: When you are writing your own timers, a common pattern involves using a `Timer`
    sample and deferring the determination of tags until it’s known whether the request
    will succeed or fail, as in [Example 4-6](part0009_split_014.html#timer_sample_exception).
    The sample holds the state of the time that the operation started for you.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当你编写自己的定时器时，常见的模式涉及使用 `Timer` 示例，并推迟标签的确定，直到已知请求是否成功或失败，例如在 [Example 4-6](part0009_split_014.html#timer_sample_exception)
    中。该示例保存了操作开始的时间状态。
- en: Example 4-6\. Determining an error and outcome tag dynamically based on the
    result of an operation
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 4-6\. 根据操作结果动态确定错误和结果标签
- en: '[PRE5]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](../images/00112.png)](part0009_split_014.html#co_charting_and_alerting_CO1-1)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](../images/00112.png)](part0009_split_014.html#co_charting_and_alerting_CO1-1)'
- en: Some monitoring systems like Prometheus expect a consistent set of tag keys
    to appear on metrics with the same name. So even though there is no exception
    here, we should tag it with some placeholder value like “none” to mirror what
    tags are present in the failure cases as well.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一些监控系统（如 Prometheus）期望在具有相同名称的度量标准上出现一致的标签键集。因此，即使在这里没有异常，我们也应该使用一些占位符值（如“none”），以反映在失败情况下存在的标签。
- en: '[![2](../images/00059.png)](part0009_split_014.html#co_charting_and_alerting_CO1-2)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](../images/00059.png)](part0009_split_014.html#co_charting_and_alerting_CO1-2)'
- en: Perhaps you have a way of better cataloging the failure conditions and can provide
    a more descriptive tag value here, but even adding the exception class name can
    go a long way toward understanding what *kinds* of failures there are. `NullPointerException`
    is a very different type of exception than a poorly handled connection timeout
    on a call to a downstream service. When error ratio spikes, it’s useful to be
    able to drill down on the exception name to get a brief glimpse at what the error
    is. From this exception name, you can hop over to your debuggability observability
    tools like logs and search for occurrences of the exception name around the time
    of the alert condition.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 或许你有更好的方法来更好地分类失败条件，并提供一个更具描述性的标签值，但即使添加异常类名也可以大大增加对失败类型的理解。`NullPointerException`和调用下游服务时出现的连接超时处理不当是两种非常不同的异常类型。当错误比例上升时，能够深入了解异常名称对于快速了解错误非常有用。通过异常名称，你可以快速转到调试和观测工具，如日志，并搜索在警报条件发生时异常名称的出现情况。
- en: Be Careful with Class.getSimpleName(), etc., as a Tag Value
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在使用 `Class.getSimpleName()` 等作为标签值时要小心
- en: Be aware of the fact that `Class.getSimpleName()` and `Class.getCanonicalName()`
    can return null or empty values, for example in the case of anonymous class instances.
    If you use one of them as a tag value, at least null/empty check the value and
    fall back on `Class.getName()`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要注意，`Class.getSimpleName()` 和 `Class.getCanonicalName()` 可能返回 null 或空值，例如匿名类实例的情况。如果你将它们之一作为标签值使用，至少要对空值进行检查，并回退到使用
    `Class.getName()`。
- en: For HTTP request metrics, for example, Spring Boot automatically tags `http.server.requests`
    with a `status` tag indicating the HTTP status code and an `outcome` tag that
    is one of `SUCCESS`, `CLIENT_ERROR`, or `SERVER_ERROR`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于HTTP请求指标，例如，Spring Boot会自动使用`status`标签表示HTTP状态码，并且使用`outcome`标签表示`SUCCESS`、`CLIENT_ERROR`或`SERVER_ERROR`之一。
- en: Based on this tag, it is possible to plot the error *rate* per interval. Error
    rate is difficult to establish an alert threshold for, because it can fluctuate
    wildly under the same failure conditions, depending on how much traffic is coming
    through the system.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此标签，可以绘制每个时间间隔的错误 *率*。错误率在相同的失败条件下可能会剧烈波动，具体取决于系统的流量量。
- en: For Atlas, use the `:and` operator to select only `SERVER_ERROR` outcomes, as
    shown in [Example 4-7](part0009_split_015.html#http_error_rate_atlas).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Atlas，请使用`:and`运算符仅选择`SERVER_ERROR`结果，如[示例 4-7](part0009_split_015.html#http_error_rate_atlas)所示。
- en: Example 4-7\. Error rate of HTTP server requests in Atlas
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-7\. Atlas中HTTP服务器请求的错误率
- en: '[PRE6]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For Prometheus, use a tag selector, as shown in [Example 4-8](part0009_split_015.html#http_error_rate_prometheus).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Prometheus，请使用标签选择器，如[示例 4-8](part0009_split_015.html#http_error_rate_prometheus)所示。
- en: Example 4-8\. Error rate of HTTP server requests in Prometheus
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-8\. Prometheus中HTTP服务器请求的错误率
- en: '[PRE7]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If every 10th request fails, and 100 requests/second are coming through the
    system, then the error rate is 10 failures/second. If 1,000 requests/second are
    coming through the system, the error rate climbs to 100 failures/second! In both
    cases, the error *ratio* relative to throughput is 10%. This error ratio normalizes
    the rate and is easy to set a fixed threshold for. In [Figure 4-23](part0009_split_015.html#error_ratio_vs_rate),
    the error ratio hovers around 10–15% in spite of the fact that throughput, and
    therefore error rate, spikes.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每10个请求中有一个失败，并且系统每秒处理100个请求，则错误率为每秒10次失败。如果系统每秒处理1,000个请求，则错误率上升到每秒100次失败！在这两种情况下，相对于吞吐量的错误
    *比率* 为10%。此错误比率对速率进行了标准化，并且易于设置固定阈值。在[图 4-23](part0009_split_015.html#error_ratio_vs_rate)中，尽管吞吐量和因此错误率激增，错误比率仍在10-15%左右。
- en: '![srej 0423](../images/00010.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0423](../images/00010.png)'
- en: Figure 4-23\. Error ratio versus error rate
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-23\. 错误比率与错误率
- en: The coarse-grained outcome tag is used to construct queries that represent the
    error ratio of the timed operation. In the case of `http.server.requests`, this
    is the ratio of `SERVER_ERROR` to the total number of requests.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 粗粒度的outcome标签用于构建代表定时操作错误比率的查询。对于`http.server.requests`，这是`SERVER_ERROR`与总请求数的比率。
- en: For Atlas, use the `:div` function to divide `SERVER_ERROR` outcomes by the
    total count of all requests, as shown in [Example 4-9](part0009_split_015.html#http_error_ratio_atlas).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Atlas，请使用`:div`函数，将`SERVER_ERROR`结果按所有请求的总数进行划分，如[示例 4-9](part0009_split_015.html#http_error_ratio_atlas)所示。
- en: Example 4-9\. Error ratio of HTTP server requests in Atlas
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-9\. Atlas中HTTP服务器请求的错误比率
- en: '[PRE8]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For Prometheus, use the `/` operator similarly, as in [Example 4-10](part0009_split_015.html#http_error_ratio_prometheus).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Prometheus，请类似地使用`/`运算符，如[示例 4-10](part0009_split_015.html#http_error_ratio_prometheus)所示。
- en: Example 4-10\. Error ratio of HTTP server requests in Prometheus
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-10\. Prometheus中HTTP服务器请求的错误比率
- en: '[PRE9]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Error Rate Is Better Than Error Ratio for Low-Throughput Services
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对于低吞吐量服务，错误率比错误比率更好
- en: In general, prefer error ratio to error rate, *unless* the endpoint has a very
    low throughput. In this case, even a small difference in errors can lead to wild
    shifts in the error ratio. It is more appropriate in these situations to pick
    a fixed error rate threshold.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，除非端点的吞吐量非常低，否则更倾向于使用错误比率，*除非*。在这种情况下，即使错误微小差异也可能导致错误比率发生剧烈变化。在这些情况下，选择一个固定的错误率阈值更为合适。
- en: Error rate and ratio are just one view of a timer. Latency is the other essential
    view.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 错误率和比率只是计时器的一种视图。延迟是另一个重要视角。
- en: Latency
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 延迟
- en: Alert on maximum latency (in this case meaning maximum observed for each interval),
    and use high-percentile approximations like the 99th percentile for comparative
    analysis, as shown in [“Automated Canary Analysis”](part0010_split_013.html#9H5VC-2d714b853a094e9a910510217e0e3d73).
    Popular Java web frameworks, as part of their “white box” (see [“Black Box Versus
    White Box Monitoring”](part0006_split_001.html#blackbox_whitebox)) autoconfiguration
    of metrics, offer instrumentation of inbound and outbound requests with rich tags.
    I’ll present details of Spring Boot’s automatic instrumentation of requests, but
    most other popular Java web frameworks have done something very similar with Micrometer.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在此情况下，对最大延迟（每个间隔内观察到的最大值）进行警报，并使用高百分位数（例如第99百分位数）进行比较分析，如在 [“自动金丝雀分析”](part0010_split_013.html#9H5VC-2d714b853a094e9a910510217e0e3d73)
    中所示。流行的 Java Web 框架作为其“白盒”（参见 [“黑盒与白盒监控”](part0006_split_001.html#blackbox_whitebox)）自动配置指标的一部分，通过丰富的标签对入站和出站请求进行仪表化。我将介绍
    Spring Boot 对请求的自动仪表化的详细信息，但大多数其他流行的 Java Web 框架与 Micrometer 类似做了某种非常类似的事情。
- en: Server (inbound) requests
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务器（入站）请求
- en: Spring Boot autoconfigures a timer metric called `http.server.requests` for
    both blocking and reactive REST endpoints. If the latency of a particular endpoint(s)
    is a key indicator of the performance of an application and it will also be used
    for comparative analysis, then add the `management.metrics.distribution.percentiles-histogram.http.server.requests=true`
    property to your `application.properties` to export percentile histograms from
    your application. To be more fine-grained about enabling percentile histograms
    for a particular set of API endpoints, you can add the `@Timed` annotation in
    Spring Boot, like in [Example 4-11](part0009_split_017.html#timed_histogram).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Boot 自动配置了一个名为 `http.server.requests` 的计时器指标，用于阻塞和响应式 REST 端点。如果特定端点的延迟是应用性能的关键指标，并且还将用于比较分析，则可以将
    `management.metrics.distribution.percentiles-histogram.http.server.requests=true`
    属性添加到您的 `application.properties` 中，以从您的应用程序导出百分位直方图。要更精细地启用特定一组 API 端点的百分位直方图，您可以像在
    [示例 4-11](part0009_split_017.html#timed_histogram) 中那样，在 Spring Boot 中添加 `@Timed`
    注解。
- en: Example 4-11\. Using @Timed to add histograms to just a single endpoint
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-11\. 使用 @Timed 为单个端点添加直方图
- en: '[PRE10]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Alternatively, you can add a `MeterFilter` that responds to a tag, as shown
    in [Example 4-12](part0009_split_017.html#meter_filter_histogram).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以添加一个响应标签的 `MeterFilter`，如 [示例 4-12](part0009_split_017.html#meter_filter_histogram)
    所示。
- en: Example 4-12\. A MeterFilter that adds percentile histograms for certain endpoints
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-12\. 一个 MeterFilter，为特定端点添加百分位直方图
- en: '[PRE11]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For Atlas, [Example 4-13](part0009_split_017.html#atlas_latency) shows how to
    compare max latency against some predetermined threshold.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Atlas，[示例 4-13](part0009_split_017.html#atlas_latency) 展示了如何将最大延迟与预定的阈值进行比较。
- en: Example 4-13\. Atlas max API latency
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-13\. Atlas 最大 API 延迟
- en: '[PRE12]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: For Prometheus, [Example 4-14](part0009_split_017.html#prometheus_latency) is
    a simple comparison.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Prometheus，[示例 4-14](part0009_split_017.html#prometheus_latency) 是一个简单的比较。
- en: Example 4-14\. Prometheus max API latency
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-14\. Prometheus 最大 API 延迟
- en: '[PRE13]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The tags that are added to `http.server.requests` are customizable. For the
    blocking Spring WebMVC model, use a `WebMvcTagsProvider`. For example, we could
    extract information about the browser and its version from the “User-Agent” request
    header, as shown in [Example 4-15](part0009_split_017.html#webmvc_tags). This
    sample uses the MIT-licensed [Browscap](https://oreil.ly/mkLpG) library to extract
    browser information from the user-agent header.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 可以自定义添加到 `http.server.requests` 的标签。对于阻塞的 Spring WebMVC 模型，请使用 `WebMvcTagsProvider`。例如，我们可以从“User-Agent”请求头中提取有关浏览器及其版本的信息，如
    [示例 4-15](part0009_split_017.html#webmvc_tags) 所示。此示例使用了 MIT 许可的 [Browscap](https://oreil.ly/mkLpG)
    库来从用户代理标头中提取浏览器信息。
- en: Example 4-15\. Adding browser tags to Spring WebMVC metrics
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-15\. 将浏览器标签添加到 Spring WebMVC 指标中
- en: '[PRE14]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For Spring WebFlux (the nonblocking reactive model), configure a `WebFluxTagsProvider`
    similarly, as in [Example 4-16](part0009_split_017.html#webflux_tags).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Spring WebFlux（非阻塞响应式模型），可以类似地配置 `WebFluxTagsProvider`，如 [示例 4-16](part0009_split_017.html#webflux_tags)
    所示。
- en: Example 4-16\. Adding browser tags to Spring WebFlux metrics
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-16\. 将浏览器标签添加到 Spring WebFlux 指标中
- en: '[PRE15]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note that the `http.server.requests` timer only begins timing a request once
    it is being processed by the service. If the request thread pool is routinely
    at capacity, requests from users are sitting in the thread pool waiting to be
    handled, and this elapse of time is very real to the user waiting for a response.
    The missing information in `http.server.requests` is one example of a larger problem
    first described by Gil Tene called coordinated omission (see [“Coordinated Omission”](part0007_split_014.html#coordinated_omission)),
    which comes in several other forms.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`http.server.requests` 计时器仅在服务处理请求时开始计时。如果请求线程池经常处于容量限制状态，则用户的请求会在线程池中等待处理，这段时间对于等待响应的用户来说是非常真实的。`http.server.requests`
    中缺失的信息是 Gil Tene 首次描述的一个更大问题的示例，称为协调省略（见[“协调省略”](part0007_split_014.html#coordinated_omission)），还有其他几种形式。
- en: It is also useful to monitor latency from the perspective of the caller (client).
    In this case, by client I generally mean service-to-service callers and not human
    consumers to your API gateway or first service interaction. A service’s view of
    its own latency doesn’t include the effects of network delays or thread pool contention
    (e.g., Tomcat’s request thread pool or the thread pool of a proxy like Nginx).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 从调用方（客户端）的视角监控延迟也很有用。在这种情况下，我通常指的是服务对服务的调用者，而不是人类消费者对您的 API 网关或第一个服务交互。服务对其自身延迟的视图不包括网络延迟或线程池争用的影响（例如
    Tomcat 的请求线程池或像 Nginx 这样的代理的线程池）。
- en: Client (outbound) requests
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 客户端（出站）请求
- en: Spring Boot also autoconfigures a timer metric called `http.client.requests`
    for both blocking and reactive *outbound* calls. This allows you to instead (or
    also) monitor a service’s latency from the perspective of all of its callers,
    provided they each make the same conclusion about what the name of the called
    service is. [Figure 4-24](part0009_split_017.html#http_client_metrics) shows three
    service instances calling the same service.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Boot 还会为阻塞和响应式 *出站* 调用自动配置一个名为 `http.client.requests` 的计时器指标。这使您可以从所有调用者的视角监控服务的延迟，只要它们每个都对所调用服务的名称达成相同的结论。[图
    4-24](part0009_split_017.html#http_client_metrics) 显示了三个服务实例调用同一服务的情况。
- en: '![srej 0424](../images/00091.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0424](../images/00091.png)'
- en: Figure 4-24\. HTTP client metrics from multiple callers
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-24\. 多个调用方的 HTTP 客户端指标
- en: We can identify the performance of a particular endpoint for the called service
    by selecting on the `uri` and `serviceName` tags. By aggregating over all other
    tags, we see the performance of the endpoint across all callers. Dimensionally
    drilling down by the `clientName` tag would show the service’s performance from
    just that client’s perspective. Even if the called service processes every request
    in the same amount of time, the client perspective could vary (e.g., if one client
    is deployed in a different zone or region). Where there is the possibility for
    this variance between clients, you can use something like Prometheus’s `topk`
    query to compare against an alert threshold so that the totality of the experience
    of an endpoint’s performance for all clients doesn’t wash away the outlier for
    some particular client, as shown in [Example 4-17](part0009_split_017.html#max_client_latency_by_client).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择 `uri` 和 `serviceName` 标签，我们可以确定被调服务特定端点的性能。通过对所有其他标签进行聚合，我们可以查看端点在所有调用者之间的性能。通过
    `clientName` 标签进行维度下钻，可以显示服务从某个客户端的视角看到的性能。即使被调服务每次请求都以相同的时间处理，客户端视角也可能不同（例如，如果一个客户端部署在不同的区域或地域）。在客户端之间存在这种差异可能性的情况下，您可以使用类似
    Prometheus 的 `topk` 查询来与警报阈值进行比较，以确保所有客户端对端点性能的整体体验不会因某些特定客户端的异常情况而被淹没，如[示例 4-17](part0009_split_017.html#max_client_latency_by_client)所示。
- en: Example 4-17\. Max outbound request latency by client name
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-17\. 按客户端名称的最大出站请求延迟
- en: '[PRE16]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: To autoconfigure HTTP client instrumentation for Spring’s `RestTemplate` (blocking)
    and `WebClient` (nonblocking) interfaces, you do need to treat path variables
    and request parameters a certain way. Specifically, you have to let the implementations
    do path variable and request parameter substitution for you rather than using
    string concatenation or a similar technique to construct a path, as shown in [Example 4-18](part0009_split_017.html#rest_template_uri_tagging).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 要为 Spring 的 `RestTemplate`（阻塞）和 `WebClient`（非阻塞）接口自动配置 HTTP 客户端仪表化，您需要以特定的方式处理路径变量和请求参数。具体来说，您必须让实现为您执行路径变量和请求参数的替换，而不是使用字符串串联或类似技术来构建路径，如[示例
    4-18](part0009_split_017.html#rest_template_uri_tagging)所示。
- en: Example 4-18\. Allowing RestTemplate to handle path variable substitution
  id: totrans-220
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-18\. 允许 RestTemplate 处理路径变量替换
- en: '[PRE17]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](../images/00112.png)](part0009_split_017.html#co_charting_and_alerting_CO2-1)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](../images/00112.png)](part0009_split_017.html#co_charting_and_alerting_CO2-1)'
- en: Sounds nefarious?
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很险恶？
- en: '[![2](../images/00059.png)](part0009_split_017.html#co_charting_and_alerting_CO2-2)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](../images/00059.png)](part0009_split_017.html#co_charting_and_alerting_CO2-2)'
- en: To take advantage of Spring Boot’s autoconfiguration of `RestTemplate` metrics,
    ensure that you are creating any custom bean wirings for `RestTemplateBuilder`
    and not `RestTemplate` (and note that Spring also provides a `RestTemplateBuilder`
    for you automatically with the defaults via autoconfiguration). Spring Boot attaches
    an additional metrics interceptor to any such beans that it finds. Once the `RestTemplate`
    is created, it is too late for this configuration to take place.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要利用 Spring Boot 自动配置的 `RestTemplate` 指标，确保为 `RestTemplateBuilder` 创建任何自定义 bean
    绑定，而不是 `RestTemplate`（请注意，Spring 也通过自动配置为您提供了 `RestTemplateBuilder` 的默认实例）。Spring
    Boot 会向它发现的任何这类 bean 附加额外的指标拦截器。一旦创建了 `RestTemplate`，对此配置的应用就太晚了。
- en: The idea is that the `uri` tag should still contain the requested path with
    path variables *pre-substitution* so that you can reason about the total number
    and latency of requests going to that endpoint regardless of what particular values
    were being looked up. Also, this is essential to controlling the total number
    of tags that the `http.client.requests` metrics contains. Allowing unbounded growth
    in unique tags would eventually overwhelm the monitoring system (or get really
    expensive for you if the monitoring system vendor charges by time series).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 思路是 `uri` 标签仍应包含带有路径变量的请求路径 *预替换*，这样您就可以理解到达该端点的请求总数和延迟，而不管正在查找的特定值是什么。此外，这对于控制
    `http.client.requests` 指标包含的标签总数至关重要。允许唯一标签的不受限增长最终会超出监控系统的能力（或者如果监控系统供应商按时间序列计费，这会变得非常昂贵）。
- en: The equivalent for the nonblocking `WebClient` is shown in [Example 4-19](part0009_split_017.html#8IMH3-2d714b853a094e9a910510217e0e3d73).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 非阻塞 `WebClient` 的等效操作在 [示例 4-19](part0009_split_017.html#8IMH3-2d714b853a094e9a910510217e0e3d73)
    中展示。
- en: Example 4-19\. Allowing WebClient to handle path variable substitution
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-19\. 允许 WebClient 处理路径变量替换
- en: '[PRE18]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](../images/00112.png)](part0009_split_017.html#co_charting_and_alerting_CO3-1)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](../images/00112.png)](part0009_split_017.html#co_charting_and_alerting_CO3-1)'
- en: Sounds nefarious?
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很险恶？
- en: '[![2](../images/00059.png)](part0009_split_017.html#co_charting_and_alerting_CO3-2)'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](../images/00059.png)](part0009_split_017.html#co_charting_and_alerting_CO3-2)'
- en: Make sure you are creating bean wirings for `WebClient.Builder` and not `WebClient`.
    Spring Boot attaches an additional metrics `WebClientCustomizer` to the builder,
    not the completed `WebClient` instance.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 确保为 `WebClient.Builder` 创建 bean 绑定，而不是 `WebClient`。Spring Boot 将附加额外的度量 `WebClientCustomizer`
    到构建器上，而不是完成的 `WebClient` 实例。
- en: While the default set of tags that Spring Boot adds to client metrics is reasonably
    complete, it is customizable. It is especially common to tag metrics with the
    value of some request header (or response header). Be sure when you add tag customizations
    that the total number of possible tag values is well bounded. You shouldn’t add
    tags for things like unique customer ID (when you can have more than maybe 1,000
    customers), a randomly generated request ID, etc. Remember, the purpose of metrics
    is to get an idea of aggregate performance, not the performance of some individual
    request.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Spring Boot 添加到客户端指标的默认标签集合相当完整，但是它是可自定义的。特别常见的是将指标与某些请求头（或响应头）的值进行标记。在添加标记自定义时，请确保可能的标签值总数是有界的。不应为诸如唯一客户
    ID（当您可能有超过 1,000 个客户时）、随机生成的请求 ID 等添加标签。记住，指标的目的是了解聚合性能，而不是某个单独请求的性能。
- en: As a slightly different example than the one we used in `http.server.requests`
    tag customization earlier, we could additionally tag the retrievals of customers
    by their subscription level, where subscription level is a response header on
    the retrieval of a customer by ID. By doing so, we could chart the latency and
    error ratio of the retrieval of premium customers versus basic customers separately.
    Perhaps the business places a higher level of expectation on the reliability or
    performance of requests to premium customers, manifesting in a tighter service
    level agreement based on this custom tag.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 作为与我们之前在 `http.server.requests` 标签自定义中使用的稍微不同的示例，我们还可以按订阅级别对客户的检索进行标记，其中订阅级别是在按
    ID 检索客户时的响应头。这样，我们可以分别绘制高级客户和基础客户的检索延迟和错误比例。也许业务对向高级客户发送请求的可靠性或性能有更高的期望，这体现在基于这个自定义标签的更紧密的服务水平协议中。
- en: To customize tags for `RestTemplate`, add your own `@Bean RestTemplateExchangeTagsProvider`,
    as shown in [Example 4-20](part0009_split_017.html#rest_template_tags_provider).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 要自定义 `RestTemplate` 的标签，添加你自己的 `@Bean RestTemplateExchangeTagsProvider`，如 [示例
    4-20](part0009_split_017.html#rest_template_tags_provider) 所示。
- en: Example 4-20\. Allowing RestTemplate to handle path variable substitution
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-20\. 允许 RestTemplate 处理路径变量替换
- en: '[PRE19]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](../images/00112.png)](part0009_split_017.html#co_charting_and_alerting_CO4-1)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](../images/00112.png)](part0009_split_017.html#co_charting_and_alerting_CO4-1)'
- en: Beware that `response.getHeaders().get("subscription")` can potentially return
    `null`! So whether we use `get` or `getFirst`, we need to `null` check somehow.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `response.getHeaders().get("subscription")` 可能会返回 `null`！所以无论我们使用 `get` 还是
    `getFirst`，我们都需要进行 `null` 检查。
- en: To customize tags for `WebClient`, add your own `@Bean WebClientExchangeTagsProvider`,
    as shown in [Example 4-21](part0009_split_017.html#web_client_tags_provider).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 要自定义 `WebClient` 的标签，添加你自己的 `@Bean WebClientExchangeTagsProvider`，如 [示例 4-21](part0009_split_017.html#web_client_tags_provider)
    所示。
- en: Example 4-21\. Allowing WebClient to handle path variable substitution
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-21\. 允许 WebClient 处理路径变量替换
- en: '[PRE20]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: To this point we’ve focused on latency and errors. Now let’s consider a common
    saturation measurement related to memory consumption.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直关注延迟和错误。现在让我们考虑与内存消耗相关的常见饱和度测量。
- en: Garbage Collection Pause Times
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 垃圾收集暂停时间
- en: Garbage collection (GC) pauses often delay the delivery of a response to a user
    request, and they can be a bellwether of an impending “out of memory” application
    failure. There are a few ways we can look at this indicator.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 垃圾收集（GC）暂停通常会延迟响应用户请求的交付，它们可能是即将发生的“内存不足”应用程序故障的信号。有几种方法可以查看这个指标。
- en: Max pause time
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大暂停时间
- en: Set a fixed alert threshold on the maximum GC pause time you find acceptable
    (knowing that a GC pause directly contributes to end-user response time as well),
    potentially selecting different thresholds for minor and major GC types. Plot
    the max from the `jvm.gc.pause` timer to set your thresholds, as shown in [Figure 4-25](part0009_split_018.html#gc_pause_times).
    A heatmap of pause times may also be interesting if your application undergoes
    frequent pauses and you want to understand what typical behavior looks like over
    time.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 为你认为可接受的最大 GC 暂停时间设置一个固定的警报阈值（知道垃圾收集暂停直接影响到最终用户的响应时间），可能为小型和大型 GC 类型选择不同的阈值。绘制
    `jvm.gc.pause` 定时器的最大值以设置你的阈值，如 [图 4-25](part0009_split_018.html#gc_pause_times)
    所示。如果你的应用程序经常暂停，并且你想了解随时间变化的典型行为，暂停时间的热图也可能很有趣。
- en: '![srej 0425](../images/00012.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0425](../images/00012.png)'
- en: Figure 4-25\. Max garbage collection pause times
  id: totrans-250
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-25\. 最大垃圾收集暂停时间
- en: Proportion of time spent in garbage collection
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 垃圾收集中花费的时间比例
- en: Since `jvm.gc.pause` is a timer, we can look at its sum independently. Specifically,
    we can add the increases in this sum over an interval of time and divide it by
    the interval to determine what proportion of the time the CPU is engaged in doing
    garbage collection. And since our Java process does nothing else during these
    times, when a significant enough proportion of time is spent in GC, an alert is
    warranted. [Example 4-22](part0009_split_018.html#prometheus_time_spent_in_gc)
    shows the Prometheus query for this technique.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `jvm.gc.pause` 是一个定时器，我们可以单独查看其总和。具体来说，我们可以在一个时间间隔内将其增加的值加起来，然后除以该时间间隔，来确定
    CPU 在进行垃圾收集时花费了多少时间。由于在这些时间段内我们的 Java 进程什么都不做，当垃圾收集所花费的时间比例足够大时，应该发出警报。[示例 4-22](part0009_split_018.html#prometheus_time_spent_in_gc)
    显示了这个技术的 Prometheus 查询。
- en: Example 4-22\. Prometheus query for time spent in garbage collection by cause
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-22\. 根据原因查询 Prometheus 中的垃圾收集时间
- en: '[PRE21]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](../images/00112.png)](part0009_split_018.html#co_charting_and_alerting_CO5-1)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](../images/00112.png)](part0009_split_018.html#co_charting_and_alerting_CO5-1)'
- en: Sums over all the individual causes, like “end of minor GC.”
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有单独的原因进行求和，比如“小 GC 结束”。
- en: '[![2](../images/00059.png)](part0009_split_018.html#co_charting_and_alerting_CO5-2)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](../images/00059.png)](part0009_split_018.html#co_charting_and_alerting_CO5-2)'
- en: The total time spent in an individual cause in the last minute.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去一分钟内在个别原因中所花费的总时间。
- en: '[![3](../images/00067.png)](part0009_split_018.html#co_charting_and_alerting_CO5-3)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](../images/00067.png)](part0009_split_018.html#co_charting_and_alerting_CO5-3)'
- en: This is the first time we’ve seen a Prometheus [subquery](https://oreil.ly/34AJs).
    It allows us to treat the operation on the two indicators as a range vector for
    input into `sum_over_time`.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们第一次看到 Prometheus 的 [子查询](https://oreil.ly/34AJs)。它允许我们将两个指标的操作视为 `sum_over_time`
    的输入的范围向量。
- en: '[![4](../images/00016.png)](part0009_split_018.html#co_charting_and_alerting_CO5-4)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](../images/00016.png)](part0009_split_018.html#co_charting_and_alerting_CO5-4)'
- en: Since `jvm_gc_pause_seconds_sum` has a unit of seconds (and therefore so do
    the sums) and we’ve summed over a 1-minute period, divide by 60 seconds to arrive
    at a percentage in the range [0, 1] of the time we’ve spent in GC in the last
    minute.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `jvm_gc_pause_seconds_sum` 的单位是秒（因此总和也是秒），我们已经对一个 1 分钟的时间段进行了求和，将其除以 60 秒，得到在最近一分钟内我们在
    GC 中所花费的时间的百分比，其值在 [0, 1] 范围内。
- en: This technique is flexible. You can use a tag to select particular GC causes
    and evaluate, for example, only the proportion of time spent in major GC events.
    Or, like we’ve done here, you can simply sum over all the causes and reason about
    overall GC time in a given interval. More than likely, you’re going to find that
    if you do separate these sums by cause, minor GC events don’t contribute that
    significantly to the proportion of time spent in GC. The app being monitored in
    [Figure 4-26](part0009_split_018.html#time_spent_in_gc) was undergoing minor collections
    every minute and, unsurprisingly, it still only spent 0.0182% of its time in GC-related
    activities.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术是灵活的。您可以使用标签选择特定的 GC 原因并评估，例如，仅评估在主要 GC 事件中所占时间的比例。或者，就像我们在这里所做的那样，您可以简单地对所有原因进行求和，并评估给定时间间隔内的总体
    GC 时间。很可能，您会发现，如果按原因分别计算这些总和，小 GC 事件对 GC 时间所占比例的贡献并不显著。在 [图 4-26](part0009_split_018.html#time_spent_in_gc)
    中监控的应用程序每分钟进行一次小集合，毫不奇怪，它在与 GC 相关的活动中仅花费了 0.0182% 的时间。
- en: '![srej 0426](../images/00002.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0426](../images/00002.png)'
- en: Figure 4-26\. The proportion of time spent in minor GC events
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-26\. 在小 GC 事件中所占时间的比例
- en: If you aren’t using a monitoring system that provides aggregation functions
    like `sum_over_time`, Micrometer provides a meter binder called `JvmHeapPressureMetrics`,
    shown in [Example 4-23](part0009_split_018.html#configuring_jvm_heap_pressure_binder),
    that precomputes this GC overhead and ships a gauge called `jvm.gc.overhead` that
    is a percentage in the range [0, 1] that you can then set a fixed threshold alert
    against. In a Spring Boot app, you can simply add an instance of `JvmHeapPressureMetrics`
    as a `@Bean` and it will be bound to your meter registries automatically.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有使用提供像`sum_over_time`这样的聚合函数的监控系统，Micrometer 提供了一个称为`JvmHeapPressureMetrics`的计量器绑定器，如
    [示例 4-23](part0009_split_018.html#configuring_jvm_heap_pressure_binder) 所示，预先计算了这种
    GC 开销，并提供了一个称为`jvm.gc.overhead`的仪表，其值为 [0, 1] 范围内的百分比，您可以设置一个固定的阈值警报。在 Spring
    Boot 应用中，您只需将一个`JvmHeapPressureMetrics`实例添加为`@Bean`，它将自动绑定到您的计量注册表。
- en: Example 4-23\. Configuring the JVM heap pressure meter binder
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-23\. 配置 JVM 堆压力计量器绑定器
- en: '[PRE22]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[![1](../images/00112.png)](part0009_split_018.html#co_charting_and_alerting_CO6-1)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](../images/00112.png)](part0009_split_018.html#co_charting_and_alerting_CO6-1)'
- en: Controls the lookback window.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 控制回顾窗口。
- en: The presence of any humongous allocation
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 任何庞大分配的存在
- en: In addition to choosing one of the above forms for monitoring time spent in
    GC, it’s also a good idea to set an alert on the presence of humongous allocation
    that GC causes in the G1 collector, because it indicates that somewhere in your
    code you are allocating an object >50% of the total size of the Eden space! More
    than likely, there is a way to refactor the application to avoid such an allocation
    by chunking or streaming data. A humongous allocation could occur while doing
    something like parsing an input or retrieving an object from a datastore that
    is not yet as big as the application could theoretically see, and a larger object
    very well could bring the application down.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择上述一种形式来监视 GC 中花费的时间外，还最好在 G1 收集器中设置一个警报，以便在 G1 收集器中引起巨大分配时发出警报，因为这表明在代码中的某个地方你正在分配一个
    >50% 的总 Eden 空间大小的对象！很可能，有一种方法可以重构应用程序以避免这样的分配，比如分块或流式处理数据。巨大的分配可能发生在诸如解析输入或从尚未达到应用程序可能看到的大小的数据存储中检索对象等操作中，并且更大的对象很可能会使应用程序崩溃。
- en: For this, specifically, you are looking for a nonzero count for `jvm.gc.pause`
    where the `cause` tag is equal to `G1 Humongous Allocation`.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一点，具体来说，你要查找 `jvm.gc.pause` 计数不为零的地方，其中 `cause` 标签等于 `G1 Humongous Allocation`。
- en: Way back in [“Monitoring for Availability”](part0005_split_003.html#4OIT5-2d714b853a094e9a910510217e0e3d73),
    we mentioned that saturation metrics are usually preferable to utilization metrics
    when you have a choice between the two. This is certainly true of memory consumption.
    The views of time spent in garbage collection as a measure of memory resource
    problems are easier to get right. There are some interesting things we can do
    with utilization measurements, too, if we’re careful.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“可用性监控”](part0005_split_003.html#4OIT5-2d714b853a094e9a910510217e0e3d73)中我们提到，当你在这两者之间有选择时，饱和度指标通常优于利用率指标。对于内存消耗来说，这当然是正确的。将垃圾回收中花费的时间视为内存资源问题的衡量标准更容易做到。如果我们小心的话，利用率测量也可以做一些有趣的事情。
- en: Heap Utilization
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 堆利用率
- en: 'The Java heap is separated into several pools with each pool having a defined
    size. Java object instances are created in heap space. The most important parts
    of the heap are as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Java 堆被分为多个池，每个池都有一个定义的大小。Java 对象实例是在堆空间中创建的。堆的最重要部分如下：
- en: Eden space (young generation)
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: Eden 空间（年轻代）
- en: All new objects are allocated here. A minor garbage collection event occurs
    when this space is filled.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的新对象都在这里分配。当这个空间填满时，会发生一次次要的垃圾回收事件。
- en: Survivor space
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 幸存者空间
- en: When a minor garbage collection occurs, any live objects (that demonstrably
    still have references and therefore cannot be collected) are copied to the survivor
    space. Objects that reach the survivor space have their age incremented, and after
    an age threshold is met, are promoted to old generation. Promotion may happen
    prematurely if the survivor space cannot hold all of the live objects in young
    generation (objects skip the survivor space and go straight to old generation).
    This last fact will be key in how we measure dangerous levels of allocation pressure.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 当发生次要的垃圾回收时，任何活动对象（可以明确证明仍有引用因此不能被收集的对象）都会被复制到幸存者空间。进入幸存者空间的对象会增加它们的年龄，并且在达到年龄阈值后，会提升到老年代。如果幸存者空间无法容纳年轻代中的所有活动对象（对象跳过幸存者空间直接进入老年代），提升可能会过早发生。这个事实将是我们如何衡量危险分配压力的关键。
- en: Old generation
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 老年代
- en: This is where long-surviving objects are stored. When objects are stored in
    the Eden space, an age for that object is set; and when it reaches that age, the
    object is moved to old generation.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 这是存储长期存活对象的地方。当对象存储在 Eden 空间时，会设置该对象的年龄；当达到该年龄时，该对象将被移到老年代。
- en: Fundamentally, we want to know when one or more of these spaces is getting and
    *staying* too “full.” This is a tricky thing to monitor because JVM garbage collection
    by design kicks in as spaces get full. So having a space fill up isn’t itself
    an indicator of a problem. What is concerning is when it stays full.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 根本上，我们想知道这些空间中的一个或多个何时变得过于“满”，并且*保持*太“满”。这是一个棘手的监控问题，因为 JVM 垃圾回收设计上会在空间变满时启动。因此，空间填满本身并不是问题的指标。令人担忧的是它保持满的情况。
- en: Micrometer’s `JvmMemoryMetrics` meter binder automatically collects JVM memory
    pool usage, along with the current total maximum heap size (since this can increase
    and decrease at runtime). Most Java web frameworks automatically configure this
    binder.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: Micrometer 的 `JvmMemoryMetrics` 米勒绑定器会自动收集 JVM 内存池使用情况，以及当前的总最大堆大小（因为这可以在运行时增加和减少）。大多数
    Java Web 框架会自动配置此绑定器。
- en: Several metrics are plotted in [Figure 4-27](part0009_split_019.html#memory_fixed_threshold).
    The most straightforward idea for how to measure heap pressure is to use a simple
    fixed threshold, such as a percentage of total heap consumed. As we can see, the
    fixed threshold alert will fire far too frequently. The earliest alert is triggered
    at 11:44, well before it is apparent that a memory leak is present in this application.
    Even though the heap temporarily exceeds the percentage-of-total-heap threshold
    we have set, garbage collection events routinely bring total consumption back
    under the threshold.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Figure 4-27](part0009_split_019.html#memory_fixed_threshold):'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: The solid vertical bars together are a stack graph of memory consumption by
    space.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The thin line around the 30.0 M level is the maximum heap space allowed. Notice
    how this fluctuates as the JVM attempts to pick the right value between initial
    heap size (`-Xms`) and max heap size (`-Xmx`) for the process.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bold line around 24.0 M level represents a fixed percentage of this maximum
    allowed memory. This is the threshold. It is a fixed threshold relative to the
    max, but dynamic in the sense that it is a percentage of the max which itself
    can fluctuate.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The lighter bars represent points where actual heap utilization (the top of
    the stack graph) exceeds the threshold. This is the “alert condition.”
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![The alarm fires frequently](../images/00075.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
- en: Figure 4-27\. An alert on memory utilization with a fixed threshold
  id: totrans-292
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So this simple fixed threshold won’t work. There are better options available,
    depending on the capabilities of your target monitoring system.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Rolling count occurrences of heap space filling up
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By using a feature like the rolling count function in Atlas, we can alert only
    when the heap exceeds the threshold—say, three out of the five prior intervals—indicating
    that in spite of the garbage collector’s best effort, heap consumption continues
    to be a problem (see [Figure 4-28](part0009_split_019.html#memory_rolling_count)).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, not many monitoring systems have a function like Atlas’s rolling
    count. Prometheus can do something like this with its `count_over_time` operation,
    but it is tricky to get a similar “three out of five” dynamic.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '![The alarm fires only when there is clearly a problem developing](../images/00072.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
- en: Figure 4-28\. Using rolling count to limit alert chattiness
  id: totrans-298
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There is an alternative approach that also works well.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Low pool memory after collection
  id: totrans-300
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Micrometer’s `JvmHeapPressureMetrics` adds a gauge `jvm.memory.usage.after.gc`
    for the percentage of Old Generation heap used after the last garbage collection
    event.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '`jvm.memory.usage.after.gc` is a percentage expressed in the range [0, 1].
    When it is high (a good starting alert threshold is greater than 90%), garbage
    collection isn’t able to sweep up much garbage. So long-term pause events which
    occur when Old Generation is swept can be expected to occur frequently, and frequent
    long-term pauses both significantly degrade the performance of the app and ultimately
    lead to `OutOfMemoryException` fatal errors.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '`jvm.memory.usage.after.gc` 是一个在范围 [0, 1] 中表示的百分比。当它很高时（一个良好的起始警报阈值大于90%），垃圾回收无法清理掉太多的垃圾。因此，当老年代被清理时会发生长期暂停事件，这些频繁的长期暂停显著降低了应用程序的性能，并最终导致致命的
    `OutOfMemoryException` 错误。'
- en: A subtle variation on measuring low pool memory after collection is also effective.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集后测量低内存池的微妙变化也是有效的。
- en: Low total memory
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 低总内存
- en: 'This technique involves mixing indicators from heap usage and garbage collection
    activity. A problem is indicated when they *both* exceed a threshold:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术涉及混合堆使用量和垃圾回收活动的指标。当它们*都*超过阈值时，就会指示出问题：
- en: '`jvm.gc.overhead` > 50%'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '`jvm.gc.overhead` > 50%'
- en: Notice that this is a lower alert threshold than suggested in [“Garbage Collection
    Pause Times”](part0009_split_018.html#8IMVJ-2d714b853a094e9a910510217e0e3d73)
    for the same indicator (where we suggested 90%). We can be more aggressive about
    this indicator because we are pairing it with a utilization indicator.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这比[“垃圾回收暂停时间”](part0009_split_018.html#8IMVJ-2d714b853a094e9a910510217e0e3d73)建议的相同指标更低的警报阈值（我们建议为90%）。我们可以更积极地对待这个指标，因为我们将其与利用率指标配对使用。
- en: '`jvm.memory.used/jvm.memory.max` > 90% at any time in the last 5 minutes'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '`jvm.memory.used/jvm.memory.max` 在过去 5 分钟的任何时间都 > 90%'
- en: Now we have an idea that GC overhead is going up because one or more of the
    pools keep filling up. You could constrain this to just the Old Generation pool
    as well if your app generates a lot of short-term garbage under normal circumstances.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道 GC 开销正在上升，因为一个或多个池继续填满。如果您的应用程序在正常情况下会生成大量短期垃圾，您也可以将其限制为仅老年代池。
- en: The alert criteria for the GC overhead indicator is a simple test against the
    gauge value.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: GC 开销指标的警报条件是针对测量值的简单测试。
- en: The query for total memory usage is a little less obvious. The Prometheus query
    is shown in [Example 4-24](part0009_split_019.html#prometheus_memory_used).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 总内存使用的查询略微不太明显。Prometheus 查询显示在[示例 4-24](part0009_split_019.html#prometheus_memory_used)中。
- en: Example 4-24\. Prometheus query for maximum memory used in the last five minutes
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-24\. 近五分钟内使用的最大内存的 Prometheus 查询
- en: '[PRE23]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: To understand better what `max_over_time` does, [Figure 4-29](part0009_split_019.html#prometheus_max_over_time)
    shows the total amount of Eden space (`jvm.memory.used{id="G1 Eden Space"}` in
    this case) consumed at several points in time (the dots) and the result of applying
    a one-minute `max_over_time` query to the same query (the solid line). It is a
    moving maximum window over a prescribed interval.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 要更好地理解 `max_over_time` 的作用，[图 4-29](part0009_split_019.html#prometheus_max_over_time)显示了在几个时间点消耗的伊甸空间总量（在本例中为
    `jvm.memory.used{id="G1 Eden Space"}` 的点）及应用一分钟 `max_over_time` 查询到相同查询的结果（实线）。它是一个在指定间隔内的移动最大窗口。
- en: As long as heap usage is going up (and hasn’t been below the current value in
    the lookback window), the `max_over_time` tracks it exactly. Once a garbage collection
    event happens, the current view of usage drops and `max_over_time` “sticks” at
    the higher value for the lookback window.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 只要堆使用量上升（并且在回看窗口中的当前值之下），`max_over_time`就会精确跟踪它。一旦发生垃圾回收事件，当前的使用视图下降，而`max_over_time`会在回看窗口的较高值上“粘滞”。
- en: '![srej 0429](../images/00093.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0429](../images/00093.png)'
- en: Figure 4-29\. Prometheus max_over_time looking at max Eden space used in a one-minute
    lookback
  id: totrans-317
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-29\. Prometheus 的 `max_over_time` 查看一个一分钟回溯窗口中最大的伊甸空间使用量
- en: This is also the first time we’ve considered an alert that is based on more
    than one condition. Alerting systems generally allow for the boolean combination
    of multiple criteria. In [Figure 4-30](part0009_split_019.html#low_total_memory_alert),
    assuming that the `jvm.gc.overhead` indicator represents Query A and the usage
    indicator represents Query B, an alert can be configured in Grafana on both together.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是我们第一次考虑基于多个条件的警报。警报系统通常允许多个标准的布尔组合。在[图 4-30](part0009_split_019.html#low_total_memory_alert)中，假设
    `jvm.gc.overhead` 指标表示查询 A，使用指标表示查询 B，则可以在 Grafana 中对它们一起配置警报。
- en: '![srej 0430](../images/00109.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0430](../images/00109.png)'
- en: Figure 4-30\. Configuring a Grafana alert based on two indicators for low total
    memory
  id: totrans-320
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-30\. 根据两个指标配置Grafana警报以用于低总内存
- en: Another common utilization measurement is CPU, which doesn’t have an easy saturation
    analog.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的利用率测量是CPU，它没有一个简单的饱和度类比。
- en: CPU Utilization
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPU利用率
- en: CPU usage is a common utilization alert to set, but unfortunately it is difficult
    to establish a general rule for what is a healthy amount of CPU because of the
    different programming models described below—this will have to be determined for
    each application, depending on its characteristics.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: CPU使用率是一个常见的利用率警报设置，但由于下文描述的不同编程模型的存在，很难建立一个健康的CPU使用量的一般规则——这将不得不针对每个应用程序根据其特性进行确定。
- en: For example, a typical Java microservice running on Tomcat and serving requests
    using a blocking servlet model will typically consume available threads in the
    Tomcat thread pool well before overutilizing the CPU. In these types of applications,
    high memory saturation is far more common (e.g., lots of excess garbage created
    in the handling of each request or large request/response bodies).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，运行在Tomcat上并使用阻塞Servlet模型提供请求的典型Java微服务在通常情况下会在耗尽Tomcat线程池中的可用线程之前过度利用CPU。在这些类型的应用程序中，高内存饱和度更为常见（例如，在处理每个请求或大请求/响应主体时创建大量垃圾）。
- en: A Java microservice running on Netty and using a reactive programming model
    all the way down will accept a much higher throughput per instance, and so CPU
    utilization tends to be much higher. In fact, better saturating available CPU
    resources is commonly cited as an advantage of the reactive programming model!
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 运行在Netty上并使用响应式编程模型的Java微服务每个实例都能接受更高的吞吐量，因此CPU利用率往往更高。事实上，更好地饱和可用CPU资源通常被引述为响应式编程模型的优势！
- en: On Some Platforms, Consider CPU and Memory Utilization Together Before Resizing
    Instances
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在某些平台上，在调整实例大小之前，请综合考虑CPU和内存利用率。
- en: A common feature of platform as a service is the simplification of instance
    sizing down to the amount of CPU or memory you desire with the other variable
    growing proportionally as you move up sizes. In the case of Cloud Foundry, this
    proportionality between CPU and memory was decided at a time when a blocking model
    of request handling like Tomcat was almost universal. As noted, CPU tends to be
    underused in this model. I once consulted at a company that had adopted a nonblocking
    reactive model for its application, and noticing that memory was significantly
    underutilized, I downsized the company’s Cloud Foundry instances to not consume
    as much memory. But CPU is allocated to instances on this platform proportionally
    to how much memory is requested. By picking a lower memory requirement, the company
    also inadvertently starved its reactive app of the CPU it would otherwise have
    so efficiently saturated!
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 平台即服务的一个常见特性是将实例大小简化为所需的CPU或内存量，而随着您增加大小，另一个变量会按比例增加。在Cloud Foundry的情况下，CPU和内存之间的这种比例关系是在一个几乎普遍使用阻塞请求处理模型（如Tomcat）的时代决定的。正如前面提到的，CPU在这种模型中往往被低估使用。我曾在一家公司做过咨询，他们采用了非阻塞的响应式模型用于应用程序，注意到内存被显著低估利用，我降低了公司的Cloud
    Foundry实例以减少对内存的消耗。但在这个平台上，CPU根据请求的内存量分配给实例。通过选择较低的内存要求，公司还意外地使其响应式应用程序失去了它本来可以高效饱和的CPU！
- en: Micrometer exports two key metrics for CPU monitoring, which are listed in [Table 4-1](part0009_split_021.html#processor_metrics).
    Both of these metrics are reported from Java’s operating system MXBean (`ManagementFactory.getOperatingSystemMXBean()`).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Micrometer导出了用于CPU监控的两个关键指标，这些指标在 [表 4-1](part0009_split_021.html#processor_metrics)
    中列出。这两个指标都是从Java的操作系统MXBean (`ManagementFactory.getOperatingSystemMXBean()`) 报告的。
- en: Table 4-1\. Micrometer reported processor metrics
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-1\. Micrometer报告的处理器指标
- en: '| Metric | Type | Description |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 指标 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| system.cpu.usage | Gauge | The recent CPU usage for the whole system |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| system.cpu.usage | Gauge | 整个系统的最近CPU使用率 |'
- en: '| process.cpu.usage | Gauge | The recent CPY usage for the Java virtual machine
    process |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| process.cpu.usage | Gauge | Java虚拟机进程的最近CPY使用率 |'
- en: For the most common case in the enterprise where an application is serving requests
    via a blocking servlet model, testing against a fixed threshold of 80% is reasonable.
    Reactive applications will need to be tested empirically to determine their appropriate
    saturation point.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 对于企业中应用程序通过阻塞Servlet模型提供请求的最常见情况，对80%的固定阈值进行测试是合理的。反应式应用程序需要通过实验确定它们适当的饱和点。
- en: For Atlas, use the `:gt` function, as shown in [Example 4-25](part0009_split_021.html#atlas_processor_metrics).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Atlas，使用`:gt`函数，如[示例 4-25](part0009_split_021.html#atlas_processor_metrics)中所示。
- en: Example 4-25\. Atlas CPU alert threshold
  id: totrans-336
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-25\. Atlas CPU警报阈值
- en: '[PRE24]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: For Prometheus, [Example 4-26](part0009_split_021.html#prometheus_processor_metrics)
    is just a comparison expression.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Prometheus，[示例 4-26](part0009_split_021.html#prometheus_processor_metrics)只是一个比较表达式。
- en: Example 4-26\. Prometheus CPU alert threshold
  id: totrans-339
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-26\. Prometheus CPU警报阈值
- en: '[PRE25]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Process CPU usage should be plotted as a percentage (where the monitoring system
    should expect an input in the range 0–1 to appropriately draw the *y*-axis). Take
    note of the *y*-axis in [Figure 4-31](part0009_split_021.html#process_cpu_usage)
    for what this should look like.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 进程CPU使用率应该绘制为百分比（监控系统应该期望在0–1的范围内输入以适当绘制*y*轴）。请注意[图 4-31](part0009_split_021.html#process_cpu_usage)中的*y*轴应该是什么样子。
- en: '![srej 0431](../images/00086.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0431](../images/00086.png)'
- en: Figure 4-31\. Process CPU usage as a percentage
  id: totrans-343
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-31\. 进程CPU使用率作为百分比
- en: In Grafana, “percent” is one of the units selectable in the “Visualization”
    tab. Make sure to select the option for “percent (0.0-1.0),” as shown in [Figure 4-32](part0009_split_021.html#grafana_percentage_unit).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在Grafana中，“percent”是“可视化”选项卡中可选择的单位之一。请确保选择“percent (0.0-1.0)”选项，如[图 4-32](part0009_split_021.html#grafana_percentage_unit)所示。
- en: '![srej 0432](../images/00106.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0432](../images/00106.png)'
- en: Figure 4-32\. Grafana percent unit
  id: totrans-346
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-32\. Grafana百分比单位
- en: There is one more resource-based indicator you should measure on every application
    related to file descriptors.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个与文件描述符相关的资源指标您应该在每个应用程序上测量。
- en: File Descriptors
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文件描述符
- en: The “ulimits” Unix feature limits how many resources a single user can use,
    including concurrently open file descriptors. File descriptors are not just consumed
    for file access, but also for network connections, database connections, etc.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: “ulimits” Unix特性限制单个用户可以使用的资源数量，包括同时打开的文件描述符。文件描述符不仅仅用于文件访问，还用于网络连接、数据库连接等。
- en: You can view your shell’s current ulimits with `ulimit -a`. The output is shown
    in [Example 4-27](part0009_split_022.html#ulimit_output). On many operating systems
    1,024 is the default limit for open file descriptors. Scenarios like each service
    request requiring access to read or write a file where the number of concurrent
    threads can exceed the operating system limit are vulnerable to this issue. Throughput
    in the thousands of simultaneous requests is not unreasonable for a modern microservice,
    especially a nonblocking one.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`ulimit -a`命令查看您的shell当前的ulimits。输出如[示例 4-27](part0009_split_022.html#ulimit_output)所示。在许多操作系统上，1,024是打开文件描述符的默认限制。像每个服务请求都需要访问读或写文件的情况，其中并发线程数可能会超过操作系统限制，这些情况容易受到这个问题的影响。对于现代微服务，特别是非阻塞微服务，同时数以千计的请求吞吐量并不是不合理的。
- en: Example 4-27\. Output of ulimit -a in a Unix shell
  id: totrans-351
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-27\. 在Unix shell中运行ulimit -a的输出
- en: '[PRE26]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[![1](../images/00112.png)](part0009_split_022.html#co_charting_and_alerting_CO7-1)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](../images/00112.png)](part0009_split_022.html#co_charting_and_alerting_CO7-1)'
- en: This represents the number of *allowable* open files, not the number of currently
    open files.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示*允许*打开文件的数量，而不是当前打开文件的数量。
- en: This problem isn’t necessarily common, but the impact of reaching the file descriptor
    limit can be fatal, causing the application to stop responding entirely, depending
    on how file descriptors are used. Unlike an out-of-memory error or fatal exception,
    often the application will simply block but appear to be in service still, so
    this problem is especially pernicious. Because monitoring file descriptor utilization
    is so cheap, alert on this on every application. Applications using common techniques
    and web frameworks will probably never exceed 5% file descriptor utilization (and
    sometimes much lower); but when a problem sneaks in, it is trouble.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题不一定常见，但达到文件描述符限制的影响可能是致命的，可能导致应用程序完全停止响应，具体取决于文件描述符的使用方式。与内存不足错误或致命异常不同，通常应用程序可能会简单地阻塞，但仍然显示为在服务中，因此这个问题特别棘手。由于监控文件描述符利用率非常廉价，在每个应用程序上都应发出警报。使用常见技术和Web框架的应用程序可能永远不会超过5%的文件描述符利用率（有时更低），但一旦问题悄然而至，就会带来麻烦。
- en: Experiencing the File Descriptor Problem While Writing This Book
  id: totrans-356
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在编写本书时遇到文件描述符问题
- en: I’ve known for some time to monitor this, but never actually experienced a problem
    myself until writing this book. A Go build step involved in building Grafana from
    source repeatedly hung, never completing. Evidently the Go dependency resolution
    mechanism doesn’t carefully limit the number of open file descriptors!
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我很久以来一直在监控这个问题，但直到写这本书之前从未亲身经历过问题。一个涉及构建 Grafana 的 Go 构建步骤反复挂起，永远无法完成。显然，Go
    依赖解析机制没有仔细限制开放文件描述符的数量！
- en: An application which may have sockets open to hundreds of callers, HTTP connections
    to downstream services, connections open to datasources, and data files open could
    hit the limit of file descriptors. When a process runs out of file descriptors,
    it tends not to end well. You may see errors in logs like [Example 4-28](part0009_split_023.html#tomcat_exhausted_file_descriptors)
    and [Example 4-29](part0009_split_023.html#java_exhausted_file_descriptors).
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序可能对数百个调用方开放套接字、向下游服务的 HTTP 连接、连接到数据源和打开的数据文件进行监控，可能会达到文件描述符的限制。当一个进程耗尽文件描述符时，通常情况并不好。你可能会在日志中看到类似
    [4-28](part0009_split_023.html#tomcat_exhausted_file_descriptors) 和 [4-29](part0009_split_023.html#java_exhausted_file_descriptors)
    的错误。
- en: Example 4-28\. Tomcat exhausted file descriptors accepting a new HTTP connection
  id: totrans-359
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-28\. Tomcat 因接受新的 HTTP 连接而耗尽文件描述符
- en: '[PRE27]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Example 4-29\. Java failing to open a file when file descriptors are exhausted
  id: totrans-361
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-29\. 当文件描述符耗尽时 Java 无法打开文件
- en: '[PRE28]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Micrometer reports two metrics shown in [Table 4-2](part0009_split_023.html#file_descriptor_metrics)
    to alert you to a file descriptor problem in your applications.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: Micrometer 报告了两个在 [表 4-2](part0009_split_023.html#file_descriptor_metrics) 中显示的指标，用于提醒您应用程序中的文件描述符问题。
- en: Table 4-2\. Micrometer-reported file descriptor metrics
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-2\. Micrometer 报告的文件描述符指标
- en: '| Metric | Type | Description |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| Metric | Type | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| process.max.fds | Gauge | Maximum allowable open file descriptors, correspondng
    to `ulimit -a` output |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| process.max.fds | Gauge | 最大允许的开放文件描述符，对应于 `ulimit -a` 输出 |'
- en: '| process.open.fds | Gauge | Number of open file descriptors |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| process.open.fds | Gauge | 开放文件描述符数量 |'
- en: Typically, open file descriptors should remain below the maximum, so a test
    against a fixed threshold like 80% is a good indicator of an impending problem.
    This alert should be set on *every application*, as file limits are a universally
    applicable hard limit that will take your application out of service.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，开放文件描述符应保持在最大值以下，因此对于像 80% 这样的固定阈值进行测试是预示即将发生问题的良好指标。此警报应设置在*每个应用程序*上，因为文件限制是一个普遍适用的硬限制，可能会使您的应用程序停止服务。
- en: For Atlas, use the `:div` and `:gt` functions, as shown in [Example 4-30](part0009_split_023.html#atlas_file_descriptors).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Atlas，使用 `:div` 和 `:gt` 函数，如示例 [4-30](part0009_split_023.html#atlas_file_descriptors)
    所示。
- en: Example 4-30\. Atlas file descriptor alert threshold
  id: totrans-371
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-30\. Atlas 文件描述符警戒阈值
- en: '[PRE29]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: For Prometheus, [Example 4-31](part0009_split_023.html#prometheus_file_descriptors)
    looks even simpler.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Prometheus，[4-31](part0009_split_023.html#prometheus_file_descriptors) 看起来更加简单。
- en: Example 4-31\. Prometheus file descriptor alert threshold
  id: totrans-374
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-31\. Prometheus 文件描述符警戒阈值
- en: '[PRE30]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: At this point, we’ve covered the signals that are applicable to most every Java
    microservice. The ones that follow are commonly useful, but not as ubiquitous.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经覆盖了适用于大多数 Java 微服务的信号。接下来的信号通常有用，但不是普遍存在的。
- en: Suspicious Traffic
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可疑流量
- en: One other simple indicator that can be derived from metrics like `http.server.requests`
    involves watching the occurrence of unusual status codes. A rapid succession of
    HTTP 403 Forbidden (and similar) or HTTP 404 Not Found may indicate an intrusion
    attempt.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个简单的指标，可以从类似 `http.server.requests` 的指标中推导出来，涉及观察异常状态码的出现。快速连续出现的 HTTP 403
    Forbidden（以及类似的）或 HTTP 404 Not Found 可能表明有入侵尝试。
- en: Unlike plotting errors, monitor total occurrences of a suspicious status code
    as a *rate* and not a ratio relative to total throughput. It’s probably safe to
    say that 10,000 HTTP 403s per second is equally suspicious if the system normally
    processes 15,000 requests per second or 15 million requests per second, so don’t
    let overall throughput hide the anomaly.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于绘制错误，监控可疑状态码的总出现次数应作为*速率*，而不是相对于总吞吐量的比率。可以说，每秒出现 10,000 次 HTTP 403 状态码同样可疑，无论系统正常处理每秒
    15,000 请求还是 15 百万请求，因此不要让整体吞吐量掩盖异常。
- en: The Atlas query in [Example 4-32](part0009_split_024.html#atlas_suspicious_403),
    is similar to the error rate query we discussed earlier, but looks at the `status`
    tag for more granularity than the `outcome` tag.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 4-32](part0009_split_024.html#atlas_suspicious_403)中的 Atlas 查询，类似于我们之前讨论的错误率查询，但查看
    `status` 标签比 `outcome` 标签具有更精细的粒度。
- en: Example 4-32\. Suspicious 403s in HTTP server requests in Atlas
  id: totrans-381
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-32\. Atlas 中 HTTP 服务器请求中的可疑 403 错误
- en: '[PRE31]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Use the Prometheus `rate` function to achieve the same result in Prometheus,
    as in [Example 4-33](part0009_split_024.html#prometheus_suspicious_403).
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Prometheus 中使用 `rate` 函数可以实现相同的结果，如在[示例 4-33](part0009_split_024.html#prometheus_suspicious_403)中所示。
- en: Example 4-33\. Suspicious 403s in HTTP server requests in Prometheus
  id: totrans-384
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-33\. Prometheus 中 HTTP 服务器请求中的可疑 403 错误
- en: '[PRE32]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The next indicator is specialized to a particular kind of application but is
    still common enough to include.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个指标是专门针对特定类型的应用程序，但仍然常见到足以包含在内。
- en: Batch Runs or Other Long-Running Tasks
  id: totrans-387
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理运行或其他长时间运行任务
- en: One of the biggest risks of any long-running task is that it runs for significantly
    longer than expected. Earlier in my career, I was routinely on call for production
    deployments, which were always performed after a series of midnight batch runs.
    Under normal circumstances, the batch sequence should have completed maybe at
    1:00 a.m. The deployment schedule was built around this assumption. So a network
    administrator manually uploading the deployed artifact (this is before [Chapter 5](part0010_split_000.html#9H5K4-2d714b853a094e9a910510217e0e3d73))
    needed to be at a computer ready to perform the task at 1:00 a.m. As the representative
    of the product’s engineering team, I needed to be ready to perform a brief smoke
    test at approximately 1:15 a.m. and be available to help remediate any issues
    that arose. At this time, I lived in a rural area without internet access, so
    I traveled along a state highway toward a population center until I could get
    a reliable enough cell signal to tether to my phone and connect to the VPN. When
    the batch processes didn’t complete in a reasonable amount of time, I sometimes
    spent hours sitting in my car on some country road waiting for them to complete.
    On days when production deployments weren’t happening, perhaps nobody knew that
    the batch cycle failed until the next business day.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 任何长时间运行任务的最大风险之一是其运行时间显著超过预期。在我早期的职业生涯中，我经常需要为生产部署值班，这些部署通常在一系列午夜批处理运行后执行。在正常情况下，批处理序列应该在凌晨1点左右完成。部署时间表是基于这一假设构建的。因此，网络管理员需要在1点准备好在计算机上手动上传已部署的工件（这是在[第5章](part0010_split_000.html#9H5K4-2d714b853a094e9a910510217e0e3d73)之前），以执行该任务。作为产品工程团队的代表，我需要在凌晨1:15左右进行简短的烟雾测试，并随时准备帮助解决任何出现的问题。那时，我住在一个没有互联网接入的农村地区，因此我沿着州际公路向人口中心前进，直到我能够获得足够可靠的手机信号来连接到我的电话和VPN。当批处理过程没有在合理的时间内完成时，有时我会在一些乡村道路上坐上几个小时等待它们完成。在没有生产部署的日子里，也许直到下一个工作日才会知道批处理周期失败了。
- en: If we wrap a long-running task in a Micrometer `Timer`, we won’t know that the
    SLO has been exceeded until the task actually completes. So if the task was supposed
    to take no more than 1 hour, but it actually runs for 16 hours, then we won’t
    see this appear on a monitoring chart until the first publishing interval *after*
    16 hours when the sample is recorded to the timer.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将一个长时间运行的任务包装在 Micrometer 的 `Timer` 中，我们在任务实际完成之前不会知道是否超出了 SLO。因此，如果任务应该不超过1小时，但实际运行了16小时，那么我们直到第一个发布间隔
    *之后* 16小时才会在监控图表上看到这一情况记录到定时器中。
- en: To monitor long-running tasks, it is better to look at the running time of in-flight
    or active tasks. `LongTaskTimer` performs this kind of measurement. We can add
    this kind of timing to a potentially long-running task, as in [Example 4-34](part0009_split_025.html#add_ltt).
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 要监控长时间运行的任务，最好查看正在运行的或活动任务的运行时间。`LongTaskTimer` 执行这种类型的测量。我们可以像在[示例 4-34](part0009_split_025.html#add_ltt)中那样，向潜在长时间运行的任务中添加这种定时。
- en: Example 4-34\. An annotation-based long task timer for a scheduled operation
  id: totrans-391
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-34\. 基于注解的计划操作的长任务定时器
- en: '[PRE33]'
  id: totrans-392
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Long task timers ship several distribution statistics: active task count, the
    maximum in-flight request duration, the sum of all in-flight request durations,
    and optionally percentile and histogram information about in-flight requests.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 长任务定时器提供几种分布统计数据：活动任务计数、最长的请求持续时间、所有请求持续时间的总和，以及关于请求持续时间的百分位数和直方图信息（可选）。
- en: For Atlas, test against our expectation of one hour in nanoseconds, as shown
    in [Example 4-35](part0009_split_025.html#atlas_ltt_max).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Atlas，根据我们对一小时的纳秒的预期进行测试，如[示例 4-35](part0009_split_025.html#atlas_ltt_max)所示。
- en: Example 4-35\. Atlas long task timer maximum alert threshold
  id: totrans-395
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-35\. Atlas 长任务定时器最大警报阈值
- en: '[PRE34]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[![1](../images/00112.png)](part0009_split_025.html#co_charting_and_alerting_CO8-1)'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](../images/00112.png)](part0009_split_025.html#co_charting_and_alerting_CO8-1)'
- en: One hour in nanoseconds
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 一小时的纳秒
- en: For Prometheus, [Example 4-36](part0009_split_025.html#prometheus_ltt_max) is
    tested against one hour in seconds.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Prometheus，[示例 4-36](part0009_split_025.html#prometheus_ltt_max) 被测试为一小时的秒数。
- en: Example 4-36\. Prometheus long task timer maximum alert threshold
  id: totrans-400
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-36\. Prometheus 长任务定时器最大警报阈值
- en: '[PRE35]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We’ve seen some examples of effective indicators to look at, and at this point
    hopefully you have one or more of them plotted on a dashboard and can see some
    meaningful insights. Next we turn to how to automate alerting when these indicators
    go awry so that you don’t have to watch your dashboards all the time to know when
    something isn’t right.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了一些有效的指标示例，希望您现在已经在仪表板上绘制了一个或多个指标，并且能够看到一些有意义的见解。接下来，我们将转向如何在这些指标出现异常时自动发出警报，以便您不必一直观察您的仪表板就知道有什么不对劲。
- en: Building Alerts Using Forecasting Methods
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预测方法构建警报
- en: Fixed alert thresholds are often difficult to determine *a priori*, and since
    system performance is subject to drift over time, can be something that continually
    needs to be retuned. If performance over time tends to decline (but in such a
    way that the decline is still within acceptable levels), then a fixed alert threshold
    can easily become too chatty. If performance tends to improve, then the threshold
    is no longer as reliable of a measure of expected performance unless it is tuned.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 固定的警报阈值通常很难*事先*确定，并且由于系统性能随时间漂移，可能需要不断进行调整。如果随着时间的推移，性能倾向于下降（但以一种仍在可接受水平内的方式），那么固定的警报阈值很容易变得太啰嗦。如果性能倾向于改善，那么该阈值将不再是可靠的预期性能度量，除非进行调整。
- en: 'Machine learning is the subject of a lot of hype that the monitoring system
    will automatically determine alert thresholds, but it hasn’t produced the promised
    results. For time series data, simpler classical statistical methods are still
    incredibly powerful. Surprisingly, the paper by S. Makridakis et al., [“Statistical
    and Machine Learning Forecasting Methods: Concerns and Ways Forward”](https://oreil.ly/J43UW),
    shows that statistical methods have a lower prediction error (as shown in [Figure 4-33](part0009_split_026.html#forecasting_error))
    than machine learning methods do.'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '机器学习是一个颇受吹捧的话题，监控系统将自动确定警报阈值，但它并没有产生承诺的结果。对于时间序列数据，更简单的经典统计方法仍然非常强大。令人惊讶的是，S.
    Makridakis 等人的论文 [“Statistical and Machine Learning Forecasting Methods: Concerns
    and Ways Forward”](https://oreil.ly/J43UW) 表明，统计方法的预测误差比机器学习方法低（如[图 4-33](part0009_split_026.html#forecasting_error)所示）。'
- en: '![srej 0433](../images/00049.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0433](../images/00049.png)'
- en: Figure 4-33\. One-step forecasting error of statistical versus machine learning
    techniques
  id: totrans-407
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-33\. 统计学与机器学习技术的一步预测误差
- en: Let’s cover a few of these statistical methods, starting with the least predictive
    naive method, which can be used with any monitoring system. Later approaches have
    less universal support from monitoring systems since their math is complicated
    enough to require built-in query functions.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要介绍一些统计方法，从最不具预测性的朴素方法开始，该方法可与任何监控系统一起使用。随后的方法由于其数学复杂性需要内置的查询功能，因此在监控系统中的通用支持较少。
- en: Naive Method
  id: totrans-409
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素方法
- en: 'The [naive method](https://oreil.ly/W_D82) is a simple heuristic that predicts
    the next value based on the last observed value:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[朴素方法](https://oreil.ly/W_D82) 是一个简单的启发式方法，根据最后观察到的值来预测下一个值：'
- en: <math alttext="ModifyingAbove y With caret Subscript upper T plus 1 vertical-bar
    upper T Baseline equals alpha y Subscript upper T" display="block"><mrow><msub><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover> <mrow><mi>T</mi><mo>+</mo><mn>1</mn><mo>|</mo><mi>T</mi></mrow></msub>
    <mo>=</mo> <mi>α</mi> <msub><mi>y</mi> <mi>T</mi></msub></mrow></math>
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove y With caret Subscript upper T plus 1 vertical-bar
    upper T Baseline equals alpha y Subscript upper T" display="block"><mrow><msub><mover
    accent="true"><mi>y</mi> <mo>^</mo></mover> <mrow><mi>T</mi><mo>+</mo><mn>1</mn><mo>|</mo><mi>T</mi></mrow></msub>
    <mo>=</mo> <mi>α</mi> <msub><mi>y</mi> <mi>T</mi></msub></mrow></math>
- en: A dynamic alert threshold can be determined with the naive method by multiplying
    a time series offset by some factor. Then we can test whether the true line ever
    drops below (or exceeds if the multiplier is greater than one) the forecast line.
    For example, if the true line is a measure of throughput through a system, a sudden
    substantial drop in throughput may indicate an outage.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过将时间序列偏移乘以某个因子来确定朴素方法的动态警报阈值。然后我们可以测试真实线是否曾经低于（如果乘数大于一则超过）预测线。例如，如果真实线是通过系统的吞吐量测量的，则吞吐量突然大幅下降可能表明发生了故障。
- en: The alert criteria for Atlas is then whenever the query in [Example 4-37](part0009_split_027.html#forecasting_naive_method_atlas)
    returns `1`. The query is designed against Atlas’s test dataset, so it’s easy
    for you to test out and try different multipliers to observe the effect.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: Atlas 的警戒标准是每当示例 [4-37](part0009_split_027.html#forecasting_naive_method_atlas)
    返回 `1` 时。该查询设计针对 Atlas 的测试数据集，因此您可以轻松测试并尝试不同的乘数以观察效果。
- en: Example 4-37\. Atlas alert criteria for the naive forecasting method
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-37\. Atlas 天真预测方法的警戒标准
- en: '[PRE36]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[![1](../images/00112.png)](part0009_split_027.html#co_charting_and_alerting_CO9-1)'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](../images/00112.png)](part0009_split_027.html#co_charting_and_alerting_CO9-1)'
- en: The tightness of the threshold is set with this factor.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个因子来设置阈值的紧密度。
- en: '[![2](../images/00059.png)](part0009_split_027.html#co_charting_and_alerting_CO9-2)'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](../images/00059.png)](part0009_split_027.html#co_charting_and_alerting_CO9-2)'
- en: “Look back” to some prior interval for the forecast.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: “回溯”到预测的某个前期间隔。
- en: The effect of the naive method can be seen in [Figure 4-34](part0009_split_027.html#forecasting_naive_method).
    The multiplicative factor (0.5 in the example query) controls how close to the
    true value we want to set the threshold and also reduces by the same amount the
    spikiness of the forecast (i.e., the looser the threshold, the less spiky the
    forecast). Since the method’s smoothing is proportional to looseness of fit, the
    alert threshold is still tripped four times in this time window (indicated by
    the vertical bars in the middle of the chart), even though we’ve allowed for a
    50% drift from “normal.”
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 天真方法的影响可以在 [图 4-34](part0009_split_027.html#forecasting_naive_method) 中看到。乘法因子（例如查询中的0.5）控制我们希望将阈值设置多接近真实值，同时也以同样的量减少预测的尖锐度（即阈值越宽松，预测的尖锐度越低）。由于该方法的平滑程度与拟合松紧成正比，所以即使我们允许50%的“正常”漂移，警戒阈值在这个时间窗口内仍会触发四次（在图表中间垂直条指示）。
- en: '![srej 0434](../images/00027.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0434](../images/00027.png)'
- en: Figure 4-34\. Forecasting with the naive method
  id: totrans-422
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-34\. 利用天真方法进行预测
- en: In order to prevent a chatty alert, we’d have to reduce the tightness of the
    forecast’s fit to our indicator (in this case a 0.45 multiplier silences the alert
    for this time window). Of course, doing so also allows for more drift from “normal”
    before an alert is fired.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免过于喋喋不休的警报，我们必须减少预测与指标的拟合紧密度（在本例中，0.45 倍数可以让警报在这个时间窗口内保持静默）。当然，这样做也会允许在触发警报之前对“正常”更多的漂移。
- en: Single-Exponential Smoothing
  id: totrans-424
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单指数平滑
- en: By smoothing the original indicator before multiplying it by some factor, we
    can fit the threshold closer to the indicator. Single-exponential smoothing is
    defined by [Equation 4-1](part0009_split_028.html#single_exponential_eq).
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在乘以某个因子之前对原始指标进行平滑，我们可以更紧密地拟合阈值到指标上。单指数平滑由 [方程 4-1](part0009_split_028.html#single_exponential_eq)
    定义。
- en: Equation 4-1\. Where <math alttext="0 less-than-or-equal-to alpha less-than-or-equal-to
    1"><mrow><mn>0</mn> <mo>≤</mo> <mi>α</mi> <mo>≤</mo> <mn>1</mn></mrow></math>
  id: totrans-426
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 4-1\. 其中 <math alttext="0 less-than-or-equal-to alpha less-than-or-equal-to
    1"><mrow><mn>0</mn> <mo>≤</mo> <mi>α</mi> <mo>≤</mo> <mn>1</mn></mrow></math>
- en: <math alttext="ModifyingAbove y With caret Subscript upper T plus 1 vertical-bar
    upper T Baseline equals alpha y Subscript upper T Baseline plus alpha left-parenthesis
    1 minus alpha right-parenthesis y Subscript upper T minus 1 Baseline plus alpha
    left-parenthesis 1 minus alpha right-parenthesis squared y Subscript upper T minus
    2 Baseline plus ellipsis equals alpha sigma-summation Underscript n equals 0 Overscript
    k Endscripts left-parenthesis 1 minus a right-parenthesis Superscript n Baseline
    y Subscript upper T minus n" display="block"><mrow><msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover> <mrow><mi>T</mi><mo>+</mo><mn>1</mn><mo>|</mo><mi>T</mi></mrow></msub>
    <mo>=</mo> <mi>α</mi> <msub><mi>y</mi> <mi>T</mi></msub> <mo>+</mo> <mi>α</mi>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>α</mi> <mo>)</mo></mrow> <msub><mi>y</mi>
    <mrow><mi>T</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <mi>α</mi> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>α</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <msub><mi>y</mi> <mrow><mi>T</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <mo>+</mo> <mo>...</mo> <mo>=</mo> <mi>α</mi> <munderover><mo>∑</mo> <mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>k</mi></munderover> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>a</mi><mo>)</mo></mrow>
    <mi>n</mi></msup> <msub><mi>y</mi> <mrow><mi>T</mi><mo>-</mo><mi>n</mi></mrow></msub></mrow></math>
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ModifyingAbove y With caret Subscript upper T plus 1 vertical-bar
    upper T Baseline equals alpha y Subscript upper T Baseline plus alpha left-parenthesis
    1 minus alpha right-parenthesis y Subscript upper T minus 1 Baseline plus alpha
    left-parenthesis 1 minus alpha right-parenthesis squared y Subscript upper T minus
    2 Baseline plus ellipsis equals alpha sigma-summation Underscript n equals 0 Overscript
    k Endscripts left-parenthesis 1 minus a right-parenthesis Superscript n Baseline
    y Subscript upper T minus n" display="block"><mrow><msub><mover accent="true"><mi>y</mi>
    <mo>^</mo></mover> <mrow><mi>T</mi><mo>+</mo><mn>1</mn><mo>|</mo><mi>T</mi></mrow></msub>
    <mo>=</mo> <mi>α</mi> <msub><mi>y</mi> <mi>T</mi></msub> <mo>+</mo> <mi>α</mi>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>α</mi> <mo>)</mo></mrow> <msub><mi>y</mi>
    <mrow><mi>T</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo> <mi>α</mi> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>α</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <msub><mi>y</mi> <mrow><mi>T</mi><mo>-</mo><mn>2</mn></mrow></msub>
    <mo>+</mo> <mo>...</mo> <mo>=</mo> <mi>α</mi> <munderover><mo>∑</mo> <mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>k</mi></munderover> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>a</mi><mo>)</mo></mrow>
    <mi>n</mi></msup> <msub><mi>y</mi> <mrow><mi>T</mi><mo>-</mo><mi>n</mi></mrow></msub></mrow></math>
- en: <math alttext="alpha"><mi>α</mi></math> is a smoothing parameter. When <math
    alttext="alpha equals 1"><mrow><mi>α</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    , all the terms except the first are zeroed and we are left with the naive method.
    Values less than 1 suggest how important previous samples should be.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="alpha"><mi>α</mi></math> 是平滑参数。当 <math alttext="alpha equals
    1"><mrow><mi>α</mi> <mo>=</mo> <mn>1</mn></mrow></math> 时，除第一个外的所有项都被清零，这时就得到了天真方法。小于1的值表明前一样本的重要性。
- en: Like for the naive method, the alert criteria for Atlas is whenever the query
    in [Example 4-38](part0009_split_028.html#forecasting_single_exponential_method_atlas)
    returns `1`.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 与天真方法类似，Atlas 的警戒标准是每当示例 [4-38](part0009_split_028.html#forecasting_single_exponential_method_atlas)
    返回 `1` 时。
- en: Example 4-38\. Atlas alert criteria for single-exponential smoothing
  id: totrans-430
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 4-38\. 单指数平滑的 Atlas 警戒标准
- en: '[PRE37]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[![1](../images/00112.png)](part0009_split_028.html#co_charting_and_alerting_CO10-1)'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](../images/00112.png)](part0009_split_028.html#co_charting_and_alerting_CO10-1)'
- en: The tightness of the threshold is set with this factor.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个因子来设置阈值的紧密度。
- en: The summation <math alttext="alpha sigma-summation Underscript n equals 0 Overscript
    k Endscripts left-parenthesis 1 minus a right-parenthesis Superscript n"><mrow><mi>α</mi>
    <msubsup><mo>∑</mo> <mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow> <mi>k</mi></msubsup>
    <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>a</mi><mo>)</mo></mrow> <mi>n</mi></msup></mrow></math>
    is a geometric series that converges to 1\. For example, for <math alttext="alpha
    equals 0.5"><mrow><mi>α</mi> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn></mrow></math>
    , see [Table 4-3](part0009_split_028.html#alpha_zero_five_convergence).
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-3\. The convergence to 1 of the geometric series where <math alttext="alpha
    equals 0.5"><mrow><mi>α</mi> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn></mrow></math>
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '| <math alttext="upper T"><mi>T</mi></math> | <math alttext="left-parenthesis
    1 minus alpha right-parenthesis Superscript upper T"><msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>α</mi><mo>)</mo></mrow>
    <mi>T</mi></msup></math> | <math alttext="alpha sigma-summation Underscript n
    equals 0 Overscript k Endscripts left-parenthesis 1 minus a right-parenthesis
    Superscript n"><mrow><mi>α</mi> <msubsup><mo>∑</mo> <mrow><mi>n</mi><mo>=</mo><mn>0</mn></mrow>
    <mi>k</mi></msubsup> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>a</mi><mo>)</mo></mrow>
    <mi>n</mi></msup></mrow></math> |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.5 | 0.5 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.25 | 0.75 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.125 | 0.88 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.063 | 0.938 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.031 | 0.969 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.016 | 0.984 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
- en: Since we don’t include *all* values of `T`, the smoothed function is effectively
    already multiplied by a factor equal to the cumulative sum of this geometric series
    up to the number of terms we have chosen. [Figure 4-35](part0009_split_028.html#single_exponential_scaling_effect)
    shows summations of one term and two terms in the series relative to the true
    value (respectively from bottom to top).
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0435](../images/00009.png)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
- en: Figure 4-35\. Scaling effect of choosing a limited summation
  id: totrans-446
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 4-36](part0009_split_028.html#single_exponential_different_constants)
    shows how different selections of <math alttext="alpha"><mi>α</mi></math> and
    <math alttext="upper T"><mi>T</mi></math> affect the dynamic threshold, in terms
    of both how smoothed it is and its approximate scaling factor relative to the
    true indicator.'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0436](../images/00105.png)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
- en: Figure 4-36\. Smoothing and scaling effect when choosing different <math alttext="alpha"><mi>α</mi></math>
    and <math alttext="upper T"><mi>T</mi></math>
  id: totrans-449
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Universal Scalability Law
  id: totrans-450
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we are going to shift our mindset entirely from smoothing datapoints
    that happened in the *past* (which we used as dynamic alert thresholds) to a technique
    that allows us to predict what *future* performance will look like if concurrency/throughput
    increases beyond current levels, using only a small set of samples of what performance
    has looked like at already-seen concurrency levels. In this way, we can set *predictive*
    alerts as we approach a service-level objective boundary to hopefully head off
    problems rather than reacting to something already exceeding the boundary. In
    other words, this technique allows us to test a *predicted* service-level indicator
    value against our SLO at a throughput that we haven’t yet experienced.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将完全改变思维方式，不再仅仅平滑过去发生的数据点（我们将其用作动态警报阈值），而是使用一种技术来预测未来性能会如何，如果并发/吞吐量超过当前水平，仅使用一小组已看到的性能样本。通过这种方式，我们可以设置预测警报，当接近服务级别目标边界时，希望能够预防问题，而不是对已经超出边界的问题做出反应。换句话说，这种技术允许我们测试一个*预测的*服务级别指标值与我们的SLO相比，这是我们尚未经历过的吞吐量。
- en: This technique is based on a mathematical principle known as Little’s Law and
    the Universal Scalability Law (USL). We’ll keep the mathematical explanation here
    to a minimum. What little is discussed you can skip past. For more details, Baron
    Schwartz’s freely available *Practical Scalability Analysis with the Universal
    Scalability Law* (VividCortex) is a great reference.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术基于一个称为Little法则和通用可扩展性定律（USL）的数学原理。我们将在这里对数学解释保持最少。你可以跳过所讨论的内容。有关更多细节，Baron
    Schwartz的免费可用的*使用通用可扩展性定律进行实用的可扩展性分析*（VividCortex）是一个很好的参考资料。
- en: Using Universal Scalability Law in the Delivery Pipeline
  id: totrans-453
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在交付流水线中使用通用可扩展性定律
- en: In addition to predicting impending SLA violations in production systems, we
    can use the same telemetry in a delivery pipeline to throw some traffic at a piece
    of software that doesn’t need to be anywhere close to the maximum traffic it might
    see in production and predict whether production-level traffic will meet an SLA.
    And we can do this before deploying a new version of the software to production!
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在生产系统中预测即将发生的SLA违规外，我们还可以在交付流水线中使用相同的遥测数据，向一个不需要接近其可能在生产中看到的最大流量的软件发送一些流量，并预测生产级别的流量是否能够达到SLA。而且我们可以在将软件的新版本部署到生产环境之前就做到这一点！
- en: 'Little’s Law, [Equation 4-2](part0009_split_030.html#littles_law), describes
    the behavior of queues as a relationship between three variables: queue size (
    <math alttext="upper N"><mi>N</mi></math> ), latency ( <math alttext="upper R"><mi>R</mi></math>
    ), and throughput ( <math alttext="upper X"><mi>X</mi></math> ). If the application
    of queuing theory to SLI predictions seems a little mind-bending, don’t worry
    (because it is). But for our purposes in predicting an SLI, <math alttext="upper
    N"><mi>N</mi></math> will represent the concurrency level of requests passing
    through our system, <math alttext="upper X"><mi>X</mi></math> the throughput,
    and <math alttext="upper R"><mi>R</mi></math> a latency measure like average or
    a high-percentile value. Since this is a relationship between three variables,
    provided any two we can derive the third. Since we care about predicting latency
    ( <math alttext="upper R"><mi>R</mi></math> ), we’d need to forecast this in the
    two dimensions of concurrency ( <math alttext="upper N"><mi>N</mi></math> ) and
    throughput ( <math alttext="upper X"><mi>X</mi></math> ).'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: Little法则，[方程式 4-2](part0009_split_030.html#littles_law)，描述了队列的行为，涉及三个变量之间的关系：队列大小（
    <math alttext="upper N"><mi>N</mi></math> ），延迟（ <math alttext="upper R"><mi>R</mi></math>
    ），和吞吐量（ <math alttext="upper X"><mi>X</mi></math> ）。如果将排队理论应用于SLI预测似乎有点令人费解，不用担心（因为确实如此）。但是对于我们预测SLI的目的，<math
    alttext="upper N"><mi>N</mi></math> 将表示通过我们系统的请求的并发级别，<math alttext="upper X"><mi>X</mi></math>
    表示吞吐量，而 <math alttext="upper R"><mi>R</mi></math> 则是诸如平均值或高百分位值的延迟度量。因为这是三个变量之间的关系，只要提供其中两个，我们就能推导出第三个。因为我们关心预测延迟（
    <math alttext="upper R"><mi>R</mi></math> ），我们需要在并发（ <math alttext="upper N"><mi>N</mi></math>
    ）和吞吐量（ <math alttext="upper X"><mi>X</mi></math> ）的两个维度上进行预测。
- en: Equation 4-2\. Little’s Law
  id: totrans-456
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 4-2\. Little法则
- en: <math alttext="StartLayout 1st Row 1st Column upper N 2nd Column equals 3rd
    Column upper X upper R 2nd Row 1st Column upper X 2nd Column equals 3rd Column
    upper N slash upper R 3rd Row 1st Column upper R 2nd Column equals 3rd Column
    upper N slash upper X EndLayout" display="block"><mtable><mtr><mtd columnalign="left"><mi>N</mi></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mi>X</mi> <mi>R</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mi>X</mi></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mi>N</mi>
    <mo>/</mo> <mi>R</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mi>R</mi></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mi>N</mi> <mo>/</mo> <mi>X</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column upper N 2nd Column equals 3rd
    Column upper X upper R 2nd Row 1st Column upper X 2nd Column equals 3rd Column
    upper N slash upper R 3rd Row 1st Column upper R 2nd Column equals 3rd Column
    upper N slash upper X EndLayout" display="block"><mtable><mtr><mtd columnalign="left"><mi>N</mi></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mi>X</mi> <mi>R</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mi>X</mi></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mi>N</mi>
    <mo>/</mo> <mi>R</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mi>R</mi></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><mi>N</mi> <mo>/</mo> <mi>X</mi></mrow></mtd></mtr></mtable></math>
- en: 'The Universal Scalability Law, [Equation 4-3](part0009_split_030.html#usl_formula),
    allows us instead to project latency in terms of only a single variable: either
    throughput or concurrency. This equation requires three coefficients, which will
    be derived and updated from a model maintained by Micrometer based on real observations
    about the system’s performance to this point. USL defines <math alttext="kappa"><mi>κ</mi></math>
    to be the cost of crosstalk, <math alttext="phi"><mi>ϕ</mi></math> to be the cost
    of contention, and <math alttext="lamda"><mi>λ</mi></math> to be how fast the
    system operates under unloaded conditions. The coefficients become fixed values
    making predictions on latency, throughput, or concurrency dependent on only one
    of the other three. Micrometer will also publish the values of these coefficients
    as they change over time, so you can compare the system’s major governing performance
    characteristics over time.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 通用可扩展性定律，[方程式 4-3](part0009_split_030.html#usl_formula)，允许我们只根据单一变量（吞吐量或并发性）来预测延迟。该方程式需要三个系数，这些系数将从
    Micrometer 维护的模型中根据系统到目前为止的实际性能观察中得出并更新。USL 定义<math alttext="kappa"><mi>κ</mi></math>作为串扰成本，<math
    alttext="phi"><mi>ϕ</mi></math>作为争用成本，<math alttext="lamda"><mi>λ</mi></math>作为系统在未加载条件下操作的速度。这些系数成为固定值，从而使延迟、吞吐量或并发性的预测仅依赖于另外三个中的一个。Micrometer
    还将随着时间的推移发布这些系数的值，因此您可以比较系统的主要性能特征随时间的变化。
- en: Equation 4-3\. Universal Scalability Law
  id: totrans-459
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 4-3\. 通用可扩展性定律
- en: <math alttext="upper X left-parenthesis upper N right-parenthesis equals StartFraction
    lamda upper N Over 1 plus phi left-parenthesis upper N minus 1 right-parenthesis
    plus kappa upper N left-parenthesis upper N minus 1 right-parenthesis EndFraction"
    display="block"><mrow><mi>X</mi> <mrow><mo>(</mo> <mi>N</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mrow><mi>λ</mi><mi>N</mi></mrow> <mrow><mn>1</mn><mo>+</mo><mi>ϕ</mi><mo>(</mo><mi>N</mi><mo>-</mo><mn>1</mn><mo>)</mo><mo>+</mo><mi>κ</mi><mi>N</mi><mo>(</mo><mi>N</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper X left-parenthesis upper N right-parenthesis equals StartFraction
    lamda upper N Over 1 plus phi left-parenthesis upper N minus 1 right-parenthesis
    plus kappa upper N left-parenthesis upper N minus 1 right-parenthesis EndFraction"
    display="block"><mrow><mi>X</mi> <mrow><mo>(</mo> <mi>N</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mrow><mi>λ</mi><mi>N</mi></mrow> <mrow><mn>1</mn><mo>+</mo><mi>ϕ</mi><mo>(</mo><mi>N</mi><mo>-</mo><mn>1</mn><mo>)</mo><mo>+</mo><mi>κ</mi><mi>N</mi><mo>(</mo><mi>N</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></mfrac></mrow></math>
- en: With a series of substitutions, we get to express <math alttext="upper R"><mi>R</mi></math>
    in terms of <math alttext="upper X"><mi>X</mi></math> or <math alttext="upper
    N"><mi>N</mi></math> (see [Equation 4-4](part0009_split_030.html#usl_predicted_latency)).
    Again, don’t think too hard about these relationships, because Micrometer is going
    to do these calculations for you.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一系列替换，我们可以将<math alttext="upper R"><mi>R</mi></math>表示为<math alttext="upper
    X"><mi>X</mi></math>或<math alttext="upper N"><mi>N</mi></math>的函数（参见[方程式 4-4](part0009_split_030.html#usl_predicted_latency)）。再次，请不要过多思考这些关系，因为
    Micrometer 将为您执行这些计算。
- en: Equation 4-4\. Predicted latency as a function of either throughput or concurrency
  id: totrans-462
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 4-4\. 预测的延迟作为吞吐量或并发性的函数。
- en: <math alttext="StartLayout 1st Row 1st Column upper R left-parenthesis upper
    N right-parenthesis 2nd Column equals 3rd Column StartFraction 1 plus phi left-parenthesis
    upper N minus 1 right-parenthesis plus kappa upper N left-parenthesis upper N
    minus 1 right-parenthesis Over lamda EndFraction 2nd Row 1st Column upper R left-parenthesis
    upper X right-parenthesis 2nd Column equals 3rd Column StartFraction minus StartRoot
    upper X squared left-parenthesis kappa squared plus 2 kappa left-parenthesis phi
    minus 2 right-parenthesis plus phi squared right-parenthesis plus 2 lamda upper
    X left-parenthesis kappa minus phi right-parenthesis plus lamda squared EndRoot
    plus kappa upper X plus lamda minus phi upper X Over 2 kappa upper X squared EndFraction
    EndLayout" display="block"><mtable><mtr><mtd columnalign="left"><mrow><mi>R</mi>
    <mo>(</mo> <mi>N</mi> <mo>)</mo></mrow></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mfrac><mrow><mn>1</mn><mo>+</mo><mi>ϕ</mi><mo>(</mo><mi>N</mi><mo>-</mo><mn>1</mn><mo>)</mo><mo>+</mo><mi>κ</mi><mi>N</mi><mo>(</mo><mi>N</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow>
    <mi>λ</mi></mfrac></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>R</mi> <mo>(</mo>
    <mi>X</mi> <mo>)</mo></mrow></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mfrac><mrow><mo>-</mo><msqrt><mrow><msup><mi>X</mi>
    <mn>2</mn></msup> <mrow><mo>(</mo><msup><mi>κ</mi> <mn>2</mn></msup> <mo>+</mo><mn>2</mn><mi>κ</mi><mrow><mo>(</mo><mi>ϕ</mi><mo>-</mo><mn>2</mn><mo>)</mo></mrow><mo>+</mo><msup><mi>ϕ</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow><mo>+</mo><mn>2</mn><mi>λ</mi><mi>X</mi><mrow><mo>(</mo><mi>κ</mi><mo>-</mo><mi>ϕ</mi><mo>)</mo></mrow><mo>+</mo><msup><mi>λ</mi>
    <mn>2</mn></msup></mrow></msqrt> <mo>+</mo><mi>κ</mi><mi>X</mi><mo>+</mo><mi>λ</mi><mo>-</mo><mi>ϕ</mi><mi>X</mi></mrow>
    <mrow><mn>2</mn><mi>κ</mi><msup><mi>X</mi> <mn>2</mn></msup></mrow></mfrac></mtd></mtr></mtable></math>
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column upper R left-parenthesis upper
    N right-parenthesis 2nd Column equals 3rd Column StartFraction 1 plus phi left-parenthesis
    upper N minus 1 right-parenthesis plus kappa upper N left-parenthesis upper N
    minus 1 right-parenthesis Over lamda EndFraction 2nd Row 1st Column upper R left-parenthesis
    upper X right-parenthesis 2nd Column equals 3rd Column StartFraction minus StartRoot
    upper X squared left-parenthesis kappa squared plus 2 kappa left-parenthesis phi
    minus 2 right-parenthesis plus phi squared right-parenthesis plus 2 lamda upper
    X left-parenthesis kappa minus phi right-parenthesis plus lamda squared EndRoot
    plus kappa upper X plus lamda minus phi upper X Over 2 kappa upper X squared EndFraction
    EndLayout" display="block"><mtable><mtr><mtd columnalign="left"><mrow><mi>R</mi>
    <mo>(</mo> <mi>N</mi> <mo>)</mo></mrow></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mfrac><mrow><mn>1</mn><mo>+</mo><mi>ϕ</mi><mo>(</mo><mi>N</mi><mo>-</mo><mn>1</mn><mo>)</mo><mo>+</mo><mi>κ</mi><mi>N</mi><mo>(</mo><mi>N</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow>
    <mi>λ</mi></mfrac></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>R</mi> <mo>(</mo>
    <mi>X</mi> <mo>)</mo></mrow></mtd> <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mfrac><mrow><mo>-</mo><msqrt><mrow><msup><mi>X</mi>
    <mn>2</mn></msup> <mrow><mo>(</mo><msup><mi>κ</mi> <mn>2</mn></msup> <mo>+</mo><mn>2</mn><mi>κ</mi><mrow><mo>(</mo><mi>ϕ</mi><mo>-</mo><mn>2</mn><mo>)</mo></mrow><mo>+</mo><msup><mi>ϕ</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow><mo>+</mo><mn>2</mn><mi>λ</mi><mi>X</mi><mrow><mo>(</mo><mi>κ</mi><mo>-</mo><mi>ϕ</mi><mo>)</mo></mrow><mo>+</mo><msup><mi>λ</mi>
    <mn>2</mn></msup></mrow></msqrt> <mo>+</mo><mi>κ</mi><mi>X</mi><mo>+</mo><mi>λ</mi><mo>-</mo><mi>ϕ</mi><mi>X</mi></mrow>
    <mrow><mn>2</mn><mi>κ</mi><msup><mi>X</mi> <mn>2</mn></msup></mrow></mfrac></mtd></mtr></mtable></math>
- en: What we’ll get is a nice two-dimensional projection instead, as shown in [Figure 4-37](part0009_split_030.html#usl_prediction).
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将得到一个很好的二维投影，如[图 4-37](part0009_split_030.html#usl_prediction)所示。
- en: '![srej 0437](../images/00098.png)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0437](../images/00098.png)'
- en: Figure 4-37\. USL prediction of latency based on different throughput levels
  id: totrans-466
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-37\. 基于不同吞吐量水平的延迟的 USL 预测。
- en: 'USL forecasting is a form of “derived” `Meter` in Micrometer and can be enabled
    as shown in [Example 4-39](part0009_split_030.html#usl_micrometer_config). Micrometer
    will publish a set of `Gauge` meters forming a series of forecasts at various
    throughput/concurrency levels for each publishing interval. Throughput and concurrency
    are correlated measurements, so think of them interchangeably from this point
    on. When you select a related group of timers (which will always have the same
    name) to publish a forecast for, Micrometer will publish several additional metrics
    using the common metric name as a prefix:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: USL 预测是 Micrometer 中的一种“派生”`Meter`，可以按照[示例 4-39](part0009_split_030.html#usl_micrometer_config)中所示启用。Micrometer
    将发布一组`Gauge`计量器，形成每个发布间隔各种吞吐量/并发性水平的预测系列。吞吐量和并发性是相关的测量，因此从这一点开始可以互换地考虑它们。当您选择发布一个与预测相关的计时器相关组时（这些计时器将始终具有相同的名称），Micrometer
    将使用公共指标名称作为前缀发布几个附加指标：
- en: timer.name.forecast
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: timer.name.forecast
- en: A series of `Gauge` meters with a tag `throughput` or `concurrency` based on
    the type of independent variable selected. At a certain time interval, plotting
    these gauges would generate a visualization like [Figure 4-37](part0009_split_030.html#usl_prediction).
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列带有标签`throughput`或`concurrency`的`Gauge`计量器，基于所选独立变量的类型。在特定时间间隔内，绘制这些计量器将生成类似[图 4-37](part0009_split_030.html#usl_prediction)的可视化效果。
- en: timer.name.crosstalk
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: timer.name.crosstalk
- en: 'A direct measure of the system’s crosstalk (e.g., fan-out in a distributed
    system like that described in the paper by S. Cho et al., [“Moolle: Fan-out Control
    for Scalable Distributed Data Stores”](https://oreil.ly/y6qO9)).'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '系统串扰的直接测量（例如，在 S. Cho 等人论文中描述的分布式系统中的扇出控制，[“Moolle: 可扩展分布式数据存储的扇出控制”](https://oreil.ly/y6qO9)）。'
- en: timer.name.contention
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: timer.name.contention
- en: A direct measure of the system’s contention (e.g., locking on relational database
    tables and in general any other form of lock synchronization).
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 系统争用的直接测量（例如，在关系数据库表上的锁定以及一般的任何其他形式的锁同步）。
- en: timer.name.unloaded.performance
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: Improvements in ideal unloaded performance (e.g., framework performance improvements)
    can be expected to yield improvements in loaded conditions as well.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-39\. Universal scalability law forecast configuration in Micrometer
  id: totrans-476
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[![1](../images/00112.png)](part0009_split_030.html#co_charting_and_alerting_CO11-1)'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: The forecast will be based on the results of a Micrometer meter search for one
    or more timers with name `http.server.requests` (remember, there may be several
    such timers with different tag values).
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](../images/00059.png)](part0009_split_030.html#co_charting_and_alerting_CO11-2)'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: We can further limit the set of timers to base the forecast on by only matching
    on timers that have a specific key-value tag pair.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](../images/00067.png)](part0009_split_030.html#co_charting_and_alerting_CO11-3)'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: Like with any search, the tag value can be constrained with a lambda as well.
    A good example is constraining the forecast to any “2xx” HTTP statuses.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](../images/00016.png)](part0009_split_030.html#co_charting_and_alerting_CO11-4)'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: The domain of the `Gauge` histogram will be either `UniversalScalabilityLawForecast.Variable.CONCURRENCY`
    or `UniversalScalabilityLawForecast.Variable.THROUGHPUT`, defaulting to `THROUGHPUT`.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: The latency an application is experiencing at its current throughput in one
    of these time slices will closely follow the “predicted” latency from the forecast.
    We can set an alert based on some scaled-up value of whatever the current throughput
    is to determine if the predicted latency at that scaled-up throughput would still
    be under our SLO.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: In addition to predicting an SLI under increased throughput, the modeled values
    for crosstalk, contention, and unloaded performance are a strong indicator of
    where performance improvements can be made in an application. After all, decreases
    in crosstalk and contention and increases in unloaded performance directly impact
    the system’s predicted and actual latency under various levels of load.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-488
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter has presented you with the tools you need to start monitoring every
    Java microservice for availability with signals that are included in Java frameworks
    like Spring Boot. We’ve also discussed more generally how to alert on and visualize
    classes of metrics like counters and timers.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: Though you should strive for finding ways of measuring microservice availability
    in terms of business-focused metrics, using these basic signals is a huge step
    forward over simply looking at box metrics in terms of understanding how your
    service is performing.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: Organizationally, you’ve committed to standing up a dashboarding/alerting tool.
    We showed Grafana in this chapter. Its open source availability and datasources
    for a wide array of popular monitoring systems make it a solid choice to build
    on top of without locking yourself in completely to a particular vendor.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’re going to transition to delivery automation, where
    we’ll see how some of these availability signals are used in making decisions
    about the fitness of new microservice releases. Effective delivery isn’t strictly
    about the motion of deploying; it turns monitoring into action.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将转向交付自动化，看看这些可用性信号如何在决策新微服务发布的适用性方面发挥作用。有效的交付不仅仅是部署的动作；它将监控转化为行动。
