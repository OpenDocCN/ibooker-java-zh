- en: Chapter 6\. Data Processing with Streams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Almost any program has to deal with processing data, most likely in the form
    of collections. An imperative approach uses loops to iterate over elements, working
    with each element in sequence. Functional languages, though, prefer a declarative
    approach and sometimes don’t even have a classical loop statement, to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: The *Streams API*, introduced in Java 8, provides a fully declarative and lazily
    evaluated approach to processing data that benefits from Java’s functional additions
    by utilizing higher-order functions for most of its operations.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will teach you the differences between imperative and declarative
    data processing. You will then have a visual introduction to Streams that highlights
    their underlying concepts and shows you how to get the most out of their flexibility
    to achieve a more functional approach to data processing.
  prefs: []
  type: TYPE_NORMAL
- en: Data Processing with Iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Processing data is an everyday task you’ve probably encountered a million times
    before and will continue to do so in the future.
  prefs: []
  type: TYPE_NORMAL
- en: From a broad point of view, any type of data processing works like a pipeline,
    with a data structure like a collection providing elements, one or more operations
    like filtering or transforming elements, and finally, delivering some form of
    a result. The result might be another data structure or even using it to run another
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with a simple data processing example.
  prefs: []
  type: TYPE_NORMAL
- en: External Iteration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Say that we need to find the three science-fiction books before 1970 sorted
    by title from a collection of `Book` instances. [Example 6-1](#_02-data-processing_for-loop)
    shows how to do this using a typical imperative approach with a `for`-loop.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-1\. Finding books with a `for`-loop
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_data_processing_with_streams_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: An unsorted Collection of books. It must be mutable, so it can be sorted in-place
    in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_data_processing_with_streams_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The collection has to be sorted first, or the elements in `result` won’t be
    the first three titles in alphabetical order of the original collection.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_data_processing_with_streams_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Ignore any unwanted books, like the ones not published before 1970 or non-science-fiction.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_data_processing_with_streams_CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The book title is all we are interested in.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_data_processing_with_streams_CO1-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Restrict the found titles to a maximum of three.
  prefs: []
  type: TYPE_NORMAL
- en: Although the code works for what it needs to do, it has several shortcomings
    compared to other approaches. The most obvious downside is the amount of boilerplate
    code required for an iteration-based loop.
  prefs: []
  type: TYPE_NORMAL
- en: Loop statements, either a `for`- or `while`-loop, contain their data processing
    logic in their body, to create a new scope for each iteration. Depending on your
    requirements, the loop’s body contains multiple statements, including decision-making
    about the iteration process itself in the form of `continue` and `break`. Overall,
    the data processing code is obscured by all this boilerplate and doesn’t present
    itself fluently or is easily followable, especially for a more complex loop than
    the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: The origin of these problems is blending “what you are doing” (working with
    data) and “how it’s done” (iterating over elements). This kind of iteration is
    called *external iteration*. Behind the scenes, the `for`-loop, in this case,
    the `for-each` variant, uses a `java.util.Iterator<E>` to traverse the collection.
    The traversal process calls `hasNext` and `next` to control the iteration, as
    illustrated in [Figure 6-1](#_02-data-processing_external-iteration).
  prefs: []
  type: TYPE_NORMAL
- en: '![External iteration](assets/afaj_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. External iteration
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the case of a “traditional” `for`-loop, you have to manage going over the
    elements until an end condition is reached yourself, which in a way is similar
    to an `Iterator<E>` and the `hasNext` and `next` method.
  prefs: []
  type: TYPE_NORMAL
- en: If you count the number of code lines that have to do with “what you’re doing”
    and “how it’s done,” you’d notice that it spends more time on traversal management
    than data processing, as listed in, as detailed in [Table 6-1](#_02-data-processing_loc-imp).
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-1\. Lines of code per data processing per task
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Lines of code |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Data preparation** Sorting the initial data and preparing a result Collection
    | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| **Traversal process** Looping and controlling the loop with `continue` and
    `break` | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| **Data processing** Choosing, transforming, and gathering the correct elements
    and data | 4 |'
  prefs: []
  type: TYPE_TB
- en: However, requiring a lot of boilerplate code to traverse isn’t the only drawback
    associated with external iteration. Another downside is the inherent serial traversal
    process. You need to rework the whole loop if you require parallel data processing
    and deal with all the associated gotchas, like the dreaded `ConcurrentModificationException`.
  prefs: []
  type: TYPE_NORMAL
- en: Internal Iteration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The opposite approach to *external* iteration is, predictably, *internal* iteration.
    With internal iteration, you give up explicit control of the traversal process
    and let the data source itself handle “how it’s done,” as illustrated in [Figure 6-2](#_02-data-processing_internal-iteration).
  prefs: []
  type: TYPE_NORMAL
- en: '![Internal iteration](assets/afaj_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. Internal iteration
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Instead of using an iterator to control the traversal, the data processing logic
    is prepared beforehand to build a pipeline that does the iteration by itself.
    The iteration process becomes more opaque, but the logic influences which elements
    traverse the pipeline. This way, you can focus your energy and code on “what you
    want to do” rather than on the tedious and often repetitive details of “how it’s
    done.”
  prefs: []
  type: TYPE_NORMAL
- en: Streams are such data pipelines with internal iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Streams as Functional Data Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Streams, as a data processing approach, get the job done like any other one
    but have specific advantages due to having an internal iterator. These advantages
    are especially beneficial from a functional point of view. The advantages are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Declarative approach
  prefs: []
  type: TYPE_NORMAL
- en: Build concise and comprehensible multi-step data processing pipelines with a
    single fluent call chain.
  prefs: []
  type: TYPE_NORMAL
- en: Composability
  prefs: []
  type: TYPE_NORMAL
- en: Stream operations provide a scaffold made of higher-order functions to be filled
    with data processing logic. They can be mixed as needed. If you design their logic
    in a functional way, you automatically gain all their advantages, like composability.
  prefs: []
  type: TYPE_NORMAL
- en: Laziness
  prefs: []
  type: TYPE_NORMAL
- en: Instead of iteration over all elements, they get pulled one by one through the
    pipeline after the last operation is attached to it, reducing the required amount
    of operations to a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Performance optimization
  prefs: []
  type: TYPE_NORMAL
- en: Streams optimize the traversal process automatically depending on their data
    source and different kinds of operations used, including short-circuiting operations
    if possible.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel data processing
  prefs: []
  type: TYPE_NORMAL
- en: Built-in support for parallel processing is used by simply changing a single
    call in the call chain.
  prefs: []
  type: TYPE_NORMAL
- en: In concept, Streams could be considered just another alternative to traditional
    loop constructs for data processing. In reality, though, Streams are special in
    *how* they go about providing those data processing capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to consider is the overall Stream workflow. Streams can be summed
    up as *lazy sequential data pipelines*. Such pipelines are a higher-level abstraction
    for traversing sequential data. They are sequences of higher-order functions to
    process their elements in a fluent, expressive, and functional way. The general
    workflow is representable by three steps, as seen in [Figure 6-3](#_02-data-processing_concept).
  prefs: []
  type: TYPE_NORMAL
- en: '![Different aspects of Java Streams](assets/afaj_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. The Basic Concept of Java Streams
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: (1) Creating a Stream
  prefs: []
  type: TYPE_NORMAL
- en: The first step is creating a Stream out of an existing data source. Streams
    aren’t limited to collection-like types, though. Any data source that can provide
    sequential elements is a possible data source for a Stream.
  prefs: []
  type: TYPE_NORMAL
- en: (2) Doing the Work
  prefs: []
  type: TYPE_NORMAL
- en: So-called *intermediate operations* — higher-order functions available as methods
    on the `java.util.stream.Stream<T>` — work on the elements passing through the
    pipeline, doing different tasks, like filtering, mapping, sorting, etc. Each one
    returns a new Stream, which can be connected with as many intermediate operations
    as needed.
  prefs: []
  type: TYPE_NORMAL
- en: (3) Getting a Result
  prefs: []
  type: TYPE_NORMAL
- en: To finish the data processing pipeline, a final — *terminal* — operation is
    needed to get back a result instead of a Stream. Such a terminal operation completes
    the Stream pipeline blueprint and starts the actual data processing.
  prefs: []
  type: TYPE_NORMAL
- en: To see this in action, let’s revisit the earlier task of finding three science-fiction
    book titles from 1999. This time, instead of using a `for`-loop as we did in [Example 6-1](#_02-data-processing_for-loop),
    we will use a Stream pipeline in [Example 6-2](#_02-data-processing_stream). Don’t
    worry too much about the Stream code yet; I’ll explain the various methods shortly.
    Read through it, and you should be able to get the gist of it for now.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-2\. Finding books with a Stream
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_data_processing_with_streams_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: An unsorted collection of books.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_data_processing_with_streams_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Ignore any books not published in 1999.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_data_processing_with_streams_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Ignore any non-science-fiction books.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_data_processing_with_streams_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Transform the element from the whole `Book` element to its `title` value.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_data_processing_with_streams_CO2-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Sort the titles.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_data_processing_with_streams_CO2-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Restrict the found titles to a maximum of three.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_data_processing_with_streams_CO2-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Aggregate the titles into a `List<String>`.
  prefs: []
  type: TYPE_NORMAL
- en: From a high-level point of view, both implementations shown in [Example 6-1](#_02-data-processing_for-loop)
    and [Example 6-2](#_02-data-processing_stream) represent pipelines that elements
    can traverse, with multiple exit points for unneeded data. But, notice how the
    functionality of the `for`-loop with its multiple statements is now condensed
    into a singular fluent Stream call?
  prefs: []
  type: TYPE_NORMAL
- en: This leads us to how Streams optimize the flow of their elements. You don’t
    have to explicitly manage the traversal with `continue` or `break` because the
    elements will traverse the pipeline depending on the result of the operations.
    [Figure 6-4](#_02-data-processing_albums-stream-steps) illustrates how the different
    Stream operations affect the element flow of [Example 6-2](#_02-data-processing_stream).
  prefs: []
  type: TYPE_NORMAL
- en: '![Element Flow of Book Stream](assets/afaj_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. Element Flow of Book Stream
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The elements flow one by one through the Stream and are funneled to the least
    amount needed to process the data.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of needing to prepare the data beforehand and wrapping the processing
    logic in a loop statement’s body, Streams are built with a fluent class of the
    different processing steps. Like other functional approaches, Stream code reflects
    “what” is happening in a more expressive and declarative fashion, without the
    typical verbiage of “how” it’s actually done.
  prefs: []
  type: TYPE_NORMAL
- en: Stream Features
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Streams are a functional API with specific behaviors and expectations built
    in. In a way, this confines their possibilities, at least, compared to the blank
    canvas of traditional loops. By being non-blank canvases, though, they provide
    you with lots of pre-defined building blocks and guaranteed properties that you
    would have to do yourself with alternative approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Lazy Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most significant advantage of Streams over loops is their laziness. Each
    time you call an intermediate operation on a Stream, it’s not applied immediately.
    Instead, the call simply “extends” the pipeline further and returns a new lazily
    evaluated Stream. The pipeline accumulates all operations, and no work starts
    before you call its terminal operation, which will trigger the actual element
    traversal, as seen in [Figure 6-5](#_02-streams_laziness).
  prefs: []
  type: TYPE_NORMAL
- en: '![Lazy Evaluation of Streams](assets/afaj_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. Lazy evaluation of Streams
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Instead of providing all elements to a code block, like a loop, the terminal
    operation is asking for more data as needed, and the Stream tries to comply. Streams,
    as a data source, don’t have to “over-provide” or buffer any elements if no one
    is requesting more elements. If you look back at [Figure 6-4](#_02-data-processing_albums-stream-steps),
    that means not every element will traverse through every operation.
  prefs: []
  type: TYPE_NORMAL
- en: The flow of Stream elements follows a “depth-first” approach, reducing the required
    CPU cycles, memory footprint, and stack depth. This way, even infinite data sources
    are possible because the pipeline is responsible for requesting the required elements
    and terminating the Stream.
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about the importance of laziness in functional programming
    in [Chapter 11](ch11.xhtml#_02-lazy-evaluation).
  prefs: []
  type: TYPE_NORMAL
- en: (Mostly) Stateless and Non-Interfering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you’ve learned in [Chapter 4](ch04.xhtml#_02-data-structures), an immutable
    state is an essential functional programming concept, and Streams do their best
    to adhere. Almost all intermediate operations are stateless and detached from
    the rest of the pipeline, only having access to the current element they’re processing.
    Certain intermediate operations, however, require some form of state to fulfill
    their purpose, like `limit` or `skip`.
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of using Streams is their separation of the data source and
    the elements themselves. That way, operations won’t affect the underlying data
    source in any way, nor does the Stream store any elements itself.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Even though you can create Java stateful lambdas with side effects, you should
    strive to design the behavioral arguments of your data manipulation pipelines
    stateless and as pure functions. Any dependence on an out-of-scope state can severely
    impact safety and performance and make the whole pipeline nondeterministic and
    incorrect due to unintended side effects. One exception is certain terminal operations
    for doing “side-effect only” code, which can help immensely fit functional Stream
    pipelines in existing imperative designs.
  prefs: []
  type: TYPE_NORMAL
- en: Streams are *non-interfering* and *pass-through* pipelines that will let their
    elements traverse as freely as possible without interference, if not absolutely
    necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizations included
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The internal iteration and fundamental design of higher-order functions allow
    Streams to optimize themselves quite efficiently. They utilize multiple techniques
    to improve their performance:'
  prefs: []
  type: TYPE_NORMAL
- en: Fusion^([1](ch06.xhtml#idm45115238989296)) of (stateless) operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removal of redundant operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Short-circuiting pipeline paths
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iteration-related code optimizations aren’t restricted to Streams, though. Traditional
    loops get optimized by the JVM, too, if possible^([2](ch06.xhtml#idm45115238985136)).
  prefs: []
  type: TYPE_NORMAL
- en: Also, loops like `for` and `while` are language features, and can therefore
    be optimized to another degree. Streams are ordinary types with all the costs
    affiliated with them. They still need to be created by wrapping a data source,
    and the pipeline is a call chain requiring a new stack frame for each call. In
    most real-world scenarios, their general advantages outweigh the possible performance
    impact of such an overhead compared to a built-in statement like `for` or `while`.
  prefs: []
  type: TYPE_NORMAL
- en: Less boilerplate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As seen in [Example 6-2](#_02-data-processing_stream), Streams condense data
    processing into a singular fluent method call chain. The call is designed to consist
    of small and on-point operations like `filter`, `map`, or `findFirst`, providing
    an expressive and straightforward scaffold around the data processing logic. Call
    chains should be easy to grasp, both visually and conceptually. Therefore, a Stream
    pipeline consumes as little visual real estate and cognitive bandwidth as necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Non-Reusable
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Stream pipelines are *single-use* only. They’re bound to their data source and
    traverse them exactly once after the terminal operation is called.
  prefs: []
  type: TYPE_NORMAL
- en: If you try to use a Stream again, an `IllegalStateException` gets thrown. You
    can’t check if a Stream is already consumed, though.
  prefs: []
  type: TYPE_NORMAL
- en: As Streams don’t change or affect their underlying data source, you can always
    create another Stream from the same data source.
  prefs: []
  type: TYPE_NORMAL
- en: Primitive Streams
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As with the functional interfaces introduced in [Chapter 2](ch02.xhtml#_01-functional-java),
    the Stream API contains specialized variants for dealing with primitives to minimize
    autoboxing overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Both `Stream` and the specialized variants `IntStream`, `LongStream`, and `DoubleStream`,
    share a common base interface, `BaseStream`, as illustrated in [Figure 6-6](#_02-data-processing_type-hierachy).
    Many of the available primitive Stream operations mirror their non-primitive counterpart,
    but not all of them.
  prefs: []
  type: TYPE_NORMAL
- en: '![Stream type hierarchy](assets/afaj_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. Stream type hierarchy
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: That’s why I discuss in [Chapter 7](ch07.xhtml#_02-streams) when to use a primitive
    Stream and how to switch between non-primitive and primitive Streams with a single
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: Easy Parallelization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data processing with traditional loop constructs is inherently serial. Concurrency
    is hard to do right and easy to do wrong, especially if you have to do it yourself.
    Streams are designed to support parallel execution from the ground up, utilizing
    the [Fork/Join framework](https://openjdk.java.net/projects/jdk7/features/#f515)
    introduced with Java 7.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing a Stream is done by simply calling the `parallel` method at any
    point of the pipeline. Although not every Stream pipeline is a good match for
    parallel processing. The Stream source must have enough elements, and the operations
    have to be costly enough to justify the overhead of multiple threads. Switching
    threads — so-called [context switches](https://en.wikipedia.org/wiki/Context_switch#Cost) — is
    an expensive task.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 8](ch08.xhtml#_01-parallel-streams), you’ll learn more about parallel
    Stream processing and concurrency in general.
  prefs: []
  type: TYPE_NORMAL
- en: (Lack of) Exception Handling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Streams do a great job of reducing the verbosity of your code by introducing
    a functional approach to data processing. However, this doesn’t make them immune
    to dealing with exceptions in their operations.
  prefs: []
  type: TYPE_NORMAL
- en: Lambda expressions, and therefore the logic of Stream operations, don’t have
    any special considerations or syntactic sugar to handle exceptions more concisely
    than you’re used to with `try`-`catch`. You can read more about the general problem
    of exceptions in functional Java code and how to handle them in different ways
    in [Chapter 10](ch10.xhtml#_02-exception-handling).
  prefs: []
  type: TYPE_NORMAL
- en: Spliterator, the Backbone of Streams
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just like “traditional” *for-each*-loop is built around the `Iterator<T>` type
    for traversing a sequence of elements, Streams have their own iteration interface:
    `java.util.Spliterator<T>`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `Iterator<T>` interface is solely based on the concept of “next” with only
    a few methods, which makes it a universal iterator for Java’s Collection API.
    The concept behind `Spliterator<T>`, however, is that it has the ability to split
    off a subsequence of its elements into another `Spliterator<T>` based on certain
    characteristics. This particular advantage over the `Iterator<T>` type makes it
    the core of the Stream API and allows Streams to process such subsequences in
    parallel, and still be able to iterate over Java Collection API types.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 6-3](#_02-streams_spliterator-interface) shows a simplified variant
    of `java.util.Spliterator`.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-3\. The `java.util.Spliterator` interface
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For the iteration process, the `boolean tryAdvance(Consumer action)` and `Spliterator<T>
    trySplit()` methods are the most important ones. Still, a Spliterator’s characteristics
    decree the capabilities of all its operations.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding Streams, the Spliterator’s characteristics are responsible for how
    a Stream iterates internally and what optimizations it supports. There are eight
    combinable characteristics, defined as `static int` constants on the `Spliterator<T>`
    type, as listed in [Table 6-2](#_02-data-processing-characteristics). Even though
    it looks like the characteristics match expected Stream behavior, not all of them
    are actually used in the current Stream implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-2\. Spliterator characteristics
  prefs: []
  type: TYPE_NORMAL
- en: '| Characteristic | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `CONCURRENT` | The underlying data source can safely be concurrently modified
    during traversal. Only affects the data source itself and has no implications
    for Stream-behavior. |'
  prefs: []
  type: TYPE_TB
- en: '| `DISTINCT` | The data source only contains unique elements, like a `Set<T>`.
    Any pair of elements in a Stream is guaranteed to be `x.equals(y) == false`. |'
  prefs: []
  type: TYPE_TB
- en: '| `IMMUTABLE` | The data source itself is immutable. No element can be added,
    replaced, or removed during traversal. Only affects the data source itself and
    has no implications for Stream-behavior. |'
  prefs: []
  type: TYPE_TB
- en: '| `NONNULL` | The underlying data source guarantees not to contain any `null`
    values. Only affects the data source itself and has no implications for Stream-behavior.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `ORDERED` | There is a defined order for the elements of the data source.
    During traversal, the encountered elements will be in that particular order. |'
  prefs: []
  type: TYPE_TB
- en: '| `SORTED` | If the `Spliterator<T>` is `SORTED`, its `getComparator()` method
    returns the associated `Comparator<T>`, or `null`, if the source is naturally
    sorted. `SORTED` `Spliterators` must also be `ORDERED`. |'
  prefs: []
  type: TYPE_TB
- en: '| `SIZED` | The data source knows its exact size. `estimateSize()` returns
    the actual size, not an estimate. |'
  prefs: []
  type: TYPE_TB
- en: '| `SUBSIZED` | Signifies that all split up chunk after calling `trySplit()`
    are also `SIZED`. Only affects the data source itself and has no implications
    for Stream-behavior. |'
  prefs: []
  type: TYPE_TB
- en: Stream characteristics don’t have to be fixed and can depend on the underlying
    data source. `HashSet` is an example of a Spliterator with dynamic characteristics.
    It uses the nested `HashMap.KeySpliterator` class which depends on the actual
    data, as seen in [Example 6-4](#_02-data-processing_spliterator_characteristics-hashset).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-4\. Spliterator characteristics of HashSet<T>
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The way `HashSet` creates its `KeySpliterator` shows that a Spliterator can
    use its surrounding context to make an informed decision about its capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: You don’t need to think much about a Stream’s characteristics most of the time.
    Usually, the underlying capabilities of a data source won’t change *magically*
    just because it’s traversed with a Stream. A `Set<T>` will still provide distinct
    elements in an unordered fashion, regardless of being used with a `for`-loop or
    a Stream. So choose the most fitting data source for the task, no matter the form
    of traversal used.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using Streams, you usually don’t need to create a Spliterator yourself,
    as the convenience methods I’m going to discuss in the next chapter will do it
    behind the scenes for you. Still, if you need to create a Spliterator for a custom
    data structure, you don’t necessarily have to implement the interface yourself,
    either. You can use one of the many convenience methods of `java.util.Spliterators`,
    instead. The easiest variant is the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The resulting Spliterator might not be the most optimized Spliterator with only
    limited parallel support, but it’s the simplest way to use existing `Iterator`-compatible
    data structures in Streams that don’t support them out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the official [documentation](https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/Spliterators.xhtml)
    for more information about the 20+ convenience methods provided by the `java.util.Spliterators`
    type.
  prefs: []
  type: TYPE_NORMAL
- en: Building Stream Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Stream API is extensive, and a detailed explanation of each operation and
    possible use case could easily fill a book itself. Let’s take a higher-level view
    of building Stream pipelines with the available higher-order functions instead.
    This overview will still help you to replace many data processing tasks with Stream
    pipelines in your code, especially those following the *map/filter/reduce* philosophy.
  prefs: []
  type: TYPE_NORMAL
- en: The Stream API actually has operations named `map`, `filter`, and `reduce`.
    Still, it provides a lot more operations than these three. The logic of most of
    these additional operations can be replicated by `map`/`filter`/`reduce`, and
    internally, that’s often the case. The extra operations give you a convenient
    way to avoid implementing common use cases yourself, with many different specialized
    operations readily available to you.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Stream
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every Stream pipeline starts with creating a new Stream instance from an existing
    data source. The most commonly used data source are collection types. That’s why
    the three methods `Stream<E> stream()`, `Stream<E> parallelStream()`, and `Spliterator<E>
    spliterator()` were retrofitted to `java.util.Collection` with the introduction
    of Streams in Java 8, as seen in [Example 6-5](#_02-data-processing_how-to-create-retrofit).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-5\. Simplified Stream creation for Collection types
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `stream` method is the simplest way to create a new Stream instance from
    any `Collection`-based data structure, like `List` or `Set`. It utilizes an `IMMUTABLE`
    and `CONCURRENT` `Spliterator` as its default implementation. However, many `Collection`
    types provide their own implementations with optimized characteristics and behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Even though the `stream` method on `Collection` might be the most convenient
    method to create a Stream, the JDK provides many other ways to create Streams
    as `static` convenience methods, like `Stream.of(T…​ values)`. In [Chapter 7](ch07.xhtml#_02-streams),
    you’ll learn more ways to create Streams for different use cases, like infinite
    Streams or working with I/O.
  prefs: []
  type: TYPE_NORMAL
- en: Doing the Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have a Stream, the next step is working with its elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Working with Stream elements is done by *intermediate operations*, which fall
    into three categories: transforming (*map*) elements, selecting (*filter*) elements,
    or modifying general Stream behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All Stream operations are aptly named and have ample [documentation](https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/stream/Stream.xhtml)
    and examples. Many methods use the “not-yet a standard” addition to JavaDoc^([3](ch06.xhtml#idm45115238384128))
    `@implSpec` to refer to implementation-specific behavior. So make sure to check
    out either the online documentation or the JavaDoc itself in case of your IDE
    isn’t rendering all of the documentation correctly.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, I will be using a simple `Shape` Record, shown in [Example 6-6](#_01-streams_operations_shape),
    to demonstrate the different operations.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-6\. A simple Shape type
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There won’t be a dedicated code example for every operation, as there are just
    too many. However, each operation and its element flow is illustrated.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting Elements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first common task of data processing is selecting the correct elements,
    either by filtering with a `Predicate` or by choosing based on the number of elements.
  prefs: []
  type: TYPE_NORMAL
- en: '`Stream<T> filter(Predicate<? super T> predicate)`'
  prefs: []
  type: TYPE_NORMAL
- en: The most straightforward way of filtering elements. If the `Predicate` evaluates
    to `true`, the element is considered for further processing. The `static` method
    `Predicate<T>.not(Predicate<T>)` allows for an easy negation of a Predicate without
    losing the advantage of method references. Common tasks, like `null` checks, are
    available via the `java.util.Objects` class and are usable as method references.
    See [Figure 6-7](#_02-data-processing_intermediate-ops_filter).
  prefs: []
  type: TYPE_NORMAL
- en: '![Stream<T> filter(Predicate<? super T> predicate)](assets/afaj_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. `Stream<T> filter(Predicate<? super T> predicate)`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`Stream<T> dropWhile(Predicate<? super T> predicate)`'
  prefs: []
  type: TYPE_NORMAL
- en: Discards — or *drops* — any element passing through the operation as long as
    the `Predicate` evaluates to `true`. This operation is designed for `ORDERED`
    Streams. The dropped elements won’t be deterministic if the Stream isn’t `ORDERED`.
    For sequential Streams, dropping elements is a cheap operation. A parallel Stream,
    though, has to coordinate between the underlying threads, making the operation
    quite costly. The operation was introduced with Java 9. See [Figure 6-8](#_02-data-processing_intermediate-ops_dropWhile).
  prefs: []
  type: TYPE_NORMAL
- en: '![Stream<T> dropWhile(Predicate<? super T> predicate)](assets/afaj_0608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-8\. `Stream<T> dropWhile(Predicate<? super T> predicate)`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`Stream<T> takeWhile(Predicate<? super T> predicate)`'
  prefs: []
  type: TYPE_NORMAL
- en: The antagonist to `dropWhile`, choosing elements until the `Predicate` evaluates
    to `false`. The operation was introduced with Java 9. See [Figure 6-9](#_02-data-processing_intermediate-ops_takeWhile).
  prefs: []
  type: TYPE_NORMAL
- en: '![Stream<T> takeWhile(Predicate<? super T> predicate)](assets/afaj_0609.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-9\. `Stream<T> takeWhile(Predicate<? super T> predicate)`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`Stream<T> limit(long maxSize)`'
  prefs: []
  type: TYPE_NORMAL
- en: Limits the maximum amount of elements passing through this operation to `maxSize`.
    See [Figure 6-10](#_02-data-processing_intermediate-ops_limit).
  prefs: []
  type: TYPE_NORMAL
- en: '![Stream<T> limit(long maxSize)](assets/afaj_0610.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-10\. `Stream<T> limit(long maxSize)`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`Stream<T> skip(long n)`'
  prefs: []
  type: TYPE_NORMAL
- en: The antagonist to `limit`, skipping `n` elements before passing all remaining
    elements to the subsequent Stream operations. See [Figure 6-11](#_02-data-processing_intermediate-ops_skip).
  prefs: []
  type: TYPE_NORMAL
- en: '![Stream<T> skip(long n)](assets/afaj_0611.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-11\. `Stream<T> skip(long n)`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`Stream<T> distinct()`'
  prefs: []
  type: TYPE_NORMAL
- en: Compares elements with `Object#equals(Object)` to return only distinct elements.
    This operation needs to buffer all elements passing through to compare them. There’s
    no integrated way to provide a custom `Comparator<T>` to determine distinctness.
    See [Figure 6-12](#_02-data-processing_intermediate-ops_distinct).
  prefs: []
  type: TYPE_NORMAL
- en: '![Stream<T> distinct()](assets/afaj_0612.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-12\. `Stream<T> distinct()`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`Stream<T> sorted()`'
  prefs: []
  type: TYPE_NORMAL
- en: Sorts the elements in their natural order if they conform to `java.util.Comparable`.
    Otherwise, a `java.lang.ClassCastException` is thrown on Stream consumption. [Figure 6-13](#_02-data-processing_intermediate-ops_sorted)
    assumes the natural sorting for shapes is by their number of corners. This operation
    needs to buffer all elements passing through to sort them. See [Figure 6-13](#_02-data-processing_intermediate-ops_sorted).
  prefs: []
  type: TYPE_NORMAL
- en: '![Stream<T> sorted()](assets/afaj_0613.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-13\. `Stream<T> sorted()`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`Stream<T> sorted(Comparator<? super T> comparator)`'
  prefs: []
  type: TYPE_NORMAL
- en: A more flexible version of `sorted` where you can provide a custom `comparator`.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping Elements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another significant category of operation is *mapping* — or transforming — elements.
    Not many Streams and their elements start out in the desired form. Sometimes you
    need a different representation or are only interested in a subset of an element’s
    properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initially, only two mapping operations were available to Streams:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Stream<R> map(Function<? super T, ? extends R> mapper)`'
  prefs: []
  type: TYPE_NORMAL
- en: The `mapper` function is applied to the elements, and the new element is returned
    down the Stream. See [Figure 6-14](#_02-data-processing_intermediate-ops_map).
  prefs: []
  type: TYPE_NORMAL
- en: '![Stream<R> map(Function<? super T, ? extends R> mapper)](assets/afaj_0614.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-14\. `Stream<R> map(Function<? super T, ? extends R> mapper)`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '`Stream<R> flatMap(Function<? super T, ? extends Stream<? extends R>> mapper)`'
  prefs: []
  type: TYPE_NORMAL
- en: The `mapper` function is still applied to the elements. However, instead of
    returning a new element, a `Stream<R>` needs to be returned. If `map` were used,
    the result would be a nested `Stream<Stream<R>>`, which is most likely not what
    you want. The `flatMap` operation “flattens” a container-like element, like a
    collection or Optional, into a new Stream of multiple elements which are used
    in subsequent operations. See [Figure 6-15](#_02-data-processing_intermediate-ops_flatMap).
  prefs: []
  type: TYPE_NORMAL
- en: '![Stream<R> flatMap(Function<? super T, ? extends Stream<? extends R>> mapper)](assets/afaj_0615.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-15\. `Stream<R> flatMap(Function<? super T, ? extends Stream<? extends
    R>> mapper)`
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Java 16 introduced an additional mapping method (and its three primitive counterparts)
    that has a similar role as `flatMap`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Stream<R> mapMulti(BiConsumer<? super T, ? super Consumer<R>> mapper)`'
  prefs: []
  type: TYPE_NORMAL
- en: The `mapMulti` operation doesn’t require the mapper to return a Stream instance.
    Instead, a `Consumer<R>` conveys the elements further down the Stream.
  prefs: []
  type: TYPE_NORMAL
- en: In its current form, the `Shape` type doesn’t lead to cleaner code when the
    `mapMulti` operation is used, as seen in [Example 6-7](#_02-data-processing_flatmap-vs-mapMulti).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-7\. Shape `flatMap` versus `mapMulti`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The winner in terms of conciseness and readability is clearly `flatMap`. Still,
    the main advantage of `multiMap` is that it condenses two operations, `map` and
    `flatMap`, into a single one.
  prefs: []
  type: TYPE_NORMAL
- en: The default implementation of `mapMulti` actually uses `flatMap` to create a
    new Stream for you, so your mapped elements don’t need to know how to create a
    Stream themselves. By calling the downstream `Consumer` yourself, *you* decide
    which mapped elements belong to the new Stream, and the pipeline is responsible
    for creating it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `mapMulti` operations aren’t supposed to replace `flatMap` operations.
    They are merely a complementary addition to Stream’s repertoire of operations.
    There are use-cases where `mapMulti` is preferable to `flatMap`, though:'
  prefs: []
  type: TYPE_NORMAL
- en: Only a small number of elements, or even zero, are mapped down the Stream pipeline.
    Using `mapMulti` avoids the overhead of creating a new Stream for every group
    of mapped elements, as done by `flatMap`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When an iterative approach to providing the mapped results is more straightforward
    than creating a new Stream instance. This gives you more freedom for the mapping
    process before feeding an element to the `Consumer`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peeking into a Stream
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One intermediate operation doesn’t fit into the *map/filter/reduce* philosophy:
    `peek`.'
  prefs: []
  type: TYPE_NORMAL
- en: The conciseness of Streams can pack a lot of functionality into a singular fluent
    call. Even though that’s one of their main selling points, debugging them is way
    more challenging than traditional imperative loop constructs. To ease this pain
    point, the Stream API includes a particular operation, `peek(Consumer<? super
    T> action)`, to, well, “peek” into the Stream without interfering with the elements,
    as seen in [Example 6-8](#_02-data-processing_peek)
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-8\. Peeking into a Stream
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `peek` operation is mainly intended to support debugging. It might get skipped
    for optimizing the Stream if the operation isn’t necessarily required for the
    final result, like counting elements, and the pipeline can get short-circuited.
  prefs: []
  type: TYPE_NORMAL
- en: The short-circuiting of operations will be explained more in [“The Cost of Operations”](#_02-data-processing_order-matters).
  prefs: []
  type: TYPE_NORMAL
- en: Terminating the Stream
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A *terminal* operation is the final step of a Stream pipeline that initiates
    the actual processing of the elements to produce a result or side effect. Unlike
    intermediate operations and their delayed nature, terminal operations evaluate
    eagerly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The available terminal operations fall into four different groups:'
  prefs: []
  type: TYPE_NORMAL
- en: Reductions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding and matching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consuming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing Elements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Reduction operations*, also known as *fold* operations, reduce the Stream’s
    elements to a single result by repeatedly applying an *accumulator* operator.
    Such an operator uses the previous result to combine it with the current element
    to generate a new result, as shown in [Figure 6-16](#_02-data-processing_intermediate-ops_reduce).
    The *accumulator* is supposed to always return a new value without requiring an
    intermediate data structure.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reducing shapes by combining them next to each other](assets/afaj_0616.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-16\. Reducing shapes by combining them next to each other
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Like many functional tools, reductions often feel alien at first due to their
    nomenclature, especially if you come from an imperative background. The simplest
    way to better understand the general concept behind such tools is by looking at
    the involved parts and how they would work in a more familiar form.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of reduction, there are three parts involved:'
  prefs: []
  type: TYPE_NORMAL
- en: The elements
  prefs: []
  type: TYPE_NORMAL
- en: Data processing is, well, about processing data elements. The familiar equivalent
    to a Stream would be any collection type.
  prefs: []
  type: TYPE_NORMAL
- en: The initial value
  prefs: []
  type: TYPE_NORMAL
- en: The accumulation of data has to start somewhere. Sometimes this initial value
    is explicit, but certain reduction variants omit it by replacing it with the first
    element or allowing for an optional result if no element is present.
  prefs: []
  type: TYPE_NORMAL
- en: The accumulator function
  prefs: []
  type: TYPE_NORMAL
- en: The reduction logic solely works with the current element and the previous result
    or initial value. Depending only on its input to create a new value makes this
    a pure function.
  prefs: []
  type: TYPE_NORMAL
- en: Take finding the biggest value of a `Collection<Integer>` for an example. You
    have to go through each element and compare it with the next one, returning the
    greater number at each step, as shown in [Example 6-9](#_02-data-processing_terminal-ops_reduce-for-loop-specific).
    All three parts of a reduction are represented.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-9\. Finding the biggest number in a `Collection<Integer>`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_data_processing_with_streams_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The initial value depends on the required task. In this case, comparing against
    the smallest possible `int` value is the logical choice to find the greatest number.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_data_processing_with_streams_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The reduction logic has to be applied to each element.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_data_processing_with_streams_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The actual reduction logic, representing the accumulator function.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_data_processing_with_streams_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The reduced value.
  prefs: []
  type: TYPE_NORMAL
- en: To better reflect a reduction operation in general, the previous example allows
    you to derive a generic reduction operation as shown in [Example 6-10](#_02-data-processing_terminal-ops_reduce-for-loop).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-10\. Reduce-like `for`-loop
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The generic variant again highlights that a functional approach separates *how*
    a task is done from *what* the task is actually doing. This way, the previous
    example of finding the maximum value can be simplified to a single method call
    by using the generic variant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `max` method is also an example of why the Stream API provides more than
    just a `reduce` method: specialization to cover common use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: Even though all the specialized Stream operations can be implemented with one
    of the three available `reduce` methods — some of them actually are --⁠, the specialized
    variants create a more expressive fluent Stream call for typical reduction operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Stream API has three different explicit `reduce` operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`T reduce(T identity, BinaryOperator<T> accumulator)`'
  prefs: []
  type: TYPE_NORMAL
- en: The `identity` is the seed — initial — value for the chain of `accumulator`
    operations. Although it’s equivalent to [Example 6-10](#_02-data-processing_terminal-ops_reduce-for-loop),
    it’s not constrained by the sequential nature of a `for`-loop.
  prefs: []
  type: TYPE_NORMAL
- en: '`Optional<T> reduce(BinaryOperator<T> accumulator)`'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of requiring a seed value, this operation picks the first encountered
    element as its initial value. That’s why it returns an `Optional<T>`, which you
    will learn more about in [Chapter 9](ch09.xhtml#_02-optionals). An empty `Optional<T>`
    is returned if the Stream doesn’t contain any elements.
  prefs: []
  type: TYPE_NORMAL
- en: '`U reduce(U identity, BiFunction<U, ? super T, U> accumulator, BinaryOperator<U>
    combiner)`'
  prefs: []
  type: TYPE_NORMAL
- en: This variant combines a `map` and `reduce` operation, which is required if the
    Stream contains elements of type `T`, but the desired reduced result is of type
    `U`. Alternatively, you can use an explicit `map` and `reduce` operation separately.
    Such a Stream pipeline might be more straightforward than using the combined `reduce`
    operations, as seen in [Example 6-11](#_02-data-processing_terminal-ops_reduce-map-reduce)
    for summing up all characters in a `Stream<String>`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-11\. Three-arguments `reduce` operation versus `map` + two-arguments
    `reduce`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Which to choose — a single `reduce` or separate `map` and `reduce` — depends
    on your preferences and if the lambda expressions can be generalized or refactored,
    so you could use method references instead.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, some typical reduction tasks are available as specialized
    operations, including any variants for primitive Streams, as listed in [Table 6-3](#_02-data-processing_terminal-typical-operations).
    The listed methods belong to `IntStream` but are also available for `LongStream`
    and `DoubleStream` with their related types.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-3\. Typical reduction operations
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Stream<T> |'
  prefs: []
  type: TYPE_TB
- en: '| `Optional<T> min(Comparator<? super T> comparator) Optional<T> max(Comparator<?
    super T> comparator)` | Returns the minimum/maximum element of the Stream according
    to the provided `comparator`. An empty `Optional<T>` is returned if no elements
    reach the operation. |'
  prefs: []
  type: TYPE_TB
- en: '| `long count()` | Returns the element count present at the end of the Stream
    pipeline. Be aware that certain Stream implementations may choose *not* to execute
    all intermediate operations if the count is determinable from the Stream itself,
    e.g., its characteristics contain `SIZED`, and no filtering is going on in the
    pipeline. |'
  prefs: []
  type: TYPE_TB
- en: '| Primitive Streams |'
  prefs: []
  type: TYPE_TB
- en: '| `int sum()` | Sums up the elements of the Stream. |'
  prefs: []
  type: TYPE_TB
- en: '| `OptionalDouble average()` | Calculates the arithmetic mean of the Stream
    elements. If the Stream contains no elements at the point of the terminal operation,
    an empty `OptionalDouble` is returned. |'
  prefs: []
  type: TYPE_TB
- en: '| `IntSummaryStatistics summaryStatistics()` | Returns a summary of the Stream
    elements, containing the *count*, *sum*, *min*, and *max* of the Stream elements.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Even after migrating your code towards a more functional approach, reduction
    operations might not be your go-to operations for terminating a Stream. That’s
    because there’s another type of reduction operation available that feels more
    common to the ways you’re used to: *aggregation operations*.'
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating Elements with Collectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A ubiquitous step for every data processing task, be it Streams or an imperative
    approach with loops, is aggregating the resulting elements into a new data structure.
    Most commonly, you want the resulting elements in a new `List`, a unique `Set`,
    or some form of `Map`.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the elements to a new value, in this case, a collection-like type,
    fits the bill of a reduction operation from the previous section, as shown in
    [Example 6-12](#_02-data-processing_aggregation-reduce).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-12\. Aggregate elements with a `reduce` operation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_data_processing_with_streams_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The three-argument `reduce` operation is used because the resulting type isn’t
    the same type as the Stream elements.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_data_processing_with_streams_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce operations are supposed to return new values, so instead of using a shared
    `ArrayList` to aggregate the elements, a new `ArrayList` is created for each accumulation
    step.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_data_processing_with_streams_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The combiner merges multiple `ArrayList` instances by creating a new one in
    the case of parallel processing.
  prefs: []
  type: TYPE_NORMAL
- en: That’s quite a lot of verbose code to reduce Stream down to a simple `List`,
    with new instances of `ArrayList` created for each element, plus additional `ArrayList`
    instances if run in parallel!
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, you could *cheat* and reuse the `ArrayList acc` variable in the
    aggregator function instead of creating and returning a new one. However, that
    would go against the general concept of `reduce` of being an *immutable* reduction
    operation. That’s why there’s a better solution available: *aggregation operations*.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Even though I call them “aggregation operations” throughout the chapter, technically,
    they’re known as “mutable reduction operations” to differentiate them from reduction
    operations known as “immutable reduction operations.”
  prefs: []
  type: TYPE_NORMAL
- en: The `Stream<T>` type’s terminal operation `collect` accepts a Collector to aggregate
    elements. Instead of reducing elements by combining Stream elements to a single
    result by repeatedly applying an *accumulator* operator, these operations use
    a *mutable results container* as an intermediate data structure, as seen in [Figure 6-17](#_02-data-processing_aggregation_collect-figure).
  prefs: []
  type: TYPE_NORMAL
- en: '![Collection Stream elements](assets/afaj_0617.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-17\. Collecting Stream elements
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The Stream’s elements are aggregated — or collected — with the help of the
    `java.util.stream.Collector<T, A, R>` type. The interface’s generic types represent
    the different parts involved in the collection process:'
  prefs: []
  type: TYPE_NORMAL
- en: '`T`: The *type* of Stream elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`A`: The *mutable result container* type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`R`: The final *result type* of the collection process which may differ from
    the intermediate container type.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A `Collector` consists of multiple steps that match perfectly to its [interface
    definition](https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/stream/Collector.xhtml),
    as seen in [Figure 6-18](#_01-sterams-terminal-collector).
  prefs: []
  type: TYPE_NORMAL
- en: '![Inner workings of a Collector<T,A,R>](assets/afaj_0618.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-18\. Inner workings of a Collector<T, A, R>
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Step 1: `Supplier<A> supplier()`'
  prefs: []
  type: TYPE_NORMAL
- en: The `Supplier` returns a new instance of the mutable result container used throughout
    the collection process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: `BiConsumer<A, T> accumulator()`'
  prefs: []
  type: TYPE_NORMAL
- en: The core of the `Collector`, as this `BiConsumer` is responsible for accumulating
    the Stream elements of type `T` into the container of type `A` by accepting the
    result container and the current element as its arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 3: `BinaryOperator<A> combiner()`'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of parallel Stream processing, where multiple accumulators may do
    their work, the returned combiner `BinaryOperator` merges partial results container
    into a single one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 4: `Function<A, R> finisher()`'
  prefs: []
  type: TYPE_NORMAL
- en: The finisher transforms the intermediate result container to the actual return
    object of type `R`. The necessity of this step depends on the implementation of
    the `Collector`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 5: The final result'
  prefs: []
  type: TYPE_NORMAL
- en: The collected instance, e.g., a `List`, a `Map`, or even a single value.
  prefs: []
  type: TYPE_NORMAL
- en: The JDK comes with the `java.util.Collectors` utility class, providing a variety
    of Collectors for many use cases. Listing and explaining them all in detail could
    fill another whole chapter. That’s why I only introduce their particular use-case
    groups here. [Chapter 7](ch07.xhtml#_02-streams) will have more examples and details
    about them and how you can create your own Collectors. Also, you should check
    out the [official documentation](https://docs.oracle.com/en/java/javase/17/docs/api/java.base/java/util/stream/Collectors.xhtml)
    for more details, including intended use-cases and examples.
  prefs: []
  type: TYPE_NORMAL
- en: Collect into a `java.util.Collection` type
  prefs: []
  type: TYPE_NORMAL
- en: 'The most used variants, collecting Stream elements into new `Collection` types
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: '`toCollection(Supplier<C> collectionFactory)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`toList()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`toSet()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`toUnmodifiableList()` (Java 10+)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`toUnmodifiableSet()` (Java 10+)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original `toList()` / `toSet()` have no guarantees on the returned collection’s
    underlying type, mutability, serializability, or thread safety. That’s why the
    `Unmodifiable` variants were introduced in Java 10 to close that gap.
  prefs: []
  type: TYPE_NORMAL
- en: Collect into a `java.util.Map` (key-value)
  prefs: []
  type: TYPE_NORMAL
- en: 'Another frequently used `Collector` task is creating a `Map<K, V>` by mapping
    the key and value from the Stream’s elements. That’s why each variant must have
    at least a key- and value mapper function: Key- and value-mapper functions must
    be provided.'
  prefs: []
  type: TYPE_NORMAL
- en: '`toMap(…​)` (3 variants)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`toConcurrentMap(…​)` (3 variants)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`toUnmodifiableMap(…​)` (2 variants, Java 10+)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like the collection-based Collector methods, the original `toMap()` variants
    do not guarantee the returned Map’s underlying type, mutability, serializability,
    or thread safety. That’s why the `Unmodifiable` variants were introduced in Java
    10 to close that gap. Concurrent variants are also available for a more efficient
    collection of parallel Streams.
  prefs: []
  type: TYPE_NORMAL
- en: Collect into a `java.util.Map` (grouped)
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of a simple key-value relationship, the following Collectors group
    the values by a key, usually with a Collection-based type as the value for the
    returned `Map`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`groupingBy()` (3 variants)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`groupingByConcurrent()` (3 variants)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collect into a `java.util.Map` (partitioned)
  prefs: []
  type: TYPE_NORMAL
- en: Partitioned maps group their elements based on a provided `Predicate`.
  prefs: []
  type: TYPE_NORMAL
- en: '`partitionBy(…​)` (2 variants)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arithmetic and comparison operations
  prefs: []
  type: TYPE_NORMAL
- en: There’s a certain overlap between the reduction operations and Collectors, like
    the arithmetic- and comparison-related Collectors.
  prefs: []
  type: TYPE_NORMAL
- en: '`averagingInt(ToIntFunction<? super T> mapper)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summingInt(ToIntFunction<? super T> mapper)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`summarizingInt(ToIntFunction<? super T> mapper)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`counting()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minBy(Comparator<? super T> comparator)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxBy(Comparator<? super T> comparator)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: String operations
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three variants for joining elements together to a singular `String`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`joining()` (3 variants)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced use cases
  prefs: []
  type: TYPE_NORMAL
- en: In more advanced use cases, like multi-level reductions or complicated groupings/partitions,
    multiple collection steps are required with the help of “downstream” Collectors.
  prefs: []
  type: TYPE_NORMAL
- en: '`reducing(…​)` (3 variants)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`collectingAndThen(Collector<T,A,R> downstream, Function<R,RR> finisher)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mapping(Function<? super T, ? extends U> mapper, Collector<? super U, A, R>
    downstream)` (Java 9+)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filtering(Predicate<? super T> predicate, Collector<? super T, A, R> downstream)`
    (Java 9+)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`teeing(Collector<? super T, ?, R1> downstream1, Collector<? super T, ?, R2>
    downstream2, BiFunction<? super R1, ? super R2, R> merger)` (Java 12+)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Chapter 7](ch07.xhtml#_02-streams) will detail how to use different Collectors
    and create complex collection workflows, including downstream collection.'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing Versus Collecting Elements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The terminal operations `reduce` and `collect` are two sides of the same coin:
    both are reduction — or fold — operations. The difference lies in the general
    approach to recombining the results: *immutable* versus *mutable* accumulation.
    This difference leads to quite different performance characteristics.'
  prefs: []
  type: TYPE_NORMAL
- en: The more abstract approach of *immutable* accumulation with the `reduce` operation
    is the best fit if sub-results are cheap to create, like summing up numbers as
    shown in [Example 6-13](#_02-data-processing_immutable-reduction-stream)
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-13\. Immutable accumulation of numbers with a Stream
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_data_processing_with_streams_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The initial value — the *seed* — is used for every parallel reduction operation.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_data_processing_with_streams_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The method reference translates into a `BiFunction<Integer, Integer, Integer>`
    to accumulate the previous (or initial) value with the current Stream element.
  prefs: []
  type: TYPE_NORMAL
- en: Every reduction operation builds upon the previous one, as seen in [Figure 6-19](#_02-data-processing_immutable-reduction).
  prefs: []
  type: TYPE_NORMAL
- en: '![Immutable accumulation of numbers](assets/afaj_0619.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-19\. Immutable accumulation of numbers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This approach isn’t feasible for all scenarios, especially if creating an intermediate
    result is costly. Take the `String` type, for example. In [Chapter 4](ch04.xhtml#_02-data-structures),
    you’ve learned about its immutable nature and why performing modifications can
    be costly. That’s why it’s usually advisable to use an optimized intermediate
    container, like `StringBuilder` or `StringBuffer`, to reduce the required processing
    power.
  prefs: []
  type: TYPE_NORMAL
- en: Concatenating a list of `String` objects with an *immutable* reduction requires
    creating a new `String` for every step, leading to a runtime of <math alttext="upper
    O left-parenthesis n squared right-parenthesis"><mrow><mi>O</mi> <mo>(</mo> <msup><mi>n</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow></math> with `n` being the number of characters.
    Let’s compare an *immutable* and *mutable* variant of `String` concatenation in
    [Example 6-14](#_02-data-processing_reduce-vs-collect-concat-strings).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-14\. Concatenating String elements with reduce and collect
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_data_processing_with_streams_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The initial value is the first `String` creation.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_data_processing_with_streams_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Every reduction step creates another new `String`, so the required processing
    power and memory scale with element count.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_data_processing_with_streams_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The first argument specifies a `Supplier<A>` for the mutable container.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_data_processing_with_streams_CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The second argument is the reduction `BiConsumer<A, T>` accepting the container
    and the current element.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_data_processing_with_streams_CO6-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The third argument defines a `BinaryOperator<A>` of how to merge multiple containers
    in the case of parallel processing.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_data_processing_with_streams_CO6-6)'
  prefs: []
  type: TYPE_NORMAL
- en: And the last argument, a `Function<A, R>` tells the `Collector` how to build
    the final result of type `R`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_data_processing_with_streams_CO6-7)'
  prefs: []
  type: TYPE_NORMAL
- en: The `java.util.stream.Collectors` utility class provides many *ready-to-use*
    Collectors, making Stream pipelines more reasonable than creating a `Collector`
    inline.
  prefs: []
  type: TYPE_NORMAL
- en: The `Collector` requires more arguments than an *immutable* reduction to do
    its work. Still, these additional arguments allow it to use a *mutable* container
    and, therefore, a different approach to reducing the Stream’s elements in the
    first place. For many common tasks, in this case, concatenating Strings, you can
    use one of the pre-defined Collectors available from `java.util.stream.Collectors`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Which type of reduction to choose — *immutable* or *mutable* — depends highly
    on your requirements. My personal *rule of thumb* is simple and stems from the
    names of the actual methods: choose `collect` if the result is a collection-based
    type, like `List` or `Map`; choose `reduce` if the result is an accumulated single
    value. But don’t forget performance and memory considerations.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 7](ch07.xhtml#_02-streams) goes into more detail about Collectors
    and how to create your own.'
  prefs: []
  type: TYPE_NORMAL
- en: Aggregate Elements Directly
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `Collector` type is a powerful and versatile tool for collecting elements
    into new data structures. Still, sometimes, a simpler solution would suffice.
    The `Stream<T>` type provides more terminal aggregation operations for common
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Returning a `List<T>`
  prefs: []
  type: TYPE_NORMAL
- en: Java 16 added the terminal operation `toList()` to simplify the most commonly
    used aggregation to create a new `List<T>`. It doesn’t use a Collector-based workflow
    to aggregate the elements, leading to fewer allocations and requiring less memory.
    That makes it optimal to use when the stream size is known in advance, and a more
    concise alternative to `collect(Collectors.toList())`. There are no guarantees
    on the implementation type of the returned list or its serializability, just like
    with using `collect(Collectors.toList())`. Unlike it, however, the return list
    is an unmodifiable variant.
  prefs: []
  type: TYPE_NORMAL
- en: Returning an array
  prefs: []
  type: TYPE_NORMAL
- en: 'Returning the Stream’s elements as an array doesn’t require a reduction or
    Collector. Instead, you can use two operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Object[] toArray()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`A[] toArray(IntFunction<A[]> generator)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second variant of `toArray` allows you to create an array of a specific
    type instead of `Object[]` by providing an “array generator,” which most likely
    is a method reference to the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Finding and Matching Elements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Besides aggregating Stream elements into a new representation, finding a particular
    element is another common task for Streams. There are multiple terminal operations
    available to either find an element or determine its existence:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Optional<T> findFirst()`'
  prefs: []
  type: TYPE_NORMAL
- en: Returns the first encountered element of the Stream. If the Stream is unordered,
    any element might be returned. Empty Streams return an empty `Optional<T>`.
  prefs: []
  type: TYPE_NORMAL
- en: '`Optional<T> findAny()`'
  prefs: []
  type: TYPE_NORMAL
- en: Returns any element of the Stream in a non-deterministic fashion. If the Stream
    itself is empty, an empty `Optional<T>` is returned.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, both methods have no arguments, so a prior `filter` operation
    might be necessary to get the desired element.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don’t require the element itself, you should use one of the matching
    operations, which matches the elements against a `Predicate<T>` instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '`boolean anyMatch(Predicate<? super T> predicate)`'
  prefs: []
  type: TYPE_NORMAL
- en: Returns `true` if *any* element of the Stream matches the `predicate`.
  prefs: []
  type: TYPE_NORMAL
- en: '`boolean allMatch(Predicate<? super T> predicate)`'
  prefs: []
  type: TYPE_NORMAL
- en: Returns `true` if *all* elements of the Stream match the `predicate`.
  prefs: []
  type: TYPE_NORMAL
- en: '`boolean noneMatch(Predicate<? super T> predicate)`'
  prefs: []
  type: TYPE_NORMAL
- en: Returns `true` if *none* of the elements match the given `predicate`.
  prefs: []
  type: TYPE_NORMAL
- en: Consuming Elements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last group of terminal operations is *side-effects-only* operations. Instead
    of returning a value, the `forEach` methods only accept a `Consumer<T>`.
  prefs: []
  type: TYPE_NORMAL
- en: '`void forEach(Consumer<? super T> action)`'
  prefs: []
  type: TYPE_NORMAL
- en: Performs the `action` for each element. The execution order is explicitly nondeterministic
    to maximize the performance, especially for parallel Streams.
  prefs: []
  type: TYPE_NORMAL
- en: '`void forEachOrdered(Consumer<? super T> action)`'
  prefs: []
  type: TYPE_NORMAL
- en: The `action` is performed for every element in the encountered order if the
    Stream is `ORDERED`.
  prefs: []
  type: TYPE_NORMAL
- en: From a functional point of view, these operations seem out of place. As a developer
    trying to transition imperative code into a more functional direction, however,
    they can be quite useful.
  prefs: []
  type: TYPE_NORMAL
- en: Localized side effects aren’t inherently harmful. Not all code is easily refactorable
    to prevent them, if even at all. Just like with all the other operations, the
    conciseness of the contained logic determines how straightforward and readable
    the Stream pipeline will be. If more than a method reference or a simple non-block
    lambda is needed, it’s always a good idea to extract/refactor the logic into a
    new method and call it instead to maintain the conciseness and readability of
    the Stream pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The Cost of Operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The beauty of Streams is their ability to concatenate multiple operations into
    a single pipeline, but you have to remember one thing: every operation might get
    called until an item gets rejected downstream.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the simple Stream pipeline in [Example 6-15](#_02-data-processing_order-matters-01).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-15\. Fruit pipeline (naïve)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_data_processing_with_streams_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Process elements to the desired form.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_data_processing_with_streams_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Sort naturally.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_data_processing_with_streams_CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Reject unwanted elements.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_data_processing_with_streams_CO7-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, work with the remaining elements.
  prefs: []
  type: TYPE_NORMAL
- en: In this fruit pipeline example, you have three intermediate and one terminal
    operation, for processing five elements. How many operation calls do you guess
    are done by this simple code? Let’s count them!
  prefs: []
  type: TYPE_NORMAL
- en: The Stream pipeline calls `map` five times, `sorted` eight times, `filter` five
    times, and finally `forEach` two times. That’s *20* operations to output *two*
    values! Even though the pipeline does what it’s supposed to, that’s ridiculous!
    Let’s rearrange the operations to reduce the overall calls significantly, as seen
    in [Example 6-16](#_02-data-processing_order-matters-02).
  prefs: []
  type: TYPE_NORMAL
- en: Example 6-16\. Fruit pipeline (improved)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_data_processing_with_streams_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Reject unwanted elements first.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_data_processing_with_streams_CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Transform elements to the desired form.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_data_processing_with_streams_CO8-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Sort naturally.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_data_processing_with_streams_CO8-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, work with the remaining elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'By filtering first, the calls of the `map` operation and the work of the stateful
    `sorted` operation are reduced to a minimum: `filter` is called five times, `map`
    two times, `sorted` one time, and `forEach` two times, saving *50%* operations
    in total without changing the result.'
  prefs: []
  type: TYPE_NORMAL
- en: Always remember that Stream elements are not being pushed through the Stream
    pipeline and its operations until they reach the terminal operation. Instead,
    the terminal operation pulls the elements through the pipeline. The fewer elements
    that flow through the pipeline, the better its performance will be. That’s why
    some operations are considered *short-circuiting* in nature, meaning they can
    cut the Stream short. Essentially, short-circuiting operations, as listed in [Table 6-4](#_02-data-processing_short-circuiting-ops),
    are operations that might carry out their intended purpose without requiring the
    Stream to traverse all of its elements.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-4\. Short-circuiting Stream operations
  prefs: []
  type: TYPE_NORMAL
- en: '| Intermediate Operations | Terminal Operations |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `limit takeWhile`  | `findAny findFirst anyMatch allMatch noneMatch` |'
  prefs: []
  type: TYPE_TB
- en: This behavior allows them to even process an infinite Stream and may still produce
    a finite Stream (intermediate ops) or finish their task in finite time (terminal
    ops).
  prefs: []
  type: TYPE_NORMAL
- en: 'A non-short-circuiting operation with heavily optimized behavior is the terminal
    `count()` operation. If the overall element count of a Stream terminated by `count()`
    is derivable from the Stream itself, any prior operations that won’t affect the
    count might get dropped, as the following code demonstrates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Even though there are three operations with a `System.out.println` call in
    the pipeline, all of them are dropped. The reasoning behind this behavior is simple:
    `map` and `peek` operations don’t inject or remove any elements in the Stream
    pipeline, so they don’t affect the final count in any way, therefore, they aren’t
    actually required.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dropping operations is at the Stream’s discretion if it deems it possible.
    For example, the preceding code runs all operations if a `filter` operation is
    added to the pipeline, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: That doesn’t mean every kind of Stream pipeline will drop *possible* unnecessary
    operations, either. If you require “side-effects” in your Stream pipeline, you
    should use one of the two `forEach` terminal operation variants, which are intended
    as “side-effects-only” operations.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying Stream Behavior
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A Stream’s characteristics, as explained in [“Spliterator, the Backbone of Streams”](#_02-data-processing_spliterator),
    are initially set on its creation. Not every Stream operation is a good match
    for every characteristic, though. Especially in parallel Streams, the encountered
    order of elements might significantly impact performance. For example, selecting
    elements with the `filter` operation is an easily parallelizable task, but `takeWhile`
    needs to synchronize between tasks if run in parallel. That’s why particular Stream
    characteristics can be switched by the intermediate operations listed in [Table 6-5](#_02-data-processing_intermediate_mod),
    which return an equivalent Stream with changed traits.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-5\. Modifying Stream Behavior
  prefs: []
  type: TYPE_NORMAL
- en: '| Operation | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `parallel()` | Enables parallel processing. May return `this` if the Stream
    is already parallel. |'
  prefs: []
  type: TYPE_TB
- en: '| `sequential()` | Enables sequential processing. May return `this` if the
    Stream is already sequential. |'
  prefs: []
  type: TYPE_TB
- en: '| `unordered()` | Returns a Stream with unordered encounter order. May return
    `this` if the Stream is already unordered. |'
  prefs: []
  type: TYPE_TB
- en: '| `onClose(Runnable closeHandler)` | Adds an additional close handler to be
    called after the Stream is finished. |'
  prefs: []
  type: TYPE_TB
- en: Switching Stream behavior is just a single method call away. However, that doesn’t
    mean it’s always a good idea. In fact, switching to parallel processing is often
    a bad idea if the pipeline and the underlying Stream aren’t designed to run in
    parallel in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: See [Chapter 8](ch08.xhtml#_01-parallel-streams) to learn how to make an informed
    decision about using parallel processing for Stream pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: To Use Streams, or Not?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Streams are an excellent way to make your data processing more expressive and
    utilize many of the functional features available in Java. You may feel a strong
    urge to (over)use Streams for all kinds of data processing. I know I certainly
    overdid it at first. You have to keep in mind, though, that not every data processing
    pipeline benefits equally from becoming a Stream.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your decision to use Streams — or not to use one — should rely always be an
    informed decision based on the following intertwined factors:'
  prefs: []
  type: TYPE_NORMAL
- en: How complex is the required task?
  prefs: []
  type: TYPE_NORMAL
- en: A simple loop that’s a few lines long won’t benefit much from being a Stream
    with one or two small operations. It depends on how easy it is to fit the whole
    task and required logic into a mental model.
  prefs: []
  type: TYPE_NORMAL
- en: If I can grasp what’s happening with ease, a simple *for-each*-loop might be
    the better choice. On the other hand, compressing a multi-page long loop into
    a more accessible Stream pipeline with well-defined operations will improve its
    readability and maintainability.
  prefs: []
  type: TYPE_NORMAL
- en: How functional is the Stream pipeline?
  prefs: []
  type: TYPE_NORMAL
- en: Stream pipelines are mere scaffolds to be filled with your logic. If the logic
    isn’t a good fit for a functional approach, like side-effect-laden code, you won’t
    get all the benefits and safety guarantees that Streams have to offer.
  prefs: []
  type: TYPE_NORMAL
- en: Refactoring or redesigning code to be more functional, pure, or immutable is
    always a good idea and makes it a better match for the Stream API. Sill, forcing
    your code to fit into a Stream pipeline without the actual need is deciding on
    a solution without really understanding the problem first. A certain degree of
    adapting your code to enable new features that benefit productivity, reasonability,
    and maintainability is good.
  prefs: []
  type: TYPE_NORMAL
- en: However, it should be a conscious decision on what’s best for your code and
    project in the long run, not just a “requirement” to use a feature.
  prefs: []
  type: TYPE_NORMAL
- en: How many elements are processed?
  prefs: []
  type: TYPE_NORMAL
- en: The overhead of creating the scaffold that holds the Stream pipeline together
    diminishes with the number of processed elements. For small data sources, the
    relation between the required instances, method calls, stack frames, and memory
    consumption is not as negligible as for processing more significant quantities
    of elements.
  prefs: []
  type: TYPE_NORMAL
- en: In a direct comparison of raw performance, a “perfectly optimized” `for`-loop
    wins out over a sequential Stream for a simple reason. Traditional Java looping
    constructs are implemented at the language level, giving the JVM more optimization
    possibilities, especially for small loops. On the other hand, Streams are implemented
    as ordinary Java types, creating an unavoidable runtime overhead. That doesn’t
    mean their execution won’t be optimized, though! As you’ve learned in this chapter,
    a Stream pipeline can short-circuit or fuse operations to maximize pipeline throughput.
  prefs: []
  type: TYPE_NORMAL
- en: None of these factors in isolation should affect your decision to use Stream,
    only in tandem. Especially the most common concern of many developers — performance — is
    seldom the most significant criterion for designing code and choosing the right
    tools.
  prefs: []
  type: TYPE_NORMAL
- en: Your code could always be more performant. Dismissing a tool out of performance
    anxiety before measuring and verifying the actual performance might deprive you
    of a better solution for your actual problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sir Tony Hoare^([4](ch06.xhtml#idm45115235918800)) once said, “We should forget
    about small efficiencies, say about 97% of the time: premature optimization is
    the root of all evil.”'
  prefs: []
  type: TYPE_NORMAL
- en: This advice can be applied when deciding whether to use Streams or loops. Most
    of the time — around 97% — you do not need to concern yourself with raw performance,
    and Streams may be the most simple and straightforward solution for you with all
    the benefits the Stream API offers. Once in a while — the 3% — you will need to
    focus on raw performance to achieve your goals, and Streams might not be the best
    solution for you. Although in [Chapter 8](ch08.xhtml#_01-parallel-streams) you
    will learn how to improve processing performance by leveraging parallel Streams.
  prefs: []
  type: TYPE_NORMAL
- en: When deciding whether or not to use Streams, you might think about how willing
    you are to use something new and unfamiliar. When you first learned to program,
    I bet all the loop constructs you’re now quite familiar with appeared to be complicated.
    Everything seemed hard at first until, over time and repeated use, you became
    familiar and more comfortable with using those loop contracts. The same is going
    to be true for using Streams. Learning the ins and outs of the Steam API will
    take some time, but it will become easier and more obvious when and how to use
    Streams efficiently to create concise and straightforward data processing pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing you have to keep in mind is that the primary goal of Streams isn’t
    to achieve the best raw performance possible or to replace all other looping constructs.
    Streams are supposed to be a more declarative and expressive way of processing
    data. They give you the equivalent of the classical map-filter-reduce pattern
    backed by Java’s strong type system but also designed with all the powerful functional
    techniques introduced in Java 8 in mind. Designing a functional Stream pipeline
    is the most straightforward and concise way to apply functional code to a sequence
    of objects.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the general idea of combining pure functions with immutable data leads
    to a looser coupling between data structures and their data processing logic.
    Each operation only needs to know how to handle a single element in its current
    form. This decoupling enables greater reusability and maintainability of smaller
    domain-specific operations that can be composed into bigger, more sophisticated
    tasks if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Takeaways
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Stream API provides a fluent and declarative way to create *map/filter/reduce*-like
    data processing pipelines without the need for external iteration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concatenable higher-order functions are the building blocks for a Stream pipeline.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streams use internal iteration, which entrusts more control over the traversal
    process to the data source itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many common and specialized operations are available besides the classical *map/filter/reduce*
    operations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streams are lazy; no work is done until a terminal operation is called.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequential processing is the default, but switching to parallel processing is
    easy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel processing might not be the best approach to all data processing problems
    and usually needs to be verified to solve the problem more efficiently.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^([1](ch06.xhtml#idm45115238989296-marker)) Brian Goetz, the Java Language Architect
    at Oracle, explains fusing operations [on StackOverflow](https://stackoverflow.com/questions/35069055/java-stream-operation-fusion-and-stateful-intermediate-operations/35070889#35070889).
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch06.xhtml#idm45115238985136-marker)) Newland, Chris and Ben Evans. 2019\.
    “Loop Unrolling: An elaborate mechanism for reducing loop iterations improves
    performance but can be thwarted by inadvertent coding.” [Java magazine](https://blogs.oracle.com/javamagazine/loop-unrolling).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch06.xhtml#idm45115238384128-marker)) Even though there are several new
    annotations used in JavaDoc since the release of Java 8, they aren’t an *official*
    standard as of writing this book. The informal proposal is available at the official
    OpenJDK bug-tracker as [JDK-8068562](https://bugs.openjdk.java.net/browse/JDK-8068562)
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch06.xhtml#idm45115235918800-marker)) Sir Charles Antony Richard Hoare
    is a British computer scientist and recipient of the Turing Award — regarded as
    the highest distinction in the field of computer science — who has made foundational
    contributions to programming languages, algorithms, operating systems, formal
    verification, and concurrent computing.
  prefs: []
  type: TYPE_NORMAL
