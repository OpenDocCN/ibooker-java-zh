- en: Chapter 3\. A Java Performance Toolbox
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance analysis is all about visibility—knowing what is going on inside
    an application and in the application’s environment. Visibility is all about tools.
    And so performance tuning is all about tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [Chapter 2](ch02.html#SampleApplications), we looked at the importance of
    taking a data-driven approach to performance: you must measure the application’s
    performance and understand what those measurements mean. Performance analysis
    must be similarly data-driven: you must have data about what, exactly, the program
    is doing in order to make it perform better. How to obtain and understand that
    data is the subject of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Hundreds of tools can provide information about what a Java application is doing,
    and looking at all of them would be impractical. Many of the most important tools
    come with the Java Development Kit (JDK), and although those tools have other
    open source and commercial competitors, this chapter focuses mostly on the JDK
    tools as a matter of expedience.
  prefs: []
  type: TYPE_NORMAL
- en: Operating System Tools and Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The starting point for program analysis is not Java-specific at all: it is
    the basic set of monitoring tools that come with the operating system. On Unix-based
    systems, these are `sar` (System Accounting Report) and its constituent tools
    like `vmstat`, `iostat`, `prstat`, and so on. Windows has graphical resource monitors
    as well as command-line utilities like `typeperf`.'
  prefs: []
  type: TYPE_NORMAL
- en: Whenever performance tests are run, data should be gathered from the operating
    system. At a minimum, information on CPU, memory, and disk usage should be collected;
    if the program uses the network, information on network usage should be gathered
    as well. If performance tests are automated, this means relying on command-line
    tools (even on Windows). But even if tests are running interactively, it is better
    to have a command-line tool that captures output, rather than eyeballing a GUI
    graph and guessing what it means. The output can always be graphed later when
    doing analysis.
  prefs: []
  type: TYPE_NORMAL
- en: CPU Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s look first at monitoring the CPU and what it tells us about Java programs.
    CPU usage is typically divided into two categories: user time and system time
    (Windows refers to this as *privileged time*). *User time* is the percentage of
    time the CPU is executing application code, while *system time* is the percentage
    of time the CPU is executing kernel code. System time is related to the application;
    if the application performs I/O, for example, the kernel will execute the code
    to read the file from disk, or write the buffered data to the network, and so
    on. Anything that uses an underlying system resource will cause the application
    to use more system time.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal in performance is to drive CPU usage as high as possible for as short
    a time as possible. That may sound a little counterintuitive; you’ve doubtless
    sat at your desktop and watched it struggle because the CPU is 100% utilized.
    So let’s consider what the CPU usage actually tells us.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to keep in mind is that the CPU usage number is an average over
    an interval—5 seconds, 30 seconds, perhaps even as little as 1 second (though
    never really less than that). Say that the average CPU usage of a program is 50%
    for the 10 minutes it takes to execute. This means that the CPU is idle for half
    the time; if we can restructure the program to not have idle patches (nor other
    bottlenecks), we can double the performance and run in 5 minutes (with the CPU
    100% busy).
  prefs: []
  type: TYPE_NORMAL
- en: If then we improve the algorithm used by the program and double performance
    again, the CPU will still be at 100% during the 2.5 minutes it takes the program
    to complete. The CPU usage number is an indication of how effectively the program
    is using the CPU, and so the higher the number, the better.
  prefs: []
  type: TYPE_NORMAL
- en: 'If I run `vmstat 1` on my Linux desktop, I will get a series of lines (one
    every second) that look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This example comes from running an application with one active thread—that makes
    the example easier to follow—but the concepts apply even if there are multiple
    threads.
  prefs: []
  type: TYPE_NORMAL
- en: 'During each second, the CPU is busy for 450 ms (42% of the time executing user
    code, and 3% of the time executing system code). Similarly, the CPU is idle for
    550 ms. The CPU can be idle for multiple reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The application might be blocked on a synchronization primitive and unable to
    execute until that lock is released.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application might be waiting for something, such as a response to come back
    from a call to the database.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application might have nothing to do.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These first two situations are always indicative of something that can be addressed.
    If contention on the lock can be reduced or the database can be tuned so that
    it sends the answer back more quickly, then the program will run faster, and the
    average CPU use of the application will go up (assuming, of course, that there
    isn’t another such issue that will continue to block the application).
  prefs: []
  type: TYPE_NORMAL
- en: 'That third point is where confusion often lies. If the application has something
    to do (and is not prevented from doing it because it is waiting for a lock or
    another resource), then the CPU will spend cycles executing the application code.
    This is a general principle, not specific to Java. Say that you write a simple
    script containing an infinite loop. When that script is executed, it will consume
    100% of a CPU. The following batch job will do just that in Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Consider what it would mean if this script did not consume 100% of a CPU. It
    would mean that the operating system had something it could do—it could print
    yet another line saying `LOOPING`—but it chose instead to be idle. Being idle
    doesn’t help anyone in that case, and if we were doing a useful (lengthy) calculation,
    forcing the CPU to be periodically idle would mean that it would take longer to
    get the answer we are after.
  prefs: []
  type: TYPE_NORMAL
- en: If you run this command on a single-CPU machine or container, much of the time
    you are unlikely to notice that it is running. But if you attempt to start a new
    program, or time the performance of another application, then you will certainly
    see the effect. Operating systems are good at time-slicing programs that are competing
    for CPU cycles, but less CPU will be available for the new program, and it will
    run more slowly. That experience sometimes leads people to think it would be a
    good idea to leave some idle CPU cycles just in case something else needs them.
  prefs: []
  type: TYPE_NORMAL
- en: But the operating system cannot guess what you want to do next; it will (by
    default) execute everything it can rather than leaving the CPU idle.
  prefs: []
  type: TYPE_NORMAL
- en: Java and single-CPU usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To return to the discussion of the Java application—what does periodic, idle
    CPU mean in that case? It depends on the type of application. If the code in question
    is a batch-style application that has a fixed amount of work, you should never
    see idle CPU, because that would mean there is no work to do. Driving the CPU
    usage higher is always the goal for batch jobs, because the job will be completed
    faster. If the CPU is already at 100%, you can still look for optimizations that
    allow the work to be completed faster (while trying also to keep the CPU at 100%).
  prefs: []
  type: TYPE_NORMAL
- en: 'If the measurement involves a server-style application that accepts requests
    from a source, idle time may occur because no work is available: for example,
    when a web server has processed all outstanding HTTP requests and is waiting for
    the next request. This is where the average time comes in. The sample `vmstat`
    output was taken during execution of a server that was receiving one request every
    second. It took 450 ms for the application server to process that request—meaning
    that the CPU was 100% busy for 450 ms, and 0% busy for 550 ms. That was reported
    as the CPU being 45% busy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it usually happens at a level of granularity that is too small to
    visualize, the expected behavior of the CPU when running a load-based application
    is to operate in short bursts like this. The same macro-level pattern will be
    seen from the reporting if the CPU received one request every half-second and
    the average time to process the request was 225 ms. The CPU would be busy for
    225 ms, idle for 275 ms, busy again for 225 ms, and idle for 275 ms: on average,
    45% busy and 55% idle.'
  prefs: []
  type: TYPE_NORMAL
- en: If the application is optimized so that each request takes only 400 ms, the
    overall CPU usage will also be reduced (to 40%). This is the only case where driving
    the CPU usage lower makes sense—when a fixed amount of load is coming into the
    system and the application is not constrained by external resources. On the other
    hand, that optimization also gives you the opportunity to add more load into the
    system, ultimately increasing the CPU utilization. And at a micro level, optimizing
    in this case is still a matter of getting the CPU usage to 100% for a short period
    of time (the 400 ms it takes to execute the request)—it’s just that the duration
    of the CPU spike is too short to effectively register as 100% using most tools.
  prefs: []
  type: TYPE_NORMAL
- en: Java and multi-CPU usage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This example has assumed a single thread running on a single CPU, but the concepts
    are the same in the general case of multiple threads running on multiple CPUs.
    Multiple threads can skew the average of the CPU in interesting ways—one such
    example is shown in [Chapter 5](ch05.html#GC), which shows the effect of the multiple
    GC threads on CPU usage. But in general, the goal for multiple threads on a multi-CPU
    machine is still to drive the CPU higher by making sure individual threads are
    not blocked, or to drive the CPU lower (over a long interval) because the threads
    have completed their work and are waiting for more work.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a multithreaded, multi-CPU case, there is one important addition regarding
    when CPUs could be idle: CPUs can be idle even when there is work to do. This
    occurs if no threads are available in the program to handle that work. The typical
    case is an application with a fixed-size thread pool running various tasks. Tasks
    for the threads get placed onto a queue; when a thread is idle and a task in the
    queue, the thread picks up that task and executes it. However, each thread can
    execute only one task at a time, and if that particular task blocks (e.g., is
    waiting for a response from the database), the thread cannot pick up a new task
    to execute in the meantime. Hence, at times we may have periods where there are
    tasks to be executed (work to be done) but no thread available to execute them;
    the result is idle CPU time.'
  prefs: []
  type: TYPE_NORMAL
- en: In that specific example, the size of the thread pool should be increased. However,
    don’t assume that just because idle CPU is available, the size of the thread pool
    should be increased in order to accomplish more work. The program may not be getting
    CPU cycles for the other two reasons previously mentioned—because of bottlenecks
    in locks or external resources. It is important to understand *why* the program
    isn’t getting CPU before determining a course of action. (See [Chapter 9](ch09.html#ThreadPerformance)
    for more details on this topic.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the CPU usage is a first step in understanding application performance,
    but it is only that: use it to see if the code is using all the CPU that can be
    expected, or if it points to a synchronization or resource issue.'
  prefs: []
  type: TYPE_NORMAL
- en: The CPU Run Queue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Both Windows and Unix systems allow you to monitor the number of threads that
    can be run (meaning that they are not blocked on I/O, or sleeping, and so on).
    Unix systems refer to this as the *run queue*, and several tools include the run
    queue length in their output. That includes the `vmstat` output in the previous
    section: the first number in each line is the length of the run queue. Windows
    refers to this number as the *processor queue* and reports it (among other ways)
    via `typeperf`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'There is an important difference in this output: the run queue length number
    on a Unix system (which was either 1 or 2 in the sample `vmstat` output) is the
    number of all threads that *are* running or that *could run* if there were an
    available CPU. In that example, there was always at least one thread that wanted
    to run: the single thread doing application work. Hence, the run queue length
    was always at least 1\. Keep in mind that the run queue represents everything
    on the machine, so sometimes there are other threads (from completely separate
    processes) that want to run, which is why the run queue length sometimes was 2
    in that sample output.'
  prefs: []
  type: TYPE_NORMAL
- en: In Windows, the processor queue length does not include the number of threads
    that are currently running. Hence, in the `typeperf` sample output, the processor
    queue number was 0, even though the machine was running the same single-threaded
    application with one thread always executing.
  prefs: []
  type: TYPE_NORMAL
- en: If there are more threads to run than available CPUs, performance begins to
    degrade. In general, then, you want the processor queue length to be 0 on Windows
    and equal to (or less than) the number of CPUs on Unix systems. That isn’t a hard-and-fast
    rule; system processes and other things will come along periodically and briefly
    raise that value without any significant performance impact. But if the run queue
    length is too high for any significant period of time, it’s an indication that
    the machine is overloaded, and you should look into reducing the amount of work
    the machine is doing (either by moving jobs to another machine or by optimizing
    the code).
  prefs: []
  type: TYPE_NORMAL
- en: Quick Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CPU time is the first thing to examine when looking at the performance of an
    application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal in optimizing code is to drive the CPU usage up (for a shorter period
    of time), not down.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand why CPU usage is low before diving in and attempting to tune an application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disk Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Monitoring disk usage has two important goals. The first pertains to the application
    itself: if the application is doing a lot of disk I/O, that I/O can easily become
    a bottleneck.'
  prefs: []
  type: TYPE_NORMAL
- en: Knowing when disk I/O is a bottleneck is tricky, because it depends on the behavior
    of the application. If the application is not efficiently buffering the data it
    writes to disk (an example is in [Chapter 12](ch12.html#Misc)), the disk I/O statistics
    will be low. But if the application is performing more I/O than the disk can handle,
    the disk I/O statistics will be high. In either situation, performance can be
    improved; be on the lookout for both.
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic I/O monitors on some systems are better than on others. Here is partial
    output of `iostat` on a Linux system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This application is writing data to disk `sda`. At first glance, the disk statistics
    look good. The `w_await`—the time to service each I/O write—is fairly low (6.08
    ms), and the disk is only 1.04% utilized. (The acceptable values for that depend
    on the physical disk, but the 5200 RPM disk in my desktop system behaves well
    when the service time is under 15 ms.) But a clue is indicating that something
    is wrong: the system is spending 37.89% of its time in the kernel. If the system
    is doing other I/O (in other programs), that’s one thing; but if all that system
    time is from the application being tested, something inefficient is happening.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The fact that the system is doing 24.2 writes per second is another clue: that
    is a lot when writing only 0.14 MB per second (MBps). I/O has become a bottleneck,
    and the next step would be to look into how the application is performing its
    writes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The other side of the coin comes if the disk cannot keep up with the I/O requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The nice thing about Linux is that it tells us immediately that the disk is
    100% utilized; it also tells us that processes are spending 47.89% of their time
    in `iowait` (that is, waiting for the disk).
  prefs: []
  type: TYPE_NORMAL
- en: 'Even on other systems with only raw data available, that data will tell us
    something is amiss: the time to complete the I/O (`w_await`) is 871 ms, the queue
    size is quite large, and the disk is writing 81 MB of data per second. This all
    points to disk I/O as a problem and that the amount of I/O in the application
    (or, possibly, elsewhere in the system) must be reduced.'
  prefs: []
  type: TYPE_NORMAL
- en: A second reason to monitor disk usage—even if the application is not expected
    to perform a significant amount of I/O—is to help monitor if the system is swapping.
    Computers have a fixed amount of physical memory, but they can run a set of applications
    that use a much larger amount of virtual memory. Applications tend to reserve
    more memory than they need, and they usually operate on only a subset of their
    memory. In both cases, the operating system can keep the unused parts of memory
    on disk, and page it into physical memory only if it is needed.
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, this kind of memory management works well, especially for
    interactive and GUI programs (which is good, or your laptop would require much
    more memory than it has). It works less well for server-based applications, since
    those applications tend to use more of their memory. And it works particularly
    badly for any kind of Java application (including a Swing-based GUI application
    running on your desktop) because of the Java heap. More details about that appear
    in [Chapter 5](ch05.html#GC).
  prefs: []
  type: TYPE_NORMAL
- en: System tools can also report if the system is swapping; for example, the `vmstat`
    output has two columns (`si`, for *swap in*, and `so`, for *swap out*) that alert
    us if the system is swapping. Disk activity is another indicator that swapping
    might be occurring. Pay close attention to those, because a system that is swapping—moving
    pages of data from main memory to disk, and vice versa—will have quite bad performance.
    Systems must be configured so that swapping never occurs.
  prefs: []
  type: TYPE_NORMAL
- en: Quick Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring disk usage is important for all applications. For applications that
    don’t directly write to disk, system swapping can still affect their performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications that write to disk can be bottlenecked both because they are writing
    data inefficiently (too little throughput) or because they are writing too much
    data (too much throughput).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network Usage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are running an application that uses the network—for example, a REST
    server—you must monitor the network traffic as well. Network usage is similar
    to disk traffic: the application might be inefficiently using the network so that
    bandwidth is too low, or the total amount of data written to a particular network
    interface might be more than the interface is able to handle.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, standard system tools are less than ideal for monitoring network
    traffic because they typically show only the number of packets and number of bytes
    that are sent and received over a particular network interface. That is useful
    information, but it doesn’t tell us if the network is under- or overutilized.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Unix systems, the basic network monitoring tool is `netstat` (and on most
    Linux distributions, `netstat` is not even included and must be obtained separately).
    On Windows, `typeperf` can be used in scripts to monitor the network usage—but
    here is a case where the GUI has an advantage: the standard Windows resource monitor
    will display a graph showing what percentage of the network is in use. Unfortunately,
    the GUI is of little help in an automated performance-testing scenario.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, many open source and commercial tools monitor network bandwidth.
    On Unix systems, one popular command-line tool is [`nicstat`](http://sourceforge.net/projects/nicstat),
    which presents a summary of the traffic on each interface, including the degree
    to which the interface is utilized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `e1000g1` interface is a 1,000 MB interface; it is not utilized very much
    (0.33%) in this example. The usefulness of this tool (and others like it) is that
    it calculates the utilization of the interface. In this output, 225.7 Kbps of
    data are being written, and 176.2 Kbps of data are being read over the interface.
    Doing the division for a 1,000 MB network yields the 0.33% utilization figure,
    and the `nicstat` tool was able to figure out the bandwidth of the interface automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Tools such as `typeperf` or `netstat` will report the amount of data read and
    written, but to figure out the network utilization, you must determine the bandwidth
    of the interface and perform the calculation in your own scripts. Be sure to remember
    that the bandwidth is measured in bits per second (bps), although tools generally
    report bytes per second (Bps). A 1,000-megabit network yields 125 megabytes (MB)
    per second. In this example, 0.22 MBps are read and 0.16 MBps are written; adding
    those and dividing by 125 yields a 0.33% utilization rate. So there is no magic
    to `nicstat` (or similar) tools; they are just more convenient to use.
  prefs: []
  type: TYPE_NORMAL
- en: Networks cannot sustain a 100% utilization rate. For local-area Ethernet networks,
    a sustained utilization rate over 40% indicates that the interface is saturated.
    If the network is packet-switched or utilizes a different medium, the maximum
    possible sustained rate will be different; consult a network architect to determine
    the appropriate goal. This goal is independent of Java, which will simply use
    the networking parameters and interfaces of the operating system.
  prefs: []
  type: TYPE_NORMAL
- en: Quick Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For network-based applications, monitor the network to make sure it hasn’t become
    a bottleneck.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications that write to the network can be bottlenecked because they are
    writing data inefficiently (too little throughput) or because they are writing
    too much data (too much throughput).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java Monitoring Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To gain insight into the JVM itself, Java monitoring tools are required. These
    tools come with the JDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '`jcmd`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prints basic class, thread, and JVM information for a Java process. This is
    suitable for use in scripts; it is executed like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Supplying the command `help` will list all possible commands, and supplying
    `help <*command*>` will give the syntax for a particular command.
  prefs: []
  type: TYPE_NORMAL
- en: '`jconsole`'
  prefs: []
  type: TYPE_NORMAL
- en: Provides a graphical view of JVM activities, including thread usage, class usage,
    and GC activities.
  prefs: []
  type: TYPE_NORMAL
- en: '`jmap`'
  prefs: []
  type: TYPE_NORMAL
- en: Provides heap dumps and other information about JVM memory usage. Suitable for
    scripting, though the heap dumps must be used in a postprocessing tool.
  prefs: []
  type: TYPE_NORMAL
- en: '`jinfo`'
  prefs: []
  type: TYPE_NORMAL
- en: Provides visibility into the system properties of the JVM, and allows some system
    properties to be set dynamically. Suitable for scripting.
  prefs: []
  type: TYPE_NORMAL
- en: '`jstack`'
  prefs: []
  type: TYPE_NORMAL
- en: Dumps the stacks of a Java process. Suitable for scripting.
  prefs: []
  type: TYPE_NORMAL
- en: '`jstat`'
  prefs: []
  type: TYPE_NORMAL
- en: Provides information about GC and class-loading activities. Suitable for scripting.
  prefs: []
  type: TYPE_NORMAL
- en: '`jvisualvm`'
  prefs: []
  type: TYPE_NORMAL
- en: A GUI tool to monitor a JVM, profile a running application, and analyze JVM
    heap dumps (which is a postprocessing activity, though `jvisualvm` can also take
    the heap dump from a live program).
  prefs: []
  type: TYPE_NORMAL
- en: All of these tools are easy to run from the same machine as the JVM. If the
    JVM is running inside a Docker container, the nongraphical tools (i.e., those
    except `jconsole` and `jvisualvm`) can be run via the `docker exec` command, or
    if you use `nsenter` to enter the Docker container. However, either case assumes
    that you have installed those tools into the Docker image, which is definitely
    recommended. It’s typical to pare down Docker images to the bare necessities of
    your application and hence to include only the JRE, but sooner or later in production
    you will need insight into that application, so it’s better to have the necessary
    tools (which are bundled with the JDK) within the Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: '`jconsole` requires a fair amount of system resources, so running it on a production
    system can interfere with that system. You can set up `jconsole` so that it can
    be run locally and attach to a remote system, which won’t interfere with that
    remote system’s performance. In a production environment, that requires installing
    certificates to enable `jconsole` to run over SSL, and setting up a secure authentication
    system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These tools fits into these broad areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic VM information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thread information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Class information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Live GC analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Heap dump postprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling a JVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you likely noticed, there is no one-to-one mapping here; many tools perform
    functions in multiple areas. So rather than exploring each tool individually,
    we’ll take a look at the functional areas of visibility that are important to
    Java and discuss how various tools provide that information. Along the way, we’ll
    discuss other tools (some open source, some commercial) that provide the same
    basic functionality but have advantages over the basic JDK tools.
  prefs: []
  type: TYPE_NORMAL
- en: Basic VM Information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'JVM tools can provide basic information about a running JVM process: how long
    it has been up, what JVM flags are in use, JVM system properties, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: Uptime
  prefs: []
  type: TYPE_NORMAL
- en: 'The length of time the JVM has been up can be found via this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: System properties
  prefs: []
  type: TYPE_NORMAL
- en: 'The set of items in `System.getProperties()` can be displayed with either of
    these commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This includes all properties set on the command line with a `-D` option, any
    properties dynamically added by the application, and the set of default properties
    for the JVM.
  prefs: []
  type: TYPE_NORMAL
- en: JVM version
  prefs: []
  type: TYPE_NORMAL
- en: 'The version of the JVM is obtained like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: JVM command line
  prefs: []
  type: TYPE_NORMAL
- en: 'The command line can be displayed in the VM summary tab of `jconsole`, or via
    `jcmd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: JVM tuning flags
  prefs: []
  type: TYPE_NORMAL
- en: 'The tuning flags in effect for an application can be obtained like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Working with tuning flags
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A lot of tuning flags can be given to a JVM, and many of those flags are a major
    focus of this book. Keeping track of those flags and their default values can
    be a little daunting; those last two examples of `jcmd` are useful in that regard.
    The `command_line` command shows which flags were specified directly on the command
    line. The `flags` command shows which flags were set on the command line, plus
    some flags that were set directly by the JVM (because their value was determined
    ergonomically). Including the `-all` option lists every flag within the JVM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hundreds of JVM tuning flags exist, and most are obscure; it is recommended
    that most of them never be changed (see [“Too Much Information?”](#toomuchinfo-sb)).
    Figuring out which flags are in effect is a frequent task when diagnosing performance
    issues, and the `jcmd` commands can do that for a running JVM. Often, you’d rather
    figure out the platform-specific defaults for a particular JVM, in which case
    using the `-XX:+PrintFlagsFinal` option on the command line is more useful. This
    easiest way to do that is to execute this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You should include any other options you intend to use on the command line because
    setting some options (particularly when setting GC-related flags) will affect
    the final value of other options. This will print out the entire list of JVM flags
    and their values (the same as is printed via the `VM.flags -all` option to `jcmd`
    for a live JVM).
  prefs: []
  type: TYPE_NORMAL
- en: 'Flag data from these commands is printed in one of the two ways shown. The
    colon in the first line of included output indicates that a nondefault value is
    in use for the flag in question. This can happen for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The flag’s value was specified directly on the command line.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some other option indirectly changed that option.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The JVM calculated the default value ergonomically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second line (without a colon) indicates that value is the default value
    for this version of the JVM. Default values for some flags may be different on
    different platforms, which is shown in the final column of this output. `product`
    means that the default setting of the flag is uniform across all platforms; `pd
    product` indicates that the default setting of the flag is platform-dependent.
  prefs: []
  type: TYPE_NORMAL
- en: Other possible values for the last column include `manageable` (the flag’s value
    can be changed dynamically during runtime) and `C2 diagnostic` (the flag provides
    diagnostic output for the compiler engineers to understand how the compiler is
    functioning).
  prefs: []
  type: TYPE_NORMAL
- en: Yet another way to see this information for a running application is with `jinfo`.
    The advantage of `jinfo` is that it allows certain flag values to be changed during
    execution of the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to retrieve the values of all the flags in the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: With the `-flags` option, `jinfo` will provide information about all flags;
    otherwise, it prints only those specified on the command line. The output from
    either of these commands isn’t as easy to read as that from the `-XX:+PrintFlagsFinal`
    option, but `jinfo` has other features to keep in mind.
  prefs: []
  type: TYPE_NORMAL
- en: '`jinfo` can inspect the value of an individual flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Although `jinfo` does not itself indicate whether a flag is manageable, flags
    that are manageable (as identified when using the `PrintFlagsFinal` argument)
    can be turned on or off via `jinfo`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Be aware that in JDK 8, `jinfo` can change the value of any flag, but that doesn’t
    mean that the JVM will respond to that change. For example, most flags that affect
    the behavior of a GC algorithm are used at startup time to determine various ways
    that the collector will behave. Altering a flag later via `jinfo` does not cause
    the JVM to change its behavior; it will continue executing based on how the algorithm
    was initialized. So this technique works only for those flags marked `manageable`
    in the output of the `PrintFlagsFinal` command. In JDK 11, `jinfo` will report
    an error if you attempt to change the value of a flag that cannot be changed.
  prefs: []
  type: TYPE_NORMAL
- en: Quick Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`jcmd` can be used to find the basic JVM information—including the value of
    all the tuning flags—for a running application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Default flag values can be found by including `-XX:+PrintFlagsFinal` on a command
    line. This is useful for determining the default ergonomic settings of flags on
    a particular platform.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jinfo` is useful for inspecting (and in some cases changing) individual flags.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thread Information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`jconsole` and `jvisualvm` display information (in real time) about the number
    of threads running in an application. It can be useful to look at the stack of
    running threads to determine if they are blocked. The stacks can be obtained via
    `jstack`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Stack information can also be obtained from `jcmd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: See [Chapter 9](ch09.html#ThreadPerformance) for more details on monitoring
    thread stacks.
  prefs: []
  type: TYPE_NORMAL
- en: Class Information
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Information about the number of classes in use by an application can be obtained
    from `jconsole` or `jstat`. `jstat` can also provide information about class compilation.
  prefs: []
  type: TYPE_NORMAL
- en: See [Chapter 12](ch12.html#Misc) for more details on class usage by applications,
    and see [Chapter 4](ch04.html#JustInTimeCompilation) for details on monitoring
    class compilation.
  prefs: []
  type: TYPE_NORMAL
- en: Live GC Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Virtually every monitoring tool reports something about GC activity. `jconsole`
    displays live graphs of the heap usage; `jcmd` allows GC operations to be performed;
    `jmap` can print heap summaries or information on the permanent generation or
    create a heap dump; and `jstat` produces a lot of views of what the garbage collector
    is doing.
  prefs: []
  type: TYPE_NORMAL
- en: See [Chapter 5](ch05.html#GC) for examples of how these programs monitor GC
    activities.
  prefs: []
  type: TYPE_NORMAL
- en: Heap Dump Postprocessing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Heap dumps can be captured from the `jvisualvm` GUI or from the command line
    using `jcmd` or `jmap`. The *heap dump* is a snapshot of the heap that can be
    analyzed with various tools, including `jvisualvm`. Heap dump processing is one
    area where third-party tools have traditionally been a step ahead of what comes
    with the JDK, so [Chapter 7](ch07.html#Memory) uses a third-party tool—the Eclipse
    Memory Analyzer Tool (mat)—to provide examples of how to postprocess heap dumps.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Profilers* are the most important tool in a performance analyst’s toolbox.
    Many profilers are available for Java, each with its own advantages and disadvantages.
    Profiling is one area where it often makes sense to use different tools—particularly
    if they are sampling profilers. Sampling profilers tend to show issues differently,
    so one may pinpoint performance issues better on some applications and worse on
    others.'
  prefs: []
  type: TYPE_NORMAL
- en: Many common Java profiling tools are themselves written in Java and work by
    “attaching” themselves to the application to be profiled. This attachment is via
    a socket or via a native Java interface called the JVM Tool Interface (JVMTI).
    The target application and the profiling tool then exchange information about
    the behavior of the target application.
  prefs: []
  type: TYPE_NORMAL
- en: This means you must pay attention to tuning the profiling tool just as you would
    tune any other Java application. In particular, if the application being profiled
    is large, it can transfer quite a lot of data to the profiling tool, so the profiling
    tool must have a sufficiently large heap to handle the data. It is often a good
    idea to run the profiling tool with a concurrent GC algorithm as well; ill-timed
    full GC pauses in the profiling tool can cause the buffers holding the data to
    overflow.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling Profilers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Profiling happens in one of two modes: sampling mode or instrumented mode.
    *Sampling mode* is the basic mode of profiling and carries the least amount of
    overhead. That’s important, since one of the pitfalls of profiling is that by
    introducing measurement into the application, you are altering its performance
    characteristics.^([1](ch03.html#idm45775556710120)) Limiting the impact of profiling
    will lead to results that more closely model the way the application behaves under
    usual circumstances.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, sampling profilers can be subject to all sorts of errors. Sampling
    profilers work when a timer periodically fires; the profiler then looks at each
    thread and determines which method the thread is executing. That method is then
    charged with having been executed since the timer previously fired.
  prefs: []
  type: TYPE_NORMAL
- en: The most common sampling error is illustrated by [Figure 3-1](#FigureProfile).
    The thread here is alternating between executing `methodA` (shown in the shaded
    bars) and `methodB` (shown in the clear bars). If the timer fires only when the
    thread happens to be in `methodB`, the profile will report that the thread spent
    all its time executing `methodB`; in reality, more time was actually spent in
    `methodA`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure of Methods Executing Alternately](assets/jp2e_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Alternate method execution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This is the most common sampling error, but it is by no means the only one.
    The way to minimize this error is to profile over a longer period of time and
    to reduce the time interval between samples. Reducing the interval between samples
    is counterproductive to the goal of minimizing the impact of profiling on the
    application; there is a balance here. Profiling tools resolve that balance differently,
    which is one reason that one profiling tool may happen to report much different
    data than another tool.
  prefs: []
  type: TYPE_NORMAL
- en: 'That kind of error is inherent to all sampling profilers, but is worse in many
    Java profilers (particularly older ones). This is due to *safepoint bias*. In
    the common Java interface for profilers, the profiler can get the stack trace
    of a thread only when the thread is at a safepoint. Threads automatically go into
    a safepoint when they are:'
  prefs: []
  type: TYPE_NORMAL
- en: Blocked on a synchronized lock
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blocked waiting for I/O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blocked waiting for a monitor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parked
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing Java Native Interface (JNI) code (unless they perform a GC locking
    function)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, the JVM can set a flag asking for threads to go into a safepoint.
    Code to check this flag (and go to a safepoint if necessary) is inserted into
    the JVM code at key locations, including during certain memory allocations and
    at loop or method transitions in compiled code. No specification indicates when
    these safepoint checks occur, and they vary between releases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The effect of this safepoint bias on sampling profilers can be profound: because
    the stack can be sampled only when the thread is at a safepoint, the sampling
    becomes even less reliable. In [Figure 3-1](#FigureProfile), it would be unlikely
    that a random profiler without safepoint bias would fire the thread samples only
    during the execution of `methodB`. But with safepoint bias, it is easier to see
    scenarios where `methodA` never goes to a safepoint and all work is therefore
    charged to `methodB`.'
  prefs: []
  type: TYPE_NORMAL
- en: Java 8 provides a different way for tools to gather stack traces (which is one
    reason older tools have safepoint bias, and newer tools tend not to have safepoint
    bias, though that does require that the newer tool be rewritten to use the new
    mechanism). In programmatic terms, this is done by using the `AsyncGetCallTrace`
    interface. Profilers that use this interface tend to be called *async profilers*.
    The *async* here refers to the way the JVM provides the stack information, and
    not anything about how the profiling tool works; it’s called *async* because the
    JVM can provide the stack at any point in time, without waiting for the thread
    to come to a (synchronous) safepoint.
  prefs: []
  type: TYPE_NORMAL
- en: Profilers that use this async interface hence have fewer sampling artifacts
    than other sampling profilers (though they’re still subject to errors like that
    in [Figure 3-1](#FigureProfile)). The async interface was made public in Java
    8 but existed as a private interface well before that.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-2](#FigureSampleProfile) shows a basic sampling profile taken to
    measure the performance of a REST server that provides sample stock data from
    the application described in [Chapter 2](ch02.html#SampleApplications). The REST
    call is configured to return a byte stream containing the compressed, serialized
    form of the stock object (part of an example we’ll explore in [Chapter 12](ch12.html#Misc)).
    We’ll use that sample program for examples throughout this section.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profile from a sampling profiler.](assets/jp2e_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. A sample-based profile
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This screenshot is from the Oracle Developer Studio profiler. This tool uses
    the async profiling interface, though it is not usually called an async profiler
    (likely for historical reasons, since it began using that interface when the interface
    was private and hence predates the popular use of the async profiler term). It
    provides various views into the data; in this view, we see the methods that consumed
    the most CPU cycles. Several of those methods are related to object serialization
    (e.g., the `ObjectOutputStream.writeObject0()` method), and many are related to
    calculating the actual data (e.g., the `Math.pow()` method).^([2](ch03.html#idm45775556676248))
    Still, the object serialization is dominating this profile; to improve performance,
    we’ll need to improve the serialization performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note carefully the last statement: it is the performance of serialization that
    must be improved, and not the performance of the `writeObject0()` method itself.
    The common assumption when looking at a profile is that improvements must come
    from optimizing the top method in the profile. However, that approach is often
    too limiting. In this case, the `writeObject0()` method is part of the JDK; its
    performance isn’t going to be improved without rewriting the JVM. But we do know
    from the profile that the serialization path is where our performance bottleneck
    lies.'
  prefs: []
  type: TYPE_NORMAL
- en: So the top method(s) in a profile should point you to the area in which to search
    for optimizations. Performance engineers aren’t going to attempt to make JVM methods
    faster, but they can figure out how to speed up object serialization in general.
  prefs: []
  type: TYPE_NORMAL
- en: We can visualize the sampling output in two additional ways; both visually display
    the call stack. The newest approach is called a *flame graph*, which is an interactive
    diagram of the call stacks within an application.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-3](#FigureFlameGraph) shows a portion of a flame graph from using
    the open source [`async-profiler` project](https://oreil.ly/DbNSL). A flame graph
    is a bottom-up diagram of the methods using the most CPU. In this section of the
    graph, the `getStockObject()` method is taking all the time. Roughly 60% of that
    time is spent in the `writeObject()` call, and 40% of the time in the constructor
    of the `StockPriceHistoryImpl` object. Similarly, we can read up the stack of
    each of those methods and locate our performance bottlenecks. The graph itself
    is interactive, so you can click lines and see information about the method—including
    the full name where that gets cut off, the CPU cycles, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: The older (though still useful) approach to visualizing the performance is a
    top-down approach known as the *call tree*. [Figure 3-4](#FigureCallTree) shows
    an example.
  prefs: []
  type: TYPE_NORMAL
- en: '![A flame graph from a sampling profiler.](assets/jp2e_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. A flame graph from a sampling profiler
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![A call tree from a sampling profiler.](assets/jp2e_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. A call tree from a sampling profiler
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this case, we have similar data starting with the top: of 100% of time,
    44% was spent by the `Errors.process()` method and its descendants. Then we drill
    into a parent and see where its children are spending time. For example, of the
    17% of total time spent in the `getStockObject()` method, 10% of that time was
    spent in `writeObject0` and 7% in the constructor.'
  prefs: []
  type: TYPE_NORMAL
- en: Quick Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sampling-based profilers are the most common kind of profiler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of their relatively low performance impact, sampling profilers introduce
    fewer measurement artifacts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling profilers that can use asynchronous stack collection will have fewer
    measurement artifacts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different sampling profiles behave differently; each may be better for a particular
    application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instrumented Profilers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Instrumented profilers* are much more intrusive than sampling profilers, but
    they can also give more beneficial information about what’s happening inside a
    program.'
  prefs: []
  type: TYPE_NORMAL
- en: Instrumented profilers work by altering the bytecode sequence of classes as
    they are loaded (inserting code to count the invocations, and so on). They are
    much more likely to introduce performance differences into the application than
    are sampling profilers. For example, the JVM will inline small methods (see [Chapter 4](ch04.html#JustInTimeCompilation))
    so that no method invocation is needed when the small-method code is executed.
    The compiler makes that decision based on the size of the code; depending on how
    the code is instrumented, it may no longer be eligible to be inlined. This may
    cause the instrumented profiler to overestimate the contribution of certain methods.
    And inlining is just one example of a decision that the compiler makes based on
    the layout of the code; in general, the more the code is instrumented (changed),
    the more likely it is that its execution profile will change.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because of the changes introduced into the code via instrumentation, it is
    best to limit its use to a few classes. This means it is best used for second-level
    analysis: a sampling profiler can point to a package or section of code, and then
    the instrumented profiler can be used to drill into that code if needed.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-5](#FigureInstrumentedProfile) uses an instrumenting profiler (which
    is not using the async interfaces) to look at the sample REST server.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profile from an instrumented profiler.](assets/jp2e_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. An instrumented profile
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A few things are different about this profiler. First, the dominant time is
    attributed to the `writeObject()` method and not the `writeObject0()` method.
    That’s because private methods are filtered out of the instrumentation. Second,
    a new method from the entity manager appears; this didn’t appear earlier because
    it was inlined into the constructor in the sampling case.
  prefs: []
  type: TYPE_NORMAL
- en: 'But the more important thing about this kind of profile is the invocation count:
    we executed a whopping 33 million calls to that entity manager method, and 166
    million calls to calculate a random number. We can have a much greater performance
    impact by reducing the total number of calls to these methods rather than speeding
    up their implementations, but we wouldn’t necessarily know that without the instrumentation
    count.'
  prefs: []
  type: TYPE_NORMAL
- en: Is this a better profile than the sampled version? It depends; there is no way
    to know in a given situation which is the more accurate profile. The invocation
    count of an instrumented profile is certainly accurate, and that additional information
    is often helpful in determining where the code is spending more time and which
    things are more fruitful to optimize.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, both the instrumented and sampled profiles point to the same
    general area of the code: object serialization. In practice, it is possible for
    different profilers to point to completely different areas of the code. Profilers
    are good estimators, but they are only making estimates: some of them will be
    wrong some of the time.'
  prefs: []
  type: TYPE_NORMAL
- en: Quick Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instrumented profilers yield more information about an application but could
    have a greater effect on the application than a sampling profiler.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instrumented profilers should be set up to instrument small sections of the
    code—a few classes or packages. That limits their impact on the application’s
    performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blocking Methods and Thread Timelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Figure 3-6](#FigureBlockProfile) shows the REST server using a different instrumented
    profiling tool: the profiler built into `jvisualvm`. Now the execution time is
    dominated by the `select()` methods (and to a lesser extent, the `run()` methods
    of the `TCPTransport` connection handler).'
  prefs: []
  type: TYPE_NORMAL
- en: '![A profile that shows blocked methods.](assets/jp2e_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. A profile with blocked methods
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Those methods (and similar blocking methods) do not consume CPU time, so they
    are not contributing to the overall CPU usage of the application. Their execution
    cannot necessarily be optimized. Threads in the application are not spending 673
    seconds executing code in the `select()` method; they are spending 673 seconds
    waiting for a selection event to occur.
  prefs: []
  type: TYPE_NORMAL
- en: For that reason, most profilers will not report methods that are blocked; those
    threads are shown as being idle. In this particular example, that is a good thing.
    Threads wait in the `select()` method because no data is flowing into the server;
    they are not being inefficient. That is their normal state.
  prefs: []
  type: TYPE_NORMAL
- en: In other cases, you do want to see the time spent in those blocking calls. The
    time that a thread spends inside the `wait()` method—waiting for another thread
    to notify it—is a significant determinant of the overall execution time of many
    applications. Most Java-based profilers have filter sets and other options that
    can be tweaked to show or hide these blocking calls.
  prefs: []
  type: TYPE_NORMAL
- en: Alternately, it is usually more fruitful to examine the execution patterns of
    threads rather than the amount of time a profiler attributes to the blocking method
    itself. [Figure 3-7](#FigureStudioTimeline) shows a thread display from the Oracle
    Developer Studio profiling tool.
  prefs: []
  type: TYPE_NORMAL
- en: '![A profile showing per-thread execution information](assets/jp2e_0307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-7\. A thread timeline profile
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Each horizontal area here is a different thread (so the figure shows nine threads:
    thread 1.14 to thread 1.22). The colored (or different grayscale) bars represent
    execution of different methods; blank areas represent places where the thread
    is not executing. At a high level, observe that thread 1.14 executed code and
    then waited for something.'
  prefs: []
  type: TYPE_NORMAL
- en: Notice too the blank areas where no thread appears to be executing. The image
    shows only nine of many threads in the application, so it is possible that these
    threads are waiting for one of those other threads to do something, or the thread
    could be executing a blocking `read()` (or similar) call.
  prefs: []
  type: TYPE_NORMAL
- en: Quick Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Threads that are blocked may or may not be a source of a performance issue;
    it is necessary to examine why they are blocked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blocked threads can be identified by the method that is blocking or by a timeline
    analysis of the thread.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Native Profilers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tools like `async-profiler` and Oracle Developer Studio have the capability
    to profile native code in addition to Java code. This has two advantages.
  prefs: []
  type: TYPE_NORMAL
- en: First, significant operations occur in native code, including within native
    libraries and native memory allocation. In [Chapter 8](ch08.html#NativeMemory),
    we’ll use a native profiler to see an example of a native memory allocation that
    caused a real-world issue. Using the native profiler to track memory usage quickly
    pinpointed the root cause.
  prefs: []
  type: TYPE_NORMAL
- en: Second, we typically profile to find bottlenecks in application code, but sometimes
    the native code is unexpectedly dominating performance. We would prefer to find
    out our code is spending too much time in GC by examining GC logs (as we’ll do
    in [Chapter 6](ch06.html#Collectors)), but if we forget that path, a profiler
    that understands native code will quickly show us we’re spending too much time
    in GC. Similarly, we would generally limit a profile to the period after the program
    has warmed up, but if compilation threads ([Chapter 4](ch04.html#JustInTimeCompilation))
    are running and taking too much CPU, a native-capable profiler will show us that.
  prefs: []
  type: TYPE_NORMAL
- en: When you looked at the flame graph for our sample REST server, I showed only
    a small portion for readability. [Figure 3-8](#FigureNativeFlameGraph) shows the
    entire graph.
  prefs: []
  type: TYPE_NORMAL
- en: At the bottom of this graph are five components. The first two (from JAX-RS
    code) are application threads and Java code. The third, though, is the GC for
    the process, and the fourth is the compiler.^([3](ch03.html#idm45775556595416))
  prefs: []
  type: TYPE_NORMAL
- en: '![A profile showing per-thread execution information](assets/jp2e_0308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-8\. A flame graph including native code
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Quick Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Native profilers provide visibility into both the JVM code and the application
    code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a native profiler shows that time in GC dominates the CPU usage, tuning the
    collector is the right thing to do. If it shows significant time in the compilation
    threads, however, that is usually not affecting the application’s performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java Flight Recorder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Java Flight Recorder* (JFR) is a feature of the JVM that performs lightweight
    performance analysis of applications while they are running. As its name suggests,
    JFR data is a history of events in the JVM that can be used to diagnose the past
    performance and operations of the JVM.'
  prefs: []
  type: TYPE_NORMAL
- en: JFR was originally a feature of the JRockit JVM from BEA Systems. It eventually
    made its way into Oracle’s HotSpot JVM; in JDK 8, only the Oracle JVM supports
    JFR (and it is licensed for use only by Oracle customers). In JDK 11, however,
    JFR is available in open source JVMs including the AdoptOpenJDK JVMs. Because
    JFR is open source in JDK 11, it is possible for it to be backported in open source
    to JDK 8, so AdoptOpenJDK and other versions of JDK 8 may someday include JFR
    (though that is not the case at least through 8u232).
  prefs: []
  type: TYPE_NORMAL
- en: The basic operation of JFR is that a set of events is enabled (for example,
    one event is that a thread is blocked waiting for a lock), and each time a selected
    event occurs, data about that event is saved (either in memory or to a file).
    The data stream is held in a circular buffer, so only the most recent events are
    available. Then you can use a tool to display those events—either taken from a
    live JVM or read from a saved file—and you can perform analysis on those events
    to diagnose performance issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of that—the kind of events, the size of the circular buffer, where it is
    stored, and so on—is controlled via various arguments to the JVM, or via tools,
    including `jcmd` commands as the program runs. By default, JFR is set up so that
    it has very low overhead: an impact below 1% of the program’s performance. That
    overhead will change as more events are enabled, as the threshold at which events
    are reported is changed, and so on. The details of all that configuration are
    discussed later in this section, but first we’ll examine what the display of these
    events look like, since that makes it easier to understand how JFR works.'
  prefs: []
  type: TYPE_NORMAL
- en: Java Mission Control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The usual tool to examine JFR recordings is *Java Mission Control* (`jmc`),
    though other tools exist, and you can use toolkits to write your own analysis
    tools. In the shift to a full open source JVM, `jmc` was moved out of the OpenJDK
    source base and into a separate project. This allows `jmc` to evolve on a separate
    release schedule and path, though at first it can be a little confusing to deal
    with the separate releases.
  prefs: []
  type: TYPE_NORMAL
- en: In JDK 8, `jmc` version 5 is bundled with Oracle’s JVM (the only JVM for which
    JFR is available). JDK 11 can use `jmc` version 7, though for now, the binaries
    for that must be obtained from the [OpenJDK project page](http://openjdk.java.net/projects/jmc).
    The plan is that eventually JDK builds will consume and bundle the appropriate
    `jmc` binaries.
  prefs: []
  type: TYPE_NORMAL
- en: The Java Mission Control program (`jmc`) starts a window that displays the JVM
    processes on the machine and lets you select one or more processes to monitor.
    [Figure 3-9](#FigureJMCMonitor) shows the Java Management Extensions (JMX) console
    of Java Mission Control monitoring our example REST server.
  prefs: []
  type: TYPE_NORMAL
- en: '![A Java Mission Control window.](assets/jp2e_0309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-9\. Java Mission Control monitoring
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This display shows basic information that Java Mission Control is monitoring:
    CPU usage, heap usage, and GC time. Note, though, that the CPU graph includes
    the total CPU on the machine. The JVM itself is using about 38% of the CPU, though
    all processes on the machine are consuming about 60% of the CPU. That is a key
    feature of the monitoring: via the JMX console, Java Mission Control has the ability
    to monitor the entire system, not just the JVM that has been selected. The upper
    dashboard can be configured to display JVM information (all kinds of statistics
    about GC, classloading, thread usage, heap usage, and so on) as well as OS-specific
    information (total machine CPU and memory usage, swapping, load averages, and
    so on).'
  prefs: []
  type: TYPE_NORMAL
- en: Like other monitoring tools, Java Mission Control can make Java Management Extensions
    calls into whatever managed beans the application has available.
  prefs: []
  type: TYPE_NORMAL
- en: JFR Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the appropriate tool, we can then look into how JFR works. This example
    uses a JFR recording taken from our REST server over a 6-minute period. As the
    recording is loaded into Java Mission Control, the first thing it displays is
    a basic monitoring overview ([Figure 3-10](#FigureJFRGeneral)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Basic Data Display from a JFR](assets/jp2e_0310.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-10\. Java Flight Recorder general information
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This display is similar to what Java Mission Control displays when doing basic
    monitoring. Above the gauges showing CPU and heap usage is a timeline of events
    (represented by a series of vertical bars). The timeline allows us to zoom into
    a particular region of interest; although in this example the recording was taken
    over a 6-minute period, I zoomed into a 38-second interval near the end of the
    recording.
  prefs: []
  type: TYPE_NORMAL
- en: 'This graph for CPU usage more clearly shows what is going on: the REST server
    is the bottom portion of the graph (averaging about 20% usage), and the machine
    is running at 38% CPU usage. Along the bottom, other tabs allow us to explore
    information about system properties and how the JFR recording was made. The icons
    that run down the left side of the window are more interesting: those icons provide
    visibility into the application behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: JFR Memory view
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The information gathered here is extensive. [Figure 3-11](#FigureJFRMemory)
    shows just one panel of the Memory view.
  prefs: []
  type: TYPE_NORMAL
- en: '![A display of the Java Flight Recorder Memory Panel](assets/jp2e_0311.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-11\. Java Flight Recorder Memory panel
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This graph shows that memory is fluctuating fairly regularly as the young generation
    is cleared (and we see that the heap overall is growing during this time: it started
    at about 340 MB and ends about at 2 GB). The lower-left panel shows all the collections
    that occurred during the recording, including their duration and type of collection
    (always a `ParallelScavenge` in this example). When one of those events is selected,
    the bottom-right panel breaks that down even further, showing all the specific
    phases of that collection and how long each took.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The various tabs on this page provide a wealth of other information: how long
    and how many reference objects were cleared, whether there are promotion or evacuation
    failures from the concurrent collectors, the configuration of the GC algorithm
    itself (including the sizes of the generations and the survivor space configurations),
    and even information on the specific kinds of objects that were allocated. As
    you read through Chapters [5](ch05.html#GC) and [6](ch06.html#Collectors), keep
    in mind how this tool can diagnose the problems that are discussed there. If you
    need to understand why the G1 GC collector bailed out and performed a full GC
    (was it due to promotion failure?), how the JVM has adjusted the tenuring threshold,
    or virtually any other piece of data about how and why GC behaved as it did, JFR
    will be able to tell you.'
  prefs: []
  type: TYPE_NORMAL
- en: JFR Code view
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Code page in Java Mission Control shows basic profiling information from
    the recording ([Figure 3-12](#FigureJFRCode)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A JFR showing where the application spent its time executing.](assets/jp2e_0312.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-12\. Java Flight Recorder Code panel
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The first tab on this page shows an aggregation by package name, which is an
    interesting feature not found in many profilers. At the bottom, other tabs present
    the traditional profile views: the hot methods and call tree of the profiled code.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other profilers, JFR offers other modes of visibility into the code.
    The Exceptions tab provides a view into the exception processing of the application
    ([Chapter 12](ch12.html#Misc) discusses why excessive exception processing can
    be bad for performance). Other tabs provide information on what the compiler is
    doing, including a view into the code cache (see [Chapter 4](ch04.html#JustInTimeCompilation)).
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, note that the packages here didn’t really show up in the
    previous profiles we looked at; conversely, the previous hot spots we saw don’t
    appear here. Because it is designed to have very low overhead, the profile sampling
    of JFR (at least in the default configuration) is quite low, and so the profiles
    are not as accurate as what we’d see from a more intrusive sampling.
  prefs: []
  type: TYPE_NORMAL
- en: There are other displays like this—for threads, I/O, and system events—but for
    the most part, these displays simply provide nice views into the actual events
    in the JFR recording.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of JFR events
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: JFR produces a stream of events that are saved as a recording. The displays
    seen so far provide views of those events, but the most powerful way to look at
    the events is on the Event panel itself, as shown in [Figure 3-13](#FigureJFREvent).
  prefs: []
  type: TYPE_NORMAL
- en: '![A display of events from a JFR.](assets/jp2e_0313.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-13\. Java Flight Recorder Event panel
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The events to display can be filtered in the left panel of this window; here,
    application-level and JVM-level events are selected. Be aware that when the recording
    is made, only certain kinds of events are included in the first place: at this
    point, we are doing postprocessing filtering (the next section shows how to filter
    the events included in the recording).'
  prefs: []
  type: TYPE_NORMAL
- en: Within the 34-second interval in this example, the application produced 878
    events from the JVM and 32 events from the JDK libraries, and the event types
    generated in that period are shown near the bottom of the window. When we looked
    at this example with profilers, we saw why the thread-park and monitor-wait events
    for this example will be high; those can be ignored (and the thread park events
    are filtered out here in the left panel). What about the other events?
  prefs: []
  type: TYPE_NORMAL
- en: Over the 34-second period, multiple threads in the application spent 34 seconds
    reading from sockets. That number doesn’t sound good, particularly because it
    will show up in JFR only if the socket read takes longer than 10 milliseconds.
    We need to look at that further, which can be done by visiting the log tab shown
    in [Figure 3-14](#FigureJFRLog).
  prefs: []
  type: TYPE_NORMAL
- en: '![A log of events from a JFR.](assets/jp2e_0314.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-14\. Java Flight Recorder Event Log panel
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It is worthwhile to look at the traces involved with those events, but it turns
    out that several threads use blocking I/O to read administrative requests that
    are expected to arrive periodically. Between those requests—for long periods of
    time—the threads sit blocked on the `read()` method. So the read time here turns
    out to be acceptable: just as when using a profiler, it is up to you to determine
    whether a lot of threads blocked in I/O is expected or indicates a performance
    issue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'That leaves the monitor-blocked events. As discussed in [Chapter 9](ch09.html#ThreadPerformance),
    contention for locks goes through two levels: first the thread spins waiting for
    the lock, and then it uses (in a process called *lock inflation*) some CPU- and
    OS-specific code to wait for the lock. A standard profiler can give hints about
    that situation, since the spinning time is included in the CPU time charged to
    a method. A native profiler can give information about the locks subject to inflation,
    but that can be hit or miss. The JVM, though, can provide all this data directly
    to JFR.'
  prefs: []
  type: TYPE_NORMAL
- en: An example of using lock visibility is shown in [Chapter 9](ch09.html#ThreadPerformance),
    but the general takeaway about JFR events is that, because they come directly
    from the JVM, they offer a level of visibility into an application that no other
    tool can provide. In Java 11, about 131 event types can be monitored with JFR.
    The exact number and types of events will vary slightly depending on release,
    but the following list details some of the more useful ones.
  prefs: []
  type: TYPE_NORMAL
- en: Each event type in the following list displays two bullet points. Events can
    collect basic information that can be collected with other tools like `jconsole`
    and `jcmd`; that kind of information is described in the first bullet. The second
    bullet describes information the event provides that is difficult to obtain outside
    JFR.
  prefs: []
  type: TYPE_NORMAL
- en: Classloading
  prefs: []
  type: TYPE_NORMAL
- en: Number of classes loaded and unloaded
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which classloader loaded the class; time required to load an individual class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thread statistics
  prefs: []
  type: TYPE_NORMAL
- en: Number of threads created and destroyed; thread dumps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which threads are blocked on locks (and the specific lock they are blocked on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throwables
  prefs: []
  type: TYPE_NORMAL
- en: Throwable classes used by the application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of exceptions and errors thrown and the stack trace of their creation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TLAB allocation
  prefs: []
  type: TYPE_NORMAL
- en: Number of allocations in the heap and size of thread-local allocation buffers
    (TLABs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specific objects allocated in the heap and the stack trace where they are allocated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: File and socket I/O
  prefs: []
  type: TYPE_NORMAL
- en: Time spent performing I/O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time spent per read/write call, the specific file or socket taking a long time
    to read or write
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor blocked
  prefs: []
  type: TYPE_NORMAL
- en: Threads waiting for a monitor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specific threads blocked on specific monitors and the length of time they are
    blocked
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code cache
  prefs: []
  type: TYPE_NORMAL
- en: Size of code cache and how much it contains
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods removed from the code cache; code cache configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code compilation
  prefs: []
  type: TYPE_NORMAL
- en: Which methods are compiled, on-stack replacement (OSR) compilation (see [Chapter 4](ch04.html#JustInTimeCompilation)),
    and length of time to compile
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nothing specific to JFR, but unifies information from several sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Garbage collection
  prefs: []
  type: TYPE_NORMAL
- en: Times for GC, including individual phases; sizes of generations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nothing specific to JFR, but unifies the information from several tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling
  prefs: []
  type: TYPE_NORMAL
- en: Instrumenting and sampling profiles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Not as much as you’d get from a true profiler, but the JFR profile provides
    a good high-order overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling JFR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JFR is initially disabled. To enable it, add the flag `-XX:+FlightRecorder`
    to the command line of the application. This enables JFR as a feature, but no
    recordings will be made until the recording process itself is enabled. That can
    occur either through a GUI or via the command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Oracle’s JDK 8, you must also specify this flag (prior to the `FlightRecorder`
    flag): `-XX:+UnlockCommercialFeatures` (default: `false`).'
  prefs: []
  type: TYPE_NORMAL
- en: If you forget to include these flags, remember that you can use `jinfo` to change
    their values and enable JFR. If you use `jmc` to start a recording, it will automatically
    change these values in the target JVM if necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling JFR via Java Mission Control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The easiest way to enable recording of a local application is through the Java
    Mission Control GUI (`jmc`). When `jmc` is started, it displays a list of all
    the JVM processes running on the current system. The JVM processes are displayed
    in a tree-node configuration; expand the node under the Flight Recorder label
    to bring up the flight recorder window shown in [Figure 3-15](#FigureJFRStart).
  prefs: []
  type: TYPE_NORMAL
- en: '![The Wizard to start a flight recording and control its parameters.](assets/jp2e_0315.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-15\. JFR Start Flight Recording window
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Flight recordings are made in one of two modes: either for a fixed duration
    (1 minute in this case) or continuously. For continuous recordings, a circular
    buffer is utilized; the buffer will contain the most recent events that are within
    the desired duration and size.'
  prefs: []
  type: TYPE_NORMAL
- en: To perform proactive analysis—meaning that you will start a recording and then
    generate some work or start a recording during a load-testing experiment after
    the JVM has warmed up—a fixed-duration recording should be used. That recording
    will give a good indication of how the JVM responded during the test.
  prefs: []
  type: TYPE_NORMAL
- en: The continuous recording is best for reactive analysis. This lets the JVM keep
    the most recent events and then dump out a recording in response to an event.
    For example, the WebLogic application server can trigger that a recording be dumped
    out in response to an abnormal event in the application server (such as a request
    that takes more than 5 minutes to process). You can set up your own monitoring
    tools to dump out the recording in response to any sort of event.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling JFR via the command line
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After enabling JFR (with the `-XX:+FlightRecorder` option), there are different
    ways to control how and when the actual recording should happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'In JDK 8, the default recording parameters can be controlled when the JVM starts
    by using the `-XX:+FlightRecorderOptions=`*`string`* parameter; this is most useful
    for reactive recordings. The *`string`* in that parameter is a list of comma-separated
    name-value pairs taken from these options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name=*name*`'
  prefs: []
  type: TYPE_NORMAL
- en: The name used to identify the recording.
  prefs: []
  type: TYPE_NORMAL
- en: '`defaultrecording=*<true|false>*`'
  prefs: []
  type: TYPE_NORMAL
- en: Whether to start the recording initially. The default value is `false`; for
    reactive analysis, this should be set to `true`.
  prefs: []
  type: TYPE_NORMAL
- en: '`settings=*path*`'
  prefs: []
  type: TYPE_NORMAL
- en: Name of the file containing the JFR settings (see the next section).
  prefs: []
  type: TYPE_NORMAL
- en: '`delay=*time*`'
  prefs: []
  type: TYPE_NORMAL
- en: The amount of time (e.g., `30s`, `1h`) before the recording should start.
  prefs: []
  type: TYPE_NORMAL
- en: '`duration=*time*`'
  prefs: []
  type: TYPE_NORMAL
- en: The amount of time to make the recording.
  prefs: []
  type: TYPE_NORMAL
- en: '`filename=*path*`'
  prefs: []
  type: TYPE_NORMAL
- en: Name of the file to write the recording to.
  prefs: []
  type: TYPE_NORMAL
- en: '`compress=*<true|false>*`'
  prefs: []
  type: TYPE_NORMAL
- en: Whether to compress (with gzip) the recording; the default is `false`.
  prefs: []
  type: TYPE_NORMAL
- en: '`maxage=*time*`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum time to keep recorded data in the circular buffer.
  prefs: []
  type: TYPE_NORMAL
- en: '`maxsize=*size*`'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum size (e.g., `1024K`, `1M`) of the recording’s circular buffer.
  prefs: []
  type: TYPE_NORMAL
- en: '`-XX:+FlightRecorderOptions` only sets the defaults for any options; individual
    recordings can override those settings.'
  prefs: []
  type: TYPE_NORMAL
- en: In both JDK 8 and JDK 11, you can start a JFR when the program initially begins
    by using the `-XX:+StartFlightRecording=*string*` flag with a similar comma-separated
    list of options.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a default recording like that can be useful in some circumstances,
    but for more flexibility, all options can be controlled with `jcmd` during a run.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start a flight recording:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `options_list` is a series of comma-separated name-value pairs that control
    how the recording is made. The possible options are exactly the same as those
    that can be specified on the command line with the `-XX:+FlightRecorderOptions=`*`string`*
    flag.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a continuous recording has been enabled, the current data in the circular
    buffer can be dumped to a file at any time via this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The list of options includes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name=*name*`'
  prefs: []
  type: TYPE_NORMAL
- en: The name under which the recording was started (see the next example for `JFR.check`).
  prefs: []
  type: TYPE_NORMAL
- en: '`filename=*path*`'
  prefs: []
  type: TYPE_NORMAL
- en: The location to dump the file to.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible that multiple JFR recordings have been enabled for a given process.
    To see the available recordings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In this example, process ID 21532 has two active JFR recordings that are named
    1 and 2\. That name can be used to identify them in other `jcmd` commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to abort a recording in process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'That command takes the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name=*name*`'
  prefs: []
  type: TYPE_NORMAL
- en: The recording name to stop.
  prefs: []
  type: TYPE_NORMAL
- en: '`discard=*boolean*`'
  prefs: []
  type: TYPE_NORMAL
- en: If `true`, discard the data rather than writing it to the previously provided
    filename (if any).
  prefs: []
  type: TYPE_NORMAL
- en: '`filename=*path*`'
  prefs: []
  type: TYPE_NORMAL
- en: Write the data to the given path.
  prefs: []
  type: TYPE_NORMAL
- en: In an automated performance-testing system, running these command-line tools
    and producing a recording is useful when it comes time to examine those runs for
    regressions.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting JFR Events
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned earlier, JFR supports many events. Often, these are periodic events:
    they occur every few milliseconds (e.g., the profiling events work on a sampling
    basis). Other events are triggered only when the duration of the event exceeds
    a certain threshold (e.g., the event for reading a file is triggered only if the
    `read()` method has taken more than a specified amount of time).'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting events naturally involves overhead. The threshold at which events
    are collected—since it increases the number of events—also plays a role in the
    overhead that comes from enabling a JFR recording. In the default recording, not
    all events are collected (the six most-expensive events are not enabled), and
    the threshold for the time-based events is somewhat high. This keeps the overhead
    of the default recording to less than 1%.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes extra overhead is worthwhile. Looking at TLAB events, for example,
    can help you determine if objects are being allocated directly to the old generation,
    but those events are not enabled in the default recording. Similarly, the profiling
    events are enabled in the default recording, but only every 20 ms—that gives a
    good overview, but it can also lead to sampling errors.^([4](ch03.html#idm45775556384040))
  prefs: []
  type: TYPE_NORMAL
- en: 'The events (and the threshold for events) that JFR captures are defined in
    a template (which is selected via the `settings` option on the command line).
    JFR ships with two templates: the default template (limiting events so that the
    overhead will be less than 1%) and a profile template (which sets most threshold-based
    events to be triggered every 10 ms). The estimated overhead of the profiling template
    is 2% (though, as always, your mileage may vary, and typically overhead is lower
    than that).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Templates are managed by the `jmc` template manager; you may have noticed a
    button to start the template manager in [Figure 3-15](#FigureJFRStart). Templates
    are stored in two locations: under the *$HOME/.jmc/<release>* directory (local
    to a user) and in the *$JAVA_HOME/jre/lib/jfr* directory (global for a JVM). The
    template manager allows you to select a global template (the template will say
    that it is “on server”), select a local template, or define a new template. To
    define a template, cycle through the available events, and enable (or disable)
    them as desired, optionally setting the threshold at which the event kicks in.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 3-16](#FigureJFRTemplate) shows that the File Read event is enabled
    with a threshold of 15 ms: file reads that take longer than that will cause an
    event to be triggered. This event has also been configured to generate a stack
    trace for the File Read events. That increases the overhead—which in turn is why
    taking a stack trace for events is a configurable option.'
  prefs: []
  type: TYPE_NORMAL
- en: '![The wizard to enable a JFR event.](assets/jp2e_0316.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-16\. A sample JFR event template
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The event templates are simple XML files, so the best way to determine which
    events are enabled in a template (and their thresholds and stack-trace configurations)
    is to read the XML file. Using an XML file also allows the local template file
    to be defined on one machine and then copied to the global template directory
    for use by others on the team.
  prefs: []
  type: TYPE_NORMAL
- en: Quick Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Java Flight Recorder provides the best possible visibility into the JVM, since
    it is built into the JVM itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Like all tools, JFR introduces some level of overhead into an application. For
    routine use, JFR can be enabled to gather a substantial amount of information
    with low overhead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JFR is useful in performance analysis, but it is also useful when enabled on
    a production system so that you can examine the events that led up to a failure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Good tools are key to good performance analysis; in this chapter, we’ve just
    scratched the surface of what tools can tell us. Here are the key things to keep
    in mind:'
  prefs: []
  type: TYPE_NORMAL
- en: No tool is perfect, and competing tools have relative strengths. Profiler X
    may be a good fit for many applications, but in some cases it will miss something
    that Profiler Y points out quite clearly. Always be flexible in your approach.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Command-line monitoring tools can gather important data automatically; be sure
    to include gathering this monitoring data in your automated performance testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tools rapidly evolve: some of the tools mentioned in this chapter are probably
    already obsolete (or at least have been superseded by new, superior tools). Keeping
    up-to-date in this area is important.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '^([1](ch03.html#idm45775556710120-marker)) Still, you must profile: how else
    will you know if the cat inside your program is still alive?'
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch03.html#idm45775556676248-marker)) You’ll see references to native C++
    code like `InstanceKlass::oop_push_contents`; we’ll look at that more in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch03.html#idm45775556595416-marker)) This particular graph is again from
    the Oracle Developer Studio tool, though `async-profiler` produced the identical
    set of native calls.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch03.html#idm45775556384040-marker)) That’s why the JFR profile we looked
    at didn’t necessarily match the more intrusive profiles from previous sections.
  prefs: []
  type: TYPE_NORMAL
