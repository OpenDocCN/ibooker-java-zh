- en: 'Chapter 11\. The Event Bus: The Backbone'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章. 事件总线：骨干
- en: In [Chapter 10](ch10.html#messaging), we discussed Reactive Messaging and utilizing
    its annotations to produce, consume, and process messages, as well as to bridge
    imperative and reactive programming. This chapter dives deeper into the backbone
    of a reactive system built with Reactive Messaging, focusing on Apache Kafka and
    Advanced Message Queuing Protocol (AMQP).^([1](ch11.html#idm45358818317616))
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](ch10.html#messaging)中，我们讨论了反应式消息传递，并利用其注解来生成、消费和处理消息，以及桥接命令式和反应式编程。本章将更深入地探讨使用反应式消息传递构建反应式系统的骨架，重点介绍Apache
    Kafka和高级消息队列协议（AMQP）的使用。^([1](ch11.html#idm45358818317616))
- en: 'Kafka or AMQP: Picking the Right Tool'
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka还是AMQP：选择合适的工具
- en: Plenty of messaging solutions let you implement event-driven architecture, event
    streaming, and reactive systems in general. Recently, Apache Kafka became a prominent
    player in this space. AMQP is another approach for messaging that should not be
    immediately ruled out. Both have pros and cons. Your choice depends entirely on
    your use cases, and to a lesser extent the existing skills and experience of a
    team.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 许多消息传递解决方案让您能够实现事件驱动架构、事件流处理以及反应式系统。最近，Apache Kafka在这一领域中成为了一个显赫的角色。AMQP是另一种消息传递方法，也不应立即被排除在外。两者各有优缺点。您的选择完全取决于您的用例，以及团队现有的技能和经验。
- en: Rather than favoring one event bus over another, this section details the characteristics
    and behaviors of each, along with their relative strengths and weaknesses. We
    want to provide you sufficient information about each system, enabling you to
    determine how they may fit into the use cases for a particular system.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本节不偏爱某个事件总线，而是详细介绍了每个事件总线的特性和行为，以及它们的优势和劣势。我们希望为您提供关于每个系统的充足信息，以帮助您确定它们如何适合特定系统的用例。
- en: At a high level, Kafka can be described as smart consumers with a dumb broker,
    while AMQP has a smart broker but dumb consumers. Sometimes the choice comes down
    to the amount of flexibility needed when implementing a solution.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，可以将Kafka描述为具有聪明消费者的愚蠢代理，而AMQP则具有聪明代理但愚蠢消费者。有时候选择的关键在于在实施解决方案时所需的灵活性。
- en: Sure it’s cliché, but there really is no one-size-fits-all event bus. Each situation
    has specific requirements and use cases to be fulfilled, requiring careful evaluation
    of the pros and cons of each. Note that other messaging solutions might be better
    for your use case, such as the [Solace PubSub+ Platform](https://solace.com),
    [Microsoft Azure Event Hubs](https://oreil.ly/eXz0E), [RabbitMQ](https://rabbitmq.com),
    or [NATS](https://nats.io).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这可能有些陈词滥调，但确实没有一种事件总线适合所有情况。每种情况都有特定的需求和用例需要满足，需要仔细评估每个事件总线的优缺点。请注意，对于您的用例，其他消息解决方案可能更合适，如[Solace
    PubSub+ Platform](https://solace.com)、[Microsoft Azure Event Hubs](https://oreil.ly/eXz0E)、[RabbitMQ](https://rabbitmq.com)或[NATS](https://nats.io)。
- en: Building Reactive Systems with Kafka
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kafka构建反应式系统
- en: Since 2011, when Apache Kafka was open sourced by LinkedIn, it has skyrocketed
    to becoming one of the most prominent actors in the event-driven space. Fueled
    by the rise of microservices, serverless architecture, and distributed systems
    in general, Kafka is a popular choice for developers needing a messaging backbone.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 自2011年LinkedIn开源Apache Kafka以来，它已迅速成为事件驱动领域中最重要的角色之一。在微服务、无服务器架构以及分布式系统普及的推动下，Kafka成为开发人员需要的消息传递支柱之一。
- en: While [Chapter 10](ch10.html#messaging) already gave examples of using Kafka,
    we didn’t explain the important details of how it works underneath, which are
    necessary to understand it well. This chapter dives a bit deeper and explains
    how to use Kafka as the connective tissue in a reactive system. We don’t intend
    to cover Kafka in full detail, but enough to appreciate how Kafka operates to
    effectively develop a reactive system. First off, we need to cover the basics
    of Kafka.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10章](ch10.html#messaging)中已经给出了使用Kafka的例子，但我们没有解释它在底层的重要细节，这些细节对于深入理解它是必要的。本章将深入探讨，解释如何在反应式系统中使用Kafka作为连接组织。我们不打算详尽覆盖Kafka的所有细节，但会介绍足够量的内容，让您能够理解Kafka运作的方式，以便有效地开发反应式系统。首先，我们需要了解Kafka的基础知识。
- en: Apache Kafka
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Apache Kafka
- en: Kafka is a powerful distributed commit log, and if you’re a developer, you’re
    probably familiar with another distributed commit log, Git! When communicating
    with Kafka, we use a *record*, or event, as the piece of information we want written
    to the log, and which we then read from the log later. The log contains a *topic*
    for each record grouping we want to track ([Figure 11-1](#image:kafka-topic)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 是一个强大的分布式提交日志，如果你是开发者，你可能熟悉另一个分布式提交日志，Git！在与 Kafka 通信时，我们使用一个*记录*或事件作为我们希望写入日志的信息片段，然后稍后从日志中读取。每个记录分组在日志中有一个*主题*，用于跟踪（[图 11-1](#image:kafka-topic)）。
- en: '![Kafka topic](assets/rsij_1101.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![Kafka 主题](assets/rsij_1101.png)'
- en: Figure 11-1\. Kafka topic
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-1\. Kafka 主题
- en: 'A record can hold only four pieces of information:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 记录只能包含四个信息片段：
- en: Key
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 关键
- en: Assigned by Kafka when writing a record into the log, but can also be used in
    partitioning, which we cover later
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当将记录写入日志时由 Kafka 分配，但也可以在分区中使用，我们稍后会涵盖这一点
- en: Value
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 值
- en: The actual value, or payload, we want to be stored in the log for retrieval
    by consumers
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望存储在日志中以供消费者检索的实际值或负载
- en: Timestamp
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 时间戳
- en: Optionally set when we create the record, or set by Kafka when the record is
    written to the log
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建记录时可选设置，或者在将记录写入日志时由 Kafka 设置
- en: Headers
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 标头
- en: Optional metadata about the record to provide extra information to Kafka, or
    for downstream consumers to utilize
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 提供给 Kafka 的关于记录的可选元数据，或者供下游消费者使用的额外信息
- en: '[Figure 11-2](#image:log-produce-consume) outlines the process of interacting
    with the log.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-2](#image:log-produce-consume) 概述了与日志交互的过程。'
- en: '![Producing and consuming records](assets/rsij_1102.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![生产和消费记录](assets/rsij_1102.png)'
- en: Figure 11-2\. Producing and consuming records
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-2\. 生产和消费记录
- en: With a record created, we write it to the log in Kafka with a producer. We can
    have one or many producer instances writing the same type of record to the log,
    because the way we write records to the log is decoupled from the way they’re
    consumed. Once a record is written to the log, we use a consumer to read the record
    from the log and perform whatever processing is needed with it.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 创建记录后，我们使用生产者将其写入 Kafka 中的日志。我们可以有一个或多个生产者实例将相同类型的记录写入日志，因为我们写入日志的方式与消费的方式是解耦的。一旦记录写入日志，我们就使用消费者从日志中读取记录，并进行需要的处理。
- en: When writing records to the log, a producer always appends. A producer cannot
    insert or delete records. An append-only approach means Kafka can offer high scalability
    for writes. Because there is no contention or locking for existing records, every
    write is a new record.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当将记录写入日志时，生产者始终进行追加。生产者无法插入或删除记录。追加方式意味着 Kafka 可以提供高可伸缩性的写入能力。因为对现有记录没有争用或锁定，每次写入都是一个新记录。
- en: Separation of producers and consumers is a key concept with Kafka. This decoupling
    of time between when records are written to the log and consumed from it is key
    for reactive systems. Granted, this is 100% achievable only when the log retention
    policies are sufficiently long enough to prevent any produced records from being
    removed before they’re consumed! We don’t want to log all the records and have
    them deleted by Kafka before we consume them years later.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kafka 中，生产者和消费者的分离是一个关键概念。这种记录写入和消费时间的解耦对于响应式系统至关重要。当然，只有当日志保留策略足够长，以防止任何生产的记录在被消费之前被
    Kafka 删除时，才能完全实现这一点！我们不希望将所有记录记录下来，然后在多年后消费之前被 Kafka 删除。
- en: In [Chapter 10](ch10.html#messaging), you saw how to produce messages with `@Outgoing`.
    Let’s modify that example slightly to also set a key for the record, as shown
    in [Example 11-1](#data::config-outgoing-meta).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 10 章](ch10.html#messaging)中，您看到如何使用 `@Outgoing` 生产消息。让我们稍微修改该示例，还设置一个记录的键，如[示例 11-1](#data::config-outgoing-meta)所示。
- en: Example 11-1\. Configure the Kafka outgoing metadata
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-1\. 配置 Kafka 出站元数据
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here we have a key to indicate whether the person is part of the *light* or
    *dark* side of the force. To switch to producing the message to Kafka, we need
    to make two changes. First, modify *pom.xml* to include the SmallRye Reactive
    Messaging for Kafka dependency ([Example 11-2](#data:connector-dependency)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有一个键，指示该人是*光明*还是*黑暗*面的一部分。要切换到将消息发送到 Kafka，我们需要进行两个更改。首先，修改 *pom.xml*
    以包括 SmallRye Reactive Messaging for Kafka 依赖项（[示例 11-2](#data:connector-dependency)）。
- en: Example 11-2\. Kafka connector dependency (*/Users/clement/Documents/book/code-repository/chapter-11/processor/pom.xml*)
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-2\. Kafka 连接器依赖项（*/Users/clement/Documents/book/code-repository/chapter-11/processor/pom.xml*）
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Lastly, configure the dependency, as shown in [Example 11-3](#data::config-dependency).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，按照[示例 11-3](#data::config-dependency)配置依赖项。
- en: Example 11-3\. Configure the Kafka connector to write records (*chapter-11/processor/src/main/resources/application.properties*)
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-3\. 配置Kafka连接器以写入记录（*chapter-11/processor/src/main/resources/application.properties*）
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The configuration indicates the connector we’re using, `smallrye-kafka`, the
    name of the topic the channel should be writing to, and the serializer for converting
    the payload content. If the topic being written to matches the name of the channel,
    we would not need the `topic` configuration, as the channel name is the default.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 配置指示我们正在使用的连接器`smallrye-kafka`，通道应该写入的主题名称，以及用于转换负载内容的序列化器。如果要写入的主题与通道名称匹配，我们将不需要`topic`配置，因为通道名称是默认值。
- en: On the consuming side, we can read the key with [Example 11-4](#data::extract-metadata).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在消费方面，我们可以通过[示例 11-4](#data::extract-metadata)读取键。
- en: Example 11-4\. Extract the incoming Kafka metadata
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-4\. 提取传入的Kafka元数据
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We also need similar configuration as outgoing for us to use Kafka; see [Example 11-5](#data::config-poll-records).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要与我们使用Kafka的出站配置相似的配置；参见[示例 11-5](#data::config-poll-records)。
- en: Example 11-5\. Configure the Kafka connector to poll records (*chapter-11/processor/src/main/resources/application.properties*)
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-5\. 配置Kafka连接器以轮询记录（*chapter-11/processor/src/main/resources/application.properties*）
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: So far, we’ve been generically referring to write records to and consume records
    from a log. As you have seen in [Figure 11-1](#image:kafka-topic), a *topic* is
    a log, a means of organizing and durably storing records. This log is for a specific
    type of record, or group of records, enabling us to customize the behavior specifically
    to the needs of those records. For instance, if the records are of extremely high
    volume and not meaningful to the application for longer than a week, we could
    change the retention policy for one topic to retain records for only that period
    of time, even if they haven’t been consumed. We could also have another topic
    that has records retained for six months or even indefinitely.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经泛泛地提到了写入记录到日志和消费记录从日志。正如您在[图 11-1](#image:kafka-topic)中看到的那样，*主题*是一个日志，用于组织和持久存储记录。此日志是特定类型记录或一组记录的日志，使我们能够根据这些记录的特定需求定制行为。例如，如果记录的数量非常大并且对应用程序长达一周的时间没有意义，我们可以更改一个主题的保留策略，仅保留该期间内的记录，即使它们没有被消费。我们还可以有另一个主题，其记录保留时间长达六个月甚至无限期。
- en: Also in [Figure 11-1](#image:kafka-topic) you can see each record as a box with
    a number; this represents the *offset*, or index, of where a record is written
    in a topic. In this instance, six records have already been written, and a producer
    is about to write the seventh, which is offset 6. We also see a consumer reading
    the record at offset 0, the first record in the topic. Though the default is for
    a new consumer to begin reading records from the first offset, we could decide
    to start at any offset we wanted.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 同样在[图 11-1](#image:kafka-topic)中，您可以看到每个记录都表示为带有编号的框；这代表了记录在主题中写入的*偏移量*或索引。在这个示例中，已经写入了六条记录，并且生产者即将写入第七条记录，其偏移量为6。我们还看到一个消费者正在读取偏移量为0的记录，即主题中的第一条记录。虽然默认情况下新消费者从第一个偏移量开始读取记录，但我们可以决定从任何偏移量开始。
- en: Another way to consider a topic is as a virtual address representing an external
    destination. When a producer writes a record to a topic, it has no knowledge of
    when, if, or even where the record will be read by a consumer. Use of a virtual
    address, or topic, provides the means of decoupling our reactive system components
    from one another in space and time.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种考虑主题的方式是将其视为表示外部目标的虚拟地址。当生产者将记录写入主题时，它不知道记录将何时、是否或甚至在何处被消费者读取。使用虚拟地址或主题提供了在空间和时间上解耦我们的反应式系统组件的方式。
- en: A consumer can be combined with others to form a *consumer group*. Any consumer
    created with the same consumer group name, or identifier, will be placed in the
    same consumer group. When creating a consumer without setting a consumer group
    identifier, we end up with a consumer group containing a single consumer by default.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 消费者可以与其他消费者结合形成一个*消费者组*。任何使用相同消费者组名称或标识符创建的消费者都将放置在同一个消费者组中。当创建一个没有设置消费者组标识符的消费者时，默认情况下会得到一个包含单个消费者的消费者组。
- en: So far, what we’ve described implies a topic with a single log of records. A
    *partition* is how we improve the problems associated with a single-log approach.
    Partitions are useful for improving the read and write performance of a topic,
    as we split a single topic into multiple partitions.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们描述的是具有单个记录日志的主题。*分区*是我们解决单一记录方法所带来问题的方式。分区有助于提高主题的读写性能，因为我们将单个主题分割为多个分区。
- en: Instead of a single partition with a single consumer, we could have three partitions
    with a separate consumer reading records from each of them. Looking at this situation
    unscientifically, we could expect there to be three times the amount of throughput
    with three partitions and three consumers, as opposed to a single partition and
    consumer.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不再使用单个分区和单个消费者，而是使用三个分区，并且每个分区都有一个独立的消费者读取记录。从非科学的角度看，我们可以预期使用三个分区和三个消费者的吞吐量是单个分区和消费者的三倍。
- en: Though we mentioned three consumers for three partitions, in [Figure 11-3](#image:kafka-topic-partitions)
    we have two consumers within a single consumer group. One consumer is assigned
    to read records from two partitions, to ensure that all partitions have consumers.
    We’ve now improved our throughput by partitioning the topic, enabling multiple
    consumers to consume records.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们提到了三个消费者对应三个分区，但在 [图 11-3](#image:kafka-topic-partitions) 中，我们有两个消费者在同一个消费者组内。一个消费者负责从两个分区读取记录，以确保所有分区都有消费者。通过对主题进行分区，我们现在通过增加消费者来提高吞吐量。
- en: '![Topic partitions](assets/rsij_1103.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![主题分区](assets/rsij_1103.png)'
- en: Figure 11-3\. Topic partitions
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-3\. 主题分区
- en: In the situation shown in [Figure 11-3](#image:kafka-topic-partitions), a producer
    can write a record to the topic, leaving the broker to decide which partition
    the record is actually written to. Alternatively, the producer can explicitly
    define the partition that a record should be written to. If records have a key
    unique to the record’s contents, such as username for a `Person` record, it can
    be efficient to determine the appropriate partition with the Kafka key-hashing
    algorithm. It will ensure that all records with an identical key are written to
    the same partition. We need to be careful, though, to ensure that any key is reasonably
    distributed. Otherwise, we risk creating a *hot partition* (for example, partitioning
    by country may see trillions of records placed in a USA partition, but only a
    few thousand records in the Andorra partition).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 11-3](#image:kafka-topic-partitions) 中所示的情况下，生产者可以将记录写入主题，由代理决定将记录实际写入到哪个分区。另外，生产者也可以明确指定记录应写入的分区。如果记录具有唯一于其内容的键，例如
    `Person` 记录的用户名，使用 Kafka 键哈希算法可以高效地确定适当的分区。这将确保具有相同键的所有记录都写入同一分区。然而，我们需要小心确保任何键都合理分布。否则，我们可能会产生*热分区*（例如，按国家分区可能会导致美国分区的记录数量达到数万亿条，而安道尔分区只有几千条记录）。
- en: Right now we have an issue with resiliency because all our partitions are on
    the same broker instance. In [Figure 11-4](#image:kafka-topic-partition-replication),
    we’ve replicated the topic across three partitions.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们在弹性方面存在问题，因为所有分区都在同一个代理实例上。在 [图 11-4](#image:kafka-topic-partition-replication)
    中，我们已经将主题复制到了三个分区上。
- en: '![Topic partition replication](assets/rsij_1104.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![主题分区复制](assets/rsij_1104.png)'
- en: Figure 11-4\. Topic partition replication
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-4\. 主题分区复制
- en: To support resiliency and ensure that consumers don’t read the same record in
    a partition from different brokers, a *leader partition* is elected for consumers
    to read from. Partition 0 in the first broker, Partition 1 in the second, and
    Partition 2 in the third broker are the leader partitions in this example.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持弹性，并确保消费者不会从不同的代理读取同一分区中的相同记录，为消费者选择了一个*领导分区*进行读取。在这个示例中，第一个代理中的分区 0，第二个代理中的分区
    1，第三个代理中的分区 2 是领导分区。
- en: With the design we have in [Figure 11-4](#image:kafka-topic-partition-replication),
    Kafka ensures that our consumers cannot read a record from a partition before
    it has been successfully replicated. It does this by tracking the *high watermark
    offset*, the offset of the last message successfully replicated across all partitions.
    The broker prevents consumers from reading beyond the high watermark offset, stopping
    unreplicated records from being read.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Point-to-Point Communication
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kafka is not a traditional messaging system. It can be confusing to implement
    the standard delivery patterns with it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: With *point-to-point communication*, we want the same message to be consumed
    once, by the same consumer or by any other consumer within the same consumer group.
    Note that when facing network failures, you cannot guarantee that records are
    consumed only once per consumer group. You need to be prepared to see duplicated
    messages.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: We use *consumer groups* to scale a consumer in Kafka to perform identical processing
    with greater throughput. In [Figure 11-5](#image:kafka-topic-consumer-groups),
    only one consumer within the group is able to read records from a single topic
    partition, conforming to the needs of point-to-point communication. Here we see
    Consumer 2 unable to read records because it’s part of the same consumer group
    as Consumer 1, effectively making Consumer 2 idle, as we have only one partition
    in this situation.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![Consumer groups](assets/rsij_1105.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: Figure 11-5\. Consumer groups
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why can’t we have two consumers in the same group reading from the same partition?
    Kafka tracks the last committed offset per partition for a given consumer group
    and uses the offset for restarting processing. However, consumers don’t commit
    the offset until they’ve completely finished processing a record. This creates
    a window where multiple consumers in the same group could read the same record,
    thus duplicating the processing of a message.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: When a new consumer subscribes to a consumer group, and Kafka does not know
    the last committed offset for the partition, there are two strategies. The strategies
    are *Earliest*, where the consumer starts reading events from the first offset
    of the partition, and *Latest*, which consumes only events received after the
    consumer subscribed.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Publish/Subscribe
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Point-to-point ensures that messages are consumed once. Another popular pattern
    dispatches a message to multiple consumers. With a *publish/subscribe* model,
    we can have many subscribers, or consumers, reading the same message, usually
    for different purposes.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-6](#image:kafka-topic-multiple-consumer-groups) has two consumer
    groups consuming messages from the same topic. One consumer group has three consumers,
    while the other has two consumers. We see each partition being read by only a
    single consumer from the same consumer group, but multiple consumers across consumer
    groups. Though the two consumer groups are connected to the same topic and its
    partitions, there is no requirement for each consumer to be at the same offset.
    Such a requirement would remove the benefits of being able to consume records
    with different groups.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-6](#image:kafka-topic-multiple-consumer-groups) 展示了两个消费者组从同一个主题消费消息。一个消费者组有三个消费者，而另一个有两个消费者。我们看到每个分区只被同一消费者组中的单个消费者读取，但跨消费者组有多个消费者。虽然这两个消费者组连接到同一主题及其分区，但每个消费者并无需在相同的偏移量处。这种要求将取消不同组能够消费记录的优势。'
- en: '![Multiple consumer groups](assets/rsij_1106.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![多个消费者组](assets/rsij_1106.png)'
- en: Figure 11-6\. Multiple consumer groups
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-6\. 多个消费者组
- en: Elasticity Patterns
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弹性模式
- en: '*Elasticity* is one of the pillars of reactive systems. The partition mechanism
    offered by Kafka lets us implement elasticity patterns. [Figure 11-6](#image:kafka-topic-multiple-consumer-groups)
    also highlights the elasticity patterns of consumer groups in Kafka. Consumer
    group 1 has three consumers, each consuming from a different partition. If a consumer
    fails for any reason, another consumer takes up the load of reading from the partition
    that is now without a consumer. Consumer elasticity ensures that all partitions
    are being consumed as long as at least one consumer is present. Granted, such
    an occurrence does reduce the throughput, but it is preferable over no records
    being consumed. Consumer group 2 could represent such an occurrence.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*弹性* 是反应式系统的支柱之一。Kafka 提供的分区机制允许我们实现弹性模式。[图 11-6](#image:kafka-topic-multiple-consumer-groups)
    也突出了 Kafka 中消费者组的弹性模式。消费者组 1 有三个消费者，每个从不同的分区消费。如果由于任何原因消费者失败，另一个消费者将接管从现在无消费者的分区读取的负载。消费者弹性确保只要至少有一个消费者存在，所有分区都在被消费。当然，这种情况确实会降低吞吐量，但比不消费任何记录要好。消费者组
    2 可能代表这种情况。'
- en: Consumer group elasticity is limited, though. As we mentioned earlier, it is
    not possible for multiple consumers within the same group to read from the same
    partition. In [Figure 11-6](#image:kafka-topic-multiple-consumer-groups), with
    three partitions we’re limited to three consumers within a single consumer group.
    Any additional consumers in the same group would be idle, as we cannot have multiple
    consumers in the same group connected to the same partition.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，消费者组的弹性是有限的。正如我们之前提到的，同一组内多个消费者不可能从同一分区读取。在 [图 11-6](#image:kafka-topic-multiple-consumer-groups)
    中，有三个分区，我们仅限于单个消费者组内的三个消费者。同一组中的任何额外消费者将处于空闲状态，因为我们不能让同一组中的多个消费者连接到同一分区。
- en: Elasticity is a key factor to consider when determining the number of partitions
    we want for a topic. With too few, we limit the throughput for processing records,
    while too many can lead to idle consumers if the records are not distributed across
    the partitions sufficiently evenly.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定主题所需的分区数时，弹性是一个重要因素。分区过少会限制处理记录的吞吐量，而过多则可能导致消费者空闲，如果记录在分区间分布不均匀的话。
- en: Dealing with Failures
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理故障
- en: Failures happen! It’s the nature of distributed systems, and not one we can
    avoid even when developing reactive systems. However, Kafka provides us with mechanisms
    for appropriately dealing with failure.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 故障在分布式系统中时常发生！这是其性质，即使在开发反应式系统时也无法避免。然而，Kafka 为我们提供了适当处理故障的机制。
- en: Commit strategies
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提交策略
- en: Each consumer periodically informs the broker of its latest *offset commit*.
    The number represents the last message that was successfully processed from a
    topic partition by the consumer. The offset commit then becomes the starting point
    for the new consumer of the partition when the current consumer fails or crashes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 每个消费者定期通知代理其最新的 *偏移量提交*。该数字代表消费者从主题分区成功处理的最后一条消息。当当前消费者失败或崩溃时，偏移量提交将成为分区的新消费者的起点。
- en: 'Committing an offset is not a cheap operation. For performance reasons, we
    recommend not committing the offset after every record processed. Quarkus provides
    a few options for commit strategies to use with Kafka:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 提交偏移量不是一项廉价的操作。出于性能考虑，我们建议不要在处理每个记录后都提交偏移量。Quarkus 提供了几种用于与 Kafka 一起使用的提交策略选项：
- en: Throttled
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 受限
- en: Ignore
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 忽略
- en: Latest
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最新
- en: The *Throttled* strategy, the default option, tracks the received records for
    a consumer and monitors their acknowledgment. When all records before a position
    are successfully processed, that position is committed to the broker as the new
    offset for that consumer group. If any record is neither acked nor nacked, it’s
    no longer possible to commit a new offset position, and records will be continually
    enqueued. Without the ability to bail out, it would lead to out-of-memory errors
    eventually. The Throttled strategy can detect this problem by reporting a failure
    to the connector, enabling the application to be marked as unhealthy. Note that
    this situation is often an application bug causing a message to be “forgotten.”
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*Throttled*策略是默认选项，跟踪消费者接收的记录并监控它们的确认。当处理完一个位置之前的所有记录时，该位置将作为该消费者组的新偏移量提交到代理。如果任何记录既没有确认也没有拒绝确认，就不再可能提交新的偏移量位置，并且记录将继续排队。如果没有能力退出，最终会导致内存不足错误。通过向连接器报告故障，Throttled策略可以检测到这个问题，使应用标记为不健康状态。请注意，这种情况通常是应用程序错误导致消息“丢失”。'
- en: The *Ignore* strategy utilizes the default offset commit of the Kafka consumer,
    which occurs periodically when polling for new records. This strategy ignores
    message acknowledgment and relies on record processing to be synchronous. This
    strategy is the default when `enabled.auto.commit=true` is used. Any asynchronous
    processing that fails will be unknown to the process that is polling for new records
    to consume.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*Ignore*策略利用Kafka消费者的默认偏移提交，该提交周期性地在轮询新记录时发生。此策略忽略消息确认，并依赖于记录处理为同步。当使用`enabled.auto.commit=true`时，此策略是默认的。任何失败的异步处理将未知于轮询新记录以消费的过程。'
- en: If we’ve set `commit-strategy` to `ignore` and `enable.auto.commit` to `false`,
    as shown in [Example 11-6](#ebus::config-commit-strat), no offset is ever committed.
    Every time a new consumer starts reading messages from a topic, it will always
    start from offset 0. In some situations, this approach is desired, but it needs
    to be a conscious choice.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将`commit-strategy`设置为`ignore`，并将`enable.auto.commit`设置为`false`，如示例[11-6](#ebus::config-commit-strat)所示，将永远不会提交任何偏移量。每次新消费者从主题读取消息时，它将始终从偏移量0开始。在某些情况下，这种方法是可取的，但需要有意识地选择。
- en: Example 11-6\. Configure the commit strategy
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-6\. 配置提交策略
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '*Latest* will commit the offset after every message is acknowledged, which
    as we described earlier will impact performance of the consumer. In lower-throughput
    scenarios, this strategy may be preferable to have a higher confidence that the
    offset is accurate.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*Latest*将在每个消息确认后提交偏移量，正如我们之前描述的那样，这会影响消费者的性能。在低吞吐量场景中，这种策略可能更可取，以提高偏移量的准确性。'
- en: Acknowledgment strategies
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确认策略
- en: In [“Acknowledgments”](ch10.html#acknowledgements_aspect), you learned how Reactive
    Messaging utilizes `ack` and `nack` to inform the upstream reactive streams of
    the record-processing status. These acknowledgment methods are part of the failure-handling
    strategies we have available for Kafka. The application configures the Kafka connector
    with one of these strategies.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在[“致谢”](ch10.html#acknowledgements_aspect)中，您了解了响应式消息如何利用`ack`和`nack`通知上游响应式流记录处理状态。这些确认方法是我们为Kafka提供的故障处理策略的一部分。应用程序使用其中一种策略配置Kafka连接器。
- en: The simplest, and default, strategy is *Fail Fast*. When an application rejects
    a message, the connector is notified of the failure, and the application is stopped.
    If the failure is transient in origin, such as network issues, restarting the
    application should allow processing to continue without an issue. However, if
    a particular record causes a consumer failure, the application will be in a perpetual
    loop of failure → stop → restart, as it will be continually trying to process
    the record causing a failure.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单且默认的策略是*Fail Fast*。当应用程序拒绝消息时，连接器会被通知失败，并停止应用程序。如果失败是短暂性的，如网络问题，重新启动应用程序应该能够使处理继续进行。但是，如果特定记录导致消费者失败，应用程序将处于失败→停止→重新启动的永久循环中，因为它将不断尝试处理导致失败的记录。
- en: Another simple strategy is *Ignore*. Any nacked message is logged and then ignored
    as the consumer continues processing new records. The Ignore strategy is beneficial
    when our application handles any failure internally, and we thus don’t need to
    inform the message producer of a failure, or when an ignored message occasionally
    is acceptable because of the type of messages being processed. If, on the other
    hand, large numbers of messages are being ignored, it is worth investigating the
    root cause as it’s likely not an intended consequence.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个简单的策略是 *忽略*。任何未确认的消息都会被记录，然后在消费者继续处理新记录时被忽略。当我们的应用程序在内部处理任何故障时，忽略策略非常有用，因此我们不需要通知消息生产者有故障发生，或者由于正在处理的消息类型允许偶尔忽略某些消息。但是，如果大量消息被忽略，值得调查其根本原因，因为这可能不是预期的后果。
- en: The last strategy for failure handling is *Dead-Letter Queue*. It sends the
    failing records to a specific topic to be handled later either automatically or
    manually.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个故障处理策略是 *死信队列*。它将失败的记录发送到特定主题，以便稍后自动或手动处理。
- en: Dead-letter queue
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 死信队列
- en: This strategy has been a part of messaging systems for as long as messaging
    systems have existed! Instead of failing straight away, or ignoring any failures,
    this strategy stores the messages that fail to a separate destination, or topic.
    Storing the failed messages enables an administration process, human or automated,
    to determine the correct cause of action to resolve the failed handling.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略在消息系统存在的时间内一直是消息系统的一部分！与其立即失败或忽略任何失败，这种策略将无法处理的消息存储到单独的目标或主题中。存储失败的消息使得管理过程（人工或自动化）能够确定正确的操作以解决处理失败。
- en: It’s important to note that the use of the Dead-Letter Queue strategy will work
    only when ordering of all messages is unnecessary, as we don’t stop processing
    new messages waiting for a message failure to be resolved off the dead-letter
    queue (DLQ).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，只有当所有消息的顺序不重要时，死信队列策略才能起作用，因为我们不会因为等待死信队列中的消息处理失败而停止处理新消息。
- en: When choosing this strategy, the default topic is named `dead-letter-topic-*[topic-name]*`.
    For our previous examples, it would be `dead-letter-topic-my-channel`. It is possible
    to configure the topic name as shown in [Example 11-7](#ebus::config-fail-strat-dlq).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当选择这种策略时，默认主题被命名为 `dead-letter-topic-*[topic-name]*`。对于我们之前的示例，将是 `dead-letter-topic-my-channel`。可以按照
    [示例 11-7](#ebus::config-fail-strat-dlq) 中所示的方式配置主题名称。
- en: Example 11-7\. Configure the failure strategy to use a DLQ
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-7\. 配置故障策略以使用 DLQ
- en: '[PRE6]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can even retrieve the failure reason associated with the message from the
    `dead-letter-reason` header ([Example 11-8](#ebus::retr-fail-reason)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以从 `dead-letter-reason` 头部检索与消息关联的失败原因（[示例 11-8](#ebus::retr-fail-reason)）。
- en: Example 11-8\. Retrieve the failure reason
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-8\. 检索失败原因
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Don’t forget that using a DLQ requires having another application or a human
    operator to process the records sent to the DLQ. The records may be reintroduced
    in the initial topic (but the order is lost) or dropped, or a mitigation logic
    would need to happen.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DLQ 需要另一个应用程序或人工操作员来处理发送到 DLQ 的记录。这些记录可以重新引入到初始主题（但顺序会丢失），也可以丢弃，或者需要进行一些减轻逻辑。
- en: Backpressure and Performance Considerations
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 背压和性能考虑
- en: There is no way to have a truly reactive system without appropriate backpressure
    to avoid overloading components. So how do we handle backpressure for Kafka?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 没有适当的背压来避免过载组件，就无法实现真正的反应式系统。那么我们如何处理 Kafka 的背压？
- en: The outbound connector for Kafka, used with `@Outgoing` or `Emitter`, uses the
    number of in-flight messages waiting for acknowledgment from the broker. *In-flight
    messages* are those the connector has sent to a Kafka broker for writing to a
    topic, but for which the connector has not received acknowledgment that the record
    was successfully stored.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Kafka 一起使用的出站连接器，用于 `@Outgoing` 或 `Emitter`，使用等待从代理收到确认的飞行消息数。*飞行消息* 是连接器已发送到
    Kafka 代理以写入主题的消息，但是尚未收到成功存储记录的确认。
- en: We tweak the number of in-flight messages to adjust the backpressure of the
    outbound Kafka connector. The default number of in-flight messages is 1,024. Too
    high a number can lead to higher memory use, potentially out-of-memory errors
    depending on the payload size, while too few causes a reduction in throughput.
    We can customize the number of in-flight messages in the connector with the property
    `max-inflight-messages`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调整在出站 Kafka 连接器中的流控以调整背压。默认的最大并发消息数为 1,024。如果数字太高，可能会导致更高的内存使用，根据负载大小可能会出现内存不足错误，而数字太低会降低吞吐量。我们可以通过属性
    `max-inflight-messages` 来自定义连接器中的最大并发消息数。
- en: On the side of the consumer, Kafka will pause the consumer and then resume it,
    according to the Reactive Streams requests. We’ve talked a lot about Kafka, so
    in the next section we explore it in Kubernetes!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在消费者端，Kafka 将根据 Reactive Streams 请求暂停和恢复消费者。我们已经讨论了很多关于 Kafka 的内容，所以下一节我们将在
    Kubernetes 中探索它！
- en: Kafka on Kubernetes
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 上的 Kafka
- en: To use Kafka on Kubernetes, we need Kafka installed. We will use the [Strimzi](https://strimzi.io)
    project for installing Kafka. This project has an operator for managing Kafka
    deployments in Kubernetes.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Kubernetes 上使用 Kafka，我们需要安装 Kafka。我们将使用 [Strimzi](https://strimzi.io) 项目来安装
    Kafka。该项目有一个用于在 Kubernetes 中管理 Kafka 部署的操作员。
- en: 'Before setting up Kafka in Kubernetes, we need a Kubernetes environment. If
    you already have one, great! If you don’t, we recommend you use minikube, as covered
    in [“The New Kids on the Block: Cloud Native and Kubernetes Native Applications”](ch03.html#distributed-system::cloud-native-kube).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中设置 Kafka 之前，我们需要一个 Kubernetes 环境。如果您已经有了一个，那太棒了！如果没有，我们建议您使用 minikube，详细信息请参见[《新生代：云原生和
    Kubernetes 原生应用》](ch03.html#distributed-system::cloud-native-kube)。
- en: Note
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Running Kafka in minikube can require more memory than usual deployments, so
    we recommend starting it with at least 4 GB of RAM:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在 minikube 上运行 Kafka 可能需要比常规部署更多的内存，因此我们建议至少启动它时配置 4 GB 的 RAM：
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With a Kubernetes environment running, we need to install Strimzi, as shown
    in [Example 11-9](#ebus::install-strimzi). Be sure to have [Helm](https://helm.sh)
    installed, as we will use it to install Strimzi.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 Kubernetes 环境时，我们需要安装 Strimzi，如[示例 11-9](#ebus::install-strimzi)所示。确保已安装
    [Helm](https://helm.sh)，因为我们将使用它来安装 Strimzi。
- en: Example 11-9\. Install Strimzi
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-9\. 安装 Strimzi
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#co_the_event_bus__the_backbone_CO1-1)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_the_event_bus__the_backbone_CO1-1)'
- en: Create a `strimzi` namespace for the Kubernetes operator.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为 Kubernetes 操作员创建 `strimzi` 命名空间。
- en: '[![2](assets/2.png)](#co_the_event_bus__the_backbone_CO1-2)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_the_event_bus__the_backbone_CO1-2)'
- en: Namespace for the Kafka cluster.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 集群的命名空间。
- en: '[![3](assets/3.png)](#co_the_event_bus__the_backbone_CO1-3)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_the_event_bus__the_backbone_CO1-3)'
- en: Add the Strimzi chart repository to Helm.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Strimzi 图表库添加到 Helm 中。
- en: '[![4](assets/4.png)](#co_the_event_bus__the_backbone_CO1-4)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_the_event_bus__the_backbone_CO1-4)'
- en: Install the Strimzi operator into the `strimzi` namespace.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Strimzi 操作员安装到 `strimzi` 命名空间中。
- en: Once the installation has succeeded, verify that the operator is running (as
    illustrated in [Example 11-10](#ebus::strimzi-op-status)).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 安装成功后，请验证运行操作员（如[示例 11-10](#ebus::strimzi-op-status)所示）。
- en: Example 11-10\. Strimzi operator status
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-10\. Strimzi 操作员状态
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now it’s time to create the Kafka cluster! First we need to define the cluster
    we want to create, as shown in [Example 11-11](#bus::kafka-cluster).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是创建 Kafka 集群的时候了！首先我们需要定义要创建的集群，如[示例 11-11](#bus::kafka-cluster)所示。
- en: Example 11-11\. Kafka cluster definition
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-11\. Kafka 集群定义
- en: '[PRE11]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_the_event_bus__the_backbone_CO2-1)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_the_event_bus__the_backbone_CO2-1)'
- en: Name of the cluster.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 集群名称。
- en: '[![2](assets/2.png)](#co_the_event_bus__the_backbone_CO2-2)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_the_event_bus__the_backbone_CO2-2)'
- en: Number of Kafka replicas to create in the cluster. In production, we would want
    more than one, but for testing this reduces the memory requirements.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中创建 Kafka 副本的数量。在生产环境中，我们希望有多个副本，但为了测试，这可以减少内存需求。
- en: '[![3](assets/3.png)](#co_the_event_bus__the_backbone_CO2-3)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_the_event_bus__the_backbone_CO2-3)'
- en: We choose ephemeral storage, again to reduce the requirements from a testing
    perspective.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择临时存储，以减少测试需求。
- en: Now we use [Example 11-11](#bus::kafka-cluster) to create a Kafka cluster matching
    the requested definition, as shown in [Example 11-12](#ebus::create-kafka-clust).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用 [示例 11-11](#bus::kafka-cluster) 来创建一个符合请求定义的 Kafka 集群，如[示例 11-12](#ebus::create-kafka-clust)所示。
- en: Example 11-12\. Create a Kafka cluster
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-12\. 创建 Kafka 集群
- en: '[PRE12]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Verify that the cluster we wanted was created ([Example 11-13](#ebus:kafka-cluster-stat)).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 验证我们想要的集群是否已创建（[示例 11-13](#ebus:kafka-cluster-stat)）。
- en: Example 11-13\. Kafka cluster status
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-13\. Kafka 集群状态
- en: '[PRE13]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: With the cluster running, we create the Kafka topics we need ([Example 11-14](#ebus::create-kafka-topics)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 集群运行后，我们创建我们需要的 Kafka 主题（[示例 11-14](#ebus::create-kafka-topics)）。
- en: Example 11-14\. Create Kafka topics
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-14\. 创建 Kafka 主题
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To show you how Kafka works with Kubernetes, we will use an example that consists
    of three services: produce a *tick* every two seconds, receive the message and
    add details of the consumer processing it, and expose all messages via SSE. These
    three services will be used to showcase consumer handling with Kafka. Follow the
    instructions in */chapter-11/README.md* under *Application deployment* for building
    the required Docker images and installing the services with Helm.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 为了向您展示 Kafka 如何与 Kubernetes 协同工作，我们将使用一个示例，该示例由三个服务组成：每两秒钟产生一个 *tick*，接收消息并添加处理它的消费者详细信息，并通过
    SSE 公开所有消息。这三个服务将用于展示 Kafka 的消费者处理。请按照 */chapter-11/README.md* 中 *应用程序部署* 部分的说明来构建所需的
    Docker 镜像并使用 Helm 安装服务。
- en: Once the services are running, it’s time to test it! Open the SSE endpoint in
    a browser, and you will see data similar to [Example 11-15](#ebus::sse-output-1).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 服务运行后，现在是测试的时候！在浏览器中打开 SSE 终端点，您将看到类似于 [示例 11-15](#ebus::sse-output-1) 的数据。
- en: 'Example 11-15\. SSE output: all the messages are consumed by the same pod'
  id: totrans-151
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-15\. SSE 输出：所有消息由同一 Pod 消费
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can see all the ticks consumed by a single consumer, even though we have
    three partitions for our topic. Let’s scale up `processor` to add more consumers
    to the same group ([Example 11-16](#ebus:inc-app-instances)).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的主题有三个分区，我们可以看到单个消费者消费的所有 ticks。让我们扩展 `processor` 来向同一组添加更多消费者（[示例 11-16](#ebus:inc-app-instances)）。
- en: Example 11-16\. Increase the number of application instances
  id: totrans-154
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-16\. 增加应用程序实例的数量
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In the browser, we now see the messages processed by three consumers of the
    same group, increasing throughput and concurrency ([Example 11-17](#ebus::sse-output-2)).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中，我们现在看到了同一组的三个消费者处理的消息，增加了吞吐量和并发性能（[示例 11-17](#ebus::sse-output-2)）。
- en: 'Example 11-17\. SSE output: the messages are consumed by the three pods'
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-17\. SSE 输出：消息由三个 Pod 消费
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If we started another `processor` instance but without `mp.messaging.incoming.ticks.group.
    id=tick-consumer` set, we would see the duplication of message numbers from the
    new consumer, as they have their own consumer group and offset position.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们启动了另一个 `processor` 实例，但没有设置 `mp.messaging.incoming.ticks.group. id=tick-consumer`，我们将看到来自新消费者的消息编号重复，因为它们有自己的消费者组和偏移位置。
- en: Building Reactive Systems with AMQP
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 AMQP 构建响应式系统
- en: '*Advanced Message Queuing Protocol*, or *AMQP*, is an application layer protocol
    for message-oriented middleware that has been around since 2002. The AMQP Broker
    is a highly advanced message broker with a tremendous amount of flexibility and
    customization dependent on application requirements.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*高级消息队列协议*（*Advanced Message Queuing Protocol*），或 *AMQP*，是自2002年以来存在的面向消息的中间件应用层协议。AMQP
    Broker 是一个高度先进的消息代理，具有极大的灵活性和根据应用需求的可定制性。'
- en: We don’t cover all the possible uses of the AMQP Broker here. With a huge array
    of possible broker topologies to support many varied use cases, there is simply
    too much information to even attempt to squeeze it all into this section! [Robert
    Godfrey on InfoQ](https://oreil.ly/xC0ar) presents the AMQP 1.0 core features
    and introduces some possibilities.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这里没有涵盖 AMQP Broker 的所有可能用途。有多种经纪人拓扑结构支持多种不同的用例，简单地试图将所有信息都压缩到本节中是不现实的！[Robert
    Godfrey on InfoQ](https://oreil.ly/xC0ar) 展示了 AMQP 1.0 的核心功能并介绍了一些可能性。
- en: Unlike Kafka, all the *smarts* are inside the AMQP Broker, which knows about
    topologies, clients, message statuses, what is delivered, and what is yet to be
    delivered.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Kafka 不同，所有的 *智能* 都在 AMQP Broker 内部，它了解拓扑结构、客户端、消息状态、已交付和尚未交付的内容。
- en: AMQP 1.0
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AMQP 1.0
- en: '*AMQP 1.0* is an open standard for passing business messages among applications
    or organizations. It consists of several layers, the lowest of which is a binary
    wire-level protocol for transferring a message between two processes. On top of
    the wire-level protocol is the messaging layer, which defines an abstract message
    format and encoding. The wire-level protocol is what enables many clients of different
    types to be able to send and receive messages with the AMQP Broker, as long as
    they support the same 1.0 version of the AMQP specification.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*AMQP 1.0*是在应用程序或组织之间传递业务消息的开放标准。它由几个层次组成，最低层是用于在两个进程之间传输消息的二进制线级协议。在线级协议之上是消息传递层，它定义了抽象消息格式和编码。线级协议使得不同类型的客户端能够与AMQP代理发送和接收消息，只要它们支持AMQP规范的相同1.0版本。'
- en: Utilizing the AMQP 1.0 connector in Quarkus requires the dependency in [Example 11-18](#bus::amqp-dep).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在Quarkus中使用AMQP 1.0连接器需要依赖项[示例 11-18](#bus::amqp-dep)。
- en: Example 11-18\. Dependency for the AMQP connector
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-18\. AMQP连接器的依赖项
- en: '[PRE18]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Point-to-Point Communication
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 点对点通信
- en: With AMQP, point-to-point communication is achieved with a *queue* and not a
    *topic*. In AMQP-speak a queue is referred to as *anycast*, meaning any consumer
    can read the message, but only one of them ([Figure 11-7](#image:amqp-queue)).
    Messages we add to a queue can be durable, as with Kafka, but they can also be
    nondurable. When a message is nondurable, it will be lost if the broker restarts
    before the message is consumed.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AMQP，点对点通信是通过*队列*而不是*主题*来实现的。在AMQP术语中，队列被称为*单播*，意味着任何消费者都可以读取消息，但其中只有一个（[图 11-7](#image:amqp-queue)）。我们添加到队列的消息可以是持久的，就像Kafka一样，但它们也可以是非持久的。当消息是非持久的时候，在消息被消费之前，如果代理在此之前重新启动，消息将会丢失。
- en: '![AMQP queue consumers](assets/rsij_1107.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![AMQP队列消费者](assets/rsij_1107.png)'
- en: Figure 11-7\. AMQP queue consumers
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-7\. AMQP队列消费者
- en: A key difference between Kafka and AMQP for point-to-point is that once a message
    is read by a consumer in AMQP, the message is removed from the queue and not retained
    in any way. AMQP temporarily stores messages until they’ve been consumed, whereas
    Kafka retains all messages in the log, at least until the log-retention policy
    begins removing older ones. This makes AMQP unsuitable for use cases that could
    require a replay of messages within the reactive system at some point.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka和AMQP在点对点通信方面的一个关键区别是，在AMQP中，一旦消息被消费者读取，消息将从队列中移除，并且不以任何方式保留。AMQP暂时存储消息直到它们被消费，而Kafka保留日志中的所有消息，至少直到日志保留策略开始删除旧消息为止。这使得AMQP不适合需要在反应系统中某个时刻重新播放消息的用例。
- en: We can also have many consumers reading messages from the same queue, but the
    broker ensures that only one of them ever reads a single message. AMQP does not
    have the same throughput restrictions as Kafka with respect to scaling consumers.
    We can have dozens of consumers reading from a single queue with AMQP, provided
    order is not important.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以有多个消费者从同一个队列中读取消息，但代理确保只有其中一个消费者读取单个消息。与Kafka相比，AMQP在缩放消费者方面没有相同的吞吐量限制。我们可以有几十个消费者从单个队列中读取消息，只要顺序不重要。
- en: Let’s send a message to AMQP! After adding the dependency we mentioned earlier
    ([Example 11-18](#bus::amqp-dep)), we need to configure the broker properties
    as shown in [Example 11-19](#ebus::config-amqp-broker), so the connector knows
    the location of the AMQP Broker.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 发送消息到AMQP！在添加我们之前提到的依赖项（[示例 11-18](#bus::amqp-dep)）后，我们需要按照[示例 11-19](#ebus::config-amqp-broker)中展示的方式配置代理属性，以便连接器知道AMQP代理的位置。
- en: Example 11-19\. Configure the AMQP Broker location and credentials
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-19\. 配置AMQP代理位置和凭据
- en: '[PRE19]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We’ve set the AMQP Broker configuration for host, port, username, and password
    globally, meaning any channel we define will use the identical AMQP Broker configuration.
    If desired, the configuration can be set on a per-channel basis. We’ve also indicated
    to use the `smallrye-amqp` connector for the `data` outgoing channel.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为主机、端口、用户名和密码设置了全局的AMQP代理配置，这意味着我们定义的任何通道都将使用相同的AMQP代理配置。如果需要，配置可以基于每个通道设置。我们还指示使用`smallrye-amqp`连接器作为`data`输出通道。
- en: By default, the channel uses durable messages for the queue, or we make them
    nondurable with `mp.messaging.outgoing.data.durable=false`. We can also override
    the message durability directly when sending the message, as shown in [Example 11-20](#ebus::use-outgoing-meta).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，通道使用队列的持久消息，或者我们可以通过`mp.messaging.outgoing.data.durable=false`使它们成为非持久消息。我们还可以在发送消息时直接覆盖消息的持久性，就像在[示例 11-20](#ebus::use-outgoing-meta)中展示的那样。
- en: Example 11-20\. Use outgoing metadata to send durable messages
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-20\. 使用出站元数据发送持久消息
- en: '[PRE20]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can then consume the message similarly to Kafka, but using the AMQP metadata
    object to retrieve more detailed information about the message ([Example 11-21](#ebus:extract-amqp-meta)).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以类似于 Kafka 消费消息，但使用 AMQP 元数据对象来检索关于消息的更详细信息（详见 [示例 11-21](#ebus:extract-amqp-meta)）。
- en: Example 11-21\. Extract AMQP metadata from incoming messages
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-21\. 从传入消息中提取 AMQP 元数据
- en: '[PRE21]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Successful receipt and processing of a message results in the connector notifying
    the broker with an `accepted` acknowledgment. On receiving this acknowledgment,
    the broker will delete the message from the queue.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 成功接收和处理消息会导致连接器向 Broker 发送 `accepted` 确认。收到此确认后，Broker 将从队列中删除消息。
- en: Publish/Subscribe
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 发布/订阅
- en: AMQP also supports a publish/subscribe model, similarly to Kafka, allowing many
    subscribers for a single queue to read messages. In AMQP, a queue can be of type
    *multicast* (the opposite of *unicast*) to indicate that many consumers can receive
    the same message.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: AMQP 同样支持发布/订阅模型，类似于 Kafka，允许多个订阅者从单个队列中读取消息。在 AMQP 中，队列可以是 *多播* 类型（与 *单播* 相反），表示多个消费者可以接收相同的消息。
- en: '[Figure 11-8](#image:amqp-topics) has three consumers of a multicast queue
    reading messages, and we see how far through the messages each of the consumers
    has gotten. As with unicast queues, the messages are durable by default but can
    also be made nondurable if desired.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11-8](#image:amqp-topics) 中有三个多播队列消费者读取消息，我们可以看到每个消费者处理消息的进度。与单播队列一样，默认情况下消息是持久的，但如果需要，也可以设置为非持久。'
- en: '![AMQP multicast queue consumers](assets/rsij_1108.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![AMQP 多播队列消费者](assets/rsij_1108.png)'
- en: Figure 11-8\. AMQP multicast queue consumers
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 11-8\. AMQP 多播队列消费者
- en: The code for sending and receiving messages from a multicast queue is identical
    to the code we used for point-to-point in [“Point-to-Point Communication”](#amqp-ptp).
    The *address* defaults to the channel name; it can be customized in configuration
    of the channel or set directly on the metadata of the message.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 从多播队列发送和接收消息的代码与我们用于点对点通信的代码完全相同，详见 [“点对点通信”](#amqp-ptp)。*地址* 默认为通道名称；可以在通道配置中自定义，或直接设置在消息元数据中。
- en: Elasticity Patterns
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弹性模式
- en: The elasticity patterns for AMQP with point-to-point communication are a little
    different. With Kafka, we can have only one consumer reading from a single partition.
    With AMQP, we can have as many consumers as we want reading from the same queue,
    given the order in which the messages are processed is not important.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: AMQP 在点对点通信的弹性模式略有不同。在 Kafka 中，我们只能有一个消费者从单个分区读取消息。而在 AMQP 中，可以有任意数量的消费者从同一队列中读取消息，只要消息处理的顺序不重要。
- en: Granted, we may not want a lot of consumers reading the queue from the same
    broker node, but we are able to cluster the brokers to spread the load across
    them. With a cluster of brokers, the broker is smart enough to shift messages
    from one broker to another if it notices the consumers of a queue on that broker
    are underutilized compared to other brokers.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可能不希望很多消费者从同一 Broker 节点读取队列，但我们可以将 Broker 进行集群化，以便将负载分布到它们之间。在 Broker 集群中，如果注意到某个
    Broker 上的队列消费者利用率低于其他 Broker，则 Broker 会智能地将消息从一个 Broker 转移到另一个 Broker。
- en: Acknowledgment and Redelivery
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确认和重发
- en: When we send a message to an AMQP Broker, it is acknowledged if the broker successfully
    committed the message. However, this will not be the case when routers are utilized
    between the producer and a broker. In this situation, it is recommended to set
    `auto-acknowledgement` to `true` to ensure that the producer receives acknowledgment
    when the message is sent to the router. Any response from the broker of `rejected`,
    `released`, or `modified` results in the message being nacked.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向 AMQP Broker 发送消息时，如果 Broker 成功提交了消息，则会进行确认。但是，在生产者和 Broker 之间使用路由器时，情况就不同了。在这种情况下，建议将
    `auto-acknowledgement` 设置为 `true`，以确保生产者在将消息发送到路由器时收到确认。如果 Broker 返回 `rejected`、`released`
    或 `modified`，则消息会被拒绝。
- en: The consumption side has a few more possibilities for acknowledgment. We can
    `fail` the message, causing the application to enter a failed state and process
    no further messages. The message being processed resulting in the failure is marked
    as `rejected` with the broker. This is the default behavior for the AMQP connector.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 消费端有几种更多的确认可能性。我们可以`fail`掉消息，导致应用程序进入失败状态并且不再处理更多的消息。处理失败的消息在代理中标记为`rejected`。这是
    AMQP 连接器的默认行为。
- en: The Accept, Release, and Reject strategies all result in the failure being logged,
    and the application to continue processing additional messages. The only difference
    between them is in the way the AMQP messages are designated on the broker. The
    Accept strategy marks the message as `accepted`, the Release one marks it as `released`,
    and, finally, the Reject one marks it as `rejected`. When we want to continue
    processing messages on failure, which of the three options you set depends on
    how you want the AMQP Broker to handle the message.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 接受、释放和拒绝策略都会导致失败被记录，并使应用程序继续处理额外的消息。它们之间唯一的区别在于 AMQP 消息在代理上的指定方式。接受策略将消息标记为`accepted`，释放策略将其标记为`released`，最后，拒绝策略将其标记为`rejected`。当我们希望在失败时继续处理消息时，你设置的三个选项中的哪一个取决于你希望
    AMQP 代理如何处理消息。
- en: What about redelivery? If we mark the AMQP message as `released`, the broker
    can redeliver the message, to the same or different consumer, at a later time.
    When setting a message as `modified`, we have two available strategies. Using
    the `modified-failed` strategy sets a `delivery-failed` attribute on the message,
    enabling the broker to attempt redelivery of the message while processing continues
    with the next message. However, using the `modified-failed-undeliverable-here`
    strategy also sets the `delivery-failed` attribute, and while the broker can attempt
    redelivery of the message, it won’t do so with this consumer.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 关于重投递的问题？如果我们将 AMQP 消息标记为`released`，代理可以在稍后的时间将消息重新投递给相同或不同的消费者。将消息设置为`modified`时，我们有两种可用的策略。使用`modified-failed`策略会在消息上设置`delivery-failed`属性，使代理能够在处理下一条消息的同时尝试重新投递该消息。然而，使用`modified-failed-undeliverable-here`策略也会设置`delivery-failed`属性，尽管代理可以尝试重新投递消息，但不会针对当前消费者进行此操作。
- en: If at any point the consumer loses the session with a broker, any in-progress
    work will be rolled back. This allows for other consumers, or restarting of the
    current consumer, to accept redelivery of any messages that were in-flight at
    the time the session with the broker was severed.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果消费者在任何时候与代理失去会话，任何正在进行中的工作都将被回滚。这允许其他消费者或当前消费者重新启动时接受在与代理会话断开连接时正在飞行中的任何消息的重新投递。
- en: Credit-Flow Backpressure Protocol
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 信用流回压协议
- en: AMQP enables backpressure in producers with a credit system. Producers are able
    to send messages to a broker only as long as they have credits available, preventing
    producers from overloading the broker with too many messages in a small amount
    of time. The credits represent the number of bytes a producer can send. For example,
    if we had 1,000 credits, representing 1,000 bytes, a producer would be able to
    send 1 message of 1,000 bytes or 10 messages of 100 bytes before the credits expired.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: AMQP 通过信用系统在生产者中实现了回压。只有在生产者有可用的信用时，它们才能向代理发送消息，以防止生产者在短时间内向代理发送过多的消息。信用代表生产者可以发送的字节数量。例如，如果我们有
    1,000 个信用，表示 1,000 字节，那么生产者可以在信用过期之前发送 1 条 1,000 字节的消息或者 10 条 100 字节的消息。
- en: When a producer has spent all its credits, it waits in a nonblocking manner
    until additional credits are granted from the broker. The default is to request
    additional credits every 2,000 ms, but this setting can be configured with the
    `credit-retrieval-period` configuration property.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 当生产者用尽所有信用时，它会以非阻塞方式等待从代理获取额外的信用。默认情况下，每 2,000 毫秒请求额外的信用，但此设置可以使用`credit-retrieval-period`配置属性进行配置。
- en: When running out of credit, the connector marks the application as `not ready`.
    This information is then reported to the application health check. If you deploy
    the application to Kubernetes, the readiness health check will fail, and Kubernetes
    will stop sending traffic to the pod until it becomes ready again.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 当信用用尽时，连接器将应用标记为`not ready`。然后将此信息报告给应用程序健康检查。如果将应用程序部署到 Kubernetes，则可用性健康检查将失败，并且
    Kubernetes 将停止向 pod 发送流量，直到其再次变为就绪状态。
- en: AMQP on Kubernetes
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 上的 AMQP
- en: Setting up a production-ready AMQP Broker on Kubernetes is not a straightforward
    task, so we’re opting to use a single Docker image to keep it simple. With a Kubernetes
    environment running, run an AMQP Broker container, as shown in [Example 11-22](#bus::amqp-broker).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上设置一个可用于生产的 AMQP Broker 并不是一件简单的任务，因此我们选择使用单个 Docker 镜像来简化。在运行
    Kubernetes 环境的情况下，运行一个 AMQP Broker 容器，如 [示例 11-22](#bus::amqp-broker) 所示。
- en: Example 11-22\. Start AMQP Broker container
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-22\. 启动 AMQP Broker 容器
- en: '[PRE22]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here, we start an AMQP Broker in a Kubernetes pod, but we need to expose the
    broker as a service to make it accessible to the services, as shown in [Example 11-23](#ebus::expose-broker).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们在 Kubernetes pod 中启动了一个 AMQP Broker，但我们需要将 Broker 暴露为服务，以便让服务可以访问，如 [示例 11-23](#ebus::expose-broker)
    所示。
- en: Example 11-23\. Expose AMQP Broker service port
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-23\. 暴露 AMQP Broker 服务端口
- en: '[PRE23]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: To be able to use AMQP, we need to switch our code to utilize a different dependency
    and configuration, but the bulk of the services remain unchanged. For each service,
    comment out the `quarkus-smallrye-reactive-messaging-kafka` dependency and uncomment
    the `quarkus-smallrye-reactive-messaging-amqp` dependency in each *pom.xml*. In
    the *application.properties* file for each service, comment out the `smallrye-kafka`
    connector configuration and uncomment the `smallrye-amqp` connector. Don’t forget
    to change both connectors in the `processor` service! Be sure to run `mvn clean
    package` on all the services after making these changes.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 要能够使用 AMQP，我们需要将我们的代码切换到使用不同的依赖项和配置，但大多数服务保持不变。对于每个服务，在每个 *pom.xml* 中注释掉 `quarkus-smallrye-reactive-messaging-kafka`
    依赖项，并取消注释每个 *application.properties* 文件中的 `smallrye-amqp` 连接器配置。在 `processor`
    服务中的两个连接器都要更改！在进行这些更改后，务必对所有服务运行 `mvn clean package`。
- en: All the AMQP Broker configuration is present in the Helm charts, with the actual
    values in *values.yaml*. Follow the instructions in */chapter-11/README.md* under
    *Application deployment* for building the required Docker images and installing
    the services. They are the same steps we used for Kafka earlier in the chapter.
    Once the services are running, it’s time to test it! Open the SSE endpoint in
    a browser to see data as we did with Kafka ([Example 11-24](#ebus::sse-output-3)).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 AMQP Broker 配置都在 Helm 图表中，实际值在 *values.yaml* 中。按照 */chapter-11/README.md*
    中 *Application deployment* 下的说明构建所需的 Docker 镜像并安装服务。这些步骤与本章前面使用 Kafka 时相同。服务运行后，现在是测试的时候了！在浏览器中打开
    SSE 端点，就像我们用 Kafka 时所做的那样（[示例 11-24](#ebus::sse-output-3)）。
- en: 'Example 11-24\. SSE output: all the messages are consumed by a single pod'
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-24\. SSE 输出：所有消息由单个 pod 消耗
- en: '[PRE24]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Let’s scale up `processor` to add more consumers, as shown in [Example 11-25](#ebus::increase-pods).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们扩展 `processor` 以增加更多的消费者，如 [示例 11-25](#ebus::increase-pods) 所示。
- en: Example 11-25\. Increase the number of application instances (pods)
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-25\. 增加应用程序实例（pod）的数量
- en: '[PRE25]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Scaling with AMQP has a different outcome from that of scaling with Kafka; see
    [Example 11-26](#ebus::sse-output-4).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AMQP 进行扩展与使用 Kafka 进行扩展有不同的结果；请参见 [示例 11-26](#ebus::sse-output-4)。
- en: 'Example 11-26\. SSE output: the messages are consumed by the three pods'
  id: totrans-220
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 11-26\. SSE 输出：消息由三个 pod 消耗
- en: '[PRE26]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We’re now seeing the same message consumed by all three producers, instead of
    a message consumed once!
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们看到同一消息被所有三个生产者消耗，而不是一条消息仅消耗一次！
- en: Summary
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter went deeper into understanding the event bus when we use AMQP or
    Kafka with Reactive Messaging. If we don’t need metadata classes for specific
    Kafka or AMQP behavior, we can easily switch between the two with a dependency
    change and modifying configuration. We covered how each of the event bus options
    support point-to-point communication, publish/subscribe, acknowledgments, failure
    handling, and backpressure. These are all key concepts in understanding the totality
    of a reactive system and its components.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了在使用 AMQP 或 Kafka 与 Reactive Messaging 时理解事件总线的更深层次。如果我们不需要特定 Kafka 或
    AMQP 行为的元数据类，我们可以轻松地通过依赖变更和修改配置在两者之间切换。我们介绍了每个事件总线选项如何支持点对点通信、发布/订阅、确认、故障处理和反压。这些都是理解反应式系统及其组件整体的关键概念。
- en: Kafka is the current popular choice for many event-driven reactive systems.
    Kafka can handle a massive number of messages and makes ordering an essential
    characteristic. AMQP does have a lot more flexibility than Kafka in the way it
    can be configured and customized. It also has higher elasticity in point-to-point
    scenarios, as the limit is not constrained by the number of partitions.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 是许多事件驱动反应系统当前流行的选择。Kafka 能处理大量消息，并且使得顺序成为一个重要特性。AMQP 在配置和定制化方面比 Kafka
    更灵活。在点对点的场景中，AMQP 的弹性更高，因为其限制不受分区数量的约束。
- en: In the next chapter, we discuss using an HTTP client with Java interfaces representing
    an external service, as well as how to use the lower-level web client and why
    that’s still useful.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论如何使用 Java 接口表示外部服务的 HTTP 客户端，以及如何使用低级别的 Web 客户端以及为什么它仍然有用。
- en: ^([1](ch11.html#idm45358818317616-marker)) Throughout this chapter, we will
    be talking about AMQP 1.0.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch11.html#idm45358818317616-marker)) 在本章中，我们将讨论 AMQP 1.0。
