- en: Chapter 2\. Application Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The complexity of distributed systems comprised of many communicating microservices
    means it is especially important to be able to observe the state of the system.
    The rate of change is high, including new code releases, independent scaling events
    with changing load, changes to infrastructure (cloud provider changes), and dynamic
    configuration changes propagating through the system. In this chapter, we will
    focus on how to measure and alert on the performance of the distributed system
    and some industry best practices to adopt.
  prefs: []
  type: TYPE_NORMAL
- en: An organization must commit at a minimum to one or more monitoring solutions.
    There are a wide range of choices including open source, commercial on-premises,
    and SaaS offerings with a broad spectrum of capabilities. The market is mature
    enough that an organization of any size and complexity can find a solution that
    fits its requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of monitoring system is important to preserve the fixed-cost characteristic
    of metrics data. The StatsD protocol, for example, requires an emission to a StatsD
    agent from an application on a per-event basis. Even if this agent is running
    as a sidecar process on the same host, the application still suffers the allocation
    cost of creating the payload on a per-event basis, so this protocol breaks at
    least this advantage of metrics telemetry. This isn’t always (or even commonly)
    catastrophic, but be aware of this cost.
  prefs: []
  type: TYPE_NORMAL
- en: Black Box Versus White Box Monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Approaches to metrics collection can be categorized according to what the method
    is able to observe:'
  prefs: []
  type: TYPE_NORMAL
- en: Black box
  prefs: []
  type: TYPE_NORMAL
- en: The collector can observe inputs and outputs (e.g., HTTP requests into a system
    and responses out of it), but the mechanism of the operation is not known to the
    collector. Black box collectors somehow intercept or wrap the observed process
    to measure it.
  prefs: []
  type: TYPE_NORMAL
- en: White box
  prefs: []
  type: TYPE_NORMAL
- en: The collector can observe inputs and outputs and also the internal mechanisms
    of the operation. White box collectors do this in application code.
  prefs: []
  type: TYPE_NORMAL
- en: Many monitoring system vendors provide agents that can be attached to application
    processes and that provide black box monitoring. Sometimes these agent collectors
    reach so deep into well-known application frameworks that they start to resemble
    white box collectors in some ways. Still, black box monitoring in whatever form
    is limited to what the writer of the agent can generalize about all applications
    that might apply the agent. For example, an agent might be able to intercept and
    time Spring Boot’s mechanism for database transactions. An agent will never be
    able to reason that a `java.util.Map` field in some class represents a form of
    near-cache and instrument it as such.
  prefs: []
  type: TYPE_NORMAL
- en: Service-mesh-based instrumentation is also black box and is generally less capable
    than an agent. While agents can observe and decorate individual method invocations,
    a service mesh’s finest-grained observation is at the RPC level.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other side, white box collection sounds like a lot of work. Some useful
    metrics are truly generalizable across applications (e.g., HTTP request timings,
    CPU utilization) and are well instrumented by black box approaches. A white box
    instrumentation library with some of these generalizations encapsulated when paired
    with an application autoconfiguration mechanism resembles a black box approach.
    White box instrumentation autoconfigured requires the same level of developer
    effort as black box instrumentation: specifically *none*!'
  prefs: []
  type: TYPE_NORMAL
- en: Good white box metrics collectors should capture everything that a black box
    collector does but also support capturing more internal details that black box
    collectors by definition cannot. The difference between the two for your engineering
    practices are minimal. For a black box agent, you must alter your delivery practice
    to package and configure the agent (or couple yourself to a runtime platform integration
    that does this for you). For autoconfigured white box metrics collection that
    captures the same set of detail, you must include a binary dependency at build
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Vendor-specific instrumentation libraries don’t tend to have this black box
    feel with a white box approach because framework and library authors aren’t inclined
    to add a wide range of proprietary instrumentation clients even as optional dependencies
    and instrument their code N different times. A vendor-neutral instrumentation
    facade like Micrometer has the advantage of the “write once, publish anywhere”
    experience for framework and library authors.
  prefs: []
  type: TYPE_NORMAL
- en: Black box and white box collectors can of course be complementary, even when
    there is some overlap between them. There is no across-the-boards requirement
    to choose one over the other.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensional Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most modern monitoring systems employ a dimensional naming scheme that consists
    of a metric name and a series of key-value tags.
  prefs: []
  type: TYPE_NORMAL
- en: While the storage mechanism varies substantially from one monitoring system
    to another, in general every unique combination of name and tags is represented
    as a distinct entry or row in storage. The total cost in storage terms of a metric
    then is the product of the cardinality of its tag set (meaning the total number
    of unique key-value tag pairs).
  prefs: []
  type: TYPE_NORMAL
- en: For example, an application-wide counter metric named `http.server.requests`
    that contains a tag for an HTTP method of which only GET and POST are ever observed,
    an HTTP status code where the service returns one of three status codes, and a
    URI of which there are two in the application results in up to <math alttext="2
    asterisk 3 asterisk 2 equals 12"><mrow><mn>2</mn> <mo>*</mo> <mn>3</mn> <mo>*</mo>
    <mn>2</mn> <mo>=</mo> <mn>12</mn></mrow></math> distinct time series sent to and
    stored in the monitoring system. This metric could be represented in storage roughly
    like in [Table 2-1](part0006_split_002.html#storage_dimensional_metric). Coordination
    between tags, like the fact that only endpoint `/a1` will ever have a `GET` method
    and only `/a2` will ever have a `POST` method can limit the total number of unique
    time series below the theoretical maximum, to only six rows in this example. In
    many dimensional time series databases, for each row representing a unique set
    of name and tags, there will be a value ring buffer that holds the samples for
    this metric over a defined period of time. When the system contains a bounded
    ring buffer like this, the total cost of your metrics is fixed to the product
    of the number of permutations of unique metric names/tags and the size of the
    ring buffer.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. The storage of a dimensional metric
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name and tags | Values |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| http.server.requests{method=GET,status=200,uri=/a1} | [10,11,10,10] |'
  prefs: []
  type: TYPE_TB
- en: '| http.server.requests{method=GET,status=400,uri=/a1} | [1,0,0,0] |'
  prefs: []
  type: TYPE_TB
- en: '| http.server.requests{method=GET,status=500,uri=/a1} | [0,0,0,4] |'
  prefs: []
  type: TYPE_TB
- en: '| http.server.requests{method=POST,status=200,uri=/a2} | [10,11,10,10] |'
  prefs: []
  type: TYPE_TB
- en: '| http.server.requests{method=POST,status=400,uri=/a2} | [0,0,0,1] |'
  prefs: []
  type: TYPE_TB
- en: '| http.server.requests{method=POST,status=500,uri=/a2} | [1,1,1,1] |'
  prefs: []
  type: TYPE_TB
- en: In some cases, metrics are periodically moved to long-term storage. At this
    point, there is an opportunity to squash or drop tags to reduce storage cost at
    the expense of some dimensional granularity.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before dimensional metrics systems became popular, many monitoring systems employed
    a hierarchical scheme. In these systems, metrics were defined only by name, with
    no key-value tag pairs. Tags are so useful that a convention emerged to append
    tag-like data to metric names with something like dot separators. So a dimensional
    metric like `httpServerRequests`, which has a `method` tag of `GET` in a dimensional
    system, might be represented as `httpServerRequests.method.GET` in a hierarchical
    system. Out of this arose query features like wildcard operators to allow simple
    aggregation across “tags,” as in [Table 2-2](part0006_split_003.html#hierarchical_metrics_aggregation).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-2\. Aggregation of hierarchical metrics with wildcards
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric query | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| httpServerRequests.method.GET | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| httpServerRequests.method.POST | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| httpServerRequests.method.* | 30 |'
  prefs: []
  type: TYPE_TB
- en: Still, tags are not a first-class citizen in hierarchical systems, and wildcarding
    like this breaks down. In particular, when an organization decides that a metric
    like `httpServerRequests` that is common to many applications across the stack
    should receive a new tag, it has the potential to break existing queries. In [Table 2-3](part0006_split_003.html#hierarchical_metrics_aggregation_failures),
    the true number of requests independent of method is 40, but since some application
    in the stack has introduced a new status tag in the metric name, it is no longer
    included in the aggregation. Even assuming we can agree as a whole organization
    to standardize on this new tag, our wildcarding queries (and therefore any dashboards
    or alerts built off of them) misrepresent the state of the system from the time
    the tag is introduced in the first application until it is fully propagated through
    the codebase and redeployed everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-3\. Failures of aggregation of hierarchical metrics with wildcards
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric query | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| httpServerRequests.method.GET | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| httpServerRequests.method.POST | 20 |'
  prefs: []
  type: TYPE_TB
- en: '| httpServerRequests.status.200.method.GET | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| httpServerRequests.method.* | 30 (!!) |'
  prefs: []
  type: TYPE_TB
- en: Effectively, the hierarchical approach has forced an ordering of tags when they
    are really independent key-value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: If you are starting with real-time application monitoring now, you should be
    using a dimensional monitoring system. This means you will also have to use a
    dimensional metrics instrumentation library in order to record metrics in a way
    that fully takes advantage of the name/tag combination that makes these systems
    so powerful. If you already have some instrumentation using a hierarchical collector,
    the most popular being Dropwizard Metrics, you are going to have to ultimately
    rewrite this instrumentation. It’s possible to flatten dimensional metrics into
    hierarchical metrics by developing a naming convention that in some way iterates
    over all the tags and combines them with the metric name. Going the other direction
    is difficult to generalize, because the lack of consistency in naming schemes
    makes it difficult to split a hierarchical name into dimensional metrics.
  prefs: []
  type: TYPE_NORMAL
- en: From this point on, we’ll be examining dimensional metrics instrumentation alone.
  prefs: []
  type: TYPE_NORMAL
- en: Micrometer Meter Registries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The remainder of this chapter will use [Micrometer](https://micrometer.io),
    a dimensional metrics instrumentation library for Java that supports many of the
    most popular monitoring systems on the market. There are only two main alternatives
    to Micrometer available:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring system vendors often provide Java API clients
  prefs: []
  type: TYPE_NORMAL
- en: While these work for white box instrumentation at the application level, there
    is little to no chance that the remainder of the Java ecosystem, especially of
    third-party open source libraries, will adopt a particular vendor’s instrumentation
    client for its metrics collection. Probably the closest we have come to this is
    some spotty adoption in open source libraries of the Prometheus client.
  prefs: []
  type: TYPE_NORMAL
- en: '[OpenTelemetry](https://oreil.ly/xV0Aa)'
  prefs: []
  type: TYPE_NORMAL
- en: OpenTelemetry is a hybrid metrics and tracing library. At the time of this writing,
    OpenTelemetry does not have a 1.0 release, and its focus has certainly been more
    on tracing than metrics, so metrics support is much more basic.
  prefs: []
  type: TYPE_NORMAL
- en: While there is some variation in capabilities from one dimensional metrics instrumentation
    library to another, most of the key concepts described apply to each of them,
    or at least you should develop an idea of how alternatives should be expected
    to mature.
  prefs: []
  type: TYPE_NORMAL
- en: In Micrometer, a `Meter` is the interface for collecting a set of measurements
    (which we individually call metrics) about your application.
  prefs: []
  type: TYPE_NORMAL
- en: Meters are created from and held in a `MeterRegistry`. Each supported monitoring
    system has an implementation of `MeterRegistry`. How a registry is created varies
    for each implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each `MeterRegistry` implementation that is supported by the Micrometer project
    has a library published to Maven Central and JCenter (e.g., `io.micrometer:micrometer-registry-prometheus`,
    `io.micrometer:micrometer-registry-atlas`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`MeterRegistry` implementations with more options contain a fluent builder
    as well, for example the InfluxDB registry shown in [Example 2-1](part0006_split_004.html#influx_fluent_builder).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-1\. Influx fluent builder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Metrics can be published to multiple monitoring systems simultaneously with
    `CompositeMeterRegistry`.
  prefs: []
  type: TYPE_NORMAL
- en: In [Example 2-2](part0006_split_004.html#composite_meter_registry), a composite
    registry is created that ships metrics to both Prometheus and Atlas. Meters should
    be created with the composite.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-2\. Composite meter registry that ships to Prometheus and Atlas
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Micrometer packs with a global static `CompositeMeterRegistry` that can be used
    in a similar way that we use an SLF4J `LoggerFactory`. The purpose of this static
    registry is to allow for instrumentation in components that cannot leak Micrometer
    as an API dependency by offering a way to dependency-inject a `MeterRegistry`.
    [Example 2-3](part0006_split_004.html#using_static_registry) shows the similarity
    between the use of the global static registry and what we are used to from logging
    libraries like SLF4J.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-3\. Using the static global registry
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: By adding any `MeterRegistry` implementations that you wire in your application
    to the global static registry, any low-level libraries using the global registry
    like this wind up registering metrics to your implementations. Composite registries
    can be added to other composite registries. In [Figure 2-1](part0006_split_004.html#global_registry_relationship),
    we’ve created a composite registry in our application that publishes metrics to
    both Prometheus and Stackdriver (i.e., we’ve called `CompositeMeterRegistry#add(MeterRegistry)`
    for both the Prometheus and Stackdriver registries). Then we’ve added *that* composite
    to the global static composite. The composite registry you created can be dependency-injected
    by something like Spring, CDI, or Guice throughout your application for your components
    to register metrics against. But other libraries are often outside of this dependency-injection
    context, and since they don’t want Micrometer to leak through their API signatures,
    they register with the static global registry. In the end, metrics registration
    flows down this hierarchy of registries. So library metrics flow down from the
    global composite to your application composite to the individual registries. Application
    metrics flow down from the application composite to the individual Prometheus
    and Stackdriver registries.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0201](../images/00111.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. Relationship between global static registry and your application’s
    registries
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Spring Boot Autoconfiguration of MeterRegistry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spring Boot autoconfigures a composite registry and adds a registry for each
    supported implementation that it finds on the classpath. A dependency on `micrometer-registry-{system}`
    in your runtime classpath along with any required configuration for that system
    causes Spring Boot to configure the registry. Spring Boot also adds any `MeterRegistry`
    found as a `@Bean` to the global static composite. In this way, any libraries
    that you add to your application that provide Micrometer instrumentation automatically
    ship their metrics to your monitoring system! This is how the black-box-like experience
    is achieved through white box instrumentation. As the developer, you don’t need
    to explicitly register these metrics; just their presence in your application
    makes it work.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Meters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Micrometer provides two styles to register metrics for each supported `Meter`
    type, depending on how many options you need. The fluent builder, as shown in
    [Example 2-4](part0006_split_006.html#meter_fluent_builder), provides the most
    options. Generally, core libraries should use the fluent builder because the extra
    verbosity required to provide robust description and base unit detail adds value
    to all of their users. In instrumentation for a particular microservice with a
    small set of engineers, opting for more compact code and less detail is fine.
    Some monitoring systems support attaching description text and base units to metrics,
    and for those, Micrometer will publish this data. Furthermore, some monitoring
    systems will use base unit information on a metric to automatically scale and
    label the *y*-axis of charts in a way that is human readable. So if you publish
    a metric with a base unit of “bytes,” a sophisticated monitoring system will recognize
    this and scale the *y*-axis to megabytes or gigabytes, or whatever is the most
    human-readable value for the range of this metric. It’s much easier to read “2
    GB” than “2147483648 bytes.” Even for those monitoring systems that don’t fundamentally
    support base units, charting user interfaces like [Grafana](https://grafana.com)
    allow you to manually specify the units of a chart, and Grafana will do this kind
    of intelligent human-readable scaling for you.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-4\. Meter fluent builder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '`MeterRegistry` contains convenience methods to construct `Meter` instances
    with a shorter form, as in [Example 2-5](part0006_split_006.html#meter_short_form).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-5\. Meter construction convenience methods
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Regardless of which of the two methods you use to construct a meter, you will
    have to decide on its name and which tags to apply.
  prefs: []
  type: TYPE_NORMAL
- en: Naming Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get the most out of metrics, they need to be structured in such a way that
    selecting just the name and aggregating over all tags yields a meaningful (if
    not always useful) value. For example, if a metric is named `http.server.requests`,
    then tags may identify application, region (in the public cloud sense), API endpoint,
    HTTP method, response status code, etc. An aggregate measure like throughput for
    this metric of all unique combinations of tags yields a measure of throughput
    for every interaction with many applications across your application stack. The
    ability to pivot on this name into various tags makes this useful. We could explode
    this metric dimensionally by region and observe a regional outage or drill down
    on a particular application—for example, for successful responses to a particular
    API endpoint to reason about throughput through that one key endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming many applications are instrumented with some metric such as `http.server.requests`,
    when building a visualization on `http.server.requests` the monitoring system
    will display an aggregate of the performance of all of the `http.server.requests`
    across all applications, regions, etc. until you decide to dimensionally drill
    down on something.
  prefs: []
  type: TYPE_NORMAL
- en: Not everything should be a tag, however. Suppose we are trying to measure the
    number of HTTP requests and the number of database calls separately.
  prefs: []
  type: TYPE_NORMAL
- en: Micrometer employs a naming convention that separates lowercase words with a
    . (dot) character. The naming shown in [Example 2-6](part0006_split_007.html#recommended_naming)
    provides enough context so that if just the name is selected the value is at least
    potentially meaningful. For example, if we select `database.queries` we can see
    the total number of calls to all databases. Then we can group by or select by
    database to drill down further or perform comparative analysis on the contribution
    of calls to each database.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-6\. Recommended approach
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: With the approach shown in [Example 2-7](part0006_split_007.html#bad_naming),
    if we select the metric `calls` we will get a value that is an aggregate of the
    number of calls to the database and HTTP requests. This value is not useful without
    dimensionally drilling down further.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2-7\. Bad practice: using a type tag where the meter name should be
    different instead'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 2-2](part0006_split_007.html#bad_metric_naming) shows the effect of
    this bad naming. Suppose for every HTTP request you make 10 database calls. If
    you just chart `calls`, you get the top-line rate of approximately 11,000 calls.
    But 11,000 is an awkward sum of two types of calls which are always an order of
    magnitude off in frequency. To get any utility out of this, we need to break down
    by type dimensionally, at which point we discover the 10x relationship of database
    calls to HTTP requests. Having to drill down right away to build an intelligible
    chart is a sign that something isn’t right about the metric naming scheme.'
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0202](../images/00020.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. The effect of bad naming on chart usability
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'It is a good practice to group related data together, such as by prefixing
    metric names with namespaces like “jvm” or “db.” For example, a suite of metrics
    related to JVM garbage collection could be prefixed with `jvm.gc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This namespacing not only helps group related metrics alphabetically in many
    monitoring system UIs and dashboarding utilities, but also can be used to affect
    a group of metrics in one pass with a `MeterFilter`. For example, to disable all
    `jvm.gc` metrics, we can apply a deny `MeterFilter` on this name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Different monitoring systems have different recommendations regarding naming
    convention, and some naming conventions may be incompatible for one system and
    not another. Recall that Micrometer employs a naming convention that separates
    lowercase words with a *.* (dot) character. Each Micrometer implementation for
    a monitoring system comes with a naming convention that transforms lowercase dot
    notation names into the monitoring system’s recommended naming convention.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, this naming convention sanitizes metric names and tags of special
    characters that are disallowed by the monitoring system. The convention turns
    out to be more than just a play at appearing more idiomatic. If shipped in this
    form without any naming convention normalization two metrics, `http.server.requests`
    and `http.client.requests`, would break Elasticsearch indexing, which treats dots
    as a form of hierarchy for the purpose of indexing. If these metrics were *not*
    shipped with dots to SignalFx, we wouldn’t be able to take advantage of UI presentation
    in SignalFx that hierarchizes metrics with dot separators in them. These two differing
    opinions on the dot character are mutually exclusive. With naming convention normalization,
    these metrics are shipped as `httpServerRequests` and `httpClientRequests` to
    Elastic and with the dot notation to SignalFx. So the application code maintains
    maximum portability without having to change instrumentation. When using Micrometer,
    meter names and tag keys should follow these guidelines:'
  prefs: []
  type: TYPE_NORMAL
- en: Always use dots to separate parts of the name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid adding unit names or words like `total` to meter names.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So choose `jvm.gc.memory.promoted` instead of `jvmGcMemoryPromoted` or `jvm_gc_memory_promoted`.
    If you prefer one of the latter (or if your monitoring system requires it), configure
    the naming convention on the registry to do this conversion. But using dot separators
    in metric names up and down the whole software stack yields consistent outcomes
    for a variety of monitoring systems.
  prefs: []
  type: TYPE_NORMAL
- en: For some monitoring systems, the presence of unit names and the like are part
    of the idiomatic naming scheme. Again, naming conventions can add these bits where
    appropriate. For example, Prometheus’s naming convention adds `_total` to the
    suffix of counters and `_seconds` to the end of timers. Also, the base unit of
    time varies based on the monitoring system. With a Micrometer `Timer`, you record
    in whatever granularity you’d like and the time values are scaled at publishing
    time. Even if you always record in a certain granularity, including the unit name
    in the meter name is inaccurate. For example, [Example 2-8](part0006_split_007.html#timer_with_unit_in_name)
    shows up as `requests_millis_seconds` in Prometheus, which is awkward.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example 2-8\. Bad practice: adding a unit to the meter name'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The default naming convention of a `MeterRegistry` can be overridden with a
    custom one, which can build upon some basic building blocks provided in the `NamingConvention`
    interface, as shown in [Example 2-9](part0006_split_007.html#custom_naming_convention).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-9\. A custom naming convention that adds base units as a suffix
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](../images/00112.png)](part0006_split_007.html#co_application_metrics_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '`NamingConvention` is a functional interface, so this can be simplified to
    a lambda, but for the sake of clarity here we leave the anonymous class.'
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in [“Dimensional Metrics”](part0006_split_002.html#dimensional_metrics_instrumentation),
    the total storage cost of a metric is the product of the cardinality of the value
    set of each of its tags. Choose tag names that help identify the failure mode
    of a piece of software. For example, if monitoring an auto insurance rating application,
    tagging policy metrics with vehicle class is more useful than tagging with a unique
    vehicle identification number. As a result of a bug or downstream service outage,
    a class of vehicles like classic trucks may start failing. Responding to an alert
    on a policy rating error ratio that exceeds a predetermined threshold, an engineer
    may quickly identify that the handling of classic trucks is problematic based
    on the top three rating failures by vehicle class.
  prefs: []
  type: TYPE_NORMAL
- en: Limit Total Unique Tag Values to Control Storage Cost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Beware of the potential for tag values coming from user-supplied sources to
    blow up the cardinality of a metric. You should always carefully normalize and
    bound user-supplied input. Sometimes the cause is sneaky. Consider the URI tag
    for recording HTTP requests on service endpoints. If we don’t constrain 404s to
    a value like “NOT_FOUND,” the dimensionality of the metric would grow with each
    resource that can’t be found. Even more tricky, an application that redirects
    all nonauthenticated requests to a login endpoint could return a 403 for a resource
    that ultimately will not be found once authenticated, and so a reasonable URI
    tag for a 403 might be “REDIRECTION.” Allowing a tag value set to grow without
    bound can result in an overrun of storage in your monitoring system, increasing
    cost and potentially destabilizing a core part of your observability stack.
  prefs: []
  type: TYPE_NORMAL
- en: In general, avoid recording tags on unique values like user ID unless it is
    known that the population is small.
  prefs: []
  type: TYPE_NORMAL
- en: Tag values must be nonnull and ideally nonblank. Even though Micrometer technically
    supports blank tag values in limited situations, like for the Datadog registry
    implementation, blank tag values are not portable to other monitoring systems
    that don’t support them.
  prefs: []
  type: TYPE_NORMAL
- en: Limit Total Unique Tag Values to Control Query Cost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to increases to storage costs with an increasing number of unique
    tag values, query costs (in both time and resources) also increase as more and
    more time series need to be aggregated in query results.
  prefs: []
  type: TYPE_NORMAL
- en: Common Tags
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When lower-level libraries provide common instrumentation (Micrometer provides
    meter binders out of the box—see [“Meter Binders”](part0007_split_023.html#meter_binders)),
    they cannot know the context of the application this instrumentation will be collected
    in. Is the app running in a private datacenter on one of a small set of named
    VMs whose names never change? On infrastructure-as-a-service public cloud resources?
    In Kubernetes? In virtually every case, there is more than one running copy of
    a particular application that might ship metrics, even if there is just one copy
    in production and one in a lower-level testing environment. It’s useful if we
    can partition in some way the metrics streaming out of these various instances
    of an application by some dimensions that allow us to attribute a behavior back
    to a particular instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Meter filters (covered in more detail in [“Meter Filters”](part0007_split_016.html#6LLJE-2d714b853a094e9a910510217e0e3d73))
    allow you to add common tags to accomplish exactly this, enriching every metric
    published from an application with additional tags. Pick common tags that help
    turn your metrics data into action. Following are common tags that are always
    useful:'
  prefs: []
  type: TYPE_NORMAL
- en: Application name
  prefs: []
  type: TYPE_NORMAL
- en: Consider that some metrics, like HTTP request metrics instrumented by the framework,
    will have the same name across various applications, e.g., `http.server.requests`
    for HTTP endpoints served by the application, and `http.client.requests` for outbound
    requests to other services. By tagging with application name, you could then,
    for example, reason about all outbound requests to some particular service endpoint
    across multiple callers.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster and server group name
  prefs: []
  type: TYPE_NORMAL
- en: In [“Delivery Pipelines”](part0010_split_003.html#delivery_definitions) we talk
    more about the formal definition of a cluster and server group. If you have such
    a topology, tagging with cluster and server group is always useful. Some organizations
    don’t have this level of complexity, and that’s OK too.
  prefs: []
  type: TYPE_NORMAL
- en: Instance name
  prefs: []
  type: TYPE_NORMAL
- en: This may be the machine’s host name in some situations, but not always (and
    this helps explain why Micrometer doesn’t preemptively tag with things like host
    name, because it really does depend on the deployed environment). In public cloud
    environments, host name may not be the right tag. AWS EC2, for example, has local
    and external host names that are distinct from the instance ID. Instance ID is
    actually the easiest of these three to locate a particular instance with in the
    AWS console, and it does uniquely identify the instance. So in this context, instance
    ID is a better tag than host name. In Kubernetes, the pod ID is the right instance-level
    tag.
  prefs: []
  type: TYPE_NORMAL
- en: Stack
  prefs: []
  type: TYPE_NORMAL
- en: “Stack” in this context means development versus production. You may have multiple
    levels of nonproduction environments. Shelter Insurance at one point had “devl,”
    “test,” “func,” and “stage” nonproduction environments, each of which served its
    own purpose (and I might be forgetting one or two). It’s nice to practice monitoring
    even on unstable lower-level environments to baseline your expectations about
    the performance and number of errors produced by a piece of code as it progresses
    along its promotion path on the route to production.
  prefs: []
  type: TYPE_NORMAL
- en: Some other ideas for tags for different deployed environments are included in
    [Table 2-4](part0006_split_010.html#tags_per_provider).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-4\. Common tags by cloud provider
  prefs: []
  type: TYPE_NORMAL
- en: '| Provider | Common tags |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| AWS | Instance ID, ASG name, region, zone, AMI ID, account |'
  prefs: []
  type: TYPE_TB
- en: '| Kubernetes | Pod ID, namespace, cluster name, Deployment/StatefulSet name,
    ReplicaSet name |'
  prefs: []
  type: TYPE_TB
- en: '| Cloud Foundry | CF app name (which may be distinct from the application name),
    organization name, space name, instance ordinal, foundation name |'
  prefs: []
  type: TYPE_TB
- en: 'This table illustrates why Micrometer doesn’t add these tags by default. The
    singular concept of “namespace” has three different names across these cloud providers:
    region, namespace, and organization/space. Where AWS and Kubernetes have a single-value
    namespace concept, CloudFoundry has two: organization and space!'
  prefs: []
  type: TYPE_NORMAL
- en: The application of common tags is a great place to begin thinking as a platform
    engineering organization about how to encapsulate and standardize for your organization.
  prefs: []
  type: TYPE_NORMAL
- en: '[Example 2-10](part0006_split_010.html#property_based_common_tags) shows how
    in Spring Boot, you can apply common tags via property-based configuration.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-10\. Adding common tags by property in Spring Boot
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you could apply tags in an autoconfigurable `@Configuration`
    class, as in [Example 2-11](part0006_split_010.html#programmatic_common_tags).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-11\. Adding common tags programmatically in Spring Boot
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](../images/00112.png)](part0006_split_010.html#co_application_metrics_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This should be sourced from the environment as well.
  prefs: []
  type: TYPE_NORMAL
- en: Assuming you have some central dynamic configuration server, like Spring Cloud
    Config Server, that applications interrogate at startup for properties, the property-based
    configuration allows you to deliver these common tag opinions across your application
    stack immediately and with no code changes or dependency requirements for each
    application.
  prefs: []
  type: TYPE_NORMAL
- en: The programmatic form can be delivered via an explicit runtime binary dependency
    from each app or by injecting a dependency into the deployed form of the app or
    a container running it, like Tomcat.
  prefs: []
  type: TYPE_NORMAL
- en: Classes of Meters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many metrics collectors provide several classes of meters, each of which may
    emit one or more metrics or statistics. The most common are gauges, counters,
    and timers/summaries. More specialized meters include long task timers, time gauges,
    and interval counters. Strive to use the most specialized meter available for
    the task. Timers, for example, always emit a count to measure throughput. There
    is little advantage to counting executions of a particular block of code, when
    timing it would have generated the same count statistic but also richer information
    about the latency distribution of that block of code.
  prefs: []
  type: TYPE_NORMAL
- en: A decision guide for which built-in meter type to choose follows the introduction
    of each type in [“Choosing the Right Meter Type”](part0007_split_011.html#choosing_meter_type).
  prefs: []
  type: TYPE_NORMAL
- en: Gauges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gauges are a measure of an instantaneous value that may increase and decrease
    over time. A time series plot of a gauge is a collection of samples of instantaneous
    values at intervals where metrics were published from the application. Because
    they are sampled instantaneous values, it is possible and even likely that the
    value would have been higher or lower if it had been sampled at a different point
    in time.
  prefs: []
  type: TYPE_NORMAL
- en: The speedometer and fuel level on a vehicle are classic examples of gauges.
    As you drive along the road, you periodically glance at the speedometer (hopefully).
    Seeing a periodic instantaneous measurement of your speed is enough to keep speed
    under control, but it is still true that you miss the variations in speed that
    occurred between looks.
  prefs: []
  type: TYPE_NORMAL
- en: In applications, typical examples for gauges would be the size of a collection
    or map or the number of threads in a running state. Memory and CPU measurements
    are also taken using gauges. [Figure 2-3](part0006_split_012.html#jvm_memory_used)
    is a gauge time series of a single metric, `jvm.memory.used`, that is tagged with
    several dimensions, including the memory space. This stack chart shows one way
    in which making a single concept like memory consumption dimensional provides
    richness to its representation in a chart.
  prefs: []
  type: TYPE_NORMAL
- en: '![An image of JVM heap usage, one meter with several tags represented in a
    stack chart](../images/00017.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Heap used by space
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Micrometer takes the stance that gauges should be sampled and not be set, so
    there is no information about what might have occurred between samples. After
    all, any intermediate values set on a gauge are lost by the time the gauge value
    is reported to a metrics backend anyway, so there seems to be little value in
    setting those intermediate values in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: Think of a gauge as a meter that only changes when it is observed. Every other
    class of meter accumulates intermediate counts toward the point where the data
    is sent to the metrics backend.
  prefs: []
  type: TYPE_NORMAL
- en: The `MeterRegistry` interface contains methods, some of which are shown in [Example 2-12](part0006_split_012.html#gauge_create),
    for building gauges to observe numeric values, functions, collections, and maps.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-12\. Creating gauges
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the first case, a slightly more common form of gauge is one that monitors
    some nonnumeric object. The last argument establishes the function that is used
    to determine the value of the gauge when the gauge is observed. Micrometer provides
    convenience methods for monitoring map and collection size, since these are such
    common cases.
  prefs: []
  type: TYPE_NORMAL
- en: Most forms of creating a gauge maintain only a *weak reference* to the object
    being observed, so as not to prevent garbage collection of the object. It is your
    responsibility to hold a strong reference to the state object that you are measuring
    with a gauge. Micrometer is careful to not create strong references to objects
    that would otherwise be garbage collected. Once the object being gauged is de-referenced
    and is garbage collected, Micrometer will start reporting a NaN or nothing for
    a gauge, depending on the registry implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Generally the returned `Gauge` instance is not useful except in testing, as
    the gauge is already set up to track a value automatically upon registration.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the shortcut methods for creating a gauge directly from a `MeterRegistry`,
    Micrometer provides a gauge fluent builder (see [Example 2-13](part0006_split_012.html#gauge_fluent_builder))
    as well, which has more options. Notice the `strongReference` option, which does
    prevent garbage collection of the monitored object, contrary to the default behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-13\. Fluent builder for gauges
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Micrometer has built-in metrics that include several gauges. Some examples are
    given in [Table 2-5](part0006_split_012.html#gauge_examples).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-5\. Examples of gauges in Micrometer built-in instrumentation
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| jvm.threads.live | The current number of live threads, including both daemon
    and nondaemon threads |'
  prefs: []
  type: TYPE_TB
- en: '| jvm.memory.used | The amount of used memory in bytes |'
  prefs: []
  type: TYPE_TB
- en: '| db.table.size | The total number of rows in a database table |'
  prefs: []
  type: TYPE_TB
- en: '| jetty.requests.active | Number of requests currently active |'
  prefs: []
  type: TYPE_TB
- en: A special kind of `Gauge` called a `TimeGauge` is specifically used for gauging
    values that are measuring time (see [Table 2-6](part0006_split_012.html#time_gauge_examples)).
    Like a `Gauge`, there is no need to set a `TimeGauge`, since its value changes
    when observed. The only difference between them is that the value of a `TimeGauge`
    will be scaled to the monitoring system’s base unit of time as it is published.
    In other cases, follow the general rule that values should be measured in whatever
    the natural base unit is (e.g., bytes for storage, connections for connection
    pool utilization). Monitoring systems only differ in their expectation of what
    the base unit is when describing time.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-6\. Examples of time gauges in Micrometer built-in instrumentation
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| process.uptime | The uptime of the Java virtual machine, as reported by Java’s
    Runtime MXBean |'
  prefs: []
  type: TYPE_TB
- en: '| kafka.consumer.fetch.latency.avg | The average time taken for a group sync,
    as calculated and reported by the Kafka Java client |'
  prefs: []
  type: TYPE_TB
- en: Kafka consumer fetch latency average is an example of where sometimes Java client
    libraries only provide coarse statistics like average where, if we could affect
    the Kafka client code directly, a timer would have been more suitable. In addition
    to seeing the average, we’d have information about decaying max latency, percentiles,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: One last special type of `Gauge` is a `MultiGauge`, which helps manage the gauging
    of a growing or shrinking list of criteria. Often this feature is used when we
    want to select a well-bounded but slightly changing set of criteria from something
    like a SQL query and report some metric for each row as a `Gauge`. It doesn’t
    have to be data fetched from a database, of course. The gauge could be built off
    a map-like structure in memory, or any other structure with rows containing a
    number of columns with at least one numeric column to use for the gauge value.
    [Example 2-14](part0006_split_012.html#multi_gauge) shows how a `MultiGauge` is
    created.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-14\. Creating a multi-gauge
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Before trying to build a gauge to report a rate at which something in your application
    is happening, consider a counter, which is better suited for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Should I Use a Gauge or a Counter?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Never gauge something you can count.
  prefs: []
  type: TYPE_NORMAL
- en: Counters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Counters report a single metric, a count. The `Counter` interface allows you
    to increment by a fixed amount, which must be positive.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible, though rare, to increment a counter by a fractional amount.
    For example, you could be counting sums of a base unit like dollars, which naturally
    have fractional amounts (although it would probably be more useful to count sales
    with a different meter type, as shown in [“Distribution Summaries”](part0007_split_009.html#distribution_summaries)).
  prefs: []
  type: TYPE_NORMAL
- en: The `MeterRegistry` interface contains convenience methods for creating counters,
    as shown in [Example 2-15](part0006_split_014.html#counter_create).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-15\. Creating counters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](../images/00112.png)](part0006_split_014.html#co_application_metrics_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This is not specific to counters—there are similar APIs we will see on other
    meter types.
  prefs: []
  type: TYPE_NORMAL
- en: The `Counter` fluent builder, as seen in [Example 2-16](part0006_split_014.html#counter_fluent_builder),
    contains more options.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-16\. Fluent builder for counters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Micrometer has built-in metrics that include several counters. Some examples
    are given in [Table 2-7](part0006_split_014.html#counter_examples).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-7\. Examples of counters in Micrometer built-in instrumentation
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| jetty.async.requests | Total number of async requests |'
  prefs: []
  type: TYPE_TB
- en: '| postgres.transactions | Total number of transactions executed (commits +
    rollbacks) |'
  prefs: []
  type: TYPE_TB
- en: '| jvm.classes.loaded | The number of classes that are currently loaded in the
    Java virtual machine |'
  prefs: []
  type: TYPE_TB
- en: '| jvm.gc.memory.promoted | Count of positive increases in the size of the old
    generation memory pool before GC to after GC |'
  prefs: []
  type: TYPE_TB
- en: When building graphs and alerts off of counters, generally you should be most
    interested in measuring the rate at which some event is occurring over a given
    time interval. Consider a simple queue. Counters could be used to measure things
    like the rate at which items are being inserted and removed.
  prefs: []
  type: TYPE_NORMAL
- en: When a Counter Measures Occurrences, It Measures Throughput
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we talk about the rate of occurrences of things happening, conceptually
    we are talking about *throughput*. When displaying counters as a rate on a chart,
    we are displaying a measure of throughput, the rapidity with which this counter
    is being incremented. When you have the opportunity to instrument an operation
    with metrics for each individual occurrence, you should almost always use timers
    (see [“Timers”](part0006_split_017.html#5N4J5-2d714b853a094e9a910510217e0e3d73))
    instead, which provide a measure of throughput in addition to other useful statistics.
  prefs: []
  type: TYPE_NORMAL
- en: You might want at first to conceive of visualizing absolute counts rather than
    a rate, but the absolute count is usually both a function of the rapidity with
    which something is used and the longevity of the application instance under instrumentation.
    Building dashboards and alerts of the rate of a counter per some interval of time
    disregards the longevity of the app, letting you see aberrant behavior long after
    the application has started.
  prefs: []
  type: TYPE_NORMAL
- en: In many cases, when we’ve dug into why engineers attempt to visualize absolute
    counts, it is to show some true business-related number (number of sales, revenue,
    etc.). Remember that metrics instrumentation is optimized for signaling availability,
    so its implementation is naturally going to trade off durability for instrumentation
    performance. Any given metrics publishing interval can fail due to things like
    (physical or virtual) machine failure or network issues between the application
    and the metrics backend, and it won’t be retried because the assumption is you’ll
    just catch up on the next interval. Even with cumulative counters, if the final
    value before shutdown doesn’t make it to the backend, it is lost. Mission-critical
    counts, such as those required for legal reporting, should use some other durable
    storage instead of being shipped as a metric (or maybe *in addition* to being
    shipped as a metric).
  prefs: []
  type: TYPE_NORMAL
- en: For some monitoring systems, like Atlas, counters are published as a rate from
    Micrometer. Querying for and plotting a counter in Atlas, as seen in [Example 2-17](part0006_split_015.html#atlas_counter_rate),
    displays a chart whose *y*-axis is a rate.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-17\. Atlas counter rate
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Still, some monitoring systems expect counters to be published as a cumulative
    statistic. It is only at query time that the counter is converted to a rate for
    display, as in [Example 2-18](part0006_split_015.html#prometheus_counter_rate).
    In this case, we have to use the specific `rate` function to convert the cumulative
    statistic into a rate. In almost every monitoring scenario, you’ll use this `rate`
    function when accessing counters.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-18\. Prometheus counter rate
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: It is tempting to count something like errors emitted from a particular method,
    or the total number of successful invocations of a method, but it is even better
    to record these events with a timer, because they include a count and other useful
    information about the latency of the operation.
  prefs: []
  type: TYPE_NORMAL
- en: Should I Use a Counter or a Timer (or a Distribution Summary)?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Never count something you can time. And if the base unit is not a unit of time,
    then the corollary is, awkwardly: never count something you can record with a
    distribution summary.'
  prefs: []
  type: TYPE_NORMAL
- en: Timers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Timers are for measuring short-duration latencies and the frequency of such
    events. All implementations of `Timer` report at least a few individual statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: Count
  prefs: []
  type: TYPE_NORMAL
- en: A measure of the number of individual recordings of this timer. For a `Timer`
    measuring an API endpoint, this count is the number of requests to the API. Count
    is a measure of throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Sum
  prefs: []
  type: TYPE_NORMAL
- en: The sum of the time it took to satisfy all requests. So if there are three requests
    to an API endpoint that took 5 ms, 10 ms, and 15 ms, then the sum is <math alttext="5
    plus 10 plus 15 equals 30 m s"><mrow><mn>5</mn> <mo>+</mo> <mn>10</mn> <mo>+</mo>
    <mn>15</mn> <mo>=</mo> <mn>30</mn> <mi>m</mi> <mi>s</mi></mrow></math> . The sum
    may be shipped literally as 30 ms, or as a rate, depending on the monitoring system.
    We’ll discuss shortly how to interpret this.
  prefs: []
  type: TYPE_NORMAL
- en: Maximum
  prefs: []
  type: TYPE_NORMAL
- en: The individual timing that took the longest, but decaying over an interval.
    Micrometer maintains a series of overlapping intervals in a ring buffer and keeps
    track of maximum in each of these intervals. The maximum value is then a little
    sticky in a sense. The important thing to keep in mind is that this isn’t a maximum
    of all samples seen from the beginning of the app (this wouldn’t be very useful),
    but the maximum value seen “recently.” It is possible to configure the recency
    to make this value decay faster or slower.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, timers can *optionally* ship other statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: Service level objective (SLO) boundaries
  prefs: []
  type: TYPE_NORMAL
- en: The count (i.e., total number) of requests observed that are less than or equal
    to a particular boundary value—for example, how many requests to an API endpoint
    took less than 100 ms. Since this is a count, it can be divided by the overall
    count to arrive at an idea of what percentage of requests met your service level
    objective, and is very cheap to calculate provided you know in advance what objectives
    you want to set.
  prefs: []
  type: TYPE_NORMAL
- en: Percentiles
  prefs: []
  type: TYPE_NORMAL
- en: Precomputed percentiles which cannot be combined with percentiles from other
    tags (e.g., percentiles for several instances in a cluster cannot be combined).
  prefs: []
  type: TYPE_NORMAL
- en: Histograms
  prefs: []
  type: TYPE_NORMAL
- en: Similar to SLO boundaries, histograms are comprised of a series of counts for
    a set of buckets. Histograms can be summed together across dimensions (e.g., sum
    the counts for like buckets across a series of instances) and be used to create
    percentile approximations by some monitoring systems. We’ll discuss histograms
    in more detail in [“Histograms”](part0007_split_007.html#6LKA8-2d714b853a094e9a910510217e0e3d73).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through a couple of examples of how these statistics might be used
    and the relationship between them.
  prefs: []
  type: TYPE_NORMAL
- en: “Count” Means “Throughput”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The count statistic of a timer is individually useful. It is a measure of *throughput*,
    the rate at which the timed operation is happening. When timing an API endpoint,
    it’s the number of requests to that endpoint. When measuring messages on a queue,
    it’s the number of messages being placed on the queue.
  prefs: []
  type: TYPE_NORMAL
- en: The count statistic should be used exactly as described in [“Counters”](part0006_split_014.html#5N4C9-2d714b853a094e9a910510217e0e3d73),
    as a rate. Depending on the monitoring system. this statistic will be either a
    cumulative count or a rate when shipped from Micrometer.
  prefs: []
  type: TYPE_NORMAL
- en: “Count” and “Sum” Together Mean “Aggregable Average”
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the one exception we’ll discuss shortly, sum is not really meaningful on
    its own. Without being considered in relation to the rate at which operations
    are occurring, a sum has no purpose. A sum of 1 second may be bad for an individual
    request to a user-facing API endpoint, but 1,000 requests of 1 ms each yielding
    a sum of 1 second sounds quite good!
  prefs: []
  type: TYPE_NORMAL
- en: Sum and count together can be used to create an *aggregable* average. If we
    instead published the average of a timer directly, it couldn’t be combined with
    the average data from other dimensions (such as other instances) to reason about
    the overall average.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the scenario described in [Figure 2-4](part0006_split_019.html#timer_sum_count),
    where a load balancer has distributed seven requests to four application instances.
    Three of these application instances are in Region 1 and, one instance is in Region
    2.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0204](../images/00120.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. Timings for requests going to a hypothetical application
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Suppose we’ve tagged the timer metrics for each of these instances with the
    instance ID and region. The monitoring system then sees time series for timers
    with four different combinations of tags:'
  prefs: []
  type: TYPE_NORMAL
- en: Instance=1, Region=1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instance=2, Region=1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instance=3, Region=1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instance=4, Region=2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There will be time series for count and sum for each of these timers. In [Table 2-8](part0006_split_019.html#timer_sum_count_table),
    after these seven requests have occurred, a cumulative monitoring system will
    have values for sum and count for their corresponding tags. Also included is the
    average for that instance individually.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-8\. Cumulative sum and count for each timer
  prefs: []
  type: TYPE_NORMAL
- en: '| Instance | Region | Count (operations) | Sum (seconds) | Average (seconds/operation)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 2 | 0.022 | 0.011 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 2 | 0.018 | 0.009 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1 | 2 | 0.020 | 0.010 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2 | 1 | 0.100 | 0.100 |'
  prefs: []
  type: TYPE_TB
- en: To find the average latency for this timer across all instances in both regions
    of this application, we add the sums and divide that by the sum of the counts
    (see [Equation 2-1](part0006_split_019.html#computing_aggregable_average)).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2-1\. Computing the cluster average
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math alttext="StartFraction 0.022 plus 0.018 plus 0.020 plus 0.100 Over 2 plus
    2 plus 2 plus 1 EndFraction equals 0.017 s e c o n d s slash o p equals 17 m i
    l l i s e c o n d s slash o p" display="block"><mrow><mfrac><mrow><mn>0</mn><mo>.</mo><mn>022</mn><mo>+</mo><mn>0</mn><mo>.</mo><mn>018</mn><mo>+</mo><mn>0</mn><mo>.</mo><mn>020</mn><mo>+</mo><mn>0</mn><mo>.</mo><mn>100</mn></mrow>
    <mrow><mn>2</mn><mo>+</mo><mn>2</mn><mo>+</mo><mn>2</mn><mo>+</mo><mn>1</mn></mrow></mfrac>
    <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>017</mn> <mi>s</mi> <mi>e</mi> <mi>c</mi>
    <mi>o</mi> <mi>n</mi> <mi>d</mi> <mi>s</mi> <mo>/</mo> <mi>o</mi> <mi>p</mi> <mo>=</mo>
    <mn>17</mn> <mi>m</mi> <mi>i</mi> <mi>l</mi> <mi>l</mi> <mi>i</mi> <mi>s</mi>
    <mi>e</mi> <mi>c</mi> <mi>o</mi> <mi>n</mi> <mi>d</mi> <mi>s</mi> <mo>/</mo> <mi>o</mi>
    <mi>p</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: If instead Micrometer only shipped averages from each instance, we wouldn’t
    have an easy way of calculating this same value. Averaging the averages, as shown
    in [Equation 2-2](part0006_split_019.html#average_of_averages), is not correct.
    The “average” here is too high. Several more requests went to Region 1 than Region
    2, and Region 1 was serving responses much more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2-2\. An INCORRECT calculation of cluster average
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math alttext="StartFraction 0.011 plus 0.009 plus 0.010 plus 0.100 Over 4 i
    n s t a n c e s EndFraction equals 0.032 s e c o n d s slash r e q u e s t equals
    32 m i l l i s e c o n d s slash r e q u e s t" display="block"><mrow><mfrac><mrow><mn>0</mn><mo>.</mo><mn>011</mn><mo>+</mo><mn>0</mn><mo>.</mo><mn>009</mn><mo>+</mo><mn>0</mn><mo>.</mo><mn>010</mn><mo>+</mo><mn>0</mn><mo>.</mo><mn>100</mn></mrow>
    <mrow><mn>4</mn><mi>i</mi><mi>n</mi><mi>s</mi><mi>t</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>s</mi></mrow></mfrac>
    <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>032</mn> <mi>s</mi> <mi>e</mi> <mi>c</mi>
    <mi>o</mi> <mi>n</mi> <mi>d</mi> <mi>s</mi> <mo>/</mo> <mi>r</mi> <mi>e</mi> <mi>q</mi>
    <mi>u</mi> <mi>e</mi> <mi>s</mi> <mi>t</mi> <mo>=</mo> <mn>32</mn> <mi>m</mi>
    <mi>i</mi> <mi>l</mi> <mi>l</mi> <mi>i</mi> <mi>s</mi> <mi>e</mi> <mi>c</mi> <mi>o</mi>
    <mi>n</mi> <mi>d</mi> <mi>s</mi> <mo>/</mo> <mi>r</mi> <mi>e</mi> <mi>q</mi> <mi>u</mi>
    <mi>e</mi> <mi>s</mi> <mi>t</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The demonstration for how average is calculated across the cluster here assumes
    that Micrometer was shipping a cumulative value for sum and count. How does it
    work out if Micrometer was shipping a rate instead? [Table 2-9](part0006_split_019.html#timer_sum_count_table_rate)
    shows rate-normalized values such as those that would be shipped to Atlas. For
    the sake of this table, assume that the seven requests shown in [Figure 2-4](part0006_split_019.html#timer_sum_count)
    happened over a one-minute interval and that this interval is aligned with the
    interval in which we push metrics to Atlas.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-9\. Rate-normalized sum and count for each timer
  prefs: []
  type: TYPE_NORMAL
- en: '| Instance | Region | Count (requests/second) | Sum (unitless) | Average (seconds/request)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0.033 | 0.00037 | 0.011 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 1 | 0.033 | 0.00030 | 0.009 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1 | 0.033 | 0.00033 | 0.010 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2 | 0.017 | 0.00167 | 0.100 |'
  prefs: []
  type: TYPE_TB
- en: The count column now has a unit of “requests/second” rather than just “requests.”
    It will be requests/second no matter what the publishing interval is. In this
    case, we’re publishing every minute; so since we saw two requests to Instance
    1, we conclude that the requests/second rate of requests to this instance is [Equation
    2-3](part0006_split_019.html#rate_avg_instance1).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2-3\. The rate of throughput to Instance 1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math alttext="2 r e q u e s t s slash m i n u t e equals 2 r e q u e s t s
    slash 60 s e c o n d s equals 0.033 r e q u e s t s slash s e c o n d" display="block"><mrow><mn>2</mn>
    <mi>r</mi> <mi>e</mi> <mi>q</mi> <mi>u</mi> <mi>e</mi> <mi>s</mi> <mi>t</mi> <mi>s</mi>
    <mo>/</mo> <mi>m</mi> <mi>i</mi> <mi>n</mi> <mi>u</mi> <mi>t</mi> <mi>e</mi> <mo>=</mo>
    <mn>2</mn> <mi>r</mi> <mi>e</mi> <mi>q</mi> <mi>u</mi> <mi>e</mi> <mi>s</mi> <mi>t</mi>
    <mi>s</mi> <mo>/</mo> <mn>60</mn> <mi>s</mi> <mi>e</mi> <mi>c</mi> <mi>o</mi>
    <mi>n</mi> <mi>d</mi> <mi>s</mi> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>033</mn>
    <mi>r</mi> <mi>e</mi> <mi>q</mi> <mi>u</mi> <mi>e</mi> <mi>s</mi> <mi>t</mi> <mi>s</mi>
    <mo>/</mo> <mi>s</mi> <mi>e</mi> <mi>c</mi> <mi>o</mi> <mi>n</mi> <mi>d</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The sum column now is unitless rather than being in seconds. It’s unitless because
    the numerator of the rate is seconds as well as the denominator, and these units
    cancel out. So for Instance 1, the sum is [Equation 2-4](part0006_split_019.html#rate_sum_instance1).
    This unitless nature of sum in a rate-normalized system serves to underscore its
    meaninglessness independent of being combined with count (or some other dimensioned
    value).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2-4\. The rate-normalized sum of Instance 1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math alttext="22 m i l l i s e c o n d s slash m i n u t e equals 0.022 s e
    c o n d s slash 60 s e c o n d s equals 0.00037" display="block"><mrow><mn>22</mn>
    <mi>m</mi> <mi>i</mi> <mi>l</mi> <mi>l</mi> <mi>i</mi> <mi>s</mi> <mi>e</mi> <mi>c</mi>
    <mi>o</mi> <mi>n</mi> <mi>d</mi> <mi>s</mi> <mo>/</mo> <mi>m</mi> <mi>i</mi> <mi>n</mi>
    <mi>u</mi> <mi>t</mi> <mi>e</mi> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>022</mn>
    <mi>s</mi> <mi>e</mi> <mi>c</mi> <mi>o</mi> <mi>n</mi> <mi>d</mi> <mi>s</mi> <mo>/</mo>
    <mn>60</mn> <mi>s</mi> <mi>e</mi> <mi>c</mi> <mi>o</mi> <mi>n</mi> <mi>d</mi>
    <mi>s</mi> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>00037</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The average per instance is the same as in the cumulative table because of the
    effect of the units canceling out.
  prefs: []
  type: TYPE_NORMAL
- en: For the purpose of average, it doesn’t matter what the interval is. If the interval
    were two minutes rather than one minute, our idea of throughput changes (i.e.,
    it’s exactly half), but the extra minute cancels out in the average calculation.
    In the case of Instance 1, the count in requests/seconds is [Equation 2-5](part0006_split_019.html#rate_avg_instance1_2min_1).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2-5\. The rate of throughput to Instance 1 over a two-minute interval
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math alttext="2 r e q u e s t s slash 2 m i n u t e s equals 2 r e q u e s
    t s slash 120 s e c o n d s equals 0.01667 r e q u e s t s slash s e c o n d"
    display="block"><mrow><mn>2</mn> <mi>r</mi> <mi>e</mi> <mi>q</mi> <mi>u</mi> <mi>e</mi>
    <mi>s</mi> <mi>t</mi> <mi>s</mi> <mo>/</mo> <mn>2</mn> <mi>m</mi> <mi>i</mi> <mi>n</mi>
    <mi>u</mi> <mi>t</mi> <mi>e</mi> <mi>s</mi> <mo>=</mo> <mn>2</mn> <mi>r</mi> <mi>e</mi>
    <mi>q</mi> <mi>u</mi> <mi>e</mi> <mi>s</mi> <mi>t</mi> <mi>s</mi> <mo>/</mo> <mn>120</mn>
    <mi>s</mi> <mi>e</mi> <mi>c</mi> <mi>o</mi> <mi>n</mi> <mi>d</mi> <mi>s</mi> <mo>=</mo>
    <mn>0</mn> <mo>.</mo> <mn>01667</mn> <mi>r</mi> <mi>e</mi> <mi>q</mi> <mi>u</mi>
    <mi>e</mi> <mi>s</mi> <mi>t</mi> <mi>s</mi> <mo>/</mo> <mi>s</mi> <mi>e</mi> <mi>c</mi>
    <mi>o</mi> <mi>n</mi> <mi>d</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The sum is [Equation 2-6](part0006_split_019.html#rate_avg_instance1_2min_2).
    But when we divide these, the average is still the same. In this division, you
    can essentially factor out the *2* from both numerator and denominator.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2-6\. The rate-normalized sum of Instance 1 over a two-minute interval
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math alttext="22 m i l l i s e c o n d s slash 2 m i n u t e s equals 0.022
    s e c o n d s slash 120 s e c o n d s equals 0.00018" display="block"><mrow><mn>22</mn>
    <mi>m</mi> <mi>i</mi> <mi>l</mi> <mi>l</mi> <mi>i</mi> <mi>s</mi> <mi>e</mi> <mi>c</mi>
    <mi>o</mi> <mi>n</mi> <mi>d</mi> <mi>s</mi> <mo>/</mo> <mn>2</mn> <mi>m</mi> <mi>i</mi>
    <mi>n</mi> <mi>u</mi> <mi>t</mi> <mi>e</mi> <mi>s</mi> <mo>=</mo> <mn>0</mn> <mo>.</mo>
    <mn>022</mn> <mi>s</mi> <mi>e</mi> <mi>c</mi> <mi>o</mi> <mi>n</mi> <mi>d</mi>
    <mi>s</mi> <mo>/</mo> <mn>120</mn> <mi>s</mi> <mi>e</mi> <mi>c</mi> <mi>o</mi>
    <mi>n</mi> <mi>d</mi> <mi>s</mi> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>00018</mn></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Average is not ideal for monitoring availability. In general, you are better
    off accepting a slightly worse average and better worst-case (e.g., greater than
    99th percentile) performance since the worst case tends to happen much more frequently
    than our intuition leads us to believe. Still, the combination of sum divided
    by count is easy to compute and possible on almost all monitoring systems, even
    those without more sophisticated math operations. So average is at least some
    baseline to have across a range of wildly different monitoring systems. If at
    all possible, don’t use average at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Average: a random number that falls somewhere between the maximum and 1/2 the
    median. Most often used to ignore reality.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Gil Tene
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Instead, for availability it is more useful to look at maximum or a high-percentile
    statistic, as we’ll see in detail in [“Timers”](part0009_split_009.html#8ILKM-2d714b853a094e9a910510217e0e3d73).
  prefs: []
  type: TYPE_NORMAL
- en: Maximum Is a Decaying Signal That Isn’t Aligned to the Push Interval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Micrometer decays the maximum rather than aligning it to the publishing interval
    like it does for sum and count. If we perfectly aligned the view of maximum time
    to the push interval, then a dropped metrics payload means we potentially miss
    out on seeing a particularly high maximum value (because in the next interval
    we’d only consider samples that occurred in that interval).
  prefs: []
  type: TYPE_NORMAL
- en: For other statistics like count, missing a publishing interval is generally
    not problematic, because the counter continues to accumulate during a period where
    a metrics payload is dropped, and the next successful payload will surface it.
  prefs: []
  type: TYPE_NORMAL
- en: Practically, there are many reasons why a high maximum latency and a dropped
    metrics payload would be correlated. For example, if the application is under
    heavy resource pressure (like a saturated network interface), a response to the
    user for an API endpoint that is being timed (and for which a maximum value is
    being tracked) may be exceedingly high at the same time that a metrics post request
    to the monitoring system fails with a read timeout. But such conditions can be
    (and many times are) temporary.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps you have a client-side load-balancing strategy that recognizes that
    (from the client’s perspective) API latency has gone up sharply for this instance
    that is under resource pressure and begins preferring other instances. By relieving
    pressure on this instance it recovers.
  prefs: []
  type: TYPE_NORMAL
- en: In some subsequent interval, after the instance has recovered, it’s nice to
    be able to push a maximum latency seen during this time of trouble that would
    otherwise have been skipped. In fact, it’s precisely these times of duress that
    we care about the most, not the maximum latency under fair-weather conditions!
  prefs: []
  type: TYPE_NORMAL
- en: The effect of this decaying though is that a maximum value will “linger” for
    a period of time after it actually occurred. In [Figure 2-5](part0006_split_020.html#max_decays),
    we see a maximum value of approximately 30 ms for a timed operation. This 30 ms
    operation occurred sometime in the metrics publishing interval immediately before
    when the line first spikes up from 0 (around 19:15).
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0205](../images/00039.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. The decaying maximum lingers on a chart for some time
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This timer was configured to decay maximum values over a period of two minutes.
    So it lingers until about 19:17\. Since this timer didn’t see any operations after
    this initial interval in which the 30 ms timing was seen, the time series disappears
    after the maximum value decays.
  prefs: []
  type: TYPE_NORMAL
- en: Micrometer keeps track of maximums in a [ring buffer](https://oreil.ly/sHbIN)
    to achieve this decaying behavior. The ring buffer has configuration options `distributionStatisticsBufferLength`
    and `distributionStatisticExpiry` on `Timer.Builder` that you can use to decay
    for longer, as shown in [Example 2-20](part0006_split_024.html#timer_fluent_builder).
    By default, Micrometer builds timers with a ring buffer of length 3, and the pointer
    will be advanced every 2 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-6](part0006_split_020.html#ring_buffer) is an illustration of a ring
    buffer of three elements. This ring buffer is just an array with a pointer to
    a particular element from which the maximum value will be polled whenever we publish
    metrics. Every `distributionStatisticExpiry`, the pointer is advanced to the next
    element in the ring buffer. The zeroth index element in this buffer has no samples
    in it. The first and second indexed elements are storing the state of the largest
    sample they have seen since their last reset, 10 ms. The darkened ring around
    the first index indicates that this index is the element that is currently being
    polled from.'
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0206](../images/00004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. A ring buffer of three elements
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure 2-7](part0006_split_020.html#ring_buffer_of_maxes) shows a timer ring
    buffer with three elements and a two-minute expiry as it evolves over an eight-minute
    period of time. Below the figure is a minute-by-minute description of how the
    values are changing where `t` is the wall time in minutes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0207](../images/00033.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-7\. A timer max ring buffer of length 3 and two-minute expiry
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: t=0
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the initial state. Each ring buffer element is empty. No timer recordings
    have been observed. Between t=0 and t=1, two timer recordings are observed: one
    at 10 ms and one at 8 ms.'
  prefs: []
  type: TYPE_NORMAL
- en: t=1
  prefs: []
  type: TYPE_NORMAL
- en: Since 10 ms is the greater of the two recordings seen and every ring buffer
    element was previously empty, all of them now are tracking 10 ms as the maximum.
    If we were to publish metrics at t=1, the max would be 10 ms. Between t=1 and
    t=2 we observe a 7 ms timing, but it is not greater than the samples in any of
    the ring buffer elements.
  prefs: []
  type: TYPE_NORMAL
- en: t=2
  prefs: []
  type: TYPE_NORMAL
- en: The zeroth ring buffer element is reset because the expiry has been reached
    and the pointer is moved to index 1\. Between t=2 and t=3 we see a timer recording
    of 6 ms. Since the zeroth element has been cleared, it now is tracking 6 ms as
    its view of the max. The polled max is 10 ms.
  prefs: []
  type: TYPE_NORMAL
- en: t=3
  prefs: []
  type: TYPE_NORMAL
- en: The oldest two ring buffer elements are still seeing 10 ms as the max, and the
    zeroth element is tracking 6 ms. The polled max is still 10 ms since the pointer
    is on index 1.
  prefs: []
  type: TYPE_NORMAL
- en: t=4
  prefs: []
  type: TYPE_NORMAL
- en: Index 1 is reset and the pointer is advanced to index 2\. Index 2 is still tracking
    10 ms, so the polled max is 10 ms.
  prefs: []
  type: TYPE_NORMAL
- en: t=5
  prefs: []
  type: TYPE_NORMAL
- en: Nothing changes. The polled max is 10 ms. Note that the timer would have reported
    a count and sum of 0 at this time and a max of 10 ms! This is what is meant by
    max not being aligned to the publishing interval in the same way that count and
    sum are.
  prefs: []
  type: TYPE_NORMAL
- en: t=6
  prefs: []
  type: TYPE_NORMAL
- en: Index 2 is reset and the pointer circles back to index 0, which is still hanging
    on to the 6 ms sample it observed between t=2 and t=3 as the max. The polled max
    is 6 ms. Between t=6 and t=7, a 12 ms sample is observed, which becomes the max
    across the ring buffer.
  prefs: []
  type: TYPE_NORMAL
- en: t=7
  prefs: []
  type: TYPE_NORMAL
- en: The polled max is 12 ms, observed shortly before t=7.
  prefs: []
  type: TYPE_NORMAL
- en: t=8
  prefs: []
  type: TYPE_NORMAL
- en: The zeroth ring buffer element is reset and the pointer moves to index 1\. The
    polled max is 12 ms.
  prefs: []
  type: TYPE_NORMAL
- en: The Sum of Sum Over an Interval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are few cases where the sum of sums is useful. In fact, I’ve only ever
    encountered one case of it, which we’ll see later in [“Proportion of time spent
    in garbage collection”](part0009_split_018.html#8IN04-2d714b853a094e9a910510217e0e3d73),
    where the sum of time spent in garbage collection (GC) is divided by the total
    amount of time in the interval in which garbage collection was happening (e.g.,
    how much time was spent in GC altogether). Even in this case, we probably could
    develop a better alert signal for garbage collection if the JVM provided us with
    discrete timings for every garbage collection event as it occurred. If it did,
    we might look at high-percentile GC times or max GC time by cause.
  prefs: []
  type: TYPE_NORMAL
- en: The Base Unit of Time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The appropriate base unit for timers varies by monitoring system. For example,
    Prometheus expects floating point second-precision data because conceptually seconds
    are a base unit of time. Atlas expects nanosecond-precision data because it can
    then accept and store integral values. Since it isn’t possible to measure a subdivision
    of a nanosecond (and in many cases it isn’t even possible to measure in true nanosecond
    precision), the backend takes advantage of this optimization. Regardless, Micrometer
    automatically scales timings to the expected base unit for each monitoring system.
  prefs: []
  type: TYPE_NORMAL
- en: Neither one of these base units is right or wrong, and neither less precise.
    It is simply a matter of convention in each. The base unit of time doesn’t affect
    the *precision* of charts. Even though Micrometer ships times in seconds to Prometheus,
    for example, the chart will often still be displayed, like in [Figure 2-8](part0006_split_022.html#timer_base_unit_of_time_scaling),
    in milliseconds for common timers like those used to monitor user-facing API endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0208](../images/00113.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-8\. A timer shipped with *seconds* base units displayed in milliseconds
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We’ll cover charting more in [Chapter 4](part0009_split_000.html#8IL24-2d714b853a094e9a910510217e0e3d73),
    but for now, just know that scaling in this way is often accomplished automatically
    by the charting interface. We just need to tell it how to interpret the statistic,
    i.e., to say that the time series being displayed is dimensioned in seconds, as
    in [Figure 2-9](part0006_split_022.html#grafana_yaxis_seconds).
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0209](../images/00008.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-9\. Informing the charting library how to interpret timer base units
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this case, Grafana is smart enough to know that it is easier for humans to
    read in milliseconds than small fractions of a second, so it scales the seconds
    data down to milliseconds. Similarly, if it is canonical to represent timings
    in nanoseconds to a particular monitoring system, the charting library would perform
    the opposite math to scale the values up to milliseconds.
  prefs: []
  type: TYPE_NORMAL
- en: Common Base Units Don’t Limit How You View the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For common units like time and data size (e.g., bytes), you shouldn’t need to
    concern yourself with how you intend to *view* the data to decide what scale you
    *record* at. It’s generally preferable to be consistent with your base units everywhere
    and allow the charting library later to do this sort of automatic scaling to a
    human-readable format. Rather than recording the payload size of a response body
    in bytes (because it will generally be small) and the size of the heap in megabytes
    (because it will generally be large), record both in bytes. They will both be
    scaled to reasonable values later when you go to view them.
  prefs: []
  type: TYPE_NORMAL
- en: For monitoring systems that do not have a clear preference for base unit of
    time, Micrometer chooses one; and it is not generally configurable because consistency
    across all applications is more important than changing the base unit of time
    when precision is not sacrificed either way.
  prefs: []
  type: TYPE_NORMAL
- en: Using Timers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `MeterRegistry` interface contains convenience methods for creating timers,
    as shown in [Example 2-19](part0006_split_024.html#timer_create).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-19\. Creating timers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `Timer` fluent builder (see [Example 2-20](part0006_split_024.html#timer_fluent_builder))
    contains more options. Most of the time you won’t use all of these options.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-20\. Fluent builder for timers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `Timer` interface exposes several convenience overloads for recording timings
    inline, such as in [Example 2-21](part0006_split_024.html#timer_record_execution).
    Additionally, a `Runnable` or `Callable` can be wrapped with instrumentation and
    returned for use later.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-21\. Recording execution with a timer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Timers Versus Distribution Summaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Timers` are really just a specialized form of distribution summaries (see
    [“Distribution Summaries”](part0007_split_009.html#distribution_summaries)) that
    are aware of how to scale durations to the base unit of time of each monitoring
    system and that have an automatically determined base unit. In almost every case
    where you want to measure time, you should use a `Timer` rather than a `DistributionSummary`.
    The rare exception to this is when recording many long-duration events in a short
    interval, such that the nanosecond-precision `Timer` would overflow ~290 years
    (since Java’s long can effectively store a maximum of 9.22e9 seconds) inside of
    a single interval.'
  prefs: []
  type: TYPE_NORMAL
- en: You may also store start state in a sample instance that can be stopped later.
    The sample records a start time based on the registry’s clock. After starting
    a sample, execute the code to be timed, and finish the operation by calling `stop(Timer)`
    on the sample.
  prefs: []
  type: TYPE_NORMAL
- en: Notice in [Example 2-22](part0006_split_025.html#timer_sample) how the timer
    that the sample is accumulating to is not determined until it is time to stop
    the sample. This allows some tags to be determined dynamically from the end state
    of the operation we are timing. The use of `Timer.Sample` is especially common
    when we are dealing with some event-driven interface with a listener pattern.
    This example is a simplified form of Micrometer’s JOOQ execution listener.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-22\. Use of timer samples for event-driven patterns
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](../images/00112.png)](part0006_split_025.html#co_application_metrics_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We would typically add some tags based on the result of the execution with data
    elements found in `ExecuteContext`.
  prefs: []
  type: TYPE_NORMAL
- en: There is also an `AutoCloseable` form of timer sample that is useful when timing
    blocks of code that contain checked exception handling, as shown in [Example 2-23](part0006_split_025.html#timer_closeable_sample).
    The pattern does require nested `try` statements, which are a bit unusual. If
    you are uncomfortable with this pattern, you can absolutely stick to a simple
    `Timer.Sample`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-23\. Use of timer samples
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](../images/00112.png)](part0006_split_025.html#co_application_metrics_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This tag will apply to both outcomes. Description text and percentile histograms
    will also apply to both outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](../images/00059.png)](part0006_split_025.html#co_application_metrics_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This nested try statement makes it possible to access the `Timer.ResourceSample`
    in the catch block for adding error tags.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](../images/00067.png)](part0006_split_025.html#co_application_metrics_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: We can add tags at each branching point in the `try/catch` block to record information
    about the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Micrometer has built-in metrics that include several timers. Some examples are
    given in [Table 2-10](part0006_split_025.html#timer_examples).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-10\. Examples of timers in Micrometer built-in instrumentation
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| http.server.requests | Spring Boot records timings for executions of WebMVC
    and WebFlux request handlers. |'
  prefs: []
  type: TYPE_TB
- en: '| jvm.gc.pause | Time spent in GC pause. |'
  prefs: []
  type: TYPE_TB
- en: '| mongodb.driver.commands | Time spent in MongoDB operations. |'
  prefs: []
  type: TYPE_TB
- en: Timers are the metrics corollary to distributed tracing (discussed in depth
    in [Chapter 3](part0008_split_000.html#7K4G4-2d714b853a094e9a910510217e0e3d73))
    in the sense that trace spans and timers can instrument the same code, as in [Example 2-24](part0007_split_000.html#trace_and_timing).
  prefs: []
  type: TYPE_NORMAL
- en: The Intersection of Tracing and Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The overlap between distributed tracing and metrics is strictly limited to timing.
  prefs: []
  type: TYPE_NORMAL
- en: The sample code uses Zipkin’s Brave instrumentation, which we’ll see again later.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-24\. Tracing and timing the same block of code
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](../images/00112.png)](part0007_split_000.html#co_application_metrics_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Brave doesn’t have an AutoCloseable construct like Micrometer, so the instrumentation
    looks a little asymmetric.
  prefs: []
  type: TYPE_NORMAL
- en: It’s natural to assume that a particular block of code, given similar inputs,
    would execute in roughly the same amount of time. Our intuition about this can
    be misleading.
  prefs: []
  type: TYPE_NORMAL
- en: Common Features of Latency Distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is important to understand some common characteristics of timings in Java
    applications. The same block of code will not execute in the same amount of time
    on each execution due to variance in the input parameters, downstream systems,
    the state of the heap, and many other variables. Nevertheless, many requests with
    similar inputs will commonly be satisfied in a similar amount of time.
  prefs: []
  type: TYPE_NORMAL
- en: Intuition may lead to a belief that timings are roughly normally distributed,
    i.e., that there is a central hump around the average with lower-probability tails
    for both faster and slower response times, like in [Figure 2-10](part0007_split_001.html#normal_distribution).
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0210](../images/00050.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-10\. The normal distribution
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In real-world cases, timings are almost always multimodal, meaning there is
    more than one “hump,” or grouping of timings, along the latency spectrum. Most
    commonly, Java timings are bimodal (two humps, as shown in [Figure 2-11](part0007_split_001.html#bimodal)),
    with the smaller, rightmost hump representing events like garbage collection and
    VM pauses. That second hump also includes a ripple effect of the multimodality
    in downstream services.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0211](../images/00056.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-11\. A typical bimodal distribution of Java latencies
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Bizarrely, the bigger, leftmost hump is often (though not always, of course)
    quite narrow and contains more than 99% of the timings. As a result, the 99th
    percentile (see [“Percentiles/Quantiles”](part0007_split_003.html#6LK44-2d714b853a094e9a910510217e0e3d73))
    will in many cases be below the average, which is skewed higher by the second
    hump.
  prefs: []
  type: TYPE_NORMAL
- en: Standard Deviation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some metrics instrumentation libraries and systems ship and display standard
    deviation. This metric only makes sense in the context of a normal distribution,
    which we’ve seen is essentially never the case. Standard deviation is not a meaningful
    statistic for real-world timings of Java executions. Ignore it! Also, ignore average.
  prefs: []
  type: TYPE_NORMAL
- en: Percentiles/Quantiles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Did I mention not to use average latency? At this point, you can probably see
    that I won’t miss an opportunity to pounce on average latency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Average: a random number that falls somewhere between the maximum and 1/2 the
    median. Most often used to ignore reality.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Gil Tene
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Average latency is a poor metric to monitor to assess latency behavior, and
    max is a useful alert threshold but can be spiky. For comparative performance,
    we can look at high-percentile values, which are less spiky. High max spikes will
    certainly be more prevalent in less-performant code, but in any case *when* they
    occur is not under your control, making side-by-side comparisons even of identical
    blocks of code running on two different instances difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the monitoring system, the term *percentile*, or *quantile*, is
    used to describe a point in a set of samples that relates to the rank order of
    its values. So the middle quantile, which is also called the median or the 50th
    percentile, is the middle value in a set of samples ranked from least to greatest.
  prefs: []
  type: TYPE_NORMAL
- en: Median Versus Average
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Average is rarely useful for monitoring timings. Statistically, the average
    is the sum of all samples divided by the total number of samples. The average
    is just a different measure of centrality than the median, not better or worse
    in general at representing centrality. Even if it were a “perfect” measure of
    centrality, for the purpose of proving our system is reliable we care more about
    the *worst* half of the distribution, not the best half.
  prefs: []
  type: TYPE_NORMAL
- en: Percentiles are a special type of quantile, described relative to 100%. In a
    list of 100 samples ordered from least to greatest, the 99th percentile (P99)
    is the 99th sample in order. In a list of 1,000 samples, the 99.9th percentile
    is the 999th sample.
  prefs: []
  type: TYPE_NORMAL
- en: By this definition, percentiles, particularly high percentiles, are useful for
    determining what *most* users are experiencing (i.e., P99 is the worst latency
    that 99 out of 100 users experienced). For timings, percentiles usefully cut out
    the spiky behavior of VM or garbage collection pauses while still preserving majority
    user experience.
  prefs: []
  type: TYPE_NORMAL
- en: It is tempting to monitor a high-percentile value like the 99th and feel at
    ease that your users are experiencing good response times. Unfortunately, our
    intuition leads us astray with these statistics. The top 1% typically hides latencies
    that are one or two orders of magnitude larger than P99.
  prefs: []
  type: TYPE_NORMAL
- en: Any single request will avoid the top 1% exactly 99% of the time. When considering
    `N` requests, the chance that at least one of these requests is in the top 1%
    is <math alttext="left-parenthesis 1 minus 0.99 Superscript upper N Baseline right-parenthesis
    asterisk 100 percent-sign"><mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mn>0</mn> <mo>.</mo>
    <msup><mn>99</mn> <mi>N</mi></msup> <mo>)</mo> <mo>*</mo> <mn>100</mn> <mo>%</mo></mrow></math>
    (assuming these probabilities are independent, of course). It takes surprisingly
    few requests for there to be a greater than majority chance that one request will
    hit the top 1%. For 100 individual requests, the chance is <math alttext="left-parenthesis
    1 minus 0.99 Superscript 100 Baseline right-parenthesis asterisk 100 percent-sign
    equals 63.3 percent-sign"><mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mn>0</mn> <mo>.</mo>
    <msup><mn>99</mn> <mn>100</mn></msup> <mo>)</mo> <mo>*</mo> <mn>100</mn> <mo>%</mo>
    <mo>=</mo> <mn>63</mn> <mo>.</mo> <mn>3</mn> <mo>%</mo></mrow></math> !
  prefs: []
  type: TYPE_NORMAL
- en: Consider the fact that a user interaction with your system likely involves many
    resource interactions (UI, API gateway, multiple microservice calls, some database
    interactions, etc.). The chance that any individual end-to-end *user interaction*
    experiences a top 1% latency on some resource in the chain of events satisfying
    their request is actually much higher than 1%. We can approximate this chance
    as <math alttext="left-parenthesis 1 minus 0.99 Superscript upper N Baseline right-parenthesis
    asterisk 100 percent-sign"><mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mn>0</mn> <mo>.</mo>
    <msup><mn>99</mn> <mi>N</mi></msup> <mo>)</mo> <mo>*</mo> <mn>100</mn> <mo>%</mo></mrow></math>
    . If a single request in a chain of microservices experiences a top 1% latency,
    then the whole user experience suffers, especially given the fact that the top
    1% tends to be an order of magnitude or more worse in performance than requests
    under the 99th percentile.
  prefs: []
  type: TYPE_NORMAL
- en: Time (Anti-)Correlation of Samples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The formulas given to determine the chance of experiencing a top 1% latency
    are only approximations. In reality, time-correlation of high/low latency leads
    to a lower chance, and anti-correlation would lead to a higher chance. In other
    words, the probabilities for each request aren’t truly independent. We almost
    never have information about this correlation, so the approximations are useful
    guides for how you should reason about your system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Micrometer supports two ways of computing percentiles for timers:'
  prefs: []
  type: TYPE_NORMAL
- en: Precompute the percentile value and ship it directly to the monitoring system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Group timings into discrete sets of latency buckets, and ship the sets of buckets
    together to the monitoring system (see [“Histograms”](part0007_split_007.html#6LKA8-2d714b853a094e9a910510217e0e3d73)).
    The monitoring system is then responsible for computing the percentile from a
    histogram.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precomputing percentile values is the most portable approach since many monitoring
    systems don’t support histogram-based percentile approximation, but it’s useful
    only in a narrow set of circumstances (we’ll see why a little later in this section).
    Precomputed percentiles can be added to a timer in a few different ways.
  prefs: []
  type: TYPE_NORMAL
- en: The `Timer` fluent builder supports adding percentiles directly as the `Timer`
    is being constructed, as shown in [Example 2-25](part0007_split_005.html#timer_percentile_fluent).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-25\. Adding percentiles to a Timer via the builder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[Example 2-26](part0007_split_005.html#timer_percentile_meter_filter) shows
    how to add percentiles with a `MeterFilter`.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-26\. Adding percentiles to a Timer via a MeterFilter
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, frameworks like Spring Boot offer property-driven `MeterFilter` equivalents
    that allow you to add percentiles to `Timers` declaratively. The configuration
    shown in [Example 2-27](part0007_split_005.html#property_driven_percentiles) adds
    percentiles to any timer prefixed with the name `requests`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-27\. Adding percentiles to metrics prefixed with “requests” in Spring
    Boot
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Adding percentiles through a `MeterFilter` allows you to add percentile support
    to `Timers` that are created not just in your application code, but in other libraries
    you are including in your application that contain Micrometer instrumentation.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Timers to Library Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are authoring a library and including timing code, do not preconfigure
    your timers with features like percentiles, histograms, and SLO boundaries. These
    features all have some performance cost, even if it is minimal. Allow the consumers
    of your library to determine if the timing is an important enough indicator to
    warrant the extra expense of these statistics. In particular, end users will want
    to turn on histograms when they intend to use the timing as part of a comparative
    measure, like in [“Automated Canary Analysis”](part0010_split_013.html#9H5VC-2d714b853a094e9a910510217e0e3d73).
    When needed, users can configure these statistics with a `MeterFilter`.
  prefs: []
  type: TYPE_NORMAL
- en: Wherever a timer metric has more than a few total unique combinations of tags,
    pre-computed percentiles are unusable because they cannot be combined or aggregated.
    On a cluster of two instances, if the 90th percentile latency for a request endpoint
    is 100 ms on one application instance and 200 ms on another, we can’t simply average
    these two values together to arrive at a cluster-wide 90th percentile latency
    of 150 ms.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2-11](part0007_split_006.html#percentiles_for_two_instances) shows why,
    using the median (50th percentile) as an example. Since the individual samples
    that went into this percentile calculation were thrown away, there is no way to
    reconstitute them to derive a cluster-wide percentile at the monitoring system.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-11\. P50 (median) request latency in a cluster of two instances
  prefs: []
  type: TYPE_NORMAL
- en: '| Instance | Individual latencies (ms) | P50 latency (ms) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | [100,110,125] | 110 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | [125,130,140] | 130 |'
  prefs: []
  type: TYPE_TB
- en: '| Whole cluster | [100,110,125,125,130,140] | 125 |'
  prefs: []
  type: TYPE_TB
- en: The best we can do with precomputed percentiles is to simply plot all of the
    values and look for outliers, as in [Figure 2-12](part0007_split_006.html#p99_not_aggregated),
    generated from the Prometheus query `requests_second{quantile=0.99}`.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0212](../images/00103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-12\. 99th percentile of a single timer for individual application instances
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This scales to a point; but as the number of instances grows (imagine we had
    a cluster of 100 instances!), the visualization quickly becomes crowded. Attempts
    to limit the number of lines displayed to select just the top *N* worst latencies
    can result in situations where the legend is still full of individual instance
    IDs. This is because, as we see in [Figure 2-13](part0007_split_006.html#p99_topk),
    where we are selecting the top three worst latencies with the Prometheus query
    `topk(3, requests_second{quantile=0.99})`, the third-worst instance changes virtually
    every interval.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0213](../images/00090.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-13\. Top three worst 99th percentile of a single timer for individual
    application instances
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Because of the limitations of precomputed percentiles, if you are working with
    a monitoring system that supports histograms, *always* use them instead, as described
    in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Histograms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Metrics are always presented in aggregated form to the monitoring system. The
    individual timings that together we represent as the latency of a block of code
    are not shipped to the monitoring system. If they were, metrics would no longer
    have a fixed cost irrespective of throughput.
  prefs: []
  type: TYPE_NORMAL
- en: We can send an approximation of what the individual timings looked like together
    in a histogram. In a histogram, the range of possible timings is divided into
    a series of buckets. For each bucket (also known as an *interval* or *bin*), the
    histogram maintains a count of how many individual timings fell into that bucket.
    The buckets are consecutive and nonoverlapping. They are not often of equal size,
    since there is generally some part of the range which we care about at a more
    fine-grained level than others. For example, for an API endpoint latency histogram,
    we care more about the distinction between 1, 10, and 100 ms latencies than we
    do about 40 s and 41 s latencies. The latency buckets will be more granular around
    the expected value than well outside the expected value.
  prefs: []
  type: TYPE_NORMAL
- en: Importantly, by accumulating all the individual timings into buckets, and controlling
    the number of buckets, we can retain the shape of the distribution while maintaining
    a fixed cost.
  prefs: []
  type: TYPE_NORMAL
- en: Histograms are represented in the monitoring system’s storage as a series of
    counters. In the case of Prometheus, as in [Table 2-12](part0007_split_007.html#histogram_bucket_storage),
    these counters have a special tag `le` that indicates that the metric is a count
    of all samples less than or equal to the tag value (in seconds).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-12\. How histogram buckets are stored in a time series database (Prometheus)
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name | Values |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| http_server_requests_seconds_bucket{status=200,le=0.1} | [10,10,12,15] |'
  prefs: []
  type: TYPE_TB
- en: '| http_server_requests_seconds_bucket{status=200,le=0.2} | [20,20,24,26] |'
  prefs: []
  type: TYPE_TB
- en: '| http_server_requests_seconds_bucket{status=200,le=0.5} | [30,30,40,67] |'
  prefs: []
  type: TYPE_TB
- en: '| http_server_requests_seconds_bucket{status=500,le=0.1} | [1,1,2,5] |'
  prefs: []
  type: TYPE_TB
- en: '| http_server_requests_seconds_bucket{status=500,le=0.2} | [1,1,2,6] |'
  prefs: []
  type: TYPE_TB
- en: '| http_server_requests_seconds_bucket{status=500,le=0.5} | [1,1,2,6] |'
  prefs: []
  type: TYPE_TB
- en: Depending on the monitoring system, a histogram may appear as either a normal
    or a cumulative histogram. A visual distinction between these two types of histograms
    is shown in [Figure 2-14](part0007_split_007.html#cumulative_vs_normal_histogram).
    Cumulative histogram buckets represent the count of all timings less than or equal
    to their boundary. Note that the timer’s count is equal to the sum of all buckets
    in a normal histogram.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0214](../images/00095.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-14\. Cumulative versus normal histogram
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Like adding any other tag, adding histogram data to a timer increases the total
    required storage by a factor equal to the number of buckets that the range is
    subdivided into. In this example, since there are three buckets (0.1 s, 0.2 s,
    and 0.5 s) there will be three times the number of permutations of other tags’
    time series stored.
  prefs: []
  type: TYPE_NORMAL
- en: This histogram is published each interval to the monitoring system. The histograms
    for each interval can be assembled into a heatmap. [Figure 2-15](part0007_split_007.html#latency_heatmap)
    shows the heatmap for the latency of an API endpoint. Most requests are served
    in ~1 ms, but there is a long tail of latencies leading all the way up to >100
    ms in each interval.
  prefs: []
  type: TYPE_NORMAL
- en: '![A heatmap of request latency](../images/00078.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-15\. Latency heatmap
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Histogram data can also be used to perform an approximation for a percentile,
    and this common use of histogram data is reflected in Micrometer’s option to enable
    histograms, `publishPercentileHistogram`. High percentiles are especially useful
    when performing comparative measurements of an application’s performance (such
    as the relative performance of two versions of an application in [“Automated Canary
    Analysis”](part0010_split_013.html#9H5VC-2d714b853a094e9a910510217e0e3d73)). Histograms
    are not enabled by default because of the additional storage cost on the monitoring
    system and heap usage in your application. Micrometer uses a bucketing function
    empirically determined at Netflix to generate reasonably low error bounds for
    these approximations.
  prefs: []
  type: TYPE_NORMAL
- en: Histogram publishing can be enabled for timers in a few ways.
  prefs: []
  type: TYPE_NORMAL
- en: The `Timer` fluent builder supports adding histograms directly as the `Timer`
    is being constructed, as shown in [Example 2-28](part0007_split_007.html#timer_histogram_fluent).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-28\. Adding histograms to a Timer via the builder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Histogram support can be added after the fact via a `MeterFilter`, as shown
    in [Example 2-29](part0007_split_007.html#timer_histogram_meter_filter). This
    ability is crucial in layering your application with effective monitoring. In
    addition to their particular business logic, applications almost always contain
    a rich binary dependency hierarchy as well. It is reasonable for authors of common
    dependencies like HikariCP for connection pooling or the RabbitMQ Java client
    to want to include instrumentation involving timers in their code. But it is impossible
    for the authors of the RabbitMQ Java client to know whether RabbitMQ interactions
    are significant enough in your application to warrant the extra cost of shipping
    distribution statistics like percentile histograms (no matter how optimized they
    may be). Allowing application developers to turn on additional distribution statistics
    via `MeterFilter` allows the RabbitMQ Java client authors to use a minimal timer
    in their code.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-29\. Adding histograms to a Timer via a MeterFilter
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](../images/00112.png)](part0007_split_007.html#co_application_metrics_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Combines percentile histogram publishing with whatever other distribution statistics
    are configured by default (or by other `MeterFilter` configurations).
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, frameworks like Spring Boot offer property-driven `MeterFilter` equivalents
    that allow you to add histograms to `Timers` declaratively. The configuration
    shown in [Example 2-30](part0007_split_007.html#property_driven_histograms) adds
    histogram support to any timer prefixed with the name `requests`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-30\. Adding percentile histograms to metrics prefixed with “requests”
    in Spring Boot
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Histograms are only shipped to monitoring systems that support percentile approximation
    based on histogram data.
  prefs: []
  type: TYPE_NORMAL
- en: For Atlas, use the `:percentiles` function, as in [Example 2-31](part0007_split_007.html#atlas_percentiles).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-31\. Atlas percentiles function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: For Prometheus, use the `histogram_quantile` function, as in [Example 2-32](part0007_split_007.html#prometheus_percentiles).
    Recall from [“Percentiles/Quantiles”](part0007_split_003.html#6LK44-2d714b853a094e9a910510217e0e3d73)
    that percentiles are just a particular type of quantile. Prometheus histograms
    contain a special bucket called `Inf` that captures all samples exceeding the
    greatest bucket that you (or Micrometer) defines. Note that the timer’s count
    is equal to the count in the `Inf` bucket.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-32\. Prometheus histogram quantile function
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Service Level Objective Boundaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to percentiles and histograms, service level objective (SLO) boundaries
    can be added either through the `Timer` fluent builder or through a `MeterFilter`.
    You may notice I said “boundaries” and not “boundary” as you might expect. In
    many circumstances, it’s reasonable to layer your objectives. Gil Tene talks about
    establishing SLO requirements in his 2013 [talk](https://oreil.ly/-LNd6) on monitoring
    latency, which I’ll paraphrase here because this is such a useful framework for
    explaining the need to layer SLOs. The SLO requirements interview is captured
    in [Example 2-33](part0007_split_008.html#slo_requirements_interview).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-33\. The SLO requirements interview
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](../images/00112.png)](part0007_split_008.html#co_application_metrics_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Oh no, you said the word “average"…we’re just going to pretend we didn’t hear
    this.
  prefs: []
  type: TYPE_NORMAL
- en: This interview eventually yields a set of SLOs.
  prefs: []
  type: TYPE_NORMAL
- en: 90% better than 20 milliseconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 99.99% better than 100 milliseconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 100% better than 2 seconds
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will configure Micrometer then to publish SLO counts for 20 milliseconds,
    100 milliseconds, and 2 seconds. And we can simply compare, for example, the ratio
    of requests less than 20 milliseconds to the total number of requests; and if
    this ratio is less than 90%, then alert.
  prefs: []
  type: TYPE_NORMAL
- en: Micrometer will ship a count for each of these boundaries that indicates how
    many requests did not exceed that boundary. A set of SLO boundaries together form
    a coarse histogram, as seen in [Figure 2-16](part0007_split_008.html#slo_histogram),
    where the latency domain is divided into buckets of zero to the lowest SLO and
    so on for consecutive SLO boundaries after that.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0216](../images/00034.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-16\. Histogram of SLO boundaries
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Shipping SLO boundaries does have an effect on total storage in the monitoring
    system and memory consumption in your application. However, because typically
    only a small set of boundaries is published, the cost is relatively low compared
    to percentile histograms.
  prefs: []
  type: TYPE_NORMAL
- en: Percentile histograms and SLOs can be used together. Adding SLO boundaries simply
    adds more buckets than would be shipped with just a percentile histogram. When
    SLO boundaries are shipped in addition to percentile histograms, the histogram
    shipped contains the buckets Micrometer decides are necessary to yield reasonable
    error bounds on percentile approximations *plus* any SLO boundaries, as shown
    in [Figure 2-17](part0007_split_008.html#mixed_percentiles_slos).
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0217](../images/00015.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-17\. Mixed histogram of service level objective boundaries and percentile
    histogram bucket boundaries
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Publishing SLO boundaries is a far cheaper (and accurate) way of testing whether
    the *N*th percentile exceeds a certain value. For example, if you determine that
    an SLO is that 99% of requests are below 100 ms, then publish a 100 ms SLO boundary.
    To set an alert on violations of the SLO boundary, simply determine whether the
    ratio of requests below the boundary to total requests is less than 99%.
  prefs: []
  type: TYPE_NORMAL
- en: This is a little inconvenient when the monitoring system expects normal histograms,
    like Atlas, because you have to select and sum all the buckets *less than* the
    SLO boundary. In [Example 2-34](part0007_split_008.html#slo_boundary_percentile_atlas),
    we want to test whether 99% of requests are less than 100 ms (0.1 seconds); but
    since it isn’t possible to treat tag values as numerical values, we can’t use
    operators like `:le` to select all time series with a tag less than 0.1 seconds.
    So we have to resort to performing numerical comparisons with a regular expression
    operator like `:re`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-34\. Atlas alert criteria using an SLO boundary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Prometheus has the advantage here, given that its histograms are expressed cumulatively.
    That is, all the samples below 100 ms are accumulated to every boundary less than
    100 ms, including this boundary.
  prefs: []
  type: TYPE_NORMAL
- en: The alert criteria is shown in [Example 2-35](part0007_split_008.html#slo_boundary_percentile_prometheus).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-35\. Prometheus alert criteria using an SLO boundary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](../images/00112.png)](part0007_split_008.html#co_application_metrics_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Division symbol
  prefs: []
  type: TYPE_NORMAL
- en: Visually, the effect of these two queries can be seen in [Figure 2-18](part0007_split_008.html#slo_percentile_atlas_vs_prometheus).
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0218](../images/00023.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-18\. SLO boundary alert queries: Atlas versus Prometheus'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: SLO publishing can be enabled for timers in a few ways.
  prefs: []
  type: TYPE_NORMAL
- en: The `Timer` fluent builder supports adding SLOs directly as the `Timer` is being
    constructed, as shown in [Example 2-36](part0007_split_008.html#timer_slo_fluent).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-36\. Adding SLO boundaries to a Timer via the builder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[Example 2-37](part0007_split_008.html#timer_slo_meter_filter) shows how to
    add SLO boundaries with a `MeterFilter`.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-37\. Adding SLO boundaries to a Timer via a MeterFilter
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](../images/00112.png)](part0007_split_008.html#co_application_metrics_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The filter will apply to this timer as it is created.
  prefs: []
  type: TYPE_NORMAL
- en: The next meter type is very similar to a timer.
  prefs: []
  type: TYPE_NORMAL
- en: Distribution Summaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A distribution summary, shown in [Example 2-38](part0007_split_009.html#distribution_summary_create),
    is used to track the distribution of events. It is similar to a timer structurally,
    but it records values that do not represent a unit of time. For example, a distribution
    summary could be used to measure the payload sizes of requests hitting a server.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-38\. Creating a distribution summary
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Micrometer also provides a fluent builder, shown in [Example 2-39](part0007_split_009.html#distribution_summary_fluent_builder)
    for distribution summaries. For maximum portability, add base units, as they are
    part of the naming convention for some monitoring systems. Optionally, you may
    provide a scaling factor that each recorded sample will be multiplied by as it
    is recorded.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-39\. The distribution summary fluent builder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Distribution summaries have all the same percentile, histogram, and SLO options
    that timers do. Timers are just a specialized distribution summary for measuring
    time. SLOs are defined as fixed values instead of durations (i.e., `1000` instead
    of `Duration.ofMillis(1000)`, where 1,000 means something, depending on what base
    unit is assigned to the distribution summary) and provide convenience methods
    for timing blocks of code. That is the only difference in the available options.
  prefs: []
  type: TYPE_NORMAL
- en: A common example of a distribution summary is payload size measured in bytes.
  prefs: []
  type: TYPE_NORMAL
- en: Much like with timers, in many real-world cases the distribution is rarely normal.
    At one point I measured payload size in bytes, which was largely normal except
    there was a sharp drop-off on the left side of the distribution, because I was
    including request headers in the payload size. So there were zero occurrences
    of request payloads less than the size of having a certain set of request headers
    present.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because distribution summaries can track any unit of measure, and the distribution
    of the measured values cannot be generally known as it can be for timers, the
    best way to alert on a distribution summary has some nuance:'
  prefs: []
  type: TYPE_NORMAL
- en: When the distribution is multimodal, as it is for timers, it is likely best
    to set alerts on maximum value so that you can keep track of where the “worst
    case” is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In most cases, it still makes sense to use high percentiles like the 99th percentile
    for comparative analysis (see [“Automated Canary Analysis”](part0010_split_013.html#9H5VC-2d714b853a094e9a910510217e0e3d73)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long Task Timers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The long task timer is a special type of timer that lets you measure time while
    an event being measured is *still running*. A timer does not record the duration
    until the task is complete. Long task timers ship several statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: Active
  prefs: []
  type: TYPE_NORMAL
- en: The number of executions that are currently in progress.
  prefs: []
  type: TYPE_NORMAL
- en: Total duration
  prefs: []
  type: TYPE_NORMAL
- en: The sum of all in-progress execution times of the block of code being measured.
  prefs: []
  type: TYPE_NORMAL
- en: Max
  prefs: []
  type: TYPE_NORMAL
- en: The longest in-progress timing. The max represents the total execution time
    of the oldest still-running execution time.
  prefs: []
  type: TYPE_NORMAL
- en: Histograms
  prefs: []
  type: TYPE_NORMAL
- en: A set of discretized buckets of in-progress tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Percentiles
  prefs: []
  type: TYPE_NORMAL
- en: Precomputed percentiles of in-progress execution times.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-19](part0007_split_010.html#long_task_timer_active_duration) shows
    how a long task timer fundamentally differs from a timer. As soon as an operation
    is complete, it no longer contributes to the total duration. In contrast, an operation
    instrumented with a timer isn’t reported *until* it is complete. Notice how at
    time t=3, total duration increases by 2 rather than just 1\. This is because we
    have two tasks that were executing beginning at t=2, so they each contribute 1
    to the total at each interval while they continue to run. At t=4, both tasks stop,
    and so total duration drops to zero, along with active count.'
  prefs: []
  type: TYPE_NORMAL
- en: Long task timer average has a different meaning than a timer average. It is
    the average of the time active operations that have been running *to this point*.
    Similarly, maximum represents the longest running task to this point, decayed
    in a similar way that timer maximum is.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0219](../images/00005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-19\. Long task timer active and total duration for two tasks
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The `MeterRegistry` interface contains convenience methods for creating long
    task timers, as shown in [Example 2-40](part0007_split_010.html#long_task_timer_create).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-40\. Creating long task timers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The `LongTaskTimer` fluent builder contains more options, shown in [Example 2-41](part0007_split_010.html#long_task_timer_fluent_builder).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-41\. Fluent builder for long task timers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Long task timers have a record method that returns a `Sample` that can later
    be stopped and convenience methods for recording a body of code wrapped in a lambda,
    as shown in [Example 2-42](part0007_split_010.html#long_task_timer_record_execution).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-42\. Recording execution with a long task timer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: A good example of a long task timer is in [Edda](https://oreil.ly/qXUkv), which
    caches AWS resources such as instances, volumes, and autoscaling groups. Normally
    all data can be refreshed in a few minutes. If the AWS services are performing
    more slowly than usual, it can take much longer. A long task timer can be used
    to track the overall time for refreshing the metadata.
  prefs: []
  type: TYPE_NORMAL
- en: In application code, it is common for such long-running processes to be implemented
    with something like Spring Boot’s `@Scheduled`, as shown in [Example 2-43](part0007_split_010.html#explicit_ltt).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-43\. An explicitly recorded long task timer for a scheduled operation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Some frameworks like Spring Boot also respond to the `@Timed` annotation to
    create long task timers when the `longTask` attribute is set to `true`, as in
    [Example 2-44](part0007_split_010.html#annotation_ltt).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-44\. An annotation-based long task timer for a scheduled operation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: If we wanted to alert when this process exceeds our threshold, with a long task
    timer we will receive that alert at the first reporting interval after we have
    exceeded the threshold. With a regular timer, we wouldn’t receive the alert until
    the first reporting interval after the process completed, over an hour later!
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the Right Meter Type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the instrumentation libraries for hierarchical metrics systems (see [“Hierarchical
    Metrics”](part0006_split_003.html#hierarchical_metrics)), typically only gauges
    and counters were supported. This led to a habit of precalculating statistics
    (most commonly rates like request throughput) and presenting them to the monitoring
    system as a gauge that could fluctuate up or down. Since Micrometer always exposes
    counters in a way that lets you derive a rate at query time, there is no need
    to perform this calculation manually in your application code.
  prefs: []
  type: TYPE_NORMAL
- en: Both `Timer` and `DistributionSummary` always publish a count of events in addition
    to other measurements. There should never be a reason to count the number of executions
    of a block of code. They should be timed instead.
  prefs: []
  type: TYPE_NORMAL
- en: Which Meter Type to Choose?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Never gauge something you can count, and never count something you can time.
  prefs: []
  type: TYPE_NORMAL
- en: As a general rule, select a `LongTaskTimer` whenever timings exceed two minutes,
    when you need to monitor in-flight requests, and especially when an operation’s
    failure may inflate the expected time from a few minutes to many minutes or hours.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling Cost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The cost of metrics grows as you instrument more and more pieces of your application.
    Perhaps at the beginning you only start with shipping basic statistics like memory
    and processor utilization, and grow to additional areas like HTTP request monitoring,
    cache performance, database interactions, and connection pool saturation,. In
    fact, many of these basic use cases are going to be increasingly automatically
    instrumented by frameworks like Spring Boot.
  prefs: []
  type: TYPE_NORMAL
- en: Expanding instrumentation to additional components isn’t the likely source of
    high cost in telemetry, however. To examine the cost of an individual metric,
    think about the unique permutations of a metric name and all of its key-value
    tag combinations. These form a set of time series on the backend, and we refer
    to the size of this set as the *cardinality* of the metric.
  prefs: []
  type: TYPE_NORMAL
- en: It is essential that you carefully bound a metric’s cardinality by considering
    all the possible key-value tags combinations that could result in a high number
    of unique values and limit them in some way. The cardinality of a metric is the
    product of the unique tag values for each tag key. The reasonable limit you place
    on tag cardinality varies from monitoring system to monitoring system, but in
    general keeping it in the thousands is a reasonable upper bound. At Netflix, the
    general advice was to keep metric cardinality under one million time series. This
    is probably at the outside edge of what most organizations would find responsible.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, metrics are intended to present an aggregate view of an indicator,
    and they shouldn’t be used to try to examine event-level or request-level performance.
    Other forms of telemetry like distributed tracing or logging are more appropriate
    for individual event-level telemetry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Framework-provided telemetry is generally carefully crafted to limit tag cardinality.
    For example, Spring Boot’s timing of Spring WebMVC and WebFlux adds a handful
    of tags but carefully limits how each contributes to the cardinality of the metric.
    The following list bexplains what each tag key means:'
  prefs: []
  type: TYPE_NORMAL
- en: Method
  prefs: []
  type: TYPE_NORMAL
- en: There are a small number of possible values for HTTP method, based on the HTTP
    specification, e.g., `GET` or `POST`.
  prefs: []
  type: TYPE_NORMAL
- en: Status
  prefs: []
  type: TYPE_NORMAL
- en: 'HTTP status codes come from the HTTP specification, so are naturally limited
    in the possible values. Also, most API endpoints are going to realistically only
    return one of a few values: 200–202 success/created/accepted, 304 not modified,
    400 bad request, 500 internal server error, and maybe 403 forbiddden. Most won’t
    return even this amount of variation.'
  prefs: []
  type: TYPE_NORMAL
- en: Outcome
  prefs: []
  type: TYPE_NORMAL
- en: A summarization of status code, e.g., `SUCCESS`, `CLIENT_ERROR`, or `SERVER_ERROR`.
  prefs: []
  type: TYPE_NORMAL
- en: URI
  prefs: []
  type: TYPE_NORMAL
- en: This tag is a good demonstration of how tag cardinality could quickly get out
    of hand. For 200–400 status codes, the URI is going to be the path that the request
    was mapped to. But when the endpoint contains a path variable or request parameters,
    the framework is careful to use the *unsubstituted* path here rather than the
    raw path of the request, e.g., `/api/customer/{id}` instead of `/api/customer/123`
    and `/api/customer/456`. Not only does this help limit the metric’s cardinality,
    but it is also more useful to reason about the performance of all requests to
    `/api/customer/{id}` in the aggregate as opposed to groups of requests that retrieved
    the particular customer with ID `123` or `456`. In addition to path substitution,
    the URI should be further constrained in cases where the server is ultimately
    going to return a 404\. Otherwise, every mistyped URI `/api/doesntexist/1`, `/api/doesntexistagain`,
    etc., results in a new tag. So Spring Boot uses a URI tag value of `NOT_FOUND`
    whenever the status code is 404\. Similarly, it uses a `REDIRECT` value when the
    status code is 403 because the server may always redirect unauthenticated requests
    to the authentication mechanism, even when the requested path doesn’t exist.
  prefs: []
  type: TYPE_NORMAL
- en: Exception
  prefs: []
  type: TYPE_NORMAL
- en: When the request results in a 500 internal server error, this tag contains the
    exception class name, which is just a simple way of grouping the general classes
    of failures—for example, null pointer exceptions versus a connection timeout on
    a downstream service request. Again, there are generally only a few possible classes
    of failures based on the implementation of the endpoint, so this is naturally
    bounded.
  prefs: []
  type: TYPE_NORMAL
- en: For HTTP server request metrics then, the total cardinality might be something
    like [Equation 2-7](part0007_split_013.html#http_server_request_cardinality).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 2-7\. Cardinality of HTTP server request metric for a single endpoint
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math alttext="2 m e t h o d s times 4 s t a t u s e s times 2 o u t c o m e
    s times 1 upper U upper R upper I times 3 e x c e p t i o n s equals 48 t a g
    v a l u e s" display="block"><mrow><mn>2</mn> <mi>m</mi> <mi>e</mi> <mi>t</mi>
    <mi>h</mi> <mi>o</mi> <mi>d</mi> <mi>s</mi> <mo>×</mo> <mn>4</mn> <mi>s</mi> <mi>t</mi>
    <mi>a</mi> <mi>t</mi> <mi>u</mi> <mi>s</mi> <mi>e</mi> <mi>s</mi> <mo>×</mo> <mn>2</mn>
    <mi>o</mi> <mi>u</mi> <mi>t</mi> <mi>c</mi> <mi>o</mi> <mi>m</mi> <mi>e</mi> <mi>s</mi>
    <mo>×</mo> <mn>1</mn> <mi>U</mi> <mi>R</mi> <mi>I</mi> <mo>×</mo> <mn>3</mn> <mi>e</mi>
    <mi>x</mi> <mi>c</mi> <mi>e</mi> <mi>p</mi> <mi>t</mi> <mi>i</mi> <mi>o</mi> <mi>n</mi>
    <mi>s</mi> <mo>=</mo> <mn>48</mn> <mi>t</mi> <mi>a</mi> <mi>g</mi> <mi>v</mi>
    <mi>a</mi> <mi>l</mi> <mi>u</mi> <mi>e</mi> <mi>s</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Resist the urge to overoptimize for cost. In particular, there is no need to
    try to limit the publication of zero values, because in some application states
    in theory all metrics could be nonzero for a particular publishing interval. It
    is at that moment of greatest saturation where you would be publishing the most
    nonzero metrics that will govern the cost of metrics to you, not the low points
    where you could publish some lesser set of metrics by excluding zero values. Also,
    reporting a zero value is a useful signal that something is just not happening
    as opposed to the service not reporting at all.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, where tag cardinality cannot be limited easily by out-of-the-box
    instrumentation, you will find the configuration of a set of metrics makes you
    define how to limit the tags responsibly. A good example is Micrometer’s instrumentation
    for the Jetty `HttpClient`. A typical request using the Jetty `HttpClient` looks
    like [Example 2-45](part0007_split_013.html#jetty_http_client_call). The `HttpClient`
    API doesn’t provide a mechanism for providing a path variable to the `POST` call
    and a collection of variable values to substitute later, so by the time Micrometer’s
    Jetty `Request.Listener` intercepts the request, path variables have already been
    substituted irreversibly.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-45\. Jersey HTTP client call with path variable string concatenation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Jetty client metrics should use the unsubstituted path for the `uri` tag for
    the same reason the URI tag on Spring Boot’s instrumentation of WebMVC and WebFlux
    was based on an unsubstituted value. It becomes the responsibility of the engineer
    using Jetty `HttpClient` to specify how to `unsubstitute` path variables for the
    purpose of tagging, as shown in [Example 2-46](part0007_split_013.html#jetty_http_client_metrics_config).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-46\. Jersey HTTP client metrics configuration
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Coordinated Omission
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: My first job in high school was as a fast-food worker. Running the drive-through
    window gave me some early firsthand experience about how to lie with statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Our performance as a store was evaluated on the basis of the *average* duration
    from the time a customer placed an order at the menu to when they departed with
    their order. Our store was fairly understaffed typically, so at times the average
    time exceeded our goal. We simply waited until a lull in activity and did laps
    around the building with one of our cars. A couple dozen three-to-four-second
    service times will lower the average quickly (again, average is a problem statistic
    more often than not)! We could make our service time look arbitrarily good. At
    some point, the corporate headquarters added minimum service time to the average
    statistic they were evaluating, and our cheating was over.
  prefs: []
  type: TYPE_NORMAL
- en: In one bizarre case, a bus came through the drive-through and each window of
    the bus ordered. Our service time was only driven by a pressure plate near the
    menu and after the service window, so it was unaware of service time per order,
    only per vehicle. Clearly we didn’t serve the bus order as quickly as we would
    serve a typical car, and the ripple effect of the bus on other service times illustrates
    a concept called *coordinated omission*, in which if we aren’t careful, we monitor
    some definition of service time but exclude wait time. There are two examples
    of coordinated omission with this bus incident, illustrated in [Figure 2-20](part0007_split_014.html#coordinated_omission_bus).
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0220](../images/00011.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-20\. Coordinated omission caused by a bus in the drive-through
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At the point where the bus is at the window receiving its orders, there are
    only three other cars whose service time is being recorded (the three that have
    activated the pressure plate at the menu). The two cars behind the menu aren’t
    being observed by the system. The actual impact of the bus’s obstruction was on
    five other customers, but we will only see a service time impact on three.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose instead of service time being determined from the menu through leaving
    the order window, we instead monitored service time as just the time spent at
    the order window. The average service time then was only affected by how long
    it took to serve the bus alone, and not the compounding effect it had on the cars
    behind it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These effects are similar to the effect that thread pools have on request timing.
    If a response time is calculated based on when a request handler begins processing
    a request and ends when the response is committed, there is no accounting for
    the time that a request sat in a queue waiting for an available request handler
    thread to begin processing it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other consequence of coordinated omission illustrated by this example is
    that blocking the drive-through lane prevents the restaurant from being totally
    overwhelmed by a steady state of customer orders. In fact, the appearance of a
    long line at the drive-through may have discouraged would-be customers from even
    attempting an order. Thread pools can have this effect as well. Coordinated omission
    arises from several sources:'
  prefs: []
  type: TYPE_NORMAL
- en: Serverless functions
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the execution of a serverless function from the perspective of that
    function of course doesn’t record the time required to launch the function.
  prefs: []
  type: TYPE_NORMAL
- en: Pauses
  prefs: []
  type: TYPE_NORMAL
- en: Pauses come in many forms—for example, the JVM pauses due to garbage collection,
    a reindexing database becomes momentarily unresponsive, and cache buffers flush
    to disk. Execution pauses are reported as higher latencies on in-flight timings,
    but operations for which timings haven’t yet started will report unrealistically
    low latencies.
  prefs: []
  type: TYPE_NORMAL
- en: Load testers
  prefs: []
  type: TYPE_NORMAL
- en: Conventional load testers back up in their own thread pools before truly saturating
    a service.
  prefs: []
  type: TYPE_NORMAL
- en: The need for accurate load tests is so common that we’ll go into a little more
    detail about them.
  prefs: []
  type: TYPE_NORMAL
- en: Load Testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some conventional load testers like Apache Bench generate requests at a particular
    rate. Aggregated statistics are generated from the set of all response times.
    When responses don’t fit inside the collection bucket interval, the next request
    will be delayed.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, a service that is becoming oversaturated doesn’t get pushed over
    the edge because the unintentional coordination of a longer response time causes
    the load test to back off. This coordination comes from the fact that these types
    of tests use a blocking request model that effectively limits concurrency to less
    than or equal to the number of cores on the machine running the test.
  prefs: []
  type: TYPE_NORMAL
- en: Real users don’t have this kind of coordination. They interact with your service
    independently of one another, and so can saturate the service to a far greater
    degree. One effective way to simulate user behavior is to saturate your service
    with a nonblocking load test. Gatling and JMeter each act this way (but Apache
    Bench does not). But to illustrate how an effective load test should work, we
    can use Spring’s nonblocking `WebClient` and Project Reactor to create a simple
    nonblocking load test. The result is shown in [Example 2-47](part0007_split_015.html#6LLAA-2d714b853a094e9a910510217e0e3d73).
    It’s in fact so easy to build these nonblocking load tests now that maybe it’s
    not worth the extra cognitive overhead of dedicated tools like JMeter and Gatling.
    You’ll have to decide this for yourself and your team.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-47\. A nonblocking load test with WebClient and Project Reactor
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](../images/00112.png)](part0007_split_015.html#co_application_metrics_CO11-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Configure a registry to ship metrics from the load test’s perspective to a monitoring
    system of choice.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](../images/00059.png)](part0007_split_015.html#co_application_metrics_CO11-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Generate an infinite flux, or if you want to run this for a particular number
    of requests, you can use `Flux.range(0, MAX_REQUESTS)` instead.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](../images/00067.png)](part0007_split_015.html#co_application_metrics_CO11-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Clamp the rate at which you want the test to send requests to the service.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between a reactive load test like this and a conventional load
    test is significant, as seen in [Figure 2-21](part0007_split_015.html#non_blocking_vs_conventional_load_test).
    Conventional load tests (because they are blocking, and therefore can only have
    a concurrency level equal to or less than the number of cores on the machine running
    the test) show a max latency less than 10 ms. A nonblocking reactive load test
    shows a tower of latencies all the way up to greater than 200 ms, with a strong
    band greater than 200 ms as the application becomes saturated.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0221](../images/00069.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-21\. Blocking load test versus a nonblocking (reactive) load test
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The effect (shown in [Figure 2-22](part0007_split_015.html#non_blocking_vs_conventional_load_test_max))
    on the alert criteria suggested in [“Latency”](part0009_split_017.html#8ILV9-2d714b853a094e9a910510217e0e3d73)
    is noticeable, as expected.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0222](../images/00068.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-22\. Difference in max latency
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The effect is also noticeable on the 99th percentile latency indicator, shown
    in [Figure 2-23](part0007_split_015.html#non_blocking_vs_conventional_load_test_p99),
    so it would show up on the key indicator we use to *compare* the response-time
    performance of two versions of the same microservice (see [“Automated Canary Analysis”](part0010_split_013.html#9H5VC-2d714b853a094e9a910510217e0e3d73)).
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0223](../images/00025.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-23\. Difference in 99th percentile
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This example also shows why a server’s view of its own throughput can be misleading.
    Throughput, shown in [Figure 2-24](part0007_split_015.html#non_blocking_vs_conventional_load_test_throughput),
    only increased from approximately 1 ops/second to 3 ops/second between the two
    tests, but this represents how many requests are being *completed*. In fact, during
    the period of the reactive load test where the service was oversaturated, many
    more requests were being queued up on the service’s Tomcat thread pool and not
    being handled in a timely fashion.
  prefs: []
  type: TYPE_NORMAL
- en: If monitoring throughput (and latency) for the sake of alert criteria in production,
    it would be better to monitor both from the perspective of the client when possible,
    as discussed in [“Latency”](part0009_split_017.html#8ILV9-2d714b853a094e9a910510217e0e3d73).
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0224](../images/00036.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-24\. Difference in throughput
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Lastly, average latency is shown in [Figure 2-25](part0007_split_015.html#non_blocking_vs_conventional_load_test_average).
    Even though the average has gone up by an order of magnitude between the two tests,
    average is still around 60 ms, which probably doesn’t seem all that bad. Average
    hides so much of what is really happening that it simply isn’t a useful measure.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0225](../images/00096.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-25\. Difference in average latency
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that we’ve seen many of the meter building blocks and some of the ways they
    are used, let’s turn our focus to application-wide customization of how metrics
    are shipped.
  prefs: []
  type: TYPE_NORMAL
- en: Meter Filters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The more instrumentation that is added to various parts of the Java stack, the
    greater the necessity of somehow controlling which metrics are published and at
    what fidelity. I first became aware of this need chatting with one one of the
    engineers on the Netflix rating team (the ones that control the thumbs up/star
    rating system on the Netflix UI). He showed me how a particular metric instrumented
    in a core platform library included by most user-facing microservices was recording
    roughly 70,000 time series per API endpoint! This was absurdly wasteful, since
    the vast majority of these time series were not used for many product teams in
    dashboards and alerts. Instrumentation had been developed for the highest-possible-fidelity
    use case. This underscores how little choice core library producers really have.
    They need to instrument for the high fidelity case and rely on somebody downstream
    of them to tune down this fidelity to something useful for them. From this experience,
    Micrometer meter filters were born.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each registry can be configured with meter filters, which allow a great degree
    of control over how and when meters are registered and what kinds of statistics
    they emit. Meter filters serve three basic functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Deny* (or accept) meters from being registered.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Transform* meter IDs (e.g., changing the name, adding or removing tags, changing
    description or base units).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Configure* distribution statistics for some meter types.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementations of `MeterFilter` are added to the registry programmatically,
    as in [Example 2-48](part0007_split_016.html#meter_filter_apply).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-48\. Applying meter filters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Meter filters are applied in order, and the results of transforming or configuring
    a meter are chained.
  prefs: []
  type: TYPE_NORMAL
- en: Apply Meter Filters Early in the Application Life Cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For performance reasons, meter filters only influence meters registered *after*
    the filter.
  prefs: []
  type: TYPE_NORMAL
- en: Deny/Accept Meters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The verbose form of an accept/deny filter is shown in [Example 2-49](part0007_split_018.html#verbose_meter_filter).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-49\. Most verbose form of an accept/deny filter
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '`MeterFilterReply` has three possible states:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DENY`'
  prefs: []
  type: TYPE_NORMAL
- en: Do not allow this meter to be registered. When you attempt to register a meter
    against a registry and the filter returns `DENY`, the registry will return a NOOP
    version of that meter (e.g., `NoopCounter`, `NoopTimer`). Your code can continue
    to interact with the NOOP meter, but anything recorded to it is discarded immediately
    with minimal overhead.
  prefs: []
  type: TYPE_NORMAL
- en: '`NEUTRAL`'
  prefs: []
  type: TYPE_NORMAL
- en: If no other meter filter has returned `DENY`, then registration of meters proceeds
    as normal.
  prefs: []
  type: TYPE_NORMAL
- en: '`ACCEPT`'
  prefs: []
  type: TYPE_NORMAL
- en: If a filter returns `ACCEPT`, the meter is immediately registered without interrogating
    any further filters’ accept methods.
  prefs: []
  type: TYPE_NORMAL
- en: '`MeterFilter` provides several convenience static builders for deny/accept
    type filters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`accept()`'
  prefs: []
  type: TYPE_NORMAL
- en: Accept every meter, overriding the decisions of any filters that follow.
  prefs: []
  type: TYPE_NORMAL
- en: '`accept(Predicate<Meter.Id>)`'
  prefs: []
  type: TYPE_NORMAL
- en: Accept any meter matching the predicate.
  prefs: []
  type: TYPE_NORMAL
- en: '`acceptNameStartsWith(String)`'
  prefs: []
  type: TYPE_NORMAL
- en: Accept every meter with a matching prefix.
  prefs: []
  type: TYPE_NORMAL
- en: '`deny()`'
  prefs: []
  type: TYPE_NORMAL
- en: Deny every meter, overriding the decisions of any filters that follow.
  prefs: []
  type: TYPE_NORMAL
- en: '`denyNameStartsWith(String)`'
  prefs: []
  type: TYPE_NORMAL
- en: Deny every meter with a matching prefix. All out-of-the-box `MeterBinder` implementations
    provided by Micrometer have names with common prefixes to allow for easy grouping
    visualization in UIs, but also to make them easy to disable/enable as a group
    with a prefix. For example, you can deny all JVM metrics with `MeterFilter.denyNameStartsWith("jvm")`.
  prefs: []
  type: TYPE_NORMAL
- en: '`deny(Predicate<Meter.Id>)`'
  prefs: []
  type: TYPE_NORMAL
- en: Deny any meter matching the predicate.
  prefs: []
  type: TYPE_NORMAL
- en: '`maximumAllowableMetrics(int)`'
  prefs: []
  type: TYPE_NORMAL
- en: Deny any meter after the registry has reached a certain number of meters.
  prefs: []
  type: TYPE_NORMAL
- en: '`maximumAllowableTags(String meterNamePrefix, String tagKey, int maximumTagValues,
    MeterFilter onMaxReached)`'
  prefs: []
  type: TYPE_NORMAL
- en: Place an upper bound on the number of tags produced by matching series.
  prefs: []
  type: TYPE_NORMAL
- en: '*Allowlisting* only a certain group of metrics is a particularly common case
    for monitoring systems that are *expensive*. This can be achieved with the static:'
  prefs: []
  type: TYPE_NORMAL
- en: '`denyUnless(Predicate<Meter.Id>)`'
  prefs: []
  type: TYPE_NORMAL
- en: Deny all meters that *don’t* match the predicate.
  prefs: []
  type: TYPE_NORMAL
- en: Meter filters are applied in the order in which they are configured on the registry,
    so it is possible to stack deny/accept filters to achieve more complex rules.
    In [Example 2-50](part0007_split_018.html#accept_only_http), we’re explicitly
    accepting any metric prefixed with `http`, and denying everything else. Because
    the first filter gives an accept decision on a meter like `http.server.requests`,
    the universal deny filter is never asked to provide an opinion.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-50\. Accept only HTTP metrics and deny everything else
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Transforming Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Meter filters can also transform a meter’s name, tags, description, and base
    units. One of the most common applications of transforming metrics is to add common
    tags. The author of a common Java library like the RabbitMQ Java client cannot
    possibly guess how you wish to identify RabbitMQ metrics arriving at your monitoring
    system by application, the deployed environment, the instance that the code is
    running on, the version of the application, etc. The possibility of applying common
    tags to all metrics streaming out of an application means that low-level-library
    authors can keep their instrumentation simple, adding only tags related to the
    piece they are instrumenting, e.g., the queue name for RabbitMQ metrics. The application
    developer can then enrich this instrumentation with other identifying information.
  prefs: []
  type: TYPE_NORMAL
- en: A transform filter is shown in [Example 2-51](part0007_split_019.html#transform_meter_filter).
    This filter adds a name prefix and an additional tag conditionally to meters starting
    with the name “test.”
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-51\. Transform meter filter
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '`MeterFilter` provides convenience builders for many common transformation
    cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '`commonTags(Iterable<Tag>)`'
  prefs: []
  type: TYPE_NORMAL
- en: Add a set of tags to all metrics. Adding common tags for app name, host, region,
    etc., is a highly recommended practice.
  prefs: []
  type: TYPE_NORMAL
- en: '`ignoreTags(String...)`'
  prefs: []
  type: TYPE_NORMAL
- en: Drop matching tag keys from every meter. This is particularly useful when a
    tag’s cardinality probably becomes too high and starts stressing your monitoring
    system or costing too much, but you can’t change all the instrumentation points
    quickly.
  prefs: []
  type: TYPE_NORMAL
- en: '`replaceTagValues(String tagKey, Function<String, String> replacement, String...
    exceptions)`'
  prefs: []
  type: TYPE_NORMAL
- en: Replace tag values according to the provided mapping for all matching tag keys.
    This can be used to reduce the total cardinality of a tag by mapping some portion
    of tag values to something else.
  prefs: []
  type: TYPE_NORMAL
- en: '`renameTag(String meterNamePrefix, String fromTagKey, String toTagKey)`'
  prefs: []
  type: TYPE_NORMAL
- en: Rename a tag key for every metric beginning with a given prefix.
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring one or more of the tags mentioned at the beginning of this section
    on Netflix core platform instrumentation, which yielded tens of thousands of tags
    per API endpoint, could have significantly cut down on cost.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Distribution Statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Timer`, `LongTaskTimer`, and `DistributionSummary` contain a set of optional
    distribution statistics in addition to the basics of count, total, and max that
    can be configured through filters. These distribution statistics include precomputed
    [“Percentiles/Quantiles”](part0007_split_003.html#6LK44-2d714b853a094e9a910510217e0e3d73),
    [“Service Level Objective Boundaries”](part0007_split_008.html#6LKH9-2d714b853a094e9a910510217e0e3d73),
    and [“Histograms”](part0007_split_007.html#6LKA8-2d714b853a094e9a910510217e0e3d73).
    Distribution statistics can be configured through a `MeterFilter`, as shown in
    [Example 2-52](part0007_split_020.html#distribution_statistics).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-52\. Configuring distribution statistics
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Generally, you should create a new `DistributionStatisticConfig` with just the
    pieces you wish to configure and then `merge` it with the input configuration.
    This allows you to drop-down on registry-provided defaults for distribution statistics
    and to chain multiple filters together, each of which configures some part of
    the distribution statistics (e.g., maybe you want a 100 ms SLO for all HTTP requests
    but only percentile histograms on a few critical endpoints).
  prefs: []
  type: TYPE_NORMAL
- en: '`MeterFilter` provides convenience builders for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`maxExpected(Duration/long)`'
  prefs: []
  type: TYPE_NORMAL
- en: Governs the upper bound of percentile histogram buckets shipped from a timer
    or summary.
  prefs: []
  type: TYPE_NORMAL
- en: '`minExpected(Duration/long)`'
  prefs: []
  type: TYPE_NORMAL
- en: Governs the lower bound of percentile histogram buckets shipped from a timer
    or summary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spring Boot offers property-based filters for configuring SLOs, percentiles,
    and percentile histograms by name prefix, as shown in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '`management.metrics.distribution.percentiles-histogram`'
  prefs: []
  type: TYPE_NORMAL
- en: Whether to publish a histogram suitable for computing aggregable (across dimension)
    percentile approximations.
  prefs: []
  type: TYPE_NORMAL
- en: '`management.metrics.distribution.minimum-expected-value`'
  prefs: []
  type: TYPE_NORMAL
- en: Publish less histogram buckets by clamping the range of expected values.
  prefs: []
  type: TYPE_NORMAL
- en: '`management.metrics.distribution.maximum-expected-value`'
  prefs: []
  type: TYPE_NORMAL
- en: Publish less histogram buckets by clamping the range of expected values.
  prefs: []
  type: TYPE_NORMAL
- en: '`management.metrics.distribution.percentiles`'
  prefs: []
  type: TYPE_NORMAL
- en: Publish percentile values computed in your application.
  prefs: []
  type: TYPE_NORMAL
- en: '`management.metrics.distribution.sla`'
  prefs: []
  type: TYPE_NORMAL
- en: Publish a cumulative histogram with buckets defined by your SLAs.
  prefs: []
  type: TYPE_NORMAL
- en: Meter filters show up in ways that demonstrate how organizational culture can
    drive even the lowest level of software configurtion. Several organizations use
    them to separate platform and application metrics, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Separating Platform and Application Metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Conway’s Law suggests that “you ship your org chart.” This means roughly that
    the way in which your system is written, deployed, and works bears some resemblance
    to your organization.
  prefs: []
  type: TYPE_NORMAL
- en: I’ve found a common pattern that I think is a good positive illustration of
    this principle. At Netflix, the operations engineering organization (what we are
    calling platform engineering in this book) built tools that solved problems otherwise
    undifferentiated among individual microservices. This organization was very much
    customer-engineer-focused, but it exercised no oversight or control over what
    individual teams did because of the overarching “freedom and responsibility” culture.
    But at many organizations (and I don’t think this is necessarily worse or better),
    platform teams do act centrally on behalf of product teams. So while the monitoring
    of an individual microservice at Netflix was wholly the responsibility of the
    product team (with as much advice and assistance as requested from the central
    team), at many organizations the responsibility for monitoring applications may
    reside wholly with a central platform team.
  prefs: []
  type: TYPE_NORMAL
- en: In other cases, the responsibility is split, with a central platform team responsible
    for monitoring certain types of signals (usually resource metrics like processor,
    memory, and maybe closer-to-business metrics like API error ratio) and application
    teams responsible for any custom indicators. You may notice how the responsibilities
    in this case roughly break down along the lines of what black box and white box
    instrumentation each excel at (with platform teams monitoring black box signals
    and product teams monitoring white box signals). This is a good illustration of
    how agent-based instrumentation shipped to some particular SaaS product may serve
    the needs of the platform team while product teams use an entirely different monitoring
    system for other signals—i.e., how these types of instrumentation can be complementary
    rather than competitive.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider an organization like this, and the impact it has on how metrics
    telemetry is configured in the application. To make this concrete, assume this
    organization is using Prometheus as its monitoring system. A platform engineer
    and a product engineer have different goals in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: Platform engineer
  prefs: []
  type: TYPE_NORMAL
- en: A platform engineer wants to monitor all microservices across the organization
    in the same way. This means that every microservice should publish a set of metrics
    the platform team intends to monitor. These metrics should have a consistent way
    of determining common tags (e.g., for the stack like test/dev/prod, region, cluster,
    server group name, application version, and an instance identifier). There is
    no value to the platform team in shipping anything more than the set of metrics
    the platform team intends to monitor. This set of metrics is unlikely to change
    significantly over time, because by the nature of the responsibility of the platform
    team, they represent general indicators that are useful to all applications and
    aren’t tied to specific business functions or features.
  prefs: []
  type: TYPE_NORMAL
- en: Product engineer
  prefs: []
  type: TYPE_NORMAL
- en: A product engineer wants to monitor their microservice(s) alone. The same set
    of common tags applicable to the platform engineer are likely beneficial to the
    product engineer as well, but there may be additional common tags that further
    differentiate individual instances at a level that isn’t relevant to the platform
    team. For example, the application team may deploy several clusters of its microservice
    that could contain the same application code but serve different segments of its
    end users (e.g., internal users versus external users). They will likely want
    to add a common tag for this distinction to their metrics as well, as there may
    be different SLOs for different end-user populations. The metrics that a product
    engineer should focus most on will be more specific to end-user experience. They
    are also likely to be feature-focused. As new features change, the set of metrics
    changes as well.
  prefs: []
  type: TYPE_NORMAL
- en: If metrics for a microservice were published through a single `MeterRegistry`,
    there is a chance that product engineer customizations to the registry would impact
    the observability of the microservice for the platform team. And, depending on
    how metrics tags are being aggregated for display by the product team, platform
    engineers adding additional common tags could impact the alerts and dashboards
    of product engineers.
  prefs: []
  type: TYPE_NORMAL
- en: Because the engineering organization is structured in this way with this division
    of responsibilities across team boundaries, it should come up with a way to split
    the publishing of metrics into distinct meter registries that best serve the individual
    responsibilities of platform and product teams. “You ship your org chart.”
  prefs: []
  type: TYPE_NORMAL
- en: Since this sort of division of responsibility is fairly common, let’s consider
    how this would be achieved. First, the platform team becomes responsible for shipping
    a common library, a JAR binary dependency that every microservice can include
    that contains this common configuration. Because presumably microservice teams
    will be on different release cycles, the platform team will naturally have to
    evolve this common configuration slowly, relative to the pace of change of any
    individual microservice. In this common platform JAR, we’d expect to see autoconfiguration
    like in [Example 2-53](part0007_split_021.html#6LM0R-2d714b853a094e9a910510217e0e3d73).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-53\. Platform team’s autoconfiguration of metrics shared with product
    teams
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](../images/00112.png)](part0007_split_021.html#co_application_metrics_CO12-1)'
  prefs: []
  type: TYPE_NORMAL
- en: In the platform team’s configuration, a Prometheus meter registry can be created
    privately, without adding it to the Spring application context, so it is not subject
    to `MeterFilter`, `MeterBinder`, and other customizations that the product team
    might configure via the application context.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](../images/00059.png)](part0007_split_021.html#co_application_metrics_CO12-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The metrics that the platform team cares about are added directly to the privately
    configured registry.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](../images/00067.png)](part0007_split_021.html#co_application_metrics_CO12-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The platform team provides a common tag meter filter that works for every microservice
    running in Kubernetes. This practice is a concern cutting all microservice teams,
    and they all benefit from the same set of common tags (though they may add their
    own additionally). The meter filter is also applied right after construction to
    the private platform meter registry. A fallback set of common tags is provided
    for when the application isn’t running in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](../images/00016.png)](part0007_split_021.html#co_application_metrics_CO12-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The platform team sets up an API endpoint for itself, common to every application
    and distinct from the typical `/actuator/prometheus` endpoint that Spring would
    autoconfigure, leaving the actuator endpoint to be under the full control of the
    product team for its own purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of Kubernetes common tags, shown in [Example 2-54](part0007_split_021.html#6LMD0-2d714b853a094e9a910510217e0e3d73),
    is applicable to the kinds of annotations that Spinnaker’s Kubernetes implementation
    configures to be placed on pods. Spinnaker will be discussed in greater detail
    in [Chapter 5](part0010_split_000.html#9H5K4-2d714b853a094e9a910510217e0e3d73).
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-54\. Kubernetes common tags, assuming the service was deployed by
    Spinnaker
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Another use case of meter filters involves adding a layer of resiliency to your
    publication of metrics themselves.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning Metrics by Monitoring System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose your organization has selected Prometheus as its primary monitoring
    system, and a dedicated platform engineering team is operating a series of Prometheus
    instances which should be pulling metrics from every application in the company’s
    inventory. Perhaps this platform team even contributes to monitoring some set
    of common indicators, on behalf of product teams, known to be broadly applicable
    to every Java microservice running in the organization.
  prefs: []
  type: TYPE_NORMAL
- en: How do these engineers prove that the Prometheus instances they think they are
    operating effectively are in fact scraping all the deployed assets successfully?
    There are different failure modes. In one case, Prometheus could simply time out
    attempting to pull metrics from a given application. This would be visible from
    Prometheus’s monitoring of itself. But another failure mode might be that we have
    misconfigured Prometheus so that it isn’t even *attempting* to scrape this application.
    In this case, Prometheus dutifully scrapes all the applications it knows about
    and reports nothing about the applications it doesn’t.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose also your organization is running its infrastructure on AWS. We could
    choose to dual-publish metrics, both to our primary monitoring system Prometheus
    and to AWS Cloudwatch. A little investigation shows that Cloudwatch (like other
    public cloud-provider-hosted monitoring solutions) is fairly expensive, billed
    by the number of time series sent. But we really only want to use Cloudwatch to
    verify that Prometheus is doing what it should.
  prefs: []
  type: TYPE_NORMAL
- en: The overall process looks like [Figure 2-26](part0007_split_022.html#prom_and_cloudwatch).
    The Prometheus team routinely queries the state of the deployed asset inventory
    (maybe through a stateful delivery automation solution as described in [Chapter 5](part0010_split_000.html#9H5K4-2d714b853a094e9a910510217e0e3d73)).
    For each application listed, the team can check for the counter that Micrometer
    maintains of Prometheus’s scrape attempts for the application. An application
    that isn’t being scraped will have a zero or empty counter.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0226](../images/00117.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-26\. Shipping metrics to both Prometheus and Cloudwatch
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Example 2-55](part0007_split_022.html#cloud_watch_meter_registry_customizer)
    shows how we can use an accept and deny `MeterFilter` pair to cost-effectively
    ship to Cloudwatch only the Prometheus metrics that are relevant in helping the
    platform team determine that the Prometheus scrape configuration is working as
    expected. It uses a Spring Boot feature called `MeterRegistryCustomizer` that
    allows us to add filter and other registry customization to specific registry
    types rather than to all of them.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-55\. Using Cloudwatch MeterRegistryCustomizer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: There is one last concept related to the organization of metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Meter Binders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many cases monitoring some subsystem or library involves more than one meter.
    Micrometer provides a simple functional interface called a `MeterBinder` that
    is designed for encapsulating a set of meters together. Spring Boot automatically
    registers the metrics from any `MeterBinder` bean that is configured to the application
    context (i.e., `@Bean MeterBinder ...`). [Example 2-56](part0007_split_023.html#meter_binder)
    illustrates a simple meter binder that encapsulates some metrics around a vehicle
    type.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-56\. Meter binder implementation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: It is best if all metrics registered in a meter binder share some common prefix
    (in this case “vehicle”), especially when the binder is packed up and shipped
    as default configuration across a wide array of applications. Some teams may not
    find the metrics to be useful in their specific case and filter them out to save
    on cost. Having a common prefix makes it easy to apply a deny meter filter by
    the common prefix, as in [Example 2-57](part0007_split_023.html#property_based_deny_filter).
    In this way, you can add and remove metrics from the meter binder over time, and
    the filter logic still has the effect of broadly including or excluding the metrics
    produced by this meter binder.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2-57\. A property-based deny filter for metrics coming from the vehicle
    meter binder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ve learned how to measure various parts of your application
    with dimensional metrics. We haven’t yet discussed specifically how to *use* this
    data. We will eventually be coming back to metrics in [Chapter 4](part0009_split_000.html#8IL24-2d714b853a094e9a910510217e0e3d73),
    where effective indicators for every Java microservice are presented along with
    how to build effective charts and alerts.
  prefs: []
  type: TYPE_NORMAL
- en: The organizational commitment you are signing up for to take advantage of all
    this dimensional metrics data involves the selection of one or more target dimensional
    monitoring systems, either picking a SaaS offering or standing up one of the available
    OSS systems on-prem. The impact on your code is limited. When using modern Java
    web frameworks like Spring Boot, prepackaged instrumentation will provide a great
    deal of detail without having to write any custom instrumentation code. All you
    need to do is add a dependency on the monitoring system implementation of your
    choice and then provide some configuration to ship metrics to it.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll see how metrics instrumentation compares to debuggability
    signals like distributed traces and logs. While reading it, keep in mind that
    the metrics instrumentation we’ve just discussed is designed to provide fixed-cost
    telemetry that helps you to understand what is happening to your system in the
    *aggregate*. These other telemetry sources will provide detailed information about
    what is happening at an individual event (or request) level.
  prefs: []
  type: TYPE_NORMAL
