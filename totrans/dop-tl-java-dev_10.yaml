- en: Chapter 10\. Continuous Deployment Patterns and Antipatterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Stephen Chin
  prefs: []
  type: TYPE_NORMAL
- en: Baruch Sadogursky
  prefs: []
  type: TYPE_NORMAL
- en: Learn from the mistakes of others. You can’t live long enough to make them all
    yourself.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Eleanor Roosevelt
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this chapter, we will give you the patterns for continuous deployment that
    you need to be successful with implementing DevOps best practices in your organization.
    It is important to understand the rationale for continuous updates to be able
    to convince others in your organization about the change needed to improve your
    deployment process.
  prefs: []
  type: TYPE_NORMAL
- en: We will also give you plenty of antipatterns from companies that have failed
    to adopt continuous update best practices. It is good to learn from the failures
    of others, and plenty of recent examples exist in the high technology industry
    of what not to do and the consequences of ignoring best practices.
  prefs: []
  type: TYPE_NORMAL
- en: After completing this chapter, you will be armed with knowledge of seven best
    practices of continuous updates that you can start using today in order to join
    [the top 26% of DevOps “Elite Performers” of the software industry](https://oreil.ly/9MMwZ).
  prefs: []
  type: TYPE_NORMAL
- en: Why Everyone Needs Continuous Updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Continuous updates are no longer an optional part of a software development
    but are a best practice to be adopted by any major project. Planning for continuous
    delivery of updates is just as important as the functional requirements of the
    project and requires a high level of automation to execute on reliably.
  prefs: []
  type: TYPE_NORMAL
- en: It was not always this way. Historically, software was delivered on a much lower
    cadence and received only critical updates. Also, installation of updates was
    often a manual and error-prone process that involved tweaking of scripts, data
    migration, and significant downtime.
  prefs: []
  type: TYPE_NORMAL
- en: This has all changed in the past decade. Now end users expect new features to
    be added constantly, which is driven by their experience with consumer devices
    and continuously updated applications. Also, the business risk associated with
    deferring critical updates is significant, as security researchers constantly
    uncover new exploits that can be used to compromise your system unless it is patched.
    Finally, continually updated software has become a business expectation in the
    cloud age, as the entire infrastructure stack is constantly being updated to improve
    security, often with the requirement that you also update your application.
  prefs: []
  type: TYPE_NORMAL
- en: Not all software projects have been as quick to adopt continuous update strategies,
    especially in industries that are used to longer technology adoption cycles. However,
    the widespread use of common hardware architectures and open source technologies
    means that these projects are at an equal risk of exposure from critical vulnerabilities.
    When exposed, this can lead to catastrophic failures that are difficult or impossible
    to recover from. Like any other software, open source projects have bugs and security
    vulnerabilities, and those are fixed and patched faster than in proprietary projects,
    but if the organization won’t update, what good will the patches do?
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will dig into the motivation for continuous updates
    in more detail. If you do not already have a continuous update strategy, the material
    in this chapter will help you convince others in your organization to adopt one.
    If you have already embraced continuous updates, you will be armed with knowledge
    to reap the business benefits of having infrastructure and DevOps processes superior
    to those of your competitors.
  prefs: []
  type: TYPE_NORMAL
- en: User Expectations on Continuous Updates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The expectations of end users on release cadence of new features has dramatically
    shifted in the last decade. This is driven by a change in the way features and
    updates are delivered on consumer devices, but translates to similar expectations
    on other software platforms, even in the enterprise. Forcing users to wait for
    a long release cycle or to perform a costly migration to take advantage of new
    features will result in dissatisfied users and put you at a competitive disadvantage.
  prefs: []
  type: TYPE_NORMAL
- en: This change in user expectations can be seen in several consumer industries,
    including cell phones. When mobile communication first started to gain popularity,
    Nokia was one of the dominant hardware manufacturers of 2G cell phones. While
    primitive by today’s standards, the phones had excellent hardware design with
    good voice quality, tactile buttons, and rugged design.
  prefs: []
  type: TYPE_NORMAL
- en: Small-form-factor mobile devices such as the Nokia 6110 accelerated adoption
    of cellular technology, but the software on these devices and users’ ability to
    update them were extremely poor. This was a common problem with early consumer
    device companies in that they considered themselves hardware companies first and
    were slow to adopt modern practices in software development.
  prefs: []
  type: TYPE_NORMAL
- en: Like many emerging technologies, the software shipped with Nokia phones was
    bare-bones and buggy, requiring patches and updates to remain usable. While Nokia
    offered a data cable, this was limited to basic operations like transferring contacts
    from the device to a computer, but didn’t allow maintenance features such as performing
    firmware updates. To get a feature update on your phone that contained important
    patches and mission-critical features (like the Snake game), you would need to
    take your phone into a service center to update your device.
  prefs: []
  type: TYPE_NORMAL
- en: It wasn’t until the iPhone came out in 2007 that the phone industry took a software-first
    approach to mobile phone design. With the ability to update the firmware and entire
    operating system from an attached computer and later over-the-air updates, Apple
    could rapidly deploy new features to existing devices.
  prefs: []
  type: TYPE_NORMAL
- en: In 2008, Apple announced the App Store, which created a vibrant app ecosystem
    and laid the foundation for modern store features like security sandboxing and
    automatic application updates, which we will come back to later in this chapter
    with a longer case study. With the release of iOS 5 in 2011, Apple embraced over-the-air
    updates; you no longer even needed a computer to install the latest version of
    the operating system.
  prefs: []
  type: TYPE_NORMAL
- en: Now the process of updating software on your phone is seamless and automated
    to the point where most consumers have no idea which version of the operating
    system or individual applications they are running. As an industry, we have trained
    the general public that continuous updates are not only expected, but required
    for functionality, productivity, and security.
  prefs: []
  type: TYPE_NORMAL
- en: This model of continuous updates has become the norm for consumer devices of
    all types, including smart TVs, home assistants, and even newer self-updating
    routers. While the car industry has been slow to adopt a continuous update strategy,
    Tesla is pushing the industry with biweekly updates to your vehicle right on your
    home network. No longer do you need to drive to a vehicle service center for a
    recall or critical software update.
  prefs: []
  type: TYPE_NORMAL
- en: Security Vulnerabilities Are the New Oil Spills
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Oil spills have had a tremendously detrimental effect on the environment over
    the past 50 years and continue to be an ongoing crisis. When running smoothly,
    oil drilling rigs are immensely profitable, but when accidents or natural disasters
    occur (particularly at sea, where environmental damage is amplified), the cost
    can be enormous. For large companies like BP, which can afford to pay or set aside
    tens of billions of dollars for fines, legal settlements, and cleanups, oil spills
    are just a cost of doing business. However, for drilling operations run by smaller
    companies, a single oil spill can spell financial disaster and put companies out
    of business with no means to address the aftermath.
  prefs: []
  type: TYPE_NORMAL
- en: This was the case for Taylor Energy, which lost an oil platform off the coast
    of Louisiana to Hurricane Ivan in 2004 and was [leaking 300 to 700 barrels per
    day](https://oreil.ly/3LOtN). This disaster continues to haunt Taylor Energy,
    which is both the recipient and instigator of multiple lawsuits surrounding the
    oil spill and ongoing containment efforts. Taylor Energy has already spent $435
    million to reduce the oil leakage in what has become the longest oil spill in
    US history, with the potential to keep leaking for the next century.
  prefs: []
  type: TYPE_NORMAL
- en: This is analogous to the risk that software vulnerabilities pose to the technology
    industry. Software systems have become increasingly complex, which means that
    there are more dependencies on open source software and third-party libraries,
    and it’s a good thing. The problem is, old-school security audit approaches don’t
    work anymore, making it virtually impossible to guarantee that a system is free
    of security vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: According to [the 2021 “Open Source Security and Risk Analysis Report” by Synopsis](https://oreil.ly/TFcnJ),
    open source software is used in 99% of enterprise projects, and 84% of those projects
    contained at least one public vulnerability, with an average of 158 vulnerabilities
    found per codebase.
  prefs: []
  type: TYPE_NORMAL
- en: So how bad are these vulnerabilities that plague commercial codebases? The top
    10 vulnerabilities allow an attacker to obtain sensitive information like authentication
    tokens and user session cookies, execute arbitrary code in the client browser,
    and trigger denial-of-service conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Organizations’ reactions to security vulnerabilities can be organized into
    three discrete steps that have to occur sequentially in order to respond:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Identify: first the organization must realize that a security issue exists
    and is currently or can potentially be exploited by an attacker.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Fix: once a security issue is identified, the development team must come up
    with a software fix to patch the issue.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Deploy: the final step is to deploy the software fix that addresses the security
    issue, often to a large set of end users or target devices that are affected by
    the vulnerability.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Going back to the Taylor Energy oil spill, you can see how difficult these
    steps are in the physical world:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify—six years
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The hurricane occurred in 2004, but it wasn’t until six years later in 2010
    that researchers observed a persistent oil slick at the Taylor site and brought
    it to public attention.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fix—eight years
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Couvillion Group won a bid for a containment system in 2018.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deploy—five months
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In April 2019, the Couvillion Group deployed a shallow 200-ton steel box containment
    system. While not a permanent fix, this containment system has been collecting
    about 1,000 gallons of resalable oil per day and reduced the visible pollutants
    on the ocean surface.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compared to a physical disaster like an oil spill, you would think that security
    vulnerabilities would be relatively easy to identify, fix, and deploy. However,
    as we will see in the following case studies, software vulnerabilities can be
    just as damaging and economically costly, and are by far much more common.
  prefs: []
  type: TYPE_NORMAL
- en: UK hospital ransomware
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s look at another security breach. [In 2017, a worldwide cyberattack](https://oreil.ly/A7sPK)
    was launched that encrypted the hacked computers and required a bitcoin “ransom”
    payment to recover the data. This attack had been exploited via the EternalBlue
    exploit on the Windows Server Message Block (SMB) service that had been previously
    discovered by the US National Security Agency (NSA) and leaked a year prior to
    the attack.
  prefs: []
  type: TYPE_NORMAL
- en: Upon infection, the virus attempted to replicate itself on the network and encrypted
    critical files, preventing their access, presenting a ransom screen. Microsoft
    had released patches for older versions of Windows that were affected by this
    exploit, but many systems were not updated because of poor maintenance or a requirement
    for 24/7 operation.
  prefs: []
  type: TYPE_NORMAL
- en: One organization critically impacted by this ransomware attack was the United
    Kingdom National Health Service (NHS) hospital system. Up to [70,000 devices](https://oreil.ly/J0NLy)
    on its network—including computers, MRI scanners, blood-storage refrigerators,
    and other critical systems—were affected by the virus. This also involved diversion
    of emergency ambulance services to hospitals and at least 139 patients who had
    an urgent referral for cancer that got cancelled.
  prefs: []
  type: TYPE_NORMAL
- en: The WannaCry ransomware attack resulted in an estimated [19,000 cancelled appointments
    and cost approximately £19 million in lost output and £73 million in IT costs](https://oreil.ly/hx7OW)
    to restore systems and data in the weeks after the attack. All of the affected
    systems were running an unpatched or unsupported version of Windows that was susceptible
    to the ransomware. The majority were on Windows 7, but many were also on Windows
    XP, which had been unsupported since 2014—a full three years prior to the attack.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we frame this with our vulnerability mitigation steps, we get the following
    timelines and impacts:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify—one year
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both the existence of the vulnerability and an available patch were available
    for a year preceding the incident. NHS IT staff didn’t realize its existence until
    the attack was launched on the world and affected the NHS.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fix—existing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the fix is simply to upgrade or patch systems with an existing fix, this
    was immediately available by the time the vulnerability was identified.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deploy—multiple years
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While critical systems were brought back online quickly, there were enough affected
    systems that it took several years for the NHS to fully upgrade and patch affected
    systems with multiple failed security audits.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this case, the security breach was at the operating system level. Assuming
    you are following industry best practices and keeping your operating system under
    maintenance and continually patched, you might believe you are safe. But what
    about application-level security vulnerabilities? This is by far the most common
    type of security vulnerability and is equally easy to exploit by an attacker—as
    happened to Equifax.
  prefs: []
  type: TYPE_NORMAL
- en: Equifax security breach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Equifax security breach is a textbook example of an application-level security
    vulnerability causing massive financial damage to a high-tech company. From March
    through July of 2017, hackers had unrestricted access to Equifax’s internal systems
    and were able to extract personal credit information for half of the total US
    population, or 143 million consumers.
  prefs: []
  type: TYPE_NORMAL
- en: This had the potential for massive identity theft, but none of the stolen Equifax
    personal data appeared on the dark web, which is the most direct monetization
    strategy. It is instead believed the data was used for international espionage
    by the Chinese government. In February 2020, four Chinese-backed military hackers
    were indicted in connection with the Equifax security breach.
  prefs: []
  type: TYPE_NORMAL
- en: For a credit agency to have a security vulnerability of this magnitude, the
    damage to its brand and reputation is incalculable. However, it is known that
    Equifax spent $1.4 billion on cleanup costs and an additional $1.38 billion to
    resolve consumer claims. Also, all of the upper executives at Equifax were quickly
    replaced after the incident.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple compounded security vulnerabilities led to this breach. The first and
    most egregious was an unpatched security vulnerability in Apache Struts that allowed
    hackers to gain access to Equifax’s dispute portal. From here, they moved to multiple
    other internal servers to access databases containing information on hundreds
    of millions of people.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second major security vulnerability was an expired public-key certificate
    that impeded the internal system that inspects encrypted traffic exiting the Equifax
    network. The certificate had expired about 10 months before the breach occurred
    and was renewed only on July 29, at which time Equifax became immediately aware
    of the obfuscated payloads being used by the attacker to extricate sensitive data.
    Here’s the Equifax timeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify—five months
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The initial security breach occurred on March 10, and while the attackers did
    not actively start exploiting this security breach until May 13, they had access
    to the system for almost five months before Equifax became aware of the data exfiltration.
    It wasn’t until July 29, when Equifax fixed its traffic-monitoring system, that
    it became aware of the breach.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fix—existing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Apache Struts security vulnerability [(CVE-2017-5638)](https://oreil.ly/FiWeh)
    was published on March 10, 2017 and fixed by Apache Struts 2.3.32 that was released
    four days prior to the CVE disclosure on March 6.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deploy—one day
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The vulnerability was patched on July 30, one day after Equifax became aware
    of the breach.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Equifax breach is particularly scary since it started with a vulnerability
    in a widely used Java library that affects many systems across the web. Even a
    year after the security vulnerability was identified, researchers at the SANS
    Internet Storm Center [found evidence of exploitation attempts](https://oreil.ly/ZCbXe)
    looking for unpatched servers or new deployments that had not been secured. Continuous
    updates can help.
  prefs: []
  type: TYPE_NORMAL
- en: Widespread chipset vulnerabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even if you are keeping up on security vulnerabilities in the application and
    operating system level, another class of vulnerabilities can affect you at the
    chipset and hardware level. The most widespread recent example of this are the
    [Meltdown and Spectre exploits](https://oreil.ly/z6E7i) discovered by Google security
    researchers.
  prefs: []
  type: TYPE_NORMAL
- en: These flaws are so fundamental to the hardware platforms we use to run everything
    from cloud workloads to mobile devices that security researchers called them catastrophic.
    Both exploits take advantage of the same underlying vulnerabilities in how speculative
    execution and caching interact to get access to data that should be protected.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of Meltdown, a malicious program can access data across the machine
    that it should not have access to, including processes with administrative privileges.
    This is an easier attack to exploit since it requires no knowledge of the programs
    you are trying to attack, but also it is easier to patch at the operating system
    level.
  prefs: []
  type: TYPE_NORMAL
- en: Upon the announcement of the Meltdown vulnerability, the latest versions of
    Linux, Windows, and Mac OS X all had security patches to prevent Meltdown from
    being exploited with some performance loss. In October 2018, [Intel announced
    hardware fixes for its newer chips](https://oreil.ly/bvCuh) (including Coffee
    Lake Refresh, Cascade Lake, and Whiskey Lake) that address various variants of
    Meltdown.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, exploiting the Spectre vulnerability requires specific information
    about the process being attacked, making it a more difficult vulnerability to
    leverage. However, it is also much trickier to patch, which means that new exploits
    based on this vulnerability continue to be identified. Also it is more dangerous
    in cloud computing applications that use VMs, since it can be used to induce a
    hypervisor to provide privileged data to a guest operating system running on it.
  prefs: []
  type: TYPE_NORMAL
- en: The result is that Meltdown and particularly Spectre have opened up a new class
    of security vulnerabilities that break the principles of software security. It
    was assumed that if you built a system with the proper security protections in
    place and could fully verify the correctness of the source code and dependent
    libraries, that system should be secure. These exploits break this assumption
    by exposing side-channel attacks hiding in the CPU and underlying hardware that
    require further analysis and software and/or hardware fixes to mitigate.
  prefs: []
  type: TYPE_NORMAL
- en: 'So getting back to our analysis for the general class of chipset side-channel
    attacks, here’s the timeline:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify—as fast as possible
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While there are generalized fixes for Meltdown and Spectre, exploits can occur
    at any time based on the architecture of your application.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fix—as fast as possible
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A software fix for Spectre often involves specially crafted code to avoid either
    accessing or leaking information in misspeculation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Deploy—as fast as possible
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Getting the fix into production quickly is the only way to mitigate damage.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Of these three variables, the one that can most easily be shortened is the deployment
    time. If you do not already have a strategy for continuous updates, creating one
    will hopefully give you the impetus to start planning for faster and more frequent
    deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Users to Update
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now hopefully convinced you that continuous updates are a good thing,
    from a feature/competitive standpoint as well as for security vulnerability mitigation.
    However, even if you deliver frequent updates, will end users accept and install
    them?
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-1](#user_model_for_updates) models the user flow for deciding whether
    to accept or reject an update.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Flow diagram showing user update flow](Images/dtjd_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. User model for update acceptance
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The first question for a user is whether they really want the update based
    on features and/or security fixes. Sometimes the model for update acceptance is
    not a binary decision, because there is a choice to stay on a maintenance line
    with patches for security, but delay major upgrades that provide larger features
    but are riskier. This is the model that Canonical uses for Ubuntu release: long-term
    support (LTS) releases come out once every two years, with public support for
    five years. If you prefer riskier, but more frequent, updates, interim releases
    occur every six months with shorter support periods.'
  prefs: []
  type: TYPE_NORMAL
- en: The second question is, how risky is the update? For security patches or minor
    upgrades, the answer is usually that it is low risk and safe to put in production
    with minimal testing. Typically, these changes are small, specifically designed
    to not touch any external or even internal APIs, and tested to make sure they
    address the security issue and don’t produce undesirable side effects before release.
    Ability to perform local rollbacks (more on them later in the chapter) mitigates
    the risk.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading may also be safe when the party releasing the upgrade verifies that
    it is a safe upgrade, as shown in the third decision box of [Figure 10-1](#user_model_for_updates).
    This is the model for operating system upgrades, such as iOS, where significant
    changes cannot be individually verified to be non-breaking. The OS vendor has
    to spend a significant amount of time testing hardware combinations, working with
    application vendors to fix compatibility issues or helping them upgrade their
    apps, and performing user trials to see what issues happen during an upgrade.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if it is both risky and the party producing the release cannot verify
    the safety, it is up to the recipient of the upgrade to do verification testing.
    Unless it can be fully automated, this is almost always a difficult and costly
    process to undertake. If the upgrade cannot be proven to be safe and bug free,
    the release may get delayed or simply skipped over in the hopes that a later release
    will be more stable.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some real-world use cases and see their continuous update strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case Study: Java Six-Month Release Cadence'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Java has historically had very long release cycles between major versions, averaging
    from one to three years. However, the release frequency has been erratic and often
    delayed, such as for Java 7, which took almost five years to be released. The
    release cadence has continued to decline as the platform has grown, due to several
    factors such as security issues and the difficulty of running and automating acceptance
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with Java 9 in September of 2017, Oracle made the dramatic move to
    a six-month feature release cycle. These releases can contain new features and
    remove deprecated features, but the general pace of innovation was intended to
    stay constant. This means that each subsequent release should contain fewer features
    and less risk, making it easier to adopt. The actual adoption numbers of each
    JDK release are shown in [Figure 10-2](#java_adoption).
  prefs: []
  type: TYPE_NORMAL
- en: Given that 67% of Java developers never made it past Java 8, which came out
    in 2014, the new release model clearly has a problem! However, hidden under the
    data are a few issues.
  prefs: []
  type: TYPE_NORMAL
- en: First, the Java ecosystem can’t handle six-month releases. As we learned in
    [Chapter 6](ch06.xhtml#package_management), virtually all Java projects are dependent
    on a large ecosystem of libraries and dependencies. In order to upgrade to a new
    Java release, all of those dependencies need to be updated and tested against
    the new Java release. For large open source libraries and complex application
    servers, this is almost impossible to accomplish in a six-month time frame.
  prefs: []
  type: TYPE_NORMAL
- en: '![Graph showing percentage adoption of each Java release](Images/dtjd_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. Developer adoption of recent Java releases^([1](ch10.xhtml#idm45310199916160))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To compound this, the OpenJDK support model provides public support for Java
    releases for only six months until the next feature release comes out. Even if
    you could upgrade every six months, you would be left without critical support
    and security patches, as detailed in [Stephen Colebourne’s blog](https://oreil.ly/Axfki).
  prefs: []
  type: TYPE_NORMAL
- en: The only exception to this is LTS releases that start with Java 11 and come
    every three years thereafter. These releases will get security patches and support
    from commercial JDK vendors such as Oracle, Red Hat, Azul, BellSoft, SAP, and
    others. Free distributions like AdoptOpenJDK and Amazon Corretto promise to provide
    Java releases and security patches at no cost. This is why Java 11 is the most
    popular release after Java 8 and none of the other six month releases have gained
    any traction.
  prefs: []
  type: TYPE_NORMAL
- en: However, in comparison to Java 8, Java 11 has not gained significant traction.
    The number of developers using Java 11, roughly two years after its release in
    September 2018, was 25%. In contrast, exactly two years after the release of Java
    8, the adoption was 64%, as shown in [Figure 10-3](#java_8_adoption). This comparison
    is also biased in favor of Java 11, because anyone who adopted Java 9 or 10 would
    likely have upgraded to Java 11, providing three full years of adoption growth.
  prefs: []
  type: TYPE_NORMAL
- en: '![Graph showing growth of Java 8 adoption](Images/dtjd_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. Developer adoption of Java 8 two years after release^([2](ch10.xhtml#idm45310199906848))
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This brings us to the second reason for the poor adoption of Java 9 and beyond,
    which is a poor value/cost trade-off. The main feature of Java 9 was the introduction
    of a new module system. The idea of a modular Java Platform was first suggested
    by [Mark Reinhold back in 2008](https://oreil.ly/22YFR) and took nine years to
    complete in the release of Java 9.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the complexity and disruptiveness of this change, it was delayed
    several times, missing both Java 7 and Java 8 as initial targets. Also Java 9
    was highly controversial on release because it was initially incompatible with
    OSGi, a competing module system released by the Eclipse Foundation targeted at
    enterprise applications.
  prefs: []
  type: TYPE_NORMAL
- en: But perhaps the bigger issue with modularity is that no one really was asking
    for it. Modularity has many benefits, including better library encapsulation,
    easier dependency management, and smaller packaged applications. However, to fully
    realize these benefits, you need to spend a lot of work rewriting your application
    to be fully modularized. Second, you need all of your dependencies to be packaged
    as modules, which has taken a while for open source projects to embrace. Finally,
    the practical benefits for most enterprise applications are small, so even after
    upgrading to a module-enabled release, it is common practice to disable modularity
    and go back to the classpath model of Java 8 and prior. [Figure 10-4](#java_8_user_model_for_updates)
    shows the simplified developer thought process on upgrading to Java 9 and beyond.
  prefs: []
  type: TYPE_NORMAL
- en: '![Flow diagram showing Java developer update flow](Images/dtjd_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. User model for java release acceptance
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Clearly, choosing whether to upgrade comes down to comparing the value for you
    in modularity or other newly introduced features versus the cost to upgrade. And
    the upgrade cost is highly dependent on how difficult it is to test your application
    after upgrading, which takes us to our first continuous update best practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case Study: iOS App Store'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have had a very different update model for content since 1990, with the creation
    of the first web browser called WorldWideWeb by Tim Berners-Lee. Using a client-server
    model, content could be retrieved dynamically and updated continuously. As JavaScript
    and CSS technologies matured, this turned into a viable app delivery platform
    for continuously updated applications.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, while desktop client applications were comparatively complex and
    rich in their user interface, updates were infrequent and manual. This created
    a situation up to the mid-2000s of having to choose between either rich client
    applications that were difficult to update in the field or simpler web applications
    that could be continuously updated to add new features or patch security vulnerabilities.
    If you are a continuous update fan (which you should be by now), you know which
    one wins.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, Apple changed all of this with the App Store on the iPhone in 2008,
    which was a game changer for deploying rich client applications to phones and
    other devices. Here is what App Store offered:'
  prefs: []
  type: TYPE_NORMAL
- en: Update in one click
  prefs: []
  type: TYPE_NORMAL
- en: Updating a desktop application requires quitting the running version, following
    some sort of guided wizard to go through a seemingly dizzying array of choices
    for the common case (e.g., desktop shortcut, start menu, optional packages), and
    often rebooting your computer after installation. Apple simplified this to a single-button
    update, and in the case of many updates, a bulk option to update them all at once.
    The app update downloads, your mobile app quits, and the app is installed, all
    in the background with no user interruption.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is only one version: latest'
  prefs: []
  type: TYPE_NORMAL
- en: Do you know what version of Microsoft Office you are running? Up until 2011,
    when Office 365 was released, you had to, and had likely not upgraded in the past
    three to five years (or more). Apple changed all of this by providing only the
    latest version in the app store so the choice of which version to upgrade to is
    entirely removed. Also, you are not even provided a version number to reference,
    so all you know is that you are on the latest with a few notes from the developer
    on what you are getting. Finally, there is no cost to upgrade once you own an
    app, so the financial disincentive to upgrade that was the norm with paid desktop
    apps was entirely removed.
  prefs: []
  type: TYPE_NORMAL
- en: Security built in
  prefs: []
  type: TYPE_NORMAL
- en: While security vulnerabilities are the number one reason to install a patch,
    security concerns are also the number one reason *not* to upgrade. Being the first
    to upgrade puts you at risk if a vulnerability in the new software is discovered,
    which is why corporate IT policies typically forbid end users from upgrading their
    desktop apps for a certain period of time. However, Apple fixed this by integrating
    a sandboxed model in which the installed applications are limited in their ability
    to access data, contacts, photos, location, camera, and many other features without
    explicitly being given permission. This, combined with the rigorous app review
    process Apple instituted on store submissions for developers, reduced malware
    and app viruses to the point where, generally speaking, security is almost not
    a concern for consumers when upgrading their apps.
  prefs: []
  type: TYPE_NORMAL
- en: The availability of simple upgrades that are low risk makes the decision to
    update simple. Add to this the fact that releases are verified by a trusted authority,
    and users almost always make the decision to upgrade, as shown in [Figure 10-5](#app_store_user_model_for_updates).
  prefs: []
  type: TYPE_NORMAL
- en: '![Flow diagram showing consumer app update flow](Images/dtjd_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. User model for iOS app update acceptance
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Apple App Store model is ubiquitous for not only mobile devices, but also
    desktop application installation. Google offered a similar model with its Android
    operating system in 2008, and both Apple and Microsoft introduced desktop app
    stores in 2011\. Many of these app stores not only make it simple to upgrade to
    the latest version, but also offer an option to automatically upgrade.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, self-updating applications are now the norm on mobile devices and
    have seen a resurgence on desktop computers, thanks to a few basic continuous
    update best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Uptime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the cloud age, one of the most important measures for business success is
    service uptime. Rather than just delivering software, many companies are moving
    to a software-as-a-service (SaaS) model, where they are also responsible for the
    infrastructure the software runs on. Unexpected interruptions of service can be
    extremely costly, both in breach of service-level agreements and in customer satisfaction
    and retention.
  prefs: []
  type: TYPE_NORMAL
- en: While uptime is important for all business-provided internet services, there
    is no place where uptime is more important than in companies that build and support
    the very infrastructure the internet relies upon. Let’s take a deeper look at
    one of the internet giants that runs global infrastructure underlying over 10%
    of the websites in the world and a myriad of applications and services that we
    rely upon on a daily basis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case Study: Cloudflare'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As internet usage has exploded, so has the need for highly reliable, globally
    distributed, and centrally managed infrastructure like content delivery networks
    (CDNs). Cloudflare’s business is providing a highly reliable content delivery
    infrastructure to businesses across the world with the promise that it can deliver
    content faster and more reliably than your own infrastructure or cloud computing
    servers can. This also means that Cloudflare has one job, which is *never* to
    go down.
  prefs: []
  type: TYPE_NORMAL
- en: While Cloudflare has had many production issues over the years involving DNS
    outages, buffer overflow data leaks, and security breaches, as its business has
    grown, the scale of the problem and resulting damage has gone up. Five of these
    outages occurred on a global scale, taking out an increasingly large portion of
    the internet. While many may secretly be happy to have a 30-minute break from
    the internet courtesy of continuous update failures (after which we promptly complain
    about them on Twitter), losing access to hundreds of millions of servers across
    the internet can cause major disruptions for businesses and huge financial loss.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to focus on the three most recent global Cloudflare outages, what
    happened, and how they could have been prevented with continuous update best practices.
  prefs: []
  type: TYPE_NORMAL
- en: 2013 Cloudflare router rule outage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In 2013, Cloudflare had 23 data centers across 14 countries serving 785,000
    websites and over 100 billion page views per month. At 9:47 UTC on March 3, Cloudflare
    had a system-wide outage affecting all of its data centers when it effectively
    dropped off the internet.
  prefs: []
  type: TYPE_NORMAL
- en: After the outage commenced, diagnosing the problem took about 30 minutes, and
    a full hour for all services to be restored at 10:49 UTC. The outage was caused
    by a [bad rule](https://oreil.ly/oQ2LF) that was deployed to the Juniper routers
    that sat on the edge of all its data centers, shown in [Example 10-1](#bad_rule).
    It was intended to prevent an ongoing distributed denial-of-service (DDos) attack
    that had unusually large packets in the range of 99,971 to 99,985 bytes. Technically,
    the packets would have been discarded after hitting the network, since the largest
    allowed packet size was 4,470, but this rule was intended to stop the attack at
    the edge before it impacted other services.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-1\. The rule that caused Cloudflare’s routers to crash
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This rule caused the Juniper edge routers to consume all RAM until they crashed.
    Removing the offending rule fixed the problem, but many routers were in a state
    where they could not be automatically rebooted and required manual power cycling.
  prefs: []
  type: TYPE_NORMAL
- en: While Cloudflare blamed Juniper networks and their FlowSpec system that deploys
    rules across a large cluster of routers, Cloudflare is the company that deployed
    an untested rule to its hardware with no ability to fail over or roll back in
    the case of failure.
  prefs: []
  type: TYPE_NORMAL
- en: 2019 Cloudflare regex outage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By 2019, Cloudflare had grown to host 16 million internet properties, serve
    1 billion IP addresses, and in totality power 10% of the Fortune 1000 companies.
    The company had a very good run of six years with no global outages until 13:42
    UTC on July 2 when Cloudflare-proxied domains started returning 502 Bad Gateway
    errors and remained down for 27 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: This time the root cause was a [bad regular expression (regex)](https://oreil.ly/5Myhx),
    shown in [Example 10-2](#regexp). When this new rule was deployed to the Cloudflare
    web application firewall (WAF), it caused the CPU usage to spike on all cores
    handling HTTP/HTTPS traffic worldwide.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-2\. The regular expression that caused Cloudflare’s outage
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Like any good regular expression, no human is capable of reading and understanding
    the series of unintelligible symbols, and certainly has no chance of verifying
    the correctness visually. In retrospect, it is obvious that the buggy part of
    the regex is `.\\*(?:.*=.\*)`. Since part of this is a noncapturing group, for
    the purposes of this bug, it can be simplified to `.*.\*=.*`. The use of a double,
    non-optional wildcard (``.*``) is known to be a performance issue with regular
    expressions since they must perform backtracking that gets super linearly harder
    as the length of the input to be matched increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the difficulty of manually verifying bugs that get deployed to global
    infrastructure, you would think that Cloudflare would have learned from its 2013
    outage and implemented progressive delivery. In fact, it had since implemented
    a complex progressive delivery system that involved three stages:'
  prefs: []
  type: TYPE_NORMAL
- en: DOG point of presence
  prefs: []
  type: TYPE_NORMAL
- en: The first line of defense on new changes used only by Cloudflare employees.
    Changes get deployed here first so issues can be detected by employees before
    getting into the real world.
  prefs: []
  type: TYPE_NORMAL
- en: PIG point of presence
  prefs: []
  type: TYPE_NORMAL
- en: A Cloudflare environment for a small subset of customer traffic; new code can
    be tested without affecting paying customers.
  prefs: []
  type: TYPE_NORMAL
- en: Canary point of presence
  prefs: []
  type: TYPE_NORMAL
- en: Three global canary environments that get a subset of worldwide traffic as a
    last line of defense before changes go global.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the WAF was primarily used for fast threat response, and as a
    result it bypassed all of these canary environments (as defined in the Canary
    Releases design pattern) and went straight to production. In this case, the regular
    expression was only run through a series of unit tests that did not check for
    CPU exhaustion before it was pushed to production. This particular change was
    not an emergency fix and thus could have done a staged rollout following the preceding
    process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The exact timeline of the problem and subsequent fix was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 13:31—Code check-in of the peer reviewed regex.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 13:37—CI server built the code and ran the tests, which passed. Well, apparently,
    these weren’t great. `¯\_(ツ)_/¯`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 13:42—The erroneous regex was deployed to the WAF in production.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 14:00—The possibility of an attacker was dismissed, and the WAF was identified
    as the root cause.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 14:02—It was decided to go to a global WAF kill.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 14:07—The kill was finally executed after delays accessing the internal systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 14:09—Service was restored for customers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To recap, let’s review the continuous update best practices that may have helped
    Cloudflare avoid another global outage.
  prefs: []
  type: TYPE_NORMAL
- en: 2020 Cloudflare backbone outage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A year after the previous Cloudflare outage, your author, Stephen, was sitting
    down to write about its 2019 outage when two peculiar things happened:'
  prefs: []
  type: TYPE_NORMAL
- en: Around 2:12 P.M. PST (21:12 UTC), the family Discord channel stops, going off
    because of a Cloudflare outage, and I become incredibly productive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A few hours later, all my searches for information on Cloudflare outages start
    turning up information on recent DNS issues instead of the articles from last
    year.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The nice folks at Cloudflare clearly recognized that good case studies come
    in threes and provided another antipattern for this chapter. On July 18, 2020,
    Cloudflare had another production outage for 27 minutes that affected 50% of its
    total network.
  prefs: []
  type: TYPE_NORMAL
- en: This time the issue was with the Cloudflare backbone, which is used to route
    the majority of traffic on its network between major geographies. To understand
    how the backbone works, it helps to understand the topology of the internet. The
    internet is not truly point-to-point but instead relies on a complex network of
    interconnected data centers to transmit information.
  prefs: []
  type: TYPE_NORMAL
- en: Cloudflare runs multiple centers in San Jose, Atlanta, Frankfurt, Paris, São
    Paulo, and other cities worldwide. These data centers are connected by a global
    backbone of direct, high-speed connections that allow them to bypass internet
    congestion and improve the quality of service between major markets.
  prefs: []
  type: TYPE_NORMAL
- en: It is the Cloudflare backbone that was the cause of the outage this time. The
    backbone is designed to be resilient to failures, such as the one that happened
    between Newark and Chicago at 20:25 UTC. However, this outage resulted in increased
    congestion between Atlanta and D.C. The attempted fix was to remove some of the
    traffic from Atlanta by executing the routing change in [Example 10-3](#routing_change).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-3\. The routing change that caused Cloudflare’s network to go down
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This routing change inactivates one line of a term script, shown in [Example 10-4](#term).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-4\. The complete term the change was made on
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The correct change would have been to inactivate the entire term. However, by
    removing the `prefix-list` line, the result was to send this route to all other
    backbone routers. This changed the `local-preference` to 200, which gave Atlanta
    priority over the other routes, which were set to 100\. The result was that rather
    than reducing traffic, Atlanta instead started attracting traffic from across
    the backbone, increasing network congestion to the point where internet service
    was disrupted for half of Cloudflare’s network.
  prefs: []
  type: TYPE_NORMAL
- en: There is a lot to say about configuration changes that can destroy your entire
    business. The core of the problem here is that Cloudflare is not treating the
    configuration of backbone routers as code that is properly peer reviewed, unit
    tested, and canary deployed.
  prefs: []
  type: TYPE_NORMAL
- en: The Hidden Cost of Manual Updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing continuous update best practices is not free, and often it can
    seem more cost-effective to delay automation and continual manual processes. In
    particular, doing automated testing, treating configuration like code, and automating
    deployment are all important, but also costly to implement.
  prefs: []
  type: TYPE_NORMAL
- en: However, what is the hidden cost of *not* automating your deployment? Manual
    deployments are fraught with errors and mistakes that cost time and effort to
    troubleshoot and business loss when they negatively impact customers. What is
    the cost of having production errors that persist for hours as staff is brought
    in to troubleshoot the issue on a live system?
  prefs: []
  type: TYPE_NORMAL
- en: In the case of Knight Capital, where the answer turned out to be $10 million
    per minute of system failure, would you trust manual updates?
  prefs: []
  type: TYPE_NORMAL
- en: 'Case Study: Knight Capital'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Knight Capital is an extreme case of a software bug going undetected, causing
    issues in production, and causing a huge amount of financial loss. However, the
    interesting thing about this bug is that the core issue was mistakes made in the
    deployment process, which was both infrequent and manual. If Knight Capital were
    practicing continuous deployment, it would have avoided a mistake that ended up
    costing it $440 million and control of the company.
  prefs: []
  type: TYPE_NORMAL
- en: Knight Capital Group was a market-making trader specializing in high-volume
    transactions, and throughout 2011 and 2012 its trading in US equity securities
    represented approximately 10% of the market volume. The company had several internal
    systems that handled trade processing, one of which was called Smart Market Access
    Routing System (SMARS). SMARS acted as a broker, taking trading requests from
    other internal systems and executing them in the market.
  prefs: []
  type: TYPE_NORMAL
- en: To support a new Retail Liquidity Program (RLP) that was set to launch on August
    1, 2012, Knight Capital upgraded its SMARS system to add in new trading functionality.
    It decided to reuse the API flag for a deprecated function called Power Peg that
    was meant for internal testing only. This change was thought to have been successfully
    deployed to all eight production servers in the week leading up to the RLP launch.
  prefs: []
  type: TYPE_NORMAL
- en: At 8:01 A.M. EST, the morning of August 1 started with some suspicious, but
    sadly ignored, email warnings about errors on pre-market trading orders that referenced
    SMARS and warned “Power Peg disabled.” Once trading commenced at 9:30 A.M. EST,
    SMARS immediately started executing a large volume of suspicious trades that would
    repeatedly buy high (at offer) and sell low (at bid), immediately losing on the
    spread. Millions of these transactions were being queued at 10 ms intervals, so
    even though the amounts were small (15 cents on every pair of trades), [the losses
    piled up extremely quickly](https://oreil.ly/w1a6K).
  prefs: []
  type: TYPE_NORMAL
- en: In a business where seconds can be costly, minutes can wipe out weeks of earnings,
    and an hour is a lifetime, Knight Capital lacked an emergency response plan. During
    this 45-minute period, it executed 4 million orders for trading 397 million shares
    of 154 stocks. This gave the company a net long position of 3.4 billion and a
    net short position of 3.15 billion. After getting 6 of the 154 stocks reversed
    and selling the remaining positions, it was left with a net loss of around $468
    million. This was a very tough period for Knight Capital.
  prefs: []
  type: TYPE_NORMAL
- en: Backtracking to the root cause of this problem, only seven of the eight production
    servers were correctly upgraded with the new RLP code. The last server had the
    old Power Peg logic enabled on the same API flag, which explains the warning emails
    earlier in the morning. For every request that hit this eighth server, an algorithm
    designed for internal testing was run that executed millions of inefficient trades
    designed to quickly bump up the price of the stock.
  prefs: []
  type: TYPE_NORMAL
- en: However, in troubleshooting this problem, the technical team erroneously thought
    that there was a bug on the newly deployed RLP logic and reverted the code on
    the other seven servers, essentially breaking 100% of the transactions and exacerbating
    the problem.
  prefs: []
  type: TYPE_NORMAL
- en: While Knight Capital did not go entirely bankrupt from this, it had to give
    up 70% of control of the company for a $400 million bailout of the company’s position.
    Before the end of the year, this turned into an acquisition by a competitor, Getco
    LLC, and the resignation of CEO Thomas Joyce.
  prefs: []
  type: TYPE_NORMAL
- en: So, what happened to Knight Capital, and how can you avoid a disaster like this?
    See the next sidebar for some additional continuous update best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous Update Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have seen the dangers of not adopting continuous update best practices
    from a variety of companies in different areas of the technology industry, it
    should be obvious why you should start implementing or continue to improve your
    continuous deployment infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a list of all the continuous update best practices along with
    the case studies that go into them in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Frequent updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The only way to get good at updating is to do it a lot.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case studies: iOS App Store, Knight Capital.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic updates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And if you are updating a lot, automating becomes cheaper and less error prone.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case study: iOS App Store.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The only way to make sure you are deploying quality is to test everything on
    every change.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case studies: Java six-month release cadence, 2020 Cloudflare backbone outage.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Progressive delivery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid catastrophic failures by deploying to a small subset of production with
    a rollback plan.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case studies: 2013 Cloudflare router rule outage, 2019 Cloudflare regex outage.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: State awareness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t assume that code is the only thing that needs to be tested; state exists
    and can wreak havoc in production.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case study: Knight Capital.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Observability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t let your customers be the ones to notify you that you are down!
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case studies: 2019 Cloudflare regex outage, 2020 Cloudflare backbone outage.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Local rollbacks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Edge devices are typically numerous and hard to fix after a bad update hits,
    so always design for a local rollback.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Case study: 2013 Cloudflare router rule outage.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you are armed with knowledge, it is time to start convincing your coworkers
    to adopt best practices today, before you become the next Knight Capital “Knightmare”
    of the high-tech industry. Making headlines is great, but do it as an elite performer
    of the DevOps industry rather than on the front page of the Register. Don’t try
    to boil the ocean, but small continuous improvement initiatives will, eventually,
    get your organization to continuous updates. Good luck!
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch10.xhtml#idm45310199916160-marker)) Brian Vermeer, “JVM Ecosystem Report
    2020,” Snyk, 2020, [*https://oreil.ly/4fN74*](https://oreil.ly/4fN74).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch10.xhtml#idm45310199906848-marker)) Eugen Paraschiv, “Java 8 Adoption
    in March 2016,” last modified March 11, 2022, [*https://oreil.ly/ab5Vv*](https://oreil.ly/ab5Vv).
  prefs: []
  type: TYPE_NORMAL
