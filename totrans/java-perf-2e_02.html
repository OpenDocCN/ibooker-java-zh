<html><head></head><body><section data-pdf-bookmark="Chapter 2. An Approach to Performance Testing" data-type="chapter" epub:type="chapter"><div class="chapter" id="SampleApplications">&#13;
<h1><span class="label">Chapter 2. </span>An Approach to Performance Testing</h1>&#13;
&#13;
&#13;
<p><a data-primary="performance testing" data-type="indexterm" id="ix_ch02-asciidoc0"/>This chapter discusses four principles of getting results from performance testing: test real applications; understand throughput, batching, and response time; understand variability; and test early and often. These principles form the basis of the&#13;
advice given in later chapters. The science of performance engineering&#13;
is covered by these principles. Executing performance tests on applications is&#13;
fine, but without scientific analysis behind those tests, they can too&#13;
often lead to incorrect or incomplete analysis. This chapter covers how to&#13;
make sure that testing produces valid analysis.</p>&#13;
&#13;
<p>Many of the examples given in later&#13;
chapters use a common application that emulates a system of stock prices;&#13;
that application is also outlined in this chapter.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Test a Real Application" data-type="sect1"><div class="sect1" id="idm45775558771832">&#13;
<h1>Test a Real Application</h1>&#13;
&#13;
<p>The first principle is that testing should occur on the actual product&#13;
in the way the product will be used. Roughly speaking, three&#13;
categories of code can be used for performance testing: microbenchmarks, macrobenchmarks, and mesobenchmarks. Each has&#13;
its own advantages and disadvantages. The category that includes the&#13;
actual application will provide the best results.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Microbenchmarks" data-type="sect2"><div class="sect2" id="idm45775558769752">&#13;
<h2>Microbenchmarks</h2>&#13;
&#13;
<p><a data-primary="benchmarks" data-secondary="microbenchmarks" data-type="indexterm" id="ix_ch02-asciidoc1"/><a data-primary="microbenchmarks" data-type="indexterm" id="ix_ch02-asciidoc2"/><a data-primary="performance testing" data-secondary="microbenchmarks" data-type="indexterm" id="ix_ch02-asciidoc3"/>A <em>microbenchmark</em> is a test&#13;
designed to measure a small unit of performance in order to decide&#13;
which of multiple alternate implementations is preferable:&#13;
the overhead in creating&#13;
a thread versus using a thread pool, the time to execute&#13;
one arithmetic algorithm versus an alternate implementation, and so on.</p>&#13;
&#13;
<p>Microbenchmarks may seem like a good idea, but the features of Java that&#13;
make it attractive to developers—namely, just-in-time compilation and&#13;
garbage collection—make it difficult to write microbenchmarks correctly.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Microbenchmarks must use their results" data-type="sect3"><div class="sect3" id="idm45775552147752">&#13;
<h3>Microbenchmarks must use their results</h3>&#13;
&#13;
<p>Microbenchmarks differ from&#13;
regular programs in various ways. First, because Java code is interpreted the first few times&#13;
it is executed, it gets faster the longer it is executed. For this reason,&#13;
all benchmarks (not just microbenchmarks) typically include a warm-up period&#13;
during which the JVM is allowed to compile the code into its optimal state.</p>&#13;
&#13;
<p>That optimal state can include a lot of optimizations. For example, here’s&#13;
a seemingly simple loop to calculate an implementation of a method that&#13;
calculates the 50th <span class="keep-together">&#13;
Fibonacci</span> number:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code> <code class="kt">void</code> <code class="nf">doTest</code><code class="o">()</code> <code class="o">{</code>&#13;
    <code class="c1">// Main Loop</code>&#13;
    <code class="kt">double</code> <code class="n">l</code><code class="o">;</code>&#13;
    <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">nWarmups</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
        <code class="n">l</code> <code class="o">=</code> <code class="n">fibImpl1</code><code class="o">(</code><code class="mi">50</code><code class="o">);</code>&#13;
    <code class="o">}</code>&#13;
    <code class="kt">long</code> <code class="n">then</code> <code class="o">=</code> <code class="n">System</code><code class="o">.</code><code class="na">currentTimeMillis</code><code class="o">();</code>&#13;
    <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">nLoops</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
        <code class="n">l</code> <code class="o">=</code> <code class="n">fibImpl1</code><code class="o">(</code><code class="mi">50</code><code class="o">);</code>&#13;
    <code class="o">}</code>&#13;
    <code class="kt">long</code> <code class="n">now</code> <code class="o">=</code> <code class="n">System</code><code class="o">.</code><code class="na">currentTimeMillis</code><code class="o">();</code>&#13;
    <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="s">"Elapsed time: "</code> <code class="o">+</code> <code class="o">(</code><code class="n">now</code> <code class="o">-</code> <code class="n">then</code><code class="o">));</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>This code wants to measure the time to execute the <code>fibImpl1()</code> method,&#13;
so it warms up the compiler first and then measures the now-compiled method.&#13;
But likely, that time will be 0 (or more likely, the time to run the <code>for</code>&#13;
loop without a body). Since the value of <code>l</code> is not read anywhere, the&#13;
compiler is free to skip its calculation altogether. That depends on what&#13;
else happens in the <code>fibImpl1()</code> method, but if it’s just a simple arithmetic&#13;
operation, it can all be skipped. It’s also possible that only parts of the&#13;
method will be executed, perhaps even producing the incorrect value for <code>l</code>;&#13;
since that value is never read, no one will know.&#13;
(Details of&#13;
how the loop is eliminated are given in <a data-type="xref" href="ch04.html#JustInTimeCompilation">Chapter 4</a>.)</p>&#13;
&#13;
<p>There is a way around that particular issue: ensure that each result&#13;
is read, not simply&#13;
written. In practice, changing the definition of <code>l</code> from a local variable&#13;
to an instance variable (declared with the <code>volatile</code> keyword) will allow&#13;
the performance of the method to be measured. (The&#13;
reason the <code>l</code> instance variable must be declared as&#13;
<span class="keep-together"><code>volatile</code></span> can be&#13;
found in <a data-type="xref" href="ch09.html#ThreadPerformance">Chapter 9</a>.)</p>&#13;
<aside class="less_space pagebreak-before" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775558431832">&#13;
<h5>Threaded Microbenchmarks</h5>&#13;
<p><a data-primary="microbenchmarks" data-type="indexterm" id="idm45775558430248"/><a data-primary="threaded microbenchmarks" data-type="indexterm" id="idm45775558429544"/>The need to use a <code>volatile</code> variable in this example&#13;
applies even when the micro<span class="keep-together">benchmark</span>&#13;
is single-threaded.</p>&#13;
&#13;
<p>Be especially wary when thinking of writing a threaded microbenchmark. When several threads are executing small bits of code,&#13;
the potential for synchronization bottlenecks (and other thread&#13;
artifacts) is quite large. Results from threaded microbenchmarks often&#13;
lead to spending&#13;
a lot of time optimizing away synchronization bottlenecks that will rarely&#13;
appear in real code—at a cost of addressing more pressing performance&#13;
needs.</p>&#13;
&#13;
<p>Consider the case of two threads calling a synchronized method in a&#13;
microbenchmark. Most of it will execute within that synchronized method, because the benchmark code is small. Even if only 50% of the total microbenchmark is within the synchronized method, the odds are high that as few as two threads will attempt to execute the synchronized method at the same time. The benchmark will run slowly as a result, and as more threads are added, the performance issues caused by the increased contention will get even worse. The net is that the test ends up measuring how the JVM handles contention rather than the goal of the microbenchmark.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Microbenchmarks must test a range of input" data-type="sect3"><div class="sect3" id="idm45775558425464">&#13;
<h3>Microbenchmarks must test a range of input</h3>&#13;
&#13;
<p>Even then, potential pitfalls exist. This code performs only one&#13;
operation: calculating the 50th Fibonacci number. A smart compiler&#13;
can figure that out and execute the loop only once—or at least discard&#13;
some iterations of the loop since those operations are redundant.</p>&#13;
&#13;
<p>Additionally, the performance of&#13;
<code class="keep-together">fibImpl1(1000)</code>&#13;
is likely to be very different from the performance of&#13;
<code class="keep-together">fibImpl1(1)</code>;&#13;
if the goal is&#13;
to compare the performance of different implementations, a range of&#13;
input values must be considered.</p>&#13;
&#13;
<p>A range of inputs could be random, like this:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">nLoops</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
    <code class="n">l</code> <code class="o">=</code> <code class="n">fibImpl1</code><code class="o">(</code><code class="n">random</code><code class="o">.</code><code class="na">nextInteger</code><code class="o">());</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>That likely isn’t what we want.&#13;
The time to calculate the random numbers is included in the time&#13;
to execute the loop, so the test now measures the time to calculate&#13;
a Fibonacci sequence&#13;
<code class="keep-together">nLoops</code>&#13;
times, plus the time to generate&#13;
<code class="keep-together">nLoops</code>&#13;
random integers.</p>&#13;
&#13;
<p class="pagebreak-before">It is better to precalculate the input values:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kt">int</code><code class="o">[]</code> <code class="n">input</code> <code class="o">=</code> <code class="k">new</code> <code class="kt">int</code><code class="o">[</code><code class="n">nLoops</code><code class="o">];</code>&#13;
<code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">nLoops</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
    <code class="n">input</code><code class="o">[</code><code class="n">i</code><code class="o">]</code> <code class="o">=</code> <code class="n">random</code><code class="o">.</code><code class="na">nextInt</code><code class="o">();</code>&#13;
<code class="o">}</code>&#13;
<code class="kt">long</code> <code class="n">then</code> <code class="o">=</code> <code class="n">System</code><code class="o">.</code><code class="na">currentTimeMillis</code><code class="o">();</code>&#13;
<code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">nLoops</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
    <code class="k">try</code> <code class="o">{</code>&#13;
        <code class="n">l</code> <code class="o">=</code> <code class="n">fibImpl1</code><code class="o">(</code><code class="n">input</code><code class="o">[</code><code class="n">i</code><code class="o">]);</code>&#13;
    <code class="o">}</code> <code class="k">catch</code> <code class="o">(</code><code class="n">IllegalArgumentException</code> <code class="n">iae</code><code class="o">)</code> <code class="o">{</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code>&#13;
<code class="kt">long</code> <code class="n">now</code> <code class="o">=</code> <code class="n">System</code><code class="o">.</code><code class="na">currentTimeMillis</code><code class="o">();</code></pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Microbenchmarks must measure the correct input" data-type="sect3"><div class="sect3" id="idm45775558392616">&#13;
<h3>Microbenchmarks must measure the correct input</h3>&#13;
&#13;
<p>You probably noticed that now the test has to check for an exception when&#13;
calling the <code>fibImpl1()</code> method: the input range includes negative&#13;
numbers (which have no Fibonacci number) and numbers greater than 1,476&#13;
(which yield a result that cannot be represented as a <code>double</code>).</p>&#13;
&#13;
<p>When that code is used in production, are those likely input values? In&#13;
this example, probably not; in your own benchmarks, your mileage may vary.&#13;
But consider the effect here: let’s say that you are testing two&#13;
implementations of this operation. The first is able to calculate&#13;
a Fibonacci number fairly quickly but doesn’t bother to check its input&#13;
parameter range. The second immediately throws an exception if the input&#13;
parameter is out of range, but then executes a slow, recursive operation to&#13;
calculate the Fibonacci number, like this:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code> <code class="kt">double</code> <code class="nf">fibImplSlow</code><code class="o">(</code><code class="kt">int</code> <code class="n">n</code><code class="o">)</code> <code class="o">{</code>&#13;
    <code class="k">if</code> <code class="o">(</code><code class="n">n</code> <code class="o">&lt;</code> <code class="mi">0</code><code class="o">)</code> <code class="k">throw</code> <code class="k">new</code> <code class="n">IllegalArgumentException</code><code class="o">(</code><code class="s">"Must be &gt; 0"</code><code class="o">);</code>&#13;
    <code class="k">if</code> <code class="o">(</code><code class="n">n</code> <code class="o">&gt;</code> <code class="mi">1476</code><code class="o">)</code> <code class="k">throw</code> <code class="k">new</code> <code class="n">ArithmeticException</code><code class="o">(</code><code class="s">"Must be &lt; 1476"</code><code class="o">);</code>&#13;
    <code class="k">return</code> <code class="nf">recursiveFib</code><code class="o">(</code><code class="n">n</code><code class="o">);</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>Comparing this implementation to the original implementation over a wide&#13;
range of input values will show this new implementation is much faster than&#13;
the original one—simply because of the range checks at the&#13;
beginning of the method.</p>&#13;
&#13;
<p>If, in the real world, users are always going to pass values less than&#13;
100 to the method, that comparison will give us the wrong answer.&#13;
In the common case, the&#13;
<code class="keep-together">fibImpl1()</code>&#13;
method will be faster, and as <a data-type="xref" href="ch01.html#Introduction">Chapter 1</a> explained, we should optimize&#13;
for the common case. (This is obviously a contrived example, and&#13;
simply adding&#13;
a bounds test to the original implementation makes it a better implementation&#13;
anyway. In the general case, that may not be possible.)</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Microbenchmark code may behave differently in production" data-type="sect3"><div class="sect3" id="idm45775552490728">&#13;
<h3>Microbenchmark code may behave differently in production</h3>&#13;
&#13;
<p>So far, the issues we’ve looked at can be overcome by carefully writing&#13;
our microbenchmark. Other things will affect the end result&#13;
of the code after it is incorporated into a larger program.&#13;
The compiler uses profile&#13;
feedback of code to determine the best optimizations to employ when compiling&#13;
a method. The profile feedback is based on which methods are frequently called,&#13;
the stack depth when they are called, the actual type (including subclasses)&#13;
of their arguments, and so on—it is dependent on the environment in which&#13;
the code actually runs.</p>&#13;
&#13;
<p>Hence, the compiler will frequently optimize&#13;
code differently in a microbenchmark than it optimizes that same code when&#13;
used in a larger application.</p>&#13;
&#13;
<p>Microbenchmarks may also exhibit very different behavior in terms of&#13;
garbage collection. Consider two microbenchmark implementations: the first one&#13;
produces fast results but also produces many&#13;
short-lived objects. The second is slightly slower but produces fewer&#13;
short-lived objects.</p>&#13;
&#13;
<p>When we run a small program to test these, the first will likely be faster.&#13;
Even though it will trigger more garbage collections, they will be quick to&#13;
discard the short-lived objects in collections of the young generation, and&#13;
the faster overall time will favor that&#13;
implementation. When we run this code in a server with multiple&#13;
threads executing simultaneously, the GC profile will look different: the&#13;
multiple threads will fill up the young generation faster.&#13;
Hence, many&#13;
of the short-lived objects that were quickly discarded in the case of the&#13;
microbenchmark may end up getting promoted into the old generation when used&#13;
in the multithreaded server environment. This, in turn, will lead to frequent&#13;
(and expensive) full GCs.&#13;
In that case, the long times spent in the&#13;
full GCs will make the first implementation perform worse than the&#13;
second, “slower” implementation that produces less garbage.</p>&#13;
&#13;
<p>Finally, there is the issue of what the microbenchmark actually means.&#13;
The overall time difference in a benchmark such as the one discussed here&#13;
may be measured in seconds for many loops, but the per iteration&#13;
difference is often measured in nanoseconds. Yes, nanoseconds add up, and&#13;
“death by 1,000 cuts” is a frequent performance issue. But particularly in&#13;
regression testing, consider whether tracking something at the nanosecond&#13;
level makes sense. It may be important to save a few nanoseconds&#13;
on each access to a collection that will be accessed millions of times&#13;
(for example,&#13;
see <a data-type="xref" href="ch12.html#Misc">Chapter 12</a>). For&#13;
an operation that occurs less frequently—for example, maybe once per request for&#13;
a REST call—fixing a nanosecond regression found by a microbenchmark&#13;
will take away&#13;
time that could be more profitably spent on optimizing other operations.</p>&#13;
&#13;
<p>Still, for all their pitfalls, microbenchmarks are popular enough that&#13;
the OpenJDK has a core framework to develop microbenchmarks: the Java&#13;
Microbenchmark Harness (<code>jmh</code>). <code>jmh</code> is used by the JDK developers to build&#13;
regression tests for the JDK itself, as well as providing a framework for the&#13;
development of general benchmarks. We’ll discuss <code>jmh</code> in more detail in the&#13;
next section.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775552481720">&#13;
<h5>What About a Warm-Up Period?</h5>&#13;
<p><a data-primary="warm-up periods" data-secondary="microbenchmarks and" data-type="indexterm" id="idm45775552480552"/>One of the performance characteristics of Java is that code performs better&#13;
the more it is executed, a topic that is covered in <a data-type="xref" href="ch04.html#JustInTimeCompilation">Chapter 4</a>.&#13;
For that reason, microbenchmarks must include a warm-up period,&#13;
which gives the compiler a chance to produce optimal code.</p>&#13;
&#13;
<p>The advantages of a warm-up period are discussed in depth&#13;
later in this chapter. For microbenchmarks, a warm-up period is required;&#13;
otherwise, the microbenchmark is measuring the performance of compilation&#13;
rather than the code it is attempting to measure.<a data-startref="ix_ch02-asciidoc3" data-type="indexterm" id="idm45775552477704"/><a data-startref="ix_ch02-asciidoc2" data-type="indexterm" id="idm45775552477000"/><a data-startref="ix_ch02-asciidoc1" data-type="indexterm" id="idm45775552476328"/></p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Macrobenchmarks" data-type="sect2"><div class="sect2" id="idm45775552475400">&#13;
<h2>Macrobenchmarks</h2>&#13;
&#13;
<p><a data-primary="benchmarks" data-secondary="macrobenchmarks" data-type="indexterm" id="ix_ch02-asciidoc4"/><a data-primary="macrobenchmarks" data-type="indexterm" id="ix_ch02-asciidoc5"/><a data-primary="performance testing" data-secondary="macrobenchmarks" data-type="indexterm" id="ix_ch02-asciidoc6"/>The best thing to use to measure performance of an application is the&#13;
application itself, in conjunction with any external resources it uses.&#13;
This is a <em>macrobenchmark</em>. If the application normally checks the credentials of a user by making&#13;
calls to a directory service (e.g., via Lightweight Directory Access Protocol, or LDAP), it should be tested in that mode. Stubbing out the LDAP calls&#13;
may make sense for module-level testing, but the application&#13;
must be tested in its full configuration.</p>&#13;
&#13;
<p>As applications grow, this maxim becomes both more important to fulfill&#13;
and more difficult to achieve. Complex systems are more than the sum of their&#13;
parts; they will behave quite differently when those parts are assembled.&#13;
Mocking out database calls, for example, may mean that you no longer have&#13;
to worry about the database performance—and hey, you’re a Java person;&#13;
why should you have to deal with the DBA’s performance problem? But&#13;
database connections consume lots of heap space for their buffers;&#13;
networks become saturated when more data is sent over them; code is optimized&#13;
differently when it calls a simpler set of methods (as opposed to&#13;
the complex code in a JDBC driver); CPUs pipeline and cache shorter code&#13;
paths more efficiently than longer code paths; and so on.</p>&#13;
&#13;
<p>The other reason to test the full application is one of resource allocation.&#13;
In a perfect world, there would be enough time to optimize every line of&#13;
code in the application. In the real world, deadlines loom, and optimizing&#13;
only one part of a complex environment may not yield immediate benefits.</p>&#13;
&#13;
<p>Consider the data flow shown in <a data-type="xref" href="#FigureBenchmark">Figure 2-1</a>. Data comes in from&#13;
a user, a proprietary business calculation is made, data based on&#13;
that is loaded from the database, more proprietary calculations are made,&#13;
changed data is stored back to the database, and an answer is sent back&#13;
to the user. The number in each box is the number of&#13;
requests per second (RPS) that the module can process when&#13;
tested in isolation.</p>&#13;
&#13;
<p>From a business perspective, the proprietary calculations are the&#13;
most important thing; they are the reason the program exists, and the reason we are paid. Yet making them 100% faster will yield absolutely no benefit in this example. Any application (including a single, standalone JVM) can be modeled as a series of steps like this, where&#13;
data flows out of a box (module, subsystem, etc.) at a rate determined by the efficiency of that box. Data flows into a subsystem at a rate determined by the output rate of the previous box.</p>&#13;
&#13;
<figure><div class="figure" id="FigureBenchmark">&#13;
<img alt="jp2e 0201" src="assets/jp2e_0201.png"/>&#13;
<h6><span class="label">Figure 2-1. </span>Typical program flow</h6>&#13;
</div></figure>&#13;
&#13;
<p>Assume that an algorithmic improvement is made to the business&#13;
calculation so&#13;
that it&#13;
can process 200 RPS; the load injected into the system is correspondingly&#13;
increased. The LDAP system can handle the increased load: so far,&#13;
so good, and 200 RPS will flow into the calculation module, which will&#13;
output 200 RPS.</p>&#13;
&#13;
<p>But the data loading can still process only 100 RPS. Even though 200 RPS&#13;
flow into the database, only 100 RPS flow out of it and into the other&#13;
modules. The total throughput of the system is still only 100 RPS, even though&#13;
the efficiency of the business logic has doubled. Further attempts to&#13;
improve the&#13;
business logic will prove futile until time is spent improving other&#13;
aspects of the environment.</p>&#13;
&#13;
<p>The time spent optimizing the calculations in this example isn’t entirely wasted: once effort is put into the bottlenecks elsewhere in&#13;
the system, the performance benefit will finally be apparent. Rather, it is a matter of priorities: without testing the entire application, it is impossible to tell where spending time on performance work will pay off.<a data-startref="ix_ch02-asciidoc6" data-type="indexterm" id="idm45775552541640"/><a data-startref="ix_ch02-asciidoc5" data-type="indexterm" id="idm45775552540936"/><a data-startref="ix_ch02-asciidoc4" data-type="indexterm" id="idm45775552540264"/></p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775552539464">&#13;
<h5>Full-System Testing with Multiple JVMs</h5>&#13;
<p><a data-primary="Java Virtual Machine (JVM)" data-secondary="full system testing with multiple JVMs" data-type="indexterm" id="idm45775552538328"/><a data-primary="performance testing" data-secondary="full system testing with multiple JVMs" data-type="indexterm" id="idm45775552537368"/>One particularly important case of testing a full application occurs when&#13;
multiple applications are run at the same time on the same&#13;
hardware. Many aspects of the JVM are tuned by default to assume that all&#13;
machine resources are available to them, and if those JVMs are tested in&#13;
isolation, they will behave well. If they are tested when other applications&#13;
are present&#13;
(including, but not limited to, other JVMs), their performance&#13;
will be quite different.</p>&#13;
&#13;
<p>Examples of this are given in later chapters, but here is one quick preview:&#13;
when executing a GC cycle, one JVM will (in its default&#13;
configuration) drive the&#13;
CPU usage on a machine to 100% of all processors. If CPU is measured as an&#13;
average during the program’s execution, the usage may average out to 40%—but that really means that the CPU is 30% busy at some times and&#13;
100% busy at other times. When the JVM is run in isolation, that may be fine,&#13;
but if the JVM is running concurrently with other applications, it will not&#13;
be able to get 100% of the machine’s CPU during GC. Its&#13;
performance will be&#13;
measurably different than when it was run by itself.</p>&#13;
&#13;
<p>This is another reason why microbenchmarks and module-level benchmarks cannot&#13;
necessarily give you the full picture of an application’s performance.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Mesobenchmarks" data-type="sect2"><div class="sect2" id="idm45775552474808">&#13;
<h2>Mesobenchmarks</h2>&#13;
&#13;
<p><a data-primary="benchmarks" data-secondary="mesobenchmarks" data-type="indexterm" id="idm45775552450408"/><a data-primary="mesobenchmarks" data-type="indexterm" id="idm45775552449432"/><a data-primary="performance testing" data-secondary="mesobenchmarks" data-type="indexterm" id="idm45775552448760"/><em>Mesobenchmarks</em> are tests that occupy a middle ground between a microbenchmark&#13;
and a full application.&#13;
I work with developers on the performance of both Java SE and large&#13;
Java applications, and each group&#13;
has a set of tests they characterize as microbenchmarks. To a Java SE&#13;
engineer, that term connotes an example even smaller than that in the first section:&#13;
the measurement of something quite small. Application developers tend to use&#13;
that term to apply to something else: benchmarks that measure one aspect of&#13;
performance but that still execute a lot of code.</p>&#13;
&#13;
<p>An example of an application microbenchmark might be something that measures&#13;
how quickly the response from a simple REST call can be returned from a&#13;
server. The code for such a request is substantial&#13;
compared to a traditional micro<span class="keep-together">benchmark</span>: there is a lot of socket-management&#13;
code, code to read the request,&#13;
code to write the answer, and so on. From a traditional standpoint,&#13;
this is not microbenchmarking.</p>&#13;
&#13;
<p>This kind of test is not a macrobenchmark either: there is no&#13;
security (e.g., the user does not log in to the application), no session&#13;
management, and no use of a host of other application features. Because it is&#13;
only a subset of an actual application, it falls somewhere in the middle—it is a mesobenchmark.&#13;
That is the term I use for benchmarks that do some real work&#13;
but are not full-fledged <span class="keep-together">applications</span>.</p>&#13;
&#13;
<p>Mesobenchmarks have fewer pitfalls than microbenchmarks and are easier to&#13;
work with than macrobenchmarks. Mesobenchmarks likely won’t contain a large amount of dead code that can be&#13;
optimized away by the compiler (unless that dead code exists in the&#13;
application, in which case optimizing it away is a good thing). Mesobenchmarks&#13;
are more easily threaded: they are still more likely to encounter&#13;
more synchronization bottlenecks than the code will encounter when run in a&#13;
full application, but those&#13;
bottlenecks are something the real application will eventually encounter on&#13;
larger hardware systems under larger load.</p>&#13;
&#13;
<p>Still, mesobenchmarks are not perfect. A developer who&#13;
uses a benchmark like this to compare the performance of two application&#13;
servers may be easily led astray. <a data-primary="REST servers" data-secondary="mesobenchmarks and" data-type="indexterm" id="idm45775552442136"/>Consider the hypothetical response times&#13;
of two REST servers shown in <a data-type="xref" href="#TableMesoBenchmark">Table 2-1</a>.</p>&#13;
<table id="TableMesoBenchmark">&#13;
<caption><span class="label">Table 2-1. </span>Hypothetical response times for two REST servers</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Test</th>&#13;
<th>Server 1</th>&#13;
<th>Server 2</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Simple REST call</p></td>&#13;
<td><p>19 ± 2.1 ms</p></td>&#13;
<td><p>50 ± 2.3 ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>REST call with authorization</p></td>&#13;
<td><p>75 ± 3.4 ms</p></td>&#13;
<td><p>50 ± 3.1 ms</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>The developer who uses only a simple REST call to compare the performance of&#13;
the two servers might not realize that server 2 is automatically performing&#13;
authorization for each request. They may then conclude that server 1 will provide the fastest performance. Yet if&#13;
their application always needs authorization (which is typical), they will have&#13;
made the incorrect choice, since it takes server 1 much longer to perform&#13;
that authorization.</p>&#13;
&#13;
<p>Even so, mesobenchmarks offer a reasonable alternative to testing a full-scale&#13;
application; their performance characteristics are much more closely aligned&#13;
to an actual application than are the performance characteristics of&#13;
microbenchmarks. And there is, of course, a continuum here. A later section&#13;
in this chapter presents the outline of a common application used for many of&#13;
the examples in subsequent chapters. That application has a server mode (for&#13;
both REST and Jakarta Enterprise Edition servers), but&#13;
those modes don’t use server facilities like authentication,&#13;
 and though it can access an enterprise&#13;
resource (i.e., a database), in most examples it just makes up random data&#13;
in place of database calls.&#13;
In batch mode, it mimics some actual (but quick) calculations: for example, no GUI or user interaction occurs.</p>&#13;
&#13;
<p>Mesobenchmarks are also good for automated testing, particularly at the&#13;
module level.</p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Good microbenchmarks are hard to write without an appropriate framework.</p>&#13;
</li>&#13;
<li>&#13;
<p>Testing an entire application is the only way to know how code will actually run.</p>&#13;
</li>&#13;
<li>&#13;
<p>Isolating performance at a modular or operational level—via a mesobenchmark—offers a reasonable approach but is no substitute for testing the full application.</p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Understand Throughput, Batching, and Response Time" data-type="sect1"><div class="sect1" id="idm45775552533496">&#13;
<h1>Understand Throughput, Batching, and Response Time</h1>&#13;
&#13;
<p>The second principle is to understand and select the appropriate test metric for the application. Performance can be measured as throughput (RPS), elapsed time (batch time), or response time, and these three metrics are interrelated. Understanding those relationships allows you to focus on the correct metric, depending on the goal of the application.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Elapsed Time (Batch) Measurements" data-type="sect2"><div class="sect2" id="idm45775552423208">&#13;
<h2>Elapsed Time (Batch) Measurements</h2>&#13;
&#13;
<p><a data-primary="elapsed time (batch) measurements" data-type="indexterm" id="idm45775552421800"/><a data-primary="performance testing" data-secondary="elapsed time (batch) measurements" data-type="indexterm" id="idm45775552421080"/>The simplest way to measure performance is to see how long it takes to&#13;
accomplish a certain task. We might, for example, want to retrieve the history of&#13;
10,000 stocks for a 25-year period and calculate the standard deviation of&#13;
those prices, produce a report of the payroll benefits for the&#13;
50,000 employees of a corporation, or execute a loop 1,000,000 times.</p>&#13;
&#13;
<p>In statically compiled languages, this testing is straightforward: the&#13;
application is&#13;
written, and the time of its execution is measured. The Java world&#13;
adds a wrinkle to this: just-in-time compilation. That process is described&#13;
in <a data-type="xref" href="ch04.html#JustInTimeCompilation">Chapter 4</a>; essentially it means that it takes&#13;
anywhere from a few seconds to a few minutes (or longer) for the code&#13;
to be fully optimized and operate at&#13;
peak performance. For that (and other) reasons, performance studies of&#13;
 Java are concerned about warm-up periods: performance is most often&#13;
measured after the code in question has been executed long enough for it&#13;
to have been compiled and optimized.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775552417592">&#13;
<h5>Other Factors for a Warm Application</h5>&#13;
<p><a data-primary="warm-up periods" data-secondary="caching and" data-type="indexterm" id="idm45775552416456"/>Warming up an application is most often discussed in terms of waiting for&#13;
the compiler to optimize the code in question, but other factors&#13;
can affect the performance of code based on how long it has run.</p>&#13;
&#13;
<p>The Java (or Jakarta) Persistence API (JPA), for example, will typically cache data it has read from the database&#13;
(see <a data-type="xref" href="ch11.html#Database">Chapter 11</a>); the second time that data is used, the operation will often&#13;
be faster because the data can be obtained from the cache rather than requiring&#13;
a trip to the database. Similarly, when an application reads a file, the&#13;
operating system typically pages that file into memory. A test that&#13;
subsequently reads the same file (e.g., in a loop to measure performance)&#13;
will run faster the second time, since the data already resides in the&#13;
computer’s main memory and needn’t be read from disk.</p>&#13;
&#13;
<p>In general, there can be many places—not all of them obvious—where&#13;
data is cached and where a warm-up period matters.</p>&#13;
</div></aside>&#13;
&#13;
<p>On the other hand, in many cases the performance of the application from&#13;
start to finish is what matters. A report generator that processes ten thousand&#13;
data elements will complete in a certain amount of time; to the end user,&#13;
it doesn’t matter if the first five thousand elements are processed 50% more slowly&#13;
than&#13;
the last five thousand elements. And even in something like a REST server—where the server’s performance will certainly improve over time—the&#13;
initial performance matters. It will take some time for a&#13;
server to reach peak performance; to the users&#13;
accessing the application during that time, the&#13;
performance during the warm-up period does matter.</p>&#13;
&#13;
<p>For those reasons, many examples in this book are batch-oriented (even if&#13;
that is a little uncommon).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Throughput Measurements" data-type="sect2"><div class="sect2" id="idm45775552410872">&#13;
<h2>Throughput Measurements</h2>&#13;
&#13;
<p><a data-primary="performance testing" data-secondary="throughput measurements" data-type="indexterm" id="idm45775552409592"/><a data-primary="throughput measurements" data-type="indexterm" id="idm45775552408616"/>A <em>throughput measurement</em> is based on the amount of work that can be&#13;
accomplished in a certain period of time. Although the most common examples&#13;
of throughput measurements involve a server processing data fed by a client,&#13;
that is not strictly necessary: a single, standalone application can measure&#13;
throughput just as easily as it measures elapsed time.</p>&#13;
&#13;
<p>In a client/server test, a throughput measurement means that clients have&#13;
no think time. If there is a single client, that client sends a request&#13;
to the server. When the client receives a response, it immediately sends a new request.&#13;
That process continues; at the end of the test, the client reports the total number of&#13;
operations it achieved. Typically, the client has multiple threads doing&#13;
the same thing, and the throughput is the aggregate measure of the number&#13;
of operations all clients achieved. Usually, this number is reported as the&#13;
number of operations per second, rather than the total number of operations&#13;
over the measurement period. This measurement is frequently referred to as&#13;
<em>transactions per second</em> (TPS), <em>requests per second</em> (RPS), or <em>operations per&#13;
second</em> (OPS).</p>&#13;
&#13;
<p>The configuration of the client in client/server tests is important; you&#13;
need to ensure that the client can send data&#13;
quickly enough to the server. This may not occur because there aren’t enough CPU&#13;
cycles on the client machine to run the desired number of client thread,&#13;
or because the client has to spend a lot of time processing the request before&#13;
it can send a new request.  In those cases, the test is effectively&#13;
measuring the client performance rather than the server performance, which&#13;
is usually not the goal.</p>&#13;
&#13;
<p>This risk depends on the amount of work that each&#13;
client thread performs (and the size and configuration of the client machine).&#13;
A zero-think-time (throughput-oriented) test is more likely to encounter&#13;
this situation, since&#13;
each client thread is performing more requests. Hence, throughput tests are&#13;
typically executed with fewer client threads (less load) than a corresponding&#13;
test that measures response time.</p>&#13;
&#13;
<p>Tests that measure throughput also commonly report the&#13;
average response time of requests. That is an interesting piece&#13;
of information,&#13;
but changes in that number don’t indicate a performance problem unless&#13;
the reported throughput is the same. A server that can sustain 500&#13;
OPS with a 0.5-second response time is performing better&#13;
than a server that reports a 0.3-second response time but only 400 OPS.</p>&#13;
&#13;
<p>Throughput measurements are almost always taken after a suitable warm-up&#13;
period, particularly because what is being measured is not a fixed set of&#13;
work.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Response-Time Tests" data-type="sect2"><div class="sect2" id="idm45775552401224">&#13;
<h2>Response-Time Tests</h2>&#13;
&#13;
<p><a data-primary="performance testing" data-secondary="response-time tests" data-type="indexterm" id="ix_ch02-asciidoc7"/><a data-primary="response-time tests" data-type="indexterm" id="ix_ch02-asciidoc8"/>The last common test is one that measures <em>response time</em>: the amount of&#13;
time that elapses between the sending of a request from a client and the receipt of the response.</p>&#13;
&#13;
<p><a data-primary="think time" data-type="indexterm" id="idm45775552396504"/>The difference between a response-time test and a throughput test (assuming&#13;
the latter is client/server based) is that client threads in a&#13;
response-time test sleep for a period of time between operations. This&#13;
is referred to as <em>think time</em>. A response-time test is designed to&#13;
mimic more&#13;
closely what a user does: the user enters a URL in a browser, spends time&#13;
reading the page that comes back, clicks a link in the page, spends&#13;
time reading that page, and so on.</p>&#13;
&#13;
<p>When think time is introduced into a test, throughput becomes&#13;
fixed: a given number of clients executing requests with a given think time&#13;
will always yield the same TPS (with slight variance; see the following sidebar).&#13;
At that point, the important measurement is the response&#13;
time for the request: the effectiveness of the server is based on how&#13;
quickly it responds to that fixed load.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775552393864">&#13;
<h5>Think Time and Throughput</h5>&#13;
<p><a data-primary="throughput, think time and" data-type="indexterm" id="idm45775552392456"/>The throughput of a test in which the clients include think time can be measured&#13;
in two ways. The simplest way is for clients to sleep for a period of time&#13;
between requests:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="k">while</code> <code class="o">(!</code><code class="n">done</code><code class="o">)</code> <code class="o">{</code>&#13;
    <code class="n">executeOperation</code><code class="o">();</code>&#13;
    <code class="n">Thread</code><code class="o">.</code><code class="na">currentThread</code><code class="o">().</code><code class="na">sleep</code><code class="o">(</code><code class="mi">30</code><code class="o">*</code><code class="mi">1000</code><code class="o">);</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>In this case, the throughput is somewhat dependent on the response time.&#13;
If the response time is 1 second, it means that the client will send a&#13;
request every 31 seconds, which will yield a throughput of 0.032&#13;
OPS. If the response time is 2 seconds, each client&#13;
will send a request every 32 seconds, yielding a throughput of 0.031&#13;
OPS.</p>&#13;
&#13;
<p><a data-primary="cycle time" data-type="indexterm" id="idm45775552378168"/>The other alternative is known as <em>cycle time</em> (rather than think time). Cycle&#13;
time sets the&#13;
total time between requests to 30 seconds, so that the time the client sleeps&#13;
depends on the response time:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="k">while</code> <code class="o">(!</code><code class="n">done</code><code class="o">)</code> <code class="o">{</code>&#13;
    <code class="n">time</code> <code class="o">=</code> <code class="n">executeOperation</code><code class="o">();</code>&#13;
    <code class="n">Thread</code><code class="o">.</code><code class="na">currentThread</code><code class="o">().</code><code class="na">sleep</code><code class="o">(</code><code class="mi">30</code><code class="o">*</code><code class="mi">1000</code> <code class="o">-</code> <code class="n">time</code><code class="o">);</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>This alternative yields a fixed throughput of 0.033 OPS per client&#13;
regardless of the response time (assuming the response time is always less&#13;
than 30 second in this example).</p>&#13;
&#13;
<p>Think times in testing tools often vary by design; they will average a&#13;
particular value but use random variation to better simulate what users&#13;
do. In addition, thread scheduling is never exactly real-time, so the&#13;
actual time between the requests a client sends will vary slightly.&#13;
As a result, even when using a tool&#13;
that provides a cycle time instead of a think time, the reported throughput&#13;
between runs will vary slightly. But if the throughput is far from the&#13;
expected value, something has gone wrong in the execution of the test.</p>&#13;
</div></aside>&#13;
&#13;
<p>We can measure response time in two ways. Response time can be reported&#13;
as an average: the individual times are added together and divided by the&#13;
number of requests. <a data-primary="percentile request" data-type="indexterm" id="idm45775552355176"/>Response time can also be reported as a <em>percentile&#13;
request</em>; for example, the&#13;
<span class="keep-together">90th%</span>&#13;
response time. If 90% of responses are less&#13;
than 1.5 seconds and 10% of responses are greater than 1.5 seconds, then&#13;
1.5 seconds is the&#13;
<span class="keep-together">90th%</span>&#13;
response time.</p>&#13;
&#13;
<p>One difference between average response time and a percentile response time&#13;
is in the way outliers affect the&#13;
calculation of the average: since they are included&#13;
as part of the&#13;
average, large outliers will have a large effect on the average response time.</p>&#13;
&#13;
<p><a data-type="xref" href="#FigureRT">Figure 2-2</a> shows a graph of 20 requests with a somewhat typical range of&#13;
response times. The response times range from 1 to 5 seconds. The average&#13;
response time (represented by the lower heavy line along the x-axis) is 2.35&#13;
seconds, and 90% of the responses occur in 4 seconds or less (represented by&#13;
the upper heavy line along the <span class="keep-together">&#13;
x-axis).</span></p>&#13;
&#13;
<p>This is the usual scenario for a well-behaved test.&#13;
Outliers can skew that analysis, as the data in <a data-type="xref" href="#FigureRTOutlier">Figure 2-3</a> shows.</p>&#13;
&#13;
<p>This data set includes a huge outlier: one request took one hundred seconds.&#13;
As a result, the positions of the&#13;
<span class="keep-together">90th%</span>&#13;
and average response times are reversed.&#13;
The average response time is a whopping 5.95 seconds, but the&#13;
<span class="keep-together">90th%</span>&#13;
response time is 1.0 second. Focus in this case should be given to reducing&#13;
the effect of the outlier (which will drive down the average response&#13;
time).</p>&#13;
&#13;
<figure><div class="figure" id="FigureRT">&#13;
<img alt="Graph of Typical Response Times" src="assets/jp2e_0202.png"/>&#13;
<h6><span class="label">Figure 2-2. </span>Typical set of response times</h6>&#13;
</div></figure>&#13;
&#13;
<figure><div class="figure" id="FigureRTOutlier">&#13;
<img alt="Graph of Response Times with Outlier" src="assets/jp2e_0203.png"/>&#13;
<h6><span class="label">Figure 2-3. </span>Set of response times with an outlier</h6>&#13;
</div></figure>&#13;
&#13;
<p><a data-primary="garbage collection (GC)" data-secondary="response time and" data-type="indexterm" id="idm45775552312680"/>Outliers like that can occur for multiple reasons, and they can&#13;
more easily occur in Java&#13;
applications because of the pause times introduced by&#13;
GC.<sup><a data-type="noteref" href="ch02.html#idm45775552311416" id="idm45775552311416-marker">1</a></sup>&#13;
In performance testing, the usual focus is on the&#13;
<span class="keep-together">90th%</span>&#13;
response time (or even&#13;
the&#13;
<span class="keep-together">95th%</span>&#13;
or&#13;
<span class="keep-together">99th%</span>&#13;
response time; there is nothing magical about 90%). If you&#13;
can focus on only one number, a&#13;
percentile-based number is the better choice, since achieving a smaller&#13;
number there&#13;
will benefit a majority of users. But it is even better to look at both the&#13;
average response time and at least one percentile-based response time,&#13;
so you do not miss cases with large outliers.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775552307912">&#13;
<h5>Load Generators</h5>&#13;
<p><a data-primary="load generators" data-type="indexterm" id="idm45775552306472"/>Many open source and commercial load-generating tools exist. The&#13;
examples in this book utilize <a href="http://faban.org">Faban</a>, an open source,&#13;
Java-based load generator. This is the code that forms the basis of load drivers&#13;
for commercial benchmarks like SPECjEnterprise by the SPEC organization.</p>&#13;
&#13;
<p>Faban comes with a simple program (<code>fhb</code>) that&#13;
can be used to measure the performance of a simple URL:</p>&#13;
<pre data-type="programlisting">&#13;
% <strong>fhb -W 1000 -r 300/300/60 -c 25  http://host:port/StockServlet?stock=SDO</strong>&#13;
ops/sec: 8.247&#13;
% errors: 0.0&#13;
avg. time: 0.022&#13;
max time: 0.045&#13;
90th %: 0.030&#13;
95th %: 0.035&#13;
99th %: 0.035&#13;
</pre>&#13;
&#13;
<p>This example measures 25 clients&#13;
(<code class="keep-together">-c 25</code>)&#13;
making requests to the stock servlet&#13;
(requesting symbol SDO); each request has a 1-second cycle time&#13;
(<code class="keep-together">-W 1000</code>).&#13;
The&#13;
benchmark has a 5-minute (300-second) warm-up period, followed by a 5-minute&#13;
measurement period and a 1-minute ramp-down period&#13;
(<code class="keep-together">-r 300/300/60</code>).&#13;
Following the test,&#13;
<code>fhb</code> reports the OPS and various response times for the&#13;
test (and because this example includes think time, the response times are&#13;
the important metric, while the OPS will be more or less constant).</p>&#13;
&#13;
<p><code>fhb</code> is good for simple tests like this (utilizing a single command-line).&#13;
A variety of more complex load-generating tools are available: Apache JMeter,&#13;
Gatling, Micro Focus LoadRunner, and a host of others.<a data-startref="ix_ch02-asciidoc8" data-type="indexterm" id="idm45775552271144"/><a data-startref="ix_ch02-asciidoc7" data-type="indexterm" id="idm45775552270440"/></p>&#13;
</div></aside>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Batch-oriented tests (or any test without a warm-up period) have been infrequently used in Java performance testing but can yield valuable results.</p>&#13;
</li>&#13;
<li>&#13;
<p>Other tests can measure either throughput or response time, depending on whether the load comes in at a fixed rate (i.e., based on emulating think time in the client).</p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Understand Variability" data-type="sect1"><div class="sect1" id="idm45775552400600">&#13;
<h1>Understand Variability</h1>&#13;
&#13;
<p><a data-primary="performance testing" data-secondary="variability of test results" data-type="indexterm" id="ix_ch02-asciidoc9"/><a data-primary="variability, of test results" data-type="indexterm" id="ix_ch02-asciidoc10"/>The third principle is to understand how test results vary over time.&#13;
Programs that process exactly the same set of data will produce a different&#13;
answer each time they are run. Background processes on the machine will&#13;
affect the application, the network will be more or less congested when the&#13;
program is run, and so on. Good benchmarks also never process exactly&#13;
the same set of data each time they are run; random behavior will be built into&#13;
the test to mimic the real world. This creates&#13;
a problem: when comparing the result from one run to the result from another&#13;
run, is the difference due to a regression or to the random variation&#13;
of the test?</p>&#13;
&#13;
<p>This problem can be solved by running the test multiple times and averaging&#13;
those results.&#13;
Then when a change is made to the code being tested, the test can be rerun&#13;
multiple times, the results averaged, and the two averages compared. It&#13;
sounds so easy.</p>&#13;
&#13;
<p>Unfortunately, it isn’t as simple as that. Understanding when a&#13;
difference is a real regression and when it is a random variation is&#13;
difficult. In this key area, science leads the way, but art&#13;
will come into play.</p>&#13;
&#13;
<p>When averages in benchmark results are&#13;
compared, it is impossible to know with absolute certainty whether the&#13;
difference in the averages is real or due to random fluctuation.&#13;
The best that can be done is to hypothesize that “the averages&#13;
are the same” and then determine the probability that such&#13;
a statement is true. If the statement is false with a high degree of&#13;
probability, we are comfortable believing the difference in the&#13;
averages (though we can never be 100% certain).</p>&#13;
&#13;
<p><a data-primary="regression testing" data-type="indexterm" id="ix_ch02-asciidoc11"/>Testing code for changes like this is called <em>regression testing</em>. In a&#13;
regression test, <a data-primary="baseline" data-type="indexterm" id="idm45775552257432"/><a data-primary="specimen" data-type="indexterm" id="idm45775552256728"/>the original code is known as the <em>baseline</em>, and the new code&#13;
is called the <em>specimen</em>. Take the case of a batch program in which the&#13;
baseline and specimen are each run three times, yielding the times given&#13;
in <a data-type="xref" href="#TableTTest1">Table 2-2</a>.</p>&#13;
<table id="TableTTest1">&#13;
<caption><span class="label">Table 2-2. </span>Hypothetical times to execute two tests</caption>&#13;
<thead>&#13;
<tr>&#13;
<th/>&#13;
<th>Baseline</th>&#13;
<th>Specimen</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>First iteration</p></td>&#13;
<td><p>1.0 second</p></td>&#13;
<td><p>0.5 second</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Second iteration</p></td>&#13;
<td><p>0.8 second</p></td>&#13;
<td><p>1.25 seconds</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Third iteration</p></td>&#13;
<td><p>1.2 seconds</p></td>&#13;
<td><p>0.5 second</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><strong>Average</strong></p></td>&#13;
<td><p><strong>1 second</strong></p></td>&#13;
<td><p><strong>0.75 second</strong></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>The average of the specimen indicates a 25% improvement in the code.&#13;
How confident can we be that the test really reflects a 25% improvement?&#13;
Things look good: two&#13;
of the three specimen values are less than the baseline average, and the size of the&#13;
improvement is&#13;
large. Yet when the analysis described in this section is performed on&#13;
those results, it turns out that the probability of the specimen and the&#13;
baseline having the same performance is 43%. When numbers like these are observed, 43% of the time the underlying performance of the two tests are&#13;
the same, and performance is different only 57% of the time. This,&#13;
by the way, is&#13;
not exactly the same thing as saying that 57% of the time the performance&#13;
is 25% better, but you’ll learn more about that later in this section.</p>&#13;
&#13;
<p>The reason these probabilities seem different than might be expected is due to&#13;
the large variation in the results.&#13;
In general, the larger the variation in&#13;
a set of results, the harder it is to guess the probability that the difference&#13;
in the averages is real or due to random chance.<sup><a data-type="noteref" href="ch02.html#idm45775552239592" id="idm45775552239592-marker">2</a></sup></p>&#13;
&#13;
<p><a data-primary="Student's t-test" data-type="indexterm" id="idm45775552238632"/>This number—43%—is based on the result of <em>Student’s t-test</em>, which is a&#13;
statistical analysis based on the series and their variances.<sup><a data-type="noteref" href="ch02.html#idm45775552237160" id="idm45775552237160-marker">3</a></sup> <a data-primary="p-value" data-type="indexterm" id="ix_ch02-asciidoc12"/>The <em>t</em>-test produces a number called the&#13;
 <em>p-value</em>, which refers to the probability that the null&#13;
hypothesis for the test is true.</p>&#13;
&#13;
<p><a data-primary="null hypothesis" data-type="indexterm" id="idm45775552233880"/>The <em>null hypothesis</em> in regression testing is the hypothesis that the two&#13;
tests have equal performance. The <em>p</em>-value for this example is roughly&#13;
43%, which means the confidence we can have that the series converge&#13;
to the same average&#13;
is 43%. Conversely, the confidence we have that the series do not converge&#13;
to the same average is 57%.</p>&#13;
&#13;
<p>What does it mean to say that 57% of the time, the series do not converge&#13;
to the same&#13;
average? Strictly speaking, it doesn’t mean that we have 57% confidence that&#13;
there is a 25% improvement in the result—it means only that we have 57%&#13;
confidence that the results are different. There may be a 25% improvement,&#13;
there may be a 125% improvement; it is even conceivable that the specimen&#13;
has worse performance than the baseline. The most probable likelihood&#13;
is that the difference in the test is similar to what has been measured&#13;
(particularly as the <em>p</em>-value goes down), but certainty can never be&#13;
achieved.</p>&#13;
&#13;
<p><a data-primary="α-value" data-primary-sortas="alpha-value" data-type="indexterm" id="idm45775552230120"/>The <em>t</em>-test is typically used in conjunction with an <em>α-value</em>, which&#13;
is a (somewhat arbitrary) point at which the result is assumed to have&#13;
statistical significance. The <em>α</em>-value is commonly set to 0.1—which&#13;
means that a result is considered statistically significant if&#13;
the specimen and baseline will be the same only 10% (0.1) of the time&#13;
(or conversely, that 90% of the time the specimen&#13;
and baseline differ).  Other commonly used&#13;
<em>α</em>-values are 0.05 (95%) or 0.01 (99%). A test is considered statistically&#13;
significant if the <em>p</em>-value is larger than 1 – <em>α</em>-value.</p>&#13;
<aside class="less_space pagebreak-before" data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775552225704">&#13;
<h5>Statistics and Semantics</h5>&#13;
<p><a data-primary="t-test, presenting results of" data-type="indexterm" id="idm45775552224216"/>The correct way to present results of a <em>t</em>-test is to phrase a statement&#13;
like this: there is a 57% probability that the specimen differs from&#13;
the baseline, and the best estimate of that difference is 25%.</p>&#13;
&#13;
<p>The common way to present these results is to say that there is a 57%&#13;
confidence level that there is a 25% improvement in the results. Though that&#13;
isn’t exactly the same thing and will drive statisticians crazy, it is&#13;
easy shorthand to adopt and isn’t that far off the mark. Probabilistic&#13;
analysis always involves uncertainty, and that uncertainty is better&#13;
understood when the semantics are precisely stated. But particularly in&#13;
an arena where the underlying issues are well understood, semantic&#13;
shortcuts will inevitably creep in.</p>&#13;
</div></aside>&#13;
&#13;
<p>Hence, the proper way to search for regressions in code is to determine&#13;
a level of statistical significance—say, 90%—and then to use the&#13;
<em>t</em>-test to determine if the specimen and baseline are different within that&#13;
degree of statistical significance. Care must be taken to&#13;
understand what it means if the test for statistical significance fails.&#13;
In the example, the <em>p</em>-value is 0.43; we cannot say that&#13;
there is statistical significance within a 90% confidence level that&#13;
the result indicates that the averages are different. The fact that&#13;
the test is not statistically significant does not mean that it is an&#13;
insignificant result; it simply means that the test is inconclusive.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775552219496">&#13;
<h5>Statistically Important</h5>&#13;
<p><a data-primary="statistical importance, statistical significance versus" data-type="indexterm" id="idm45775552218296"/>Statistical <em>significance</em> does not mean statistical <em>importance</em>. A baseline&#13;
with little variance that averages 1 second and a specimen with little&#13;
variance that averages 1.01 seconds may have a <em>p</em>-value of 0.01:&#13;
there is a 99% probability that there is a difference in the result.</p>&#13;
&#13;
<p>The difference itself is only 1%. Now say a different test shows a&#13;
10% regression between specimen and baseline, but with a <em>p</em>-value of&#13;
0.2: not statistically significant. Which test warrants the most precious&#13;
resource of all—additional time to investigate?</p>&#13;
&#13;
<p>Although there is less confidence in the case showing a 10% difference,&#13;
time is better spent investigating that test (starting, if possible,&#13;
with getting additional data to see if the result is actually&#13;
statistically significant). Just because the 1% difference is more&#13;
probable doesn’t&#13;
mean that it is more important.</p>&#13;
</div></aside>&#13;
&#13;
<p>The usual reason a test is statistically inconclusive is that the samples don’t have&#13;
enough data. So far, our example has looked at a series&#13;
with three results in the baseline and the specimen. What if three additional&#13;
results are added, yielding the data in <a data-type="xref" href="#TableTTest2">Table 2-3</a>?</p>&#13;
<table id="TableTTest2">&#13;
<caption><span class="label">Table 2-3. </span>Increased sample size of hypothetical times</caption>&#13;
<thead>&#13;
<tr>&#13;
<th/>&#13;
<th>Baseline</th>&#13;
<th>Specimen</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>First iteration</p></td>&#13;
<td><p>1.0 second</p></td>&#13;
<td><p>0.5 second</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Second iteration</p></td>&#13;
<td><p>0.8 second</p></td>&#13;
<td><p>1.25 second</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Third iteration</p></td>&#13;
<td><p>1.2 seconds</p></td>&#13;
<td><p>0.5 second</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Fourth iteration</p></td>&#13;
<td><p>1.1 seconds</p></td>&#13;
<td><p>0.8 second</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Fifth iteration</p></td>&#13;
<td><p>0.7 second</p></td>&#13;
<td><p>0.7 second</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Sixth iteration</p></td>&#13;
<td><p>1.2 seconds</p></td>&#13;
<td><p>0.75 second</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p><strong>Average</strong></p></td>&#13;
<td><p><strong>1 second</strong></p></td>&#13;
<td><p><strong>0.75 second</strong></p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>With the additional data, the <em>p</em>-value drops from 0.43 to 0.11:&#13;
the probability that the results are different has risen from 57% to 89%.&#13;
The averages haven’t changed; we just have more confidence that&#13;
the difference is not due to random variation.</p>&#13;
&#13;
<p>Running additional tests until a level of statistical significance is achieved&#13;
isn’t always practical. It isn’t, strictly speaking, necessary either.&#13;
The choice of the <em>α</em>-value that determines statistical significance is&#13;
arbitrary, even if the usual choice is common. A <em>p</em>-value of 0.11 is&#13;
not statistically significant within a 90% confidence level, but it is&#13;
statistically significant within an 89% confidence level.<a data-startref="ix_ch02-asciidoc12" data-type="indexterm" id="idm45775552190152"/></p>&#13;
&#13;
<p>Regression testing is important, but it’s not a black-and-white&#13;
science. You cannot look at a series of numbers (or their averages)&#13;
and make&#13;
a judgment that compares them without doing some statistical analysis to&#13;
understand what the numbers mean. Yet even that analysis cannot yield a&#13;
completely definitive answer, because of the laws of probabilities.&#13;
The job of a performance engineer is to look at the data, understand the&#13;
probabilities, and determine where to spend time based on all the available&#13;
data.</p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Correctly determining whether results from two tests are different requires a level of statistical analysis to make sure that perceived differences are not the result of random chance.</p>&#13;
</li>&#13;
<li>&#13;
<p>The rigorous way to accomplish that is to use Student’s <em>t</em>-test to compare the results.</p>&#13;
</li>&#13;
<li>&#13;
<p>Data from the <em>t</em>-test tells us the probability that a regression exists, but it doesn’t tell us which regressions should be ignored and which must be pursued. Finding that balance is part of the art of performance engineering<a data-startref="ix_ch02-asciidoc11" data-type="indexterm" id="idm45775552183528"/>.<a data-startref="ix_ch02-asciidoc10" data-type="indexterm" id="idm45775552182696"/><a data-startref="ix_ch02-asciidoc9" data-type="indexterm" id="idm45775552181992"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Test Early, Test Often" data-type="sect1"><div class="sect1" id="idm45775552265480">&#13;
<h1>Test Early, Test Often</h1>&#13;
&#13;
<p><a data-primary="development cycle, performance testing as integral part of" data-type="indexterm" id="ix_ch02-asciidoc13"/><a data-primary="performance testing" data-secondary="as integral part of development cycle" data-type="indexterm" id="ix_ch02-asciidoc14"/>Fourth and, finally,&#13;
performance geeks (including me) like to recommend that performance testing be&#13;
an integral part of the development cycle. In an ideal world, performance&#13;
tests would be run as part of the process when code is checked into the&#13;
central repository; code that introduces performance regressions would be&#13;
blocked from being checked in.</p>&#13;
&#13;
<p>Some inherent tension exists between that recommendation and other&#13;
recommendations in this chapter—and between that recommendation and&#13;
the real world. A good&#13;
performance test will encompass a lot of code—at least a medium-sized&#13;
mesobenchmark. It will need to be repeated multiple times to establish&#13;
confidence that any difference found between the old code and the new code is&#13;
a real difference and not just random variation. On a large project, this&#13;
can take a few days or a week, making it unrealistic to run performance&#13;
tests on code before checking it into a repository.</p>&#13;
&#13;
<p>The typical development cycle does not make things any easier. A project&#13;
schedule often establishes a feature-freeze date: all feature changes to&#13;
code must be checked into the repository at an early point in the release&#13;
cycle, and the remainder of the cycle is devoted to shaking out any bugs&#13;
(including performance issues) in the new release. This causes two problems&#13;
for early testing:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Developers are under time constraints to get code checked in to meet the schedule; they will balk at having to spend time fixing a performance issue when the schedule has time for that after all the initial code is checked in. The developer who checks in code causing a 1% regression early in the cycle will face pressure to fix that issue; the developer who waits until the evening of the feature freeze can check in code that causes a 20% regression and deal with it later.</p>&#13;
</li>&#13;
<li>&#13;
<p>Performance characteristics of code will change as the code changes. This is the same principle that argued for testing the full application (in addition to any module-level tests that may occur): heap usage will change, code compilation will change, and so on.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Despite these challenges, frequent performance testing during development is&#13;
important, even if the issues cannot be immediately addressed. A developer&#13;
who introduces code causing a 5% regression may have a plan to address that&#13;
regression as development proceeds: maybe the code depends on an&#13;
as-yet-to-be integrated feature, and when that feature is available, a small&#13;
tweak will allow the regression to go away. That’s a reasonable position,&#13;
even though it means that performance tests will have to live with that 5%&#13;
regression for a few weeks (and the unfortunate but unavoidable issue that&#13;
said regression is masking other regressions).</p>&#13;
&#13;
<p>On the other hand, if the new code causes a regression that can be fixed&#13;
with only architectural changes, it is better to catch the regression and&#13;
address it early, before the rest of the code starts to depend on the&#13;
new implementation. It’s a balancing act, requiring analytic and often&#13;
political skills.</p>&#13;
&#13;
<p>Early, frequent testing is most useful if the following guidelines are&#13;
followed:</p>&#13;
<dl>&#13;
<dt>Automate everything</dt>&#13;
<dd>&#13;
<p>All performance testing should be scripted (or programmed, though&#13;
scripting is usually easier). Scripts must be able to install the new&#13;
code, configure it into the full environment (creating database connections,&#13;
setting up user accounts, and so on), and run the set of tests. But it&#13;
doesn’t stop there: the scripts must be able to run the test multiple&#13;
times, perform <em>t</em>-test analysis on the results, and produce a report&#13;
showing the confidence level that the results are the same, and the&#13;
measured difference if they are not the same.</p>&#13;
&#13;
<p>The automation must make sure that the machine is in a known state before&#13;
tests are run: it must check that no unexpected processes are running,&#13;
that the OS configuration is correct, and so on. A performance test is&#13;
repeatable only if the environment is the same from run to run; the&#13;
automation must take care of that.</p>&#13;
</dd>&#13;
<dt>Measure everything</dt>&#13;
<dd>&#13;
<p>The automation must gather every conceivable piece of&#13;
data that will be useful for later analysis. This includes system&#13;
information sampled throughout the run: CPU usage, disk usage,&#13;
network usage, memory usage, and so on. It includes logs from the&#13;
application—both those the application produces, and the logs from&#13;
the garbage collector. Ideally, it can include Java Flight Recorder (JFR) recordings (see&#13;
<a data-type="xref" href="ch03.html#Tools">Chapter 3</a>) or other low-impact profiling information, periodic thread&#13;
stacks, and other heap&#13;
analysis data like histograms or full heap dumps (though the full&#13;
heap dumps, in particular, take a lot of space and cannot necessarily&#13;
be kept long-term).</p>&#13;
&#13;
<p>The monitoring information must also include data from other parts&#13;
of the system, if applicable: for example, if the program uses a database,&#13;
include the system statistics from the database machine as well as any&#13;
diagnostic output from the <span class="keep-together">database</span> (including performance reports&#13;
like Oracle’s Automatic Workload Repository, or AWR, reports).</p>&#13;
&#13;
<p>This data will guide the analysis of any regressions that&#13;
are uncovered. If the CPU usage has increased, it’s time to consult&#13;
the profile information to see what is taking more time. If the time&#13;
spent in GC has increased, it’s time to consult the heap profiles to&#13;
see what is consuming more memory. If CPU time and GC time have&#13;
decreased, contention somewhere has likely slowed performance:&#13;
stack data can point to particular synchronization bottlenecks&#13;
(see <a data-type="xref" href="ch09.html#ThreadPerformance">Chapter 9</a>), JFR recordings can be used to find application&#13;
latencies, or database logs can point to something&#13;
that has increased database contention.</p>&#13;
&#13;
<p>When figuring out the source of a regression, it is time&#13;
to play detective, and the more data that is available, the more clues&#13;
there are to follow. As discussed in <a data-type="xref" href="ch01.html#Introduction">Chapter 1</a>,&#13;
it isn’t necessarily&#13;
the case that the JVM is the regression. Measure everything, everywhere,&#13;
to make&#13;
sure the correct analysis can be done.</p>&#13;
</dd>&#13;
<dt>Run on the target system</dt>&#13;
<dd>&#13;
<p>A test that is run on a single-core laptop will behave differently&#13;
than a test run on a machine with 72 cores. That should&#13;
be clear in terms of threading effects: the larger machine is going to&#13;
run more threads at the same time, reducing contention among application&#13;
threads for access to the CPU. At the same time, the large system&#13;
will show synchronization bottlenecks that would be unnoticed on the&#13;
small laptop.</p>&#13;
&#13;
<p>Other performance differences are just as important, even&#13;
if they are not as immediately obvious. Many important tuning flags&#13;
calculate their defaults based on the underlying hardware the JVM is&#13;
running on. Code is compiled differently from platform to platform.&#13;
Caches—software and, more importantly, hardware—behave differently&#13;
on different systems and under different loads. And so on…</p>&#13;
&#13;
<p>Hence, the performance of a particular production environment can never be&#13;
fully known without testing the expected load on the expected hardware.&#13;
Approximations and extrapolations can be made from running smaller tests&#13;
on smaller hardware, and in the real world, duplicating a production&#13;
environment for testing can be quite difficult or expensive. But&#13;
extrapolations are simply predictions, and even in the best case,&#13;
predictions can be wrong. A large-scale system is more than the sum of&#13;
its parts, and there can be no substitute for performing adequate&#13;
load testing on the target platform.</p>&#13;
</dd>&#13;
</dl>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Frequent performance testing is important, but it doesn’t occur in a vacuum; there are trade-offs to consider regarding the normal development cycle.</p>&#13;
</li>&#13;
<li>&#13;
<p>An automated testing system that collects all possible statistics from all machines and programs will provide the necessary clues to any performance regressions.<a data-startref="ix_ch02-asciidoc14" data-type="indexterm" id="idm45775558341512"/><a data-startref="ix_ch02-asciidoc13" data-type="indexterm" id="idm45775558340808"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Benchmark Examples" data-type="sect1"><div class="sect1" id="idm45775552180680">&#13;
<h1>Benchmark Examples</h1>&#13;
&#13;
<p><a data-primary="benchmarks" data-secondary="examples" data-type="indexterm" id="ix_ch02-asciidoc15"/>Some examples in this book use <code>jmh</code> to provide a microbenchmark. In this&#13;
section, we’ll look in depth at how one such microbenchmark is developed&#13;
as an example of how to write your own <code>jmh</code> benchmark. But many&#13;
examples in this book are based on variants of a&#13;
mesobenchmark—a test that is complex enough to exercise various JVM features but less&#13;
complex than a real application. So following our exploration of <code>jmh</code>,&#13;
we’ll look through some of the common code examples of the mesobenchmark&#13;
used in later chapters so that those examples have some context.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Java Microbenchmark Harness" data-type="sect2"><div class="sect2" id="idm45775558335240">&#13;
<h2>Java Microbenchmark Harness</h2>&#13;
&#13;
<p><a data-primary="jmh" data-type="indexterm" id="ix_ch02-asciidoc16"/><a data-primary="performance testing" data-secondary="jmh" data-type="indexterm" id="ix_ch02-asciidoc17"/><code>jmh</code> is a set of classes that supply a framework for writing benchmarks.&#13;
The <em>m</em> in <code>jmh</code> used to stand for <em>microbenchmark</em>, though now <code>jmh</code> advertises&#13;
itself as suitable for nano/micro/milli/macro benchmarks. Its typical usage,&#13;
though, remains small (micro) benchmarks. Although <code>jmh</code> was announced in&#13;
conjunction with Java 9, it isn’t really tied to any specific Java release,&#13;
and no tools in the JDK support <code>jmh</code>. The class libraries that&#13;
make up <code>jmh</code> are compatible with JDK 8 and later releases.</p>&#13;
&#13;
<p><code>jmh</code> takes some of the uncertainty out of writing a good benchmark, but it is&#13;
not a silver bullet; you still must understand what you’re benchmarking and&#13;
how to write good benchmark code. But the features of <code>jmh</code> are designed to make&#13;
that easier.</p>&#13;
&#13;
<p><code>jmh</code> is used for a few examples in the book, including a test for&#13;
JVM parameters that affect string interning presented in <a data-type="xref" href="ch12.html#Misc">Chapter 12</a>.&#13;
We’ll use that example here to understand how to write a benchmark using <code>jmh</code>.</p>&#13;
&#13;
<p>It is possible to write a <code>jmh</code> benchmark from scratch, but it is easier to&#13;
start with a <code>jmh</code>-provided main class and write only the benchmark-specific&#13;
code. And while it is possible to get the necessary <code>jmh</code> classes by using a variety&#13;
of tools (and even certain IDEs), the basic method is to use Maven. The&#13;
following command will create a Maven project that we can add our benchmark&#13;
code to:</p>&#13;
&#13;
<pre data-type="programlisting">$ mvn archetype:generate \&#13;
      -DinteractiveMode=false \&#13;
      -DarchetypeGroupId=org.openjdk.jmh \&#13;
      -DarchetypeArtifactId=jmh-java-benchmark-archetype \&#13;
      -DgroupId=net.sdo \&#13;
      -DartifactId=string-intern-benchmark \&#13;
      -Dversion=1.0</pre>&#13;
&#13;
<p>This creates the Maven project in the <em>string-intern-benchmark</em> directory;&#13;
there, it creates a directory with the given <code>groupId</code> name, and&#13;
a skeleton benchmark class called <code>MyBenchmark</code>. There’s nothing special&#13;
about that name; you can create a different (or several different) classes,&#13;
since <code>jmh</code> will figure out which classes to test by looking for an annotation&#13;
called <code>Benchmark</code>.</p>&#13;
&#13;
<p>We’re interested in testing the performance of the <code>String.intern()</code> method,&#13;
so the first benchmark method we would write looks like this:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kn">import</code> <code class="nn">org.openjdk.jmh.annotations.Benchmark</code><code class="o">;</code>&#13;
<code class="kn">import</code> <code class="nn">org.openjdk.jmh.infra.Blackhole</code><code class="o">;</code>&#13;
&#13;
<code class="kd">public</code> <code class="kd">class</code> <code class="nc">MyBenchmark</code> <code class="o">{</code>&#13;
&#13;
    <code class="nd">@Benchmark</code>&#13;
    <code class="kd">public</code> <code class="kt">void</code> <code class="nf">testIntern</code><code class="o">(</code><code class="n">Blackhole</code> <code class="n">bh</code><code class="o">)</code> <code class="o">{</code>&#13;
        <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="mi">10000</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
            <code class="n">String</code> <code class="n">s</code> <code class="o">=</code> <code class="k">new</code> <code class="n">String</code><code class="o">(</code><code class="s">"String to intern "</code> <code class="o">+</code> <code class="n">i</code><code class="o">);</code>&#13;
            <code class="n">String</code> <code class="n">t</code> <code class="o">=</code> <code class="n">s</code><code class="o">.</code><code class="na">intern</code><code class="o">();</code>&#13;
            <code class="n">bh</code><code class="o">.</code><code class="na">consume</code><code class="o">(</code><code class="n">t</code><code class="o">);</code>&#13;
        <code class="o">}</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>The basic outline of the <code>testIntern()</code> method should make sense: we are&#13;
testing the time to create 10,000 interned strings. The <code>Blackhole</code>&#13;
class used here is a <code>jmh</code> feature that addresses one of the points about&#13;
microbenchmarking: if the value of an operation isn’t used, the compiler&#13;
is free to optimize out the operation. So we make sure that the values&#13;
are used by passing them to the <code>consume()</code> method of the <code>Blackhole</code>.</p>&#13;
&#13;
<p>In this example, the <code>Blackhole</code> isn’t strictly needed: we’re really&#13;
interested in only the side effects of calling the <code>intern()</code> method, which is going&#13;
to insert a string into a global hash table. That state change cannot be&#13;
compiled away even if we don’t use the return value from the <code>intern()</code>&#13;
method itself. Still, rather than puzzle through whether it’s necessary&#13;
to consume a particular value, it’s better to be in the habit of&#13;
making sure the operation will execute as we expect and consume calculated&#13;
values.</p>&#13;
&#13;
<p>To compile and run the benchmark:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>mvn package</strong>&#13;
... output from mvn...&#13;
$ <strong>java -jar target/benchmarks.jar</strong>&#13;
# Warmup: 5 iterations, 10 s each&#13;
# Measurement: 5 iterations, 10 s each&#13;
# Timeout: 10 min per iteration&#13;
# Threads: 1 thread, will synchronize iterations&#13;
# Benchmark mode: Throughput, ops/time&#13;
# Benchmark: net.sdo.MyBenchmark.testIntern&#13;
&#13;
# Run progress: 0.00% complete, ETA 00:08:20&#13;
# Fork: 1 of 5&#13;
# Warmup Iteration   1: 189.999 ops/s&#13;
# Warmup Iteration   2: 170.331 ops/s&#13;
# Warmup Iteration   3: 195.393 ops/s&#13;
# Warmup Iteration   4: 184.782 ops/s&#13;
# Warmup Iteration   5: 158.762 ops/s&#13;
Iteration   1: 182.426 ops/s&#13;
Iteration   2: 172.446 ops/s&#13;
Iteration   3: 166.488 ops/s&#13;
Iteration   4: 182.737 ops/s&#13;
Iteration   5: 168.755 ops/s&#13;
&#13;
# Run progress: 20.00% complete, ETA 00:06:44&#13;
# Fork: 2 of 5&#13;
.... similar output until ...&#13;
&#13;
Result "net.sdo.MyBenchmark.testIntern":&#13;
  177.219 ±(99.9%) 10.140 ops/s [Average]&#13;
  (min, avg, max) = (155.286, 177.219, 207.970), stdev = 13.537&#13;
  CI (99.9%): [167.078, 187.359] (assumes normal distribution)&#13;
&#13;
&#13;
Benchmark                Mode  Cnt    Score    Error  Units&#13;
MyBenchmark.testIntern  thrpt   25  177.219 ± 10.140  ops/s&#13;
</pre>&#13;
&#13;
<p>From the output, we can see how <code>jmh</code> helps avoid the pitfalls that we&#13;
discussed earlier in this chapter. First, see that we execute five warm-up&#13;
iterations of 10 seconds each, followed by five measurement iterations (also&#13;
of 10 seconds each). The warm-up iterations allow the compiler to&#13;
fully optimize the code, and then the harness will report&#13;
information from only the iterations of that compiled code.</p>&#13;
&#13;
<p>Then see that there are different forks (five in all). The harness is repeating&#13;
the test five times, each time in a separate (newly forked) JVM in order to&#13;
determine the repeatability of the results. And each JVM needs to&#13;
warm up and then measure the code. A forked test like this (with warm-up&#13;
and measurement intervals) is called a <em>trial</em>. In all, each test takes one hundred&#13;
seconds for&#13;
5 warm-up and 5 measurement cycles; that is all repeated 5 times, and the&#13;
total execution time is 8:20 minutes.</p>&#13;
&#13;
<p>Finally, we have the summary output: on average, the <code>testIntern()</code> method&#13;
executed 177 times per second. With a&#13;
confidence interval of 99.9%, we can say that the statistical average lies&#13;
between 167 and 187 operations per second. So <code>jmh</code> also helps us&#13;
with the necessary statistical analysis we need to understand if a particular&#13;
result is running with acceptable variance.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="JMH and parameters" data-type="sect3"><div class="sect3" id="idm45775558240376">&#13;
<h3>JMH and parameters</h3>&#13;
&#13;
<p><a data-primary="jmh" data-secondary="parameters and" data-type="indexterm" id="idm45775558238936"/>Often you want a range of input for a test; in this example, we’d like to&#13;
see the effect of interning 1 or 10,000 (or maybe even 1 million)&#13;
strings. Rather than&#13;
hardcoding that value in the <code>testIntern()</code> method, we can introduce a parameter:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="nd">@Param</code><code class="o">({</code><code class="s">"1"</code><code class="o">,</code><code class="s">"10000"</code><code class="o">})</code>&#13;
<code class="kd">private</code> <code class="kt">int</code> <code class="n">nStrings</code><code class="o">;</code>&#13;
&#13;
<code class="nd">@Benchmark</code>&#13;
<code class="kd">public</code> <code class="kt">void</code> <code class="nf">testIntern</code><code class="o">(</code><code class="n">Blackhole</code> <code class="n">bh</code><code class="o">)</code> <code class="o">{</code>&#13;
    <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">nStrings</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
        <code class="n">String</code> <code class="n">s</code> <code class="o">=</code> <code class="k">new</code> <code class="n">String</code><code class="o">(</code><code class="s">"String to intern "</code> <code class="o">+</code> <code class="n">i</code><code class="o">);</code>&#13;
        <code class="n">String</code> <code class="n">t</code> <code class="o">=</code> <code class="n">s</code><code class="o">.</code><code class="na">intern</code><code class="o">();</code>&#13;
        <code class="n">bh</code><code class="o">.</code><code class="na">consume</code><code class="o">(</code><code class="n">t</code><code class="o">);</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>Now <code>jmh</code> will report results for both values of the parameter:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>java -jar target/benchmarks.jar</strong>&#13;
...lots of output...&#13;
Benchmark               (nStrings)   Mode  Cnt        Score        Error  Units&#13;
MyBenchmark.testMethod           1  thrpt   25  2838957.158 ± 284042.905  ops/s&#13;
MyBenchmark.testMethod       10000  thrpt   25      202.829 ±     15.396  ops/s&#13;
</pre>&#13;
&#13;
<p>Predictably, with a loop&#13;
size of 10,000, the number of times the loop is run per second is reduced by&#13;
a factor of 10,000. In fact, the result for 10,000 strings is less than&#13;
something around 283 as we might hope, which is due to the way the string&#13;
intern table scales (which is explained when we use this benchmark in <a data-type="xref" href="ch12.html#Misc">Chapter 12</a>).</p>&#13;
&#13;
<p>It’s usually easier to have a single simple value for the parameter in the&#13;
source code and use that for testing. When you run the benchmark, you can&#13;
give it a list of values to use for each parameter, overriding the&#13;
value that is hardcoded in the Java code:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>java -jar target/benchmarks.jar -p nStrings=1,1000000</strong>&#13;
</pre>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Comparing tests" data-type="sect3"><div class="sect3" id="idm45775558134264">&#13;
<h3>Comparing tests</h3>&#13;
&#13;
<p><a data-primary="jmh" data-secondary="comparing tests" data-type="indexterm" id="idm45775558132824"/>The genesis of this benchmark is to figure out if we can make string interning&#13;
faster by using different JVM tunings. To do that, we’ll run the benchmark&#13;
with different JVM arguments by specifying those arguments on the command line:</p>&#13;
<pre data-type="programlisting">&#13;
$ <strong>java -jar target/benchmarks.jar</strong>&#13;
... output from test 1 ...&#13;
$ <strong>java -jar target/benchmarks.jar -jvmArg -XX:StringTableSize=10000000</strong>&#13;
... output from test 2 ...&#13;
</pre>&#13;
&#13;
<p>Then we can manually inspect and compare the difference the tuning has made&#13;
in our result.</p>&#13;
&#13;
<p>More commonly, you want to compare two implementations of&#13;
code. String interning is fine, but could we do better by using a simple&#13;
hash map and managing that? To test that, we would define another method in&#13;
the class and annotate that with the <code>Benchmark</code> annotation. Our first&#13;
(and suboptimal) pass at that would look like this:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">private</code> <code class="kd">static</code> <code class="n">ConcurrentHashMap</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">map</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ConcurrentHashMap</code><code class="o">&lt;&gt;();</code>&#13;
<code class="nd">@Benchmark</code>&#13;
<code class="kd">public</code> <code class="kt">void</code> <code class="nf">testMap</code><code class="o">(</code><code class="n">Blackhole</code> <code class="n">bh</code><code class="o">)</code> <code class="o">{</code>&#13;
    <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">nStrings</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
        <code class="n">String</code> <code class="n">s</code> <code class="o">=</code> <code class="k">new</code> <code class="n">String</code><code class="o">(</code><code class="s">"String to intern "</code> <code class="o">+</code> <code class="n">i</code><code class="o">);</code>&#13;
        <code class="n">String</code> <code class="n">t</code> <code class="o">=</code> <code class="n">map</code><code class="o">.</code><code class="na">putIfAbsent</code><code class="o">(</code><code class="n">s</code><code class="o">,</code> <code class="n">s</code><code class="o">);</code>&#13;
        <code class="n">bh</code><code class="o">.</code><code class="na">consume</code><code class="o">(</code><code class="n">t</code><code class="o">);</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p><code>jmh</code> will run all annotated methods through the same series of warm-up and&#13;
measurement iterations (always in newly forked JVMs) and produce a nice&#13;
comparison:</p>&#13;
&#13;
<pre data-type="programlisting">Benchmark               (nStrings)   Mode  Cnt    Score     Error  Units&#13;
MyBenchmark.testIntern       10000  thrpt   25  212.301 ± 207.550  ops/s&#13;
MyBenchmark.testMap          10000  thrpt   25  352.140 ±  84.397  ops/s</pre>&#13;
&#13;
<p>Managing the interned objects by hand has given a nice improvement here&#13;
(though beware: issues with the benchmark as written remain;&#13;
this isn’t the final word).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Setup code" data-type="sect3"><div class="sect3" id="idm45775558021976">&#13;
<h3>Setup code</h3>&#13;
&#13;
<p><a data-primary="jmh" data-secondary="setup code" data-type="indexterm" id="ix_ch02-asciidoc18"/>The previous output is abbreviated for this format, but when you run&#13;
<code>jmh</code> tests, you’ll see a long caveat before the results are printed. The gist&#13;
of this caveat is “just because you put some code into <code>jmh</code>, don’t assume&#13;
you’ve written a good benchmark: test your code and make sure it’s testing&#13;
what you expect.”</p>&#13;
&#13;
<p>Let’s look again at the benchmark definition. We wanted to test how long&#13;
it took to intern 10,000 strings, but what we are testing is the&#13;
time it takes to create (via concatenation) 10,000 strings plus the time it&#13;
takes to intern the resulting strings. The range of those strings is also fairly&#13;
limited: they are the same initial 17 characters followed by an integer. In&#13;
the same way we pre-created input for the handwritten Fibonacci microbenchmark,&#13;
we should pre-create input in this case.</p>&#13;
&#13;
<p>It could be argued that the range of&#13;
strings doesn’t matter to this benchmark and the concatenation is minor and&#13;
that hence the original test is completely accurate. That is possibly true, but&#13;
proving that point requires some work. Better just to write a benchmark&#13;
where those questions aren’t an issue than to make assumptions about what’s&#13;
going on.</p>&#13;
&#13;
<p>We also have to think deeply about what is going on in the test.&#13;
Essentially, the table holding the interned strings is a cache: the interned&#13;
string might be there (in which case it is returned), or it might not (in&#13;
which case it is inserted). And now we have a problem when we compare the&#13;
implementations: the manually managed concurrent hash map is never cleared&#13;
during a test. That means that during the first warm-up cycle, the strings&#13;
are inserted into the map, and in subsequent measurement cycles, the strings&#13;
are already there: the test has a 100% hit rate on the cache.</p>&#13;
&#13;
<p>The string intern table doesn’t work that way: the keys in the string intern&#13;
table are essentially weak references. Hence, the JVM may clear&#13;
some or all entries at any point in time (since the interned string goes out&#13;
of scope immediately after it is inserted into the table). The cache hit rate&#13;
in this case is indeterminate, but it is likely not anywhere close to 100%.&#13;
So as it&#13;
stands now, the intern test is going to do more work, since it has to update&#13;
the internal string table more frequently (both to delete and then to re-add&#13;
entries).</p>&#13;
&#13;
<p>Both of these issues will be avoided if we pre-create the strings into a&#13;
static array and then&#13;
intern them (or insert them into the hash map). Because the static array&#13;
maintains a reference to the string, the reference in the string table will&#13;
not be cleared. Then both tests will have a 100% hit rate during the&#13;
measurement cycles, and the range of strings will be more comprehensive.</p>&#13;
&#13;
<p>We need to do this initialization outside the measurement period, which&#13;
is accomplished using the <code>Setup</code> annotation:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">private</code> <code class="kd">static</code> <code class="n">ConcurrentHashMap</code><code class="o">&lt;</code><code class="n">String</code><code class="o">,</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">map</code><code class="o">;</code>&#13;
<code class="kd">private</code> <code class="kd">static</code> <code class="n">String</code><code class="o">[]</code> <code class="n">strings</code><code class="o">;</code>&#13;
&#13;
<code class="nd">@Setup</code><code class="o">(</code><code class="n">Level</code><code class="o">.</code><code class="na">Iteration</code><code class="o">)</code>&#13;
<code class="kd">public</code> <code class="kt">void</code> <code class="nf">setup</code><code class="o">()</code> <code class="o">{</code>&#13;
    <code class="n">strings</code> <code class="o">=</code> <code class="k">new</code> <code class="n">String</code><code class="o">[</code><code class="n">nStrings</code><code class="o">];</code>&#13;
    <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">nStrings</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
        <code class="n">strings</code><code class="o">[</code><code class="n">i</code><code class="o">]</code> <code class="o">=</code> <code class="n">makeRandomString</code><code class="o">();</code>&#13;
    <code class="o">}</code>&#13;
    <code class="n">map</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ConcurrentHashMap</code><code class="o">&lt;&gt;();</code>&#13;
<code class="o">}</code>&#13;
&#13;
<code class="nd">@Benchmark</code>&#13;
<code class="kd">public</code> <code class="kt">void</code> <code class="nf">testIntern</code><code class="o">(</code><code class="n">Blackhole</code> <code class="n">bh</code><code class="o">)</code> <code class="o">{</code>&#13;
    <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">nStrings</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
        <code class="n">String</code> <code class="n">t</code> <code class="o">=</code> <code class="n">strings</code><code class="o">[</code><code class="n">i</code><code class="o">].</code><code class="na">intern</code><code class="o">();</code>&#13;
        <code class="n">bh</code><code class="o">.</code><code class="na">consume</code><code class="o">(</code><code class="n">t</code><code class="o">);</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>The <code>Level</code> value given to the <code>Setup</code> annotation controls when the given&#13;
method is executed. <code>Level</code> can take one of three values:</p>&#13;
<dl>&#13;
<dt><code>Level.Trial</code></dt>&#13;
<dd>&#13;
<p>The setup is done once, when the benchmark code initializes.</p>&#13;
</dd>&#13;
<dt><code>Level.Iteration</code></dt>&#13;
<dd>&#13;
<p>The setup is done before each iteration of the benchmark (each measurement cycle).</p>&#13;
</dd>&#13;
<dt><code>Level.Invocation</code></dt>&#13;
<dd>&#13;
<p>The setup is done before each time the test method is executed.</p>&#13;
</dd>&#13;
</dl>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775557890936">&#13;
<h5>Setup Code and Low-Level Tests</h5>&#13;
<p>Some of the <code>jmh</code> tests in later chapters will report that particular operations&#13;
differ by only a few nanoseconds. Those tests are not really measuring individual&#13;
operations and reporting something that took nanoseconds; they are measuring&#13;
a lot of operations and reporting the average (within statistical variance).&#13;
<code>jmh</code> manages all of that for us.</p>&#13;
&#13;
<p>This leads to an important caveat: when there is setup code to execute on&#13;
every invocation, <code>jmh</code> can have a lot of difficulty performing this average&#13;
invocation analysis. For that (and other) reasons, it is recommended to use&#13;
the <code>Level.Invocation</code> annotation rarely, and only for test methods that&#13;
themselves take a long period of time.</p>&#13;
</div></aside>&#13;
&#13;
<p>A similar <code>Teardown</code> annotation could be used in other cases to&#13;
clear state if required.</p>&#13;
&#13;
<p><code>jmh</code> has many additional options, including measuring single&#13;
invocations of a method or measuring average time instead of throughput, passing&#13;
additional JVM arguments to a forked JVM, controlling thread synchronization,&#13;
and more. My goal isn’t to provide a complete reference for <code>jmh</code>; rather,&#13;
this example ideally shows the complexity involved even in writing&#13;
a simple microbenchmark.<a data-startref="ix_ch02-asciidoc18" data-type="indexterm" id="idm45775557884440"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Controlling execution and repeatability" data-type="sect3"><div class="sect3" id="idm45775558021384">&#13;
<h3>Controlling execution and repeatability</h3>&#13;
&#13;
<p><a data-primary="jmh" data-secondary="controlling execution and repeatability" data-type="indexterm" id="idm45775557882200"/>Once you have a correct microbenchmark, you’ll need to run it in such a way&#13;
that the results make statistical sense for what you are measuring.</p>&#13;
&#13;
<p>As you’ve just seen, by default <code>jmh</code> will run the target method for a period&#13;
of 10 seconds by executing it as many times as necessary over that interval&#13;
(so in the previous example, it was executed an average of 1,772 times over 10&#13;
seconds). Each 10-second test is an iteration, and by default there were five&#13;
warm-up iterations (where the results were discarded) and five measurement&#13;
iterations each time a new JVM was forked. And that was all repeated for&#13;
five trials.</p>&#13;
&#13;
<p>All of that is done so <code>jmh</code> can perform the statistical analysis to&#13;
calculate the confidence interval in the result. In the cases presented&#13;
earlier, the 99.9%&#13;
confidence interval has a range of about 10%, which may or may not be&#13;
sufficient when comparing to other benchmarks.</p>&#13;
&#13;
<p>We can get a smaller or larger confidence interval by varying these parameters.&#13;
For example, here are the results from running the two benchmarks with a&#13;
low number of measurement iterations and trials:</p>&#13;
&#13;
<pre data-type="programlisting">Benchmark               (nStrings)   Mode  Cnt    Score     Error  Units&#13;
MyBenchmark.testIntern       10000  thrpt    4  233.359 ±  95.304  ops/s&#13;
MyBenchmark.testMap          10000  thrpt    4  354.341 ± 166.491  ops/s</pre>&#13;
&#13;
<p>That result makes it look like using the <code>intern()</code> method is far worse than&#13;
using a map, but look at the range: it is possible that the real result of the&#13;
first case is close to 330 ops/s, while the real result of the second case is&#13;
close to 200 ops/s. Even if that’s unlikely, the ranges here are too broad to&#13;
conclusively decide which is better.</p>&#13;
&#13;
<p>That result is from having only two forked trials of two iterations each. If&#13;
we increase that to 10 iterations each, we get a better result:</p>&#13;
&#13;
<pre data-type="programlisting">MyBenchmark.testIntern       10000  thrpt   20  172.738 ± 29.058  ops/s&#13;
MyBenchmark.testMap          10000  thrpt   20  305.618 ± 22.754  ops/s</pre>&#13;
&#13;
<p>Now the ranges are discrete, and we can confidently conclude that the map&#13;
technique is superior (at least with respect to a test with a 100% cache&#13;
hit-rate and 10,000 unchanging strings).</p>&#13;
&#13;
<p>There is no hard-and-fast rule as to how many iterations, how many forked&#13;
trials, or what&#13;
length of execution will be needed to get enough data so that the results are&#13;
clear like this. If you are comparing two techniques that have little&#13;
difference between them, you’ll need a lot more iterations and trials. On the&#13;
other hand, if they’re that close together, perhaps you’re better off looking&#13;
at something that will have a greater impact on performance. This again is&#13;
a place where art influences science; at some point, you’ll have to decide&#13;
for yourself where the boundaries lie.</p>&#13;
&#13;
<p>All of these variables—number of iterations, length of each interval, etc.—are controlled via command-line arguments to the standard <code>jmh</code> benchmark.&#13;
Here are the most relevant ones:</p>&#13;
<dl>&#13;
<dt><code>-f 5</code></dt>&#13;
<dd>&#13;
<p>Number of forked trials to run (default: 5).</p>&#13;
</dd>&#13;
<dt><code>-wi 5</code></dt>&#13;
<dd>&#13;
<p>Number of warm-up iterations per trial (default: 5).</p>&#13;
</dd>&#13;
<dt><code>-i 5</code></dt>&#13;
<dd>&#13;
<p>Number of measurement iterations per trial (default: 5).</p>&#13;
</dd>&#13;
<dt><code>-r 10</code></dt>&#13;
<dd>&#13;
<p>Minimum length of time (in seconds) of each iteration; an iteration&#13;
may run longer than this, depending on the actual length of the target method.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Increasing these parameters will generally lower the variability of a result&#13;
until you have the desired confidence range. Conversely, for more stable tests,&#13;
lowering these parameters will generally reduce the time required to run&#13;
the test.</p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p><code>jmh</code> is a framework and harness for writing microbenchmarks that provides help to properly address the requirements of such benchmarks.</p>&#13;
</li>&#13;
<li>&#13;
<p><code>jmh</code> isn’t a replacement for deeply thinking about writing the code that you measure; it’s simply a useful tool in its <span class="keep-together">development.</span><a data-startref="ix_ch02-asciidoc17" data-type="indexterm" id="idm45775557860264"/><a data-startref="ix_ch02-asciidoc16" data-type="indexterm" id="idm45775557859560"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Common Code Examples" data-type="sect2"><div class="sect2" id="idm45775558334424">&#13;
<h2>Common Code Examples</h2>&#13;
&#13;
<p><a data-primary="benchmarks" data-secondary="common code examples" data-type="indexterm" id="ix_ch02-asciidoc19"/>Many of the examples in this book are based on a sample application&#13;
that calculates the “historical” high and low price of a stock over a range&#13;
of dates, as well as the standard deviation during that time.&#13;
<em>Historical</em> is in quotes here because in the application, all the data is&#13;
fictional; the prices and the stock symbols are randomly generated.</p>&#13;
&#13;
<p>The basic object within the application is a&#13;
<code class="keep-together">StockPrice</code>&#13;
object that represents the price range of a stock on a given day, along with&#13;
a collection of option prices for that stock:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code> <code class="kd">interface</code> <code class="nc">StockPrice</code> <code class="o">{</code>&#13;
    <code class="n">String</code> <code class="nf">getSymbol</code><code class="o">();</code>&#13;
    <code class="n">Date</code> <code class="nf">getDate</code><code class="o">();</code>&#13;
    <code class="n">BigDecimal</code> <code class="nf">getClosingPrice</code><code class="o">();</code>&#13;
    <code class="n">BigDecimal</code> <code class="nf">getHigh</code><code class="o">();</code>&#13;
    <code class="n">BigDecimal</code> <code class="nf">getLow</code><code class="o">();</code>&#13;
    <code class="n">BigDecimal</code> <code class="nf">getOpeningPrice</code><code class="o">();</code>&#13;
    <code class="kt">boolean</code> <code class="nf">isYearHigh</code><code class="o">();</code>&#13;
    <code class="kt">boolean</code> <code class="nf">isYearLow</code><code class="o">();</code>&#13;
    <code class="n">Collection</code><code class="o">&lt;?</code> <code class="kd">extends</code> <code class="n">StockOptionPrice</code><code class="o">&gt;</code> <code class="n">getOptions</code><code class="o">();</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>The sample applications typically deal with a collection of these prices,&#13;
representing the history of the stock over a period of time (e.g., 1 year or 25 years,&#13;
depending on the example):</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code> <code class="kd">interface</code> <code class="nc">StockPriceHistory</code> <code class="o">{</code>&#13;
    <code class="n">StockPrice</code> <code class="nf">getPrice</code><code class="o">(</code><code class="n">Date</code> <code class="n">d</code><code class="o">);</code>&#13;
    <code class="n">Collection</code><code class="o">&lt;</code><code class="n">StockPrice</code><code class="o">&gt;</code> <code class="nf">getPrices</code><code class="o">(</code><code class="n">Date</code> <code class="n">startDate</code><code class="o">,</code> <code class="n">Date</code> <code class="n">endDate</code><code class="o">);</code>&#13;
    <code class="n">Map</code><code class="o">&lt;</code><code class="n">Date</code><code class="o">,</code> <code class="n">StockPrice</code><code class="o">&gt;</code> <code class="n">getAllEntries</code><code class="o">();</code>&#13;
    <code class="n">Map</code><code class="o">&lt;</code><code class="n">BigDecimal</code><code class="o">,</code><code class="n">ArrayList</code><code class="o">&lt;</code><code class="n">Date</code><code class="o">&gt;&gt;</code> <code class="n">getHistogram</code><code class="o">();</code>&#13;
    <code class="n">BigDecimal</code> <code class="nf">getAveragePrice</code><code class="o">();</code>&#13;
    <code class="n">Date</code> <code class="nf">getFirstDate</code><code class="o">();</code>&#13;
    <code class="n">BigDecimal</code> <code class="nf">getHighPrice</code><code class="o">();</code>&#13;
    <code class="n">Date</code> <code class="nf">getLastDate</code><code class="o">();</code>&#13;
    <code class="n">BigDecimal</code> <code class="nf">getLowPrice</code><code class="o">();</code>&#13;
    <code class="n">BigDecimal</code> <code class="nf">getStdDev</code><code class="o">();</code>&#13;
    <code class="n">String</code> <code class="nf">getSymbol</code><code class="o">();</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>The basic implementation of this class loads a set of prices from the&#13;
database:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code> <code class="kd">class</code> <code class="nc">StockPriceHistoryImpl</code> <code class="kd">implements</code> <code class="n">StockPriceHistory</code> <code class="o">{</code>&#13;
    <code class="o">...</code>&#13;
    <code class="kd">public</code> <code class="nf">StockPriceHistoryImpl</code><code class="o">(</code><code class="n">String</code> <code class="n">s</code><code class="o">,</code> <code class="n">Date</code> <code class="n">startDate</code><code class="o">,</code>&#13;
        <code class="n">Date</code> <code class="n">endDate</code><code class="o">,</code> <code class="n">EntityManager</code> <code class="n">em</code><code class="o">)</code> <code class="o">{</code>&#13;
        <code class="n">Date</code> <code class="n">curDate</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Date</code><code class="o">(</code><code class="n">startDate</code><code class="o">.</code><code class="na">getTime</code><code class="o">());</code>&#13;
        <code class="n">symbol</code> <code class="o">=</code> <code class="n">s</code><code class="o">;</code>&#13;
        <code class="k">while</code> <code class="o">(!</code><code class="n">curDate</code><code class="o">.</code><code class="na">after</code><code class="o">(</code><code class="n">endDate</code><code class="o">))</code> <code class="o">{</code>&#13;
            <code class="n">StockPriceImpl</code> <code class="n">sp</code> <code class="o">=</code> <code class="n">em</code><code class="o">.</code><code class="na">find</code><code class="o">(</code><code class="n">StockPriceImpl</code><code class="o">.</code><code class="na">class</code><code class="o">,</code>&#13;
                         <code class="k">new</code> <code class="nf">StockPricePK</code><code class="o">(</code><code class="n">s</code><code class="o">,</code> <code class="o">(</code><code class="n">Date</code><code class="o">)</code> <code class="n">curDate</code><code class="o">.</code><code class="na">clone</code><code class="o">()));</code>&#13;
            <code class="k">if</code> <code class="o">(</code><code class="n">sp</code> <code class="o">!=</code> <code class="kc">null</code><code class="o">)</code> <code class="o">{</code>&#13;
                <code class="n">Date</code> <code class="n">d</code> <code class="o">=</code> <code class="o">(</code><code class="n">Date</code><code class="o">)</code> <code class="n">curDate</code><code class="o">.</code><code class="na">clone</code><code class="o">();</code>&#13;
                <code class="k">if</code> <code class="o">(</code><code class="n">firstDate</code> <code class="o">==</code> <code class="kc">null</code><code class="o">)</code> <code class="o">{</code>&#13;
                    <code class="n">firstDate</code> <code class="o">=</code> <code class="n">d</code><code class="o">;</code>&#13;
                <code class="o">}</code>&#13;
                <code class="n">prices</code><code class="o">.</code><code class="na">put</code><code class="o">(</code><code class="n">d</code><code class="o">,</code> <code class="n">sp</code><code class="o">);</code>&#13;
                <code class="n">lastDate</code> <code class="o">=</code> <code class="n">d</code><code class="o">;</code>&#13;
            <code class="o">}</code>&#13;
            <code class="n">curDate</code><code class="o">.</code><code class="na">setTime</code><code class="o">(</code><code class="n">curDate</code><code class="o">.</code><code class="na">getTime</code><code class="o">()</code> <code class="o">+</code> <code class="n">msPerDay</code><code class="o">);</code>&#13;
        <code class="o">}</code>&#13;
    <code class="o">}</code>&#13;
    <code class="o">...</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>The architecture of the samples is designed to be loaded from a database, and that functionality&#13;
will be used in&#13;
the examples in <a data-type="xref" href="ch11.html#Database">Chapter 11</a>. However, to facilitate running the examples,&#13;
most of the time they will use a mock entity manager that&#13;
generates random data for the series. In essence, most examples are&#13;
module-level meso<span class="keep-together">benchmarks</span> that are suitable for illustrating the performance&#13;
issues at hand—but we would have an idea of the actual performance&#13;
of the application only when the full application is run (as in <a data-type="xref" href="ch11.html#Database">Chapter 11</a>).</p>&#13;
&#13;
<p>One caveat is that numerous examples are therefore dependent on the&#13;
performance of the random number generator in use. Unlike the&#13;
microbenchmark example, this is by design, as it allows the&#13;
illustration of several performance issues in Java. (For that&#13;
matter, the goal of the examples is to measure the performance of an&#13;
arbitrary thing, and the performance of the random number generator fits&#13;
that goal. That is quite different from a microbenchmark, where&#13;
including the time for generating random numbers would affect the overall&#13;
calculation.)</p>&#13;
&#13;
<p>The examples also heavily depend on the performance of the&#13;
<code class="keep-together">BigDecimal</code>&#13;
class, which is used to store all the data points. This is&#13;
a standard choice for storing currency data; if the currency data is stored&#13;
as primitive&#13;
<code class="keep-together">double</code>&#13;
objects, rounding of half-pennies and smaller&#13;
amounts becomes quite problematic. From the perspective of writing examples,&#13;
that choice is also useful as it allows some “business logic” or&#13;
lengthy calculation to occur—particularly in calculating the standard&#13;
deviation of a series of prices. The standard deviation relies on knowing&#13;
the square root of a&#13;
<code class="keep-together">BigDecimal</code>&#13;
number. The standard Java API doesn’t&#13;
supply such a routine, but the examples use this method:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code> <code class="kd">static</code> <code class="n">BigDecimal</code> <code class="nf">sqrtB</code><code class="o">(</code><code class="n">BigDecimal</code> <code class="n">bd</code><code class="o">)</code> <code class="o">{</code>&#13;
    <code class="n">BigDecimal</code> <code class="n">initial</code> <code class="o">=</code> <code class="n">bd</code><code class="o">;</code>&#13;
    <code class="n">BigDecimal</code> <code class="n">diff</code><code class="o">;</code>&#13;
    <code class="k">do</code> <code class="o">{</code>&#13;
        <code class="n">BigDecimal</code> <code class="n">sDivX</code> <code class="o">=</code> <code class="n">bd</code><code class="o">.</code><code class="na">divide</code><code class="o">(</code><code class="n">initial</code><code class="o">,</code> <code class="mi">8</code><code class="o">,</code> <code class="n">RoundingMode</code><code class="o">.</code><code class="na">FLOOR</code><code class="o">);</code>&#13;
        <code class="n">BigDecimal</code> <code class="n">sum</code> <code class="o">=</code> <code class="n">sDivX</code><code class="o">.</code><code class="na">add</code><code class="o">(</code><code class="n">initial</code><code class="o">);</code>&#13;
        <code class="n">BigDecimal</code> <code class="n">div</code> <code class="o">=</code> <code class="n">sum</code><code class="o">.</code><code class="na">divide</code><code class="o">(</code><code class="n">TWO</code><code class="o">,</code> <code class="mi">8</code><code class="o">,</code> <code class="n">RoundingMode</code><code class="o">.</code><code class="na">FLOOR</code><code class="o">);</code>&#13;
        <code class="n">diff</code> <code class="o">=</code> <code class="n">div</code><code class="o">.</code><code class="na">subtract</code><code class="o">(</code><code class="n">initial</code><code class="o">).</code><code class="na">abs</code><code class="o">();</code>&#13;
        <code class="n">diff</code><code class="o">.</code><code class="na">setScale</code><code class="o">(</code><code class="mi">8</code><code class="o">,</code> <code class="n">RoundingMode</code><code class="o">.</code><code class="na">FLOOR</code><code class="o">);</code>&#13;
        <code class="n">initial</code> <code class="o">=</code> <code class="n">div</code><code class="o">;</code>&#13;
    <code class="o">}</code> <code class="k">while</code> <code class="o">(</code><code class="n">diff</code><code class="o">.</code><code class="na">compareTo</code><code class="o">(</code><code class="n">error</code><code class="o">)</code> <code class="o">&gt;</code> <code class="mi">0</code><code class="o">);</code>&#13;
    <code class="k">return</code> <code class="n">initial</code><code class="o">;</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>This is an implementation of the Babylonian method for estimating the square&#13;
root of a number. It isn’t the most efficient implementation; in particular,&#13;
the initial guess could be much better, which would save some iterations. That&#13;
is deliberate because it allows the calculation to take some time&#13;
(emulating business logic), though it does illustrate the basic point made&#13;
in <a data-type="xref" href="ch01.html#Introduction">Chapter 1</a>: often the better way to make Java code faster is to write&#13;
a better algorithm, independent of any Java tuning or Java coding practices&#13;
that are employed.</p>&#13;
&#13;
<p>The standard deviation, average price, and histogram of an implementation of&#13;
the&#13;
<code class="keep-together">StockPriceHistory</code>&#13;
interface are all derived values. In different&#13;
examples, these values will be calculated eagerly (when the data is loaded from the entity manager) or lazily (when the method to retrieve the data is called).&#13;
Similarly, the&#13;
<code class="keep-together">StockPrice</code>&#13;
interface references a&#13;
<code class="keep-together">StockOptionPrice</code>&#13;
interface, which is the price of certain options for the given stock on the&#13;
given day. Those option values can be retrieved from the entity manager&#13;
either eagerly or lazily. In both cases, the definition of these interfaces&#13;
allows these approaches to be compared in different situations.</p>&#13;
&#13;
<p>These interfaces also fit naturally into a Java REST application: users can&#13;
make a call with parameters indicating the symbol and date range for a stock&#13;
they are interested in. <a data-primary="JAX-RS (Java API for RESTful Web Services)" data-type="indexterm" id="idm45775557360344"/>In the standard example, the request will go through&#13;
a standard calling using the Java API for RESTful Web Services (JAX-RS) call that parses the input parameters, calls an embedded JPA bean to get the underlying data, and&#13;
forwards the response:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting">    <code class="nd">@GET</code>&#13;
    <code class="nd">@Produces</code><code class="o">(</code><code class="n">MediaType</code><code class="o">.</code><code class="na">APPLICATION_JSON</code><code class="o">)</code>&#13;
    <code class="kd">public</code> <code class="n">JsonObject</code> <code class="nf">getStockInfo</code><code class="o">(</code>&#13;
        <code class="nd">@DefaultValue</code><code class="o">(</code><code class="s">""</code> <code class="o">+</code> <code class="n">StockPriceHistory</code><code class="o">.</code><code class="na">STANDARD</code><code class="o">)</code>&#13;
              <code class="nd">@QueryParam</code><code class="o">(</code><code class="s">"impl"</code><code class="o">)</code> <code class="kt">int</code> <code class="n">impl</code><code class="o">,</code>&#13;
        <code class="nd">@DefaultValue</code><code class="o">(</code><code class="s">"true"</code><code class="o">)</code> <code class="nd">@QueryParam</code><code class="o">(</code><code class="s">"doMock"</code><code class="o">)</code> <code class="kt">boolean</code> <code class="n">doMock</code><code class="o">,</code>&#13;
        <code class="nd">@DefaultValue</code><code class="o">(</code><code class="s">""</code><code class="o">)</code> <code class="nd">@QueryParam</code><code class="o">(</code><code class="s">"symbol"</code><code class="o">)</code> <code class="n">String</code> <code class="n">symbol</code><code class="o">,</code>&#13;
        <code class="nd">@DefaultValue</code><code class="o">(</code><code class="s">"01/01/2019"</code><code class="o">)</code> <code class="nd">@QueryParam</code><code class="o">(</code><code class="s">"start"</code><code class="o">)</code> <code class="n">String</code> <code class="n">start</code><code class="o">,</code>&#13;
        <code class="nd">@DefaultValue</code><code class="o">(</code><code class="s">"01/01/2034"</code><code class="o">)</code> <code class="nd">@QueryParam</code><code class="o">(</code><code class="s">"end"</code><code class="o">)</code> <code class="n">String</code> <code class="n">end</code><code class="o">,</code>&#13;
        <code class="nd">@DefaultValue</code><code class="o">(</code><code class="s">"0"</code><code class="o">)</code> <code class="nd">@QueryParam</code><code class="o">(</code><code class="s">"save"</code><code class="o">)</code> <code class="kt">int</code> <code class="n">saveCount</code>&#13;
        <code class="o">)</code> <code class="kd">throws</code> <code class="n">ParseException</code> <code class="o">{</code>&#13;
&#13;
        <code class="n">StockPriceHistory</code> <code class="n">sph</code><code class="o">;</code>&#13;
        <code class="n">EntityManager</code> <code class="n">em</code><code class="o">;</code>&#13;
        <code class="n">DateFormat</code> <code class="n">df</code> <code class="o">=</code> <code class="n">localDateFormatter</code><code class="o">.</code><code class="na">get</code><code class="o">();</code>  <code class="c1">// Thread-local</code>&#13;
        <code class="n">Date</code> <code class="n">startDate</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">start</code><code class="o">);</code>&#13;
        <code class="n">Date</code> <code class="n">endDate</code> <code class="o">=</code> <code class="n">df</code><code class="o">.</code><code class="na">parse</code><code class="o">(</code><code class="n">end</code><code class="o">);</code>&#13;
&#13;
        <code class="n">em</code> <code class="o">=</code> <code class="c1">// ... get the entity manager based on the test permutation</code>&#13;
        <code class="n">sph</code> <code class="o">=</code> <code class="n">em</code><code class="o">.</code><code class="na">find</code><code class="o">(...</code><code class="na">based</code> <code class="n">on</code> <code class="n">arguments</code><code class="o">...);</code>&#13;
        <code class="k">return</code> <code class="n">JSON</code><code class="o">.</code><code class="na">createObjectBuilder</code><code class="o">()</code>&#13;
                <code class="o">.</code><code class="na">add</code><code class="o">(</code><code class="s">"symbol"</code><code class="o">,</code> <code class="n">sph</code><code class="o">.</code><code class="na">getSymbol</code><code class="o">())</code>&#13;
                <code class="o">.</code><code class="na">add</code><code class="o">(</code><code class="s">"high"</code><code class="o">,</code> <code class="n">sph</code><code class="o">.</code><code class="na">getHighPrice</code><code class="o">())</code>&#13;
                <code class="o">.</code><code class="na">add</code><code class="o">(</code><code class="s">"low"</code><code class="o">,</code> <code class="n">sph</code><code class="o">.</code><code class="na">getLowPrice</code><code class="o">())</code>&#13;
                <code class="o">.</code><code class="na">add</code><code class="o">(</code><code class="s">"average"</code><code class="o">,</code> <code class="n">sph</code><code class="o">.</code><code class="na">getAveragePrice</code><code class="o">())</code>&#13;
                <code class="o">.</code><code class="na">add</code><code class="o">(</code><code class="s">"stddev"</code><code class="o">,</code> <code class="n">sph</code><code class="o">.</code><code class="na">getStdDev</code><code class="o">())</code>&#13;
                <code class="o">.</code><code class="na">build</code><code class="o">();</code>&#13;
    <code class="o">}</code></pre>&#13;
&#13;
<p>This class can inject different implementations of the history bean&#13;
(for eager or lazy initialization, among other things); it will optionally&#13;
cache the data retrieved from the backend database (or mock entity manager).&#13;
Those are the common options when dealing with the performance of an&#13;
enterprise application (in particular, caching data in the middle tier is&#13;
sometimes considered the big performance advantage of an application&#13;
server). Examples throughout the book examine those trade-offs as well<a data-startref="ix_ch02-asciidoc19" data-type="indexterm" id="idm45775557294232"/>.<a data-startref="ix_ch02-asciidoc15" data-type="indexterm" id="idm45775557293496"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45775557858248">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Performance testing involves trade-offs. Making good choices&#13;
among competing options is crucial to successfully tracking the performance&#13;
characteristics of a system.</p>&#13;
&#13;
<p>Choosing what to test is the first area where experience with the application&#13;
and intuition will be of immeasurable help when setting up performance tests.&#13;
Micro<span class="keep-together">benchmarks</span> are helpful&#13;
to set a guideline for certain operations. That leaves a broad&#13;
continuum of other tests, from small module-level tests to a large,&#13;
multitiered environment. Tests all along that continuum have merit,&#13;
and choosing the tests along that continuum is one place where experience&#13;
and intuition will come into play. However, in the end there can be no&#13;
substitute for testing a full application as it is deployed in production;&#13;
only then can the full effect of all performance-related issues be understood.</p>&#13;
&#13;
<p>Similarly, understanding what is and is not an actual regression in code is&#13;
not always a black-and-white issue. Programs always exhibit random behavior,&#13;
and once randomness is injected into the equation, we will never be 100%&#13;
certain about what data means. Applying statistical analysis&#13;
to the results can help turn the analysis to a more objective path, but&#13;
even then some subjectivity will be involved. Understanding the underlying&#13;
probabilities and what they mean can help to reduce that subjectivity.</p>&#13;
&#13;
<p>Finally, with these foundations in place, an automated testing system&#13;
can be set up to gather full information about everything that&#13;
occurred during the test. With the knowledge of what’s going on and what the underlying tests mean, the performance analyst can apply&#13;
both science and art so that the program can exhibit the best possible&#13;
performance.<a data-startref="ix_ch02-asciidoc0" data-type="indexterm" id="idm45775557079624"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45775552311416"><sup><a href="ch02.html#idm45775552311416-marker">1</a></sup> Not that garbage collection should be expected to introduce a hundred-second delay, but particularly for tests with small average response times, the GC pauses can introduce significant outliers.</p><p data-type="footnote" id="idm45775552239592"><sup><a href="ch02.html#idm45775552239592-marker">2</a></sup> And though three data points makes it easier to understand an example, it is too small to be accurate for any real system.</p><p data-type="footnote" id="idm45775552237160"><sup><a href="ch02.html#idm45775552237160-marker">3</a></sup> <em>Student</em>, by the way, is the pen name of the scientist who first published the test; it isn’t named that way to remind you of graduate school, where you (or at least I) slept through statistics class.</p></div></div></section></body></html>