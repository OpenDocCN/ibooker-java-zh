<html><head></head><body><section data-pdf-bookmark="Chapter 9. Threading and Synchronization Performance" data-type="chapter" epub:type="chapter"><div class="chapter" id="ThreadPerformance">&#13;
<h1><span class="label">Chapter 9. </span>Threading and Synchronization Performance</h1>&#13;
&#13;
&#13;
<p><a data-primary="threading" data-type="indexterm" id="ix_ch09-asciidoc0"/><a data-primary="threading/synchronization performance" data-type="indexterm" id="ix_ch09-asciidoc1"/>From its first days, some of Java’s appeal has been because it is&#13;
multithreaded. Even in the days before multicore and multi-CPU systems&#13;
were the norm, the ability to easily write threaded programs in Java has&#13;
been considered one of its hallmark features.</p>&#13;
&#13;
<p>In performance terms, the appeal is obvious: if two CPUs are available,&#13;
an application might be able to do twice as much work or the&#13;
same amount of work&#13;
twice as fast. This assumes that the task can be broken into&#13;
discrete segments, since Java is not an autoparallelizing language that will&#13;
figure out the algorithmic parts. Fortunately, computing today is&#13;
often about discrete tasks: a server handling simultaneous requests from&#13;
discrete clients, a batch job performing the same operation on a series&#13;
of data, mathematical algorithms that break into constituent parts, and&#13;
so on.</p>&#13;
&#13;
<p>This chapter explores how to get the maximum performance out of&#13;
Java threading and synchronization facilities.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Threading and Hardware" data-type="sect1"><div class="sect1" id="idm45775549629416">&#13;
<h1>Threading and Hardware</h1>&#13;
&#13;
<p><a data-primary="threading" data-secondary="hardware and" data-type="indexterm" id="idm45775549628216"/>Recall the discussion from <a data-type="xref" href="ch01.html#Introduction">Chapter 1</a> about multicore systems and&#13;
hyper-threaded systems. Threading at the software level allows us to take&#13;
advantage of a machine’s multiple cores and hyper-threads.</p>&#13;
&#13;
<p>Doubling the cores on a machine allows us to double the performance of&#13;
our correctly written application, though as we discussed in <a data-type="xref" href="ch01.html#Introduction">Chapter 1</a>,&#13;
adding hyper-threading to a CPU does not&#13;
double its performance.</p>&#13;
&#13;
<p>Almost all examples in this chapter are run on a machine with four&#13;
single-threaded CPUs—the exception being the first example that shows the&#13;
difference between hyper-threaded and non-hyper-threaded CPUs. After that,&#13;
we will look at scaling only in terms of single-threaded CPU cores so that&#13;
we can better understand the performance effects of adding threads. That&#13;
is not to say that hyper-threaded CPUs aren’t important; the 20%–40%&#13;
performance boost from that extra hardware thread will certainly improve the&#13;
overall performance or throughput of your application. From a Java perspective,&#13;
we should still consider the hyper-threads as actual CPUs and tune our&#13;
application running on a four-core, eight hyper-thread machine as if it&#13;
had eight CPUs. But from a measurement perspective, we should be expecting&#13;
only a five-to-six times improvement compared to a single core.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Thread Pools and ThreadPoolExecutors" data-type="sect1"><div class="sect1" id="idm45775549623240">&#13;
<h1>Thread Pools and ThreadPoolExecutors</h1>&#13;
&#13;
<p><a data-primary="thread pools" data-type="indexterm" id="ix_ch09-asciidoc2"/><a data-primary="threading" data-secondary="thread pools/ThreadPoolExecutors" data-type="indexterm" id="ix_ch09-asciidoc3"/>Threads can be managed by custom code in Java, or applications&#13;
can utilize&#13;
a thread pool. Java servers are typically built&#13;
around the notion of&#13;
one or more thread pools to handle requests: each call&#13;
into the server is handled by a (potentially different) thread from the&#13;
pool. <a data-primary="ThreadPoolExecutor class" data-type="indexterm" id="idm45775549618952"/>Similarly, other applications can use Java’s&#13;
<code class="keep-together">ThreadPoolExecutor</code>&#13;
to execute tasks in parallel.</p>&#13;
&#13;
<p>In fact, some server frameworks use instances of the&#13;
<code class="keep-together">ThreadPoolExecutor</code>&#13;
class to manage&#13;
their tasks, though many have written their own thread pools (if only because&#13;
they predate the addition of&#13;
<code class="keep-together">ThreadPoolExecutor</code>&#13;
to the Java API).&#13;
Although the implementation of the pools in these cases might differ, the&#13;
basic concepts are the same, and both are discussed in this section.</p>&#13;
&#13;
<p>The key factor in using a thread pool is that tuning the&#13;
size of the pool is crucial to getting the best performance.&#13;
Thread pool performance varies depending on&#13;
basic choices about thread pool size, and under certain circumstances&#13;
an oversized thread pool will be detrimental to performance.</p>&#13;
&#13;
<p>All thread pools work in essentially the same way. Tasks are submitted to a&#13;
queue (there can&#13;
be more than one queue, but the concept is the same). Then a certain number&#13;
of threads picks up tasks from the queue and executes them. The result of the&#13;
task can be sent back to the client (e.g., in the case of a&#13;
server), stored in a database, stored in an internal data structure, or&#13;
whatever. But after finishing the task, the thread returns to the task queue&#13;
to retrieve another job to execute (and if there are no more tasks to&#13;
perform, the thread waits for a task).</p>&#13;
&#13;
<p>Thread pools have a minimum and maximum number of threads.&#13;
The minimum number of threads is kept around, waiting for tasks to be&#13;
assigned to them. Because creating a thread is a fairly expensive operation,&#13;
this speeds up the overall operation when a task is submitted: it is expected&#13;
that an already existing thread can pick it up. On the other&#13;
hand, threads require system resources—including native memory&#13;
for their stacks—and having too many idle threads can consume&#13;
resources that could be used by other processes. The&#13;
maximum number of threads also serves as a necessary throttle, preventing&#13;
too many tasks from executing at once.</p>&#13;
&#13;
<p>The terminology of the&#13;
<code class="keep-together">ThreadPoolExecutor</code>&#13;
and related classes is somewhat different. These classes refer to a&#13;
<em>core</em> pool size and <em>maximum</em> pool size, and the meaning of those&#13;
terms varies depending on how the pool is constructed. Sometimes the&#13;
core pool size is the minimum pool size, sometimes it is the maximum&#13;
pool size, and sometimes it is ignored altogether. Similarly, sometimes&#13;
the maximum pool size is the maximum size, but sometimes it is ignored.</p>&#13;
&#13;
<p>Details are given at the end of this section, but to keep&#13;
things simple, we’ll set the core and maximum sizes the same for our&#13;
tests and refer to only a maximum size. The thread pools in the&#13;
examples therefore always have the given number&#13;
of threads.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Setting the Maximum Number of Threads" data-type="sect2"><div class="sect2" id="idm45775549609288">&#13;
<h2>Setting the Maximum Number of Threads</h2>&#13;
&#13;
<p><a data-primary="thread pools" data-secondary="setting maximum number of threads" data-type="indexterm" id="ix_ch09-asciidoc4"/>Let’s address the maximum number of threads first: what is the optimal&#13;
maximum number of threads for a given workload on given hardware? There&#13;
is no simple answer; it depends on characteristics of the workload&#13;
and the hardware on which it is run.&#13;
In particular, the optimal number of threads depends on how often each&#13;
individual task will block.</p>&#13;
&#13;
<p>We’ll use a machine with four single-threaded CPUs for this discussion. Note&#13;
that it doesn’t matter if the system has only&#13;
four cores, if it has 128 cores but you want&#13;
<a data-primary="Docker container" data-type="indexterm" id="idm45775549604968"/>to utilize only four of them, or if you have a Docker container limiting the CPU&#13;
usage to four: the goal is to maximize the usage of those&#13;
four cores.</p>&#13;
&#13;
<p>Clearly, then, the maximum number of threads must be set to at least four.&#13;
Granted, some threads in the JVM are doing things other than processing these&#13;
tasks, but these threads will almost never need an entire core. One exception&#13;
is if a concurrent mode garbage collector is being used as discussed&#13;
in <a data-type="xref" href="ch05.html#GC">Chapter 5</a>—the background threads there must have enough CPU (cores) to operate,&#13;
lest they fall behind in processing the heap.</p>&#13;
&#13;
<p>Does it help to have more than four threads? This is where the characteristics&#13;
of the workload come into play. Take the simple case where the tasks&#13;
are all compute-bound: they don’t make external network calls (e.g., to a&#13;
database), nor do they have significant contention on an internal lock. The&#13;
stock price history batch program is such an application (when using a&#13;
mock entity manager):&#13;
the data on the entities can be calculated completely in parallel.</p>&#13;
&#13;
<p><a data-type="xref" href="#TablePool1">Table 9-1</a> shows the performance of calculating the history of 10,000&#13;
mock stock entities using a thread pool set to use the given number of&#13;
threads on a machine with four cores. With only a single thread in the pool,&#13;
55.2 seconds are needed to calculate the data set; with four threads, only&#13;
13.9 seconds are required. After that, a little more time is needed as&#13;
threads are added.</p>&#13;
<table id="TablePool1">&#13;
<caption><span class="label">Table 9-1. </span>Time required to calculate 10,000 mock price histories</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Number of threads</th>&#13;
<th>Seconds required</th>&#13;
<th>Percent of baseline</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>1</p></td>&#13;
<td><p>55.2 ± 0.6</p></td>&#13;
<td><p>100%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>2</p></td>&#13;
<td><p>28.3 ± 0.3</p></td>&#13;
<td><p>51.2%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>4</p></td>&#13;
<td><p>13.9 ± 0.6</p></td>&#13;
<td><p>25.1%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>8</p></td>&#13;
<td><p>14.3 ± 0.2</p></td>&#13;
<td><p>25.9%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>16</p></td>&#13;
<td><p>14.5 ± 0.3</p></td>&#13;
<td><p>26.2%</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>If the tasks in the application were completely parallel, the “Percent&#13;
of baseline” column would show 50% for two threads and 25% for four threads.&#13;
Such completely linear scaling is impossible to come by for several&#13;
reasons: if nothing else, the threads must&#13;
coordinate among themselves to pick a task from the run queue (and in&#13;
general, there is usually more synchronization among the threads). By the&#13;
time four threads are used, the system is consuming 100% of available CPU,&#13;
and although the machine may not be running any other user-level applications, various system-level processes will kick in and use some&#13;
CPU, preventing the JVM from utilizing all 100% of the cycles. Still, this application is doing a good job of scaling, and even&#13;
if the number of threads in the pool is overestimated, we have only a&#13;
slight penalty to pay.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775549583784">&#13;
<h5>The Effect of Hyper-Threading</h5>&#13;
<p><a data-primary="hyper-threaded CPU hardware" data-type="indexterm" id="idm45775549582616"/><a data-primary="thread pools" data-secondary="hyper-threaded CPU hardware" data-type="indexterm" id="idm45775549581848"/>What if those four CPUs were hyper-threaded so that we have two cores and&#13;
four hardware threads?&#13;
<a data-type="xref" href="#TablePool2">Table 9-2</a> shows the same experiment on such a machine.&#13;
As in the previous example, this appears to the JVM as a four-core&#13;
machine, but the scaling of the benchmark is quite different.</p>&#13;
<table id="TablePool2">&#13;
<caption><span class="label">Table 9-2. </span>Time required to calculate 10,000 mock price histories with hyper-threading</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Number of threads</th>&#13;
<th>Seconds required</th>&#13;
<th>Percent of baseline</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>1</p></td>&#13;
<td><p>55.7 ± 0.1</p></td>&#13;
<td><p>100%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>2</p></td>&#13;
<td><p>28.1 ± 0.4</p></td>&#13;
<td><p>50.4%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>4</p></td>&#13;
<td><p>25.5 ± 0.4</p></td>&#13;
<td><p>45.7%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>8</p></td>&#13;
<td><p>25.7 ± 0.2</p></td>&#13;
<td><p>46.1%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>16</p></td>&#13;
<td><p>26.0 ± 0.2</p></td>&#13;
<td><p>46.6%</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Now the program scales well from one to two CPUs, since they are both full&#13;
cores. But adding two hyper-threads gives us only a little&#13;
benefit. This is the worst-case scenario; the benefit of hyper-threading is&#13;
more apparent if the threads pause for I/O or wait for a lock held by&#13;
another thread. Still, adding a hyper-thread will usually give only a&#13;
20% improvement.</p>&#13;
</div></aside>&#13;
&#13;
<p>In other circumstances, though, the penalty for too many threads&#13;
can be larger. In the REST&#13;
version of the stock history calculator, having too many threads has a bigger&#13;
effect, as is shown in <a data-type="xref" href="#TablePool3">Table 9-3</a>. The application&#13;
server is configured to have the given number of threads, and&#13;
a load generator is sending 16 simultaneous requests to the server.</p>&#13;
<table id="TablePool3">&#13;
<caption><span class="label">Table 9-3. </span>Operations per second for mock stock prices through a REST server</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Number of threads</th>&#13;
<th>Operations per second</th>&#13;
<th>Percent of baseline</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>1</p></td>&#13;
<td><p>46.4</p></td>&#13;
<td><p>27%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>4</p></td>&#13;
<td><p>169.5</p></td>&#13;
<td><p>100%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>8</p></td>&#13;
<td><p>165.2</p></td>&#13;
<td><p>97%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>16</p></td>&#13;
<td><p>162.2</p></td>&#13;
<td><p>95%</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Given that the REST server has four available CPUs, maximum&#13;
throughput is achieved with that many threads in the pool.</p>&#13;
&#13;
<p><a data-type="xref" href="ch01.html#Introduction">Chapter 1</a> discussed the need to determine where the bottleneck&#13;
is when investigating performance issues. In this example, the&#13;
bottleneck is clearly the CPU: at four CPUs, the CPU is 100% utilized. Still,&#13;
the penalty for adding more threads in this case is somewhat minimal, at&#13;
least until there are four times too many threads.</p>&#13;
&#13;
<p>But what if the bottleneck is elsewhere? This example is also somewhat&#13;
unusual in that the tasks are completely CPU-bound: they do no I/O. Typically,&#13;
the threads might be expected to make calls to a database or write their&#13;
output somewhere or even rendezvous with another resource. In that case,&#13;
the CPU won’t necessarily be the bottleneck: that external resource might be.</p>&#13;
&#13;
<p>When that is the case, adding threads to the thread pool is&#13;
detrimental. Although I said (only somewhat&#13;
tongue in cheek) in <a data-type="xref" href="ch01.html#Introduction">Chapter 1</a> that the&#13;
database is always the bottleneck, the bottleneck can be any external&#13;
resource.</p>&#13;
&#13;
<p>As an example, consider the stock REST server with the roles reversed: what if&#13;
the goal is to make optimal use of the load generator machine (which, after&#13;
all, is simply running a threaded Java program)?</p>&#13;
&#13;
<p>In typical usage, if the REST application is run in a&#13;
server with four&#13;
CPUs and has only a single client requesting data, the REST server&#13;
will be about 25% busy, and the client machine will be almost idle. If the&#13;
load is&#13;
increased to four concurrent clients, the server will be&#13;
100% busy, and the client machine may be only 20% busy.</p>&#13;
&#13;
<p>Looking only at the client,&#13;
it is easy to conclude that because the client has a lot of excess CPU,&#13;
it should be possible to add more threads to the client and improve&#13;
its throughput.&#13;
<a data-type="xref" href="#TablePool4">Table 9-4</a> shows how wrong that assumption is: when&#13;
threads are added to the client, performance is drastically affected.</p>&#13;
<table id="TablePool4">&#13;
<caption><span class="label">Table 9-4. </span>Average response time for calculating mock stock price histories</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Number of client threads</th>&#13;
<th>Average response time</th>&#13;
<th>Percent of baseline</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>1</p></td>&#13;
<td><p>0.022 second</p></td>&#13;
<td><p>100%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>2</p></td>&#13;
<td><p>0.022 second</p></td>&#13;
<td><p>100%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>4</p></td>&#13;
<td><p>0.024 second</p></td>&#13;
<td><p>109%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>8</p></td>&#13;
<td><p>0.046 second</p></td>&#13;
<td><p>209%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>16</p></td>&#13;
<td><p>0.093 second</p></td>&#13;
<td><p>422%</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>32</p></td>&#13;
<td><p>0.187 second</p></td>&#13;
<td><p>885%</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Once the REST server is the bottleneck in this example (i.e., at&#13;
four client threads), adding load into the server is quite&#13;
harmful.</p>&#13;
&#13;
<p>This example may seem somewhat contrived. Who would add more client&#13;
threads when the server is already CPU-bound? But I’ve used this&#13;
example simply because it is easy to understand and uses only Java&#13;
programs. You can run it yourself to understand how it works,&#13;
without having to set up database connections and schemas and whatnot.</p>&#13;
&#13;
<p>The point is that the same principle holds here for a REST server&#13;
that is&#13;
sending requests to a database that is CPU- or I/O-bound.&#13;
You might look only at the&#13;
server’s CPU, see that is it well below 100% and that it has&#13;
additional requests to process, and assume&#13;
that increasing the number of threads in the server is a good idea.&#13;
That would lead to a big surprise,&#13;
because increasing the number of threads in&#13;
that situation will actually decrease the total throughput (and possibly&#13;
significantly), just as increasing the number of client threads did&#13;
in the Java-only example.</p>&#13;
&#13;
<p>This is another reason it is important to know where the actual&#13;
bottleneck in a system is: if load is increased into the bottleneck,&#13;
performance will decrease significantly. Conversely, if load&#13;
into the current bottleneck is reduced, performance will likely increase.</p>&#13;
&#13;
<p>This is also why self-tuning of thread pools is difficult. Thread pools&#13;
usually have some visibility into the amount of work that they&#13;
have pending and perhaps even how much CPU the machine has available—but&#13;
they usually have no visibility into other aspects of the entire environment&#13;
in which they are executing. Hence, adding threads when work is&#13;
pending—a key feature of many self-tuning thread pools (as well as&#13;
certain configurations of the&#13;
<span class="keep-together"><code>ThreadPoolExecutor</code>)</span>—is often exactly the&#13;
wrong thing to do.</p>&#13;
&#13;
<p>In <a data-type="xref" href="#TablePool4">Table 9-4</a>, the default configuration of the&#13;
REST server was to create 16 threads on the four-CPU machine. That makes sense&#13;
in a general default case, because the threads can be expected to make external&#13;
calls. When those calls block waiting for a response, other tasks can be&#13;
run, and the server will require more than four threads in order to execute&#13;
those tasks. So a default that creates a few too many threads is a reasonable&#13;
compromise: it will have a slight penalty for tasks that are primarily&#13;
CPU-bound, and it will allow increased throughput for running multiple tasks&#13;
that perform blocking I/O. Other servers might have created 32 threads by&#13;
default, which would have had a bigger penalty for our CPU-bound test, but&#13;
also had a bigger advantage for handling a load that is primarily I/O bound.</p>&#13;
&#13;
<p>Unfortunately, this is also why setting the maximum size of a&#13;
thread pool is often more art than science. In the real world,&#13;
a self-tuning thread pool may get you 80% to 90% of the possible performance of&#13;
the system under test, and overestimating the number of threads needed&#13;
in a pool may exact only a small penalty.&#13;
But when things go wrong with this sizing, they&#13;
can go wrong in a big way. Adequate testing in this regard is,&#13;
unfortunately, still a key requirement.<a data-startref="ix_ch09-asciidoc4" data-type="indexterm" id="idm45775549517624"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Setting the Minimum Number of Threads" data-type="sect2"><div class="sect2" id="idm45775549608344">&#13;
<h2>Setting the Minimum Number of Threads</h2>&#13;
&#13;
<p><a data-primary="thread pools" data-secondary="setting minimum number of threads" data-type="indexterm" id="ix_ch09-asciidoc5"/>Once the maximum number of threads in a thread pool has been determined,&#13;
it’s time to determine the minimum number of threads needed. To cut to the&#13;
chase, it rarely matters, and for simplicity sake in almost all cases,&#13;
you can set the minimum number of threads to the&#13;
same value as the maximum.</p>&#13;
&#13;
<p>The argument for setting the minimum number of threads to another value&#13;
(e.g., 1) is that it prevents the system from creating too many threads,&#13;
which saves on system resources. It is true that each thread requires a&#13;
certain amount of memory, particularly for its stack (which is discussed&#13;
later in this chapter). Again, though, following one of the general rules&#13;
from <a data-type="xref" href="ch02.html#SampleApplications">Chapter 2</a>, the system needs to be sized to handle the&#13;
maximum expected throughput, at which point it will need to&#13;
create all those threads. If the system can’t handle the&#13;
maximum number of threads, choosing a small minimum number of threads&#13;
doesn’t really help: if the system does hit the condition that requires the&#13;
maximum number of threads (and which it cannot handle), the system will&#13;
certainly be in the weeds.&#13;
Better to create all the threads that might eventually be needed and ensure&#13;
that the system can handle the maximum expected load.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775549511752">&#13;
<h5>Should You Pre-create Threads?</h5>&#13;
<p><a data-primary="thread pools" data-secondary="pre-creating threads for" data-type="indexterm" id="idm45775549510616"/><a data-primary="ThreadPoolExecutor class" data-secondary="and pre-creating threads" data-type="indexterm" id="idm45775549509576"/>By default, when you create a <code>ThreadPoolExecutor</code>, it will start with only&#13;
one thread. Consider a configuration where the pool asks for eight core threads&#13;
and sixteen maximum threads; in that case, the core setting could be considered&#13;
a minimum, since eight threads will be kept running even if they are idle.&#13;
But those eight threads still won’t be created when the pool is created;&#13;
they are created on demand, and then kept running.</p>&#13;
&#13;
<p>In a server, that means the first eight requests will be slightly delayed&#13;
while the thread is created. As you’ve seen in this section, that effect&#13;
is minor, but you can pre-create those threads (using the&#13;
<code>prestartAllCoreThreads()</code> method).</p>&#13;
</div></aside>&#13;
&#13;
<p>On the other hand, the downside to specifying a minimum number of threads&#13;
is fairly nominal. That downside occurs the first time there are multiple&#13;
tasks to execute: then the pool will&#13;
need to create a new thread. Creating threads is detrimental&#13;
to performance—which is why thread pools are needed in the first place—but this&#13;
one-time cost for creating the thread is likely to be unnoticed&#13;
as long as it then remains in the pool.</p>&#13;
&#13;
<p>In a batch application, it does not matter whether the thread is allocated when&#13;
the pool is created (which is what will occur if you set the minimum and&#13;
maximum number of&#13;
threads to the same value) or whether the thread is allocated on demand:&#13;
the time to execute the application will be the same.&#13;
In other applications, the&#13;
new threads are likely allocated during the warm-up period (and again, the total&#13;
time to allocate the threads is the same); the effect on the performance of&#13;
an application will be negligible. Even if the thread creation occurs during&#13;
the measurement cycle, as long as the thread creation is limited, it will&#13;
likely not be noticed.</p>&#13;
&#13;
<p>One other tuning that applies here is the idle time for a thread. Say that&#13;
the pool is sized with a minimum of one thread and a maximum of four.&#13;
Now suppose that usually one thread is executing a task, and then the&#13;
application starts a cycle in which every 15 seconds,&#13;
the workload has on average two tasks to execute. The first time through that&#13;
cycle, the pool will create the second thread—and now it makes sense for&#13;
that second thread to stay in the pool for at least a certain period of time.&#13;
You want to avoid the situation in which that second thread is created, finishes&#13;
its task in 5 seconds, is idle for 5 seconds, and then exits—since&#13;
5 seconds later, a second thread will be needed for the next task.&#13;
In general, after a thread is created in a pool for a minimum size, it should&#13;
stick around for at least a few minutes to handle any spike in load. To&#13;
the extent that you have a good model of the arrival rate, you can base&#13;
the idle time on that. Otherwise, plan on the idle time being measured in&#13;
minutes, at least anywhere from 10 to 30.</p>&#13;
&#13;
<p>Keeping idle threads around usually has little impact on an application.&#13;
Usually, the thread object&#13;
itself doesn’t take a very large amount of heap space. The exception&#13;
to that rule&#13;
is if the thread holds onto a large amount of thread-local storage or&#13;
if a large amount of memory is referenced through the thread’s runnable object.&#13;
In either of those cases, freeing a thread can offer significant savings&#13;
in terms of the live data left in the heap (which in turn affects the&#13;
efficiency of GC).</p>&#13;
&#13;
<p>These cases really should not happen for thread pools, however. When a&#13;
thread in a pool is&#13;
idle, it should not be referencing any runnable object anymore (if it is,&#13;
a bug exists somewhere). Depending on the pool implementation, the thread-local variables may remain in place—but while thread-local variables&#13;
can be an effective way to promote object reuse in certain circumstances&#13;
(see <a data-type="xref" href="ch07.html#Memory">Chapter 7</a>), the total&#13;
amount of memory those thread-local objects occupy should be limited.</p>&#13;
&#13;
<p>One important exception to this rule is for thread pools that can grow to&#13;
be very large (and hence run on a very large machine). Say the task queue for a thread pool is expected to average 20 tasks; 20 is then&#13;
a good minimum size for the pool. Now say the pool is running on a very&#13;
large machine and that it is designed to handle a spike of 2,000 tasks.&#13;
Keeping 2,000 idle threads around in this pool will affect its performance&#13;
when it is running only the 20 tasks—the throughput of this pool may be&#13;
as much as 50% when it contains 1,980 idle threads, as opposed to when it has&#13;
only the core 20 busy threads. Thread pools don’t usually encounter sizing&#13;
issues like that, but when they do, that’s a good time to make sure they&#13;
have a good minimum value.<a data-startref="ix_ch09-asciidoc5" data-type="indexterm" id="idm45775549499288"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Thread Pool Task Sizes" data-type="sect2"><div class="sect2" id="idm45775549516664">&#13;
<h2>Thread Pool Task Sizes</h2>&#13;
&#13;
<p><a data-primary="thread pools" data-secondary="limiting size of task queues" data-type="indexterm" id="idm45775549497176"/><a data-primary="ThreadPoolExecutor class" data-secondary="limiting size of task queues" data-type="indexterm" id="idm45775549496136"/>The tasks pending for a thread pool are held in a queue or list;&#13;
when a thread in the pool can execute a task, it pulls a task from the&#13;
queue. This can lead to an imbalance, as the number of tasks on&#13;
the queue could grow very large. If the queue is too large, tasks in the&#13;
queue will have to wait a long time until the tasks in front of them have&#13;
completed execution. Imagine a web server that is overloaded: if a&#13;
task is added to the queue and isn’t executed for 3 seconds, the user&#13;
has likely moved on to another page.</p>&#13;
&#13;
<p>As a result, thread pools typically limit the size of the queue of pending&#13;
tasks. The&#13;
<code class="keep-together">ThreadPoolExecutor</code>&#13;
does this in various ways, depending on the&#13;
data structure it is configured with (more on that in the next section);&#13;
servers usually have a tuning parameter to adjust this value.</p>&#13;
&#13;
<p>As with the maximum size of the thread pool, no universal rule indicates&#13;
how this value should be tuned. A server with 30,000 items in&#13;
its queue&#13;
and four available CPUs can clear the queue in 6 minutes if it takes only&#13;
50 ms to execute a task (assuming no new tasks arrive during&#13;
that time). That might be acceptable, but if each task requires 1 second&#13;
to execute, it will take 2 hours to clear the queue. Once again,&#13;
measuring your actual application is the only way to be sure of what value&#13;
will give you the performance you require.</p>&#13;
&#13;
<p>In any case, when the queue limit is reached, attempts to add a task to the queue will fail. A <code class="keep-together">ThreadPoolExecutor</code> has a&#13;
<code class="keep-together">rejectedExecution()</code>&#13;
method that handles that case (by default, it throws a&#13;
<code>RejectedExecutionException</code>,&#13;
but you can override that behavior).&#13;
Application servers should return a reasonable response to the user&#13;
(with a message that indicates what has happened), and REST servers should return a status code of either 429 (too many requests)&#13;
or 503 (service unavailable).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Sizing a ThreadPoolExecutor" data-type="sect2"><div class="sect2" id="idm45775549489496">&#13;
<h2>Sizing a ThreadPoolExecutor</h2>&#13;
&#13;
<p><a data-primary="thread pools" data-secondary="sizing a ThreadPoolExecutor" data-type="indexterm" id="ix_ch09-asciidoc6"/><a data-primary="ThreadPoolExecutor class" data-secondary="sizing" data-type="indexterm" id="ix_ch09-asciidoc7"/>The general behavior for a thread pool is that it starts&#13;
with a minimum number of threads, and if a task arrives when all existing&#13;
threads are busy, a new thread is started (up to the maximum number of&#13;
threads) and the task is executed immediately. If the maximum number of&#13;
threads have been started but they are all busy, the task is queued,&#13;
unless many tasks are pending already, in which case the&#13;
task is rejected. While that is the canonical behavior of a thread pool, the&#13;
<code class="keep-together">ThreadPoolExecutor</code>&#13;
can behave somewhat differently.</p>&#13;
&#13;
<p><a data-primary="queues and thread pools, types of" data-type="indexterm" id="idm45775549484120"/>The&#13;
<code class="keep-together">ThreadPoolExecutor</code>&#13;
decides when to start a new thread based on the&#13;
type of queue used to hold the tasks. There are three possibilities:</p>&#13;
<dl>&#13;
<dt><code>SynchronousQueue</code></dt>&#13;
<dd>&#13;
<p>When the executor uses&#13;
<code class="keep-together">SynchronousQueue</code>,&#13;
the thread pool behaves&#13;
as expected with respect to the number of threads: new tasks will start&#13;
a new thread if all existing threads are busy and if the pool has less than&#13;
the number of maximum threads. However, this queue has no way to hold&#13;
pending tasks: if a task arrives and the maximum number of threads is&#13;
already busy, the task is always rejected. So this choice is good for&#13;
managing a small number of tasks, but otherwise may be unsuitable. The&#13;
documentation for this class suggests specifying a very large number for the&#13;
maximum thread size—which may be OK if the tasks are completely I/O-bound&#13;
but as we’ve seen may be counterproductive in other situations. On the&#13;
other hand, if you need a thread pool in which the number of threads is easy&#13;
to tune, this is the better choice.</p>&#13;
&#13;
<p>In this case, the core value is the minimum pool size: the number of threads&#13;
that will be kept running even if they are idle. The maximum value is&#13;
the maximum number of threads in the pool.</p>&#13;
&#13;
<p>This is the type of thread pool (with an unbounded maximum thread value)&#13;
returned by the <code>newCachedThreadPool()</code>&#13;
method of the <code>Executors</code> class.</p>&#13;
</dd>&#13;
<dt>Unbounded queues</dt>&#13;
<dd>&#13;
<p>When the executor uses an unbounded queue (such as <span class="keep-together"><code>LinkedBlockedingQueue</code>),</span>&#13;
no task will ever be rejected (since the queue size is unlimited). In this&#13;
case, the executor will use at most the number of threads specified by the core&#13;
thread pool size: the maximum pool size is ignored. This essentially&#13;
mimics a traditional thread pool in which the core size is interpreted as the&#13;
maximum pool size, though because the queue is unbounded, it runs the risk&#13;
of consuming too much memory if tasks are submitted more quickly than they&#13;
can be run.</p>&#13;
&#13;
<p>This is the type of thread pool returned by the <code>newFixedThreadPool()</code>&#13;
and <code>newSingleThreadScheduledExecutor()</code> methods of the <code>Executors</code> class.&#13;
The core (or maximum) pool size of the first case is the parameter passed to construct the pool; in the second case, the core pool size is 1.</p>&#13;
</dd>&#13;
<dt>Bounded queues</dt>&#13;
<dd>&#13;
<p><a data-primary="bounded queues" data-type="indexterm" id="idm45775549470808"/>Executors that use a bounded queue (e.g., <span class="keep-together"><code>ArrayBlockingQueue</code>)</span>&#13;
employ a complicated algorithm to determine when to start a new thread.&#13;
For&#13;
example, say that the pool’s core size is 4, its maximum size is 8, and&#13;
the maximum size of <code class="keep-together">ArrayBlockingQueue</code>&#13;
is 10. As tasks arrive and&#13;
are placed in the queue, the pool will run a maximum of 4 threads (the&#13;
core pool size). Even if the queue completely fills up—so that it is&#13;
holding 10 pending tasks—the executor will utilize&#13;
4 threads.</p>&#13;
&#13;
<p>An additional thread will be started only when the queue is full, and a new&#13;
task is added to the queue. Instead of rejecting the task (since the queue&#13;
is full), the executor starts a new thread. That new thread runs the first&#13;
task on the queue, making room for the pending task to be added to the queue.</p>&#13;
&#13;
<p>In this example, the only way the pool&#13;
will end up with 8 threads (its specified maximum) is if there are&#13;
7 tasks in progress, 10 tasks in the queue, and a new task is added&#13;
to the queue.</p>&#13;
&#13;
<p>The idea behind this algorithm is that the pool will operate with only the core&#13;
threads (four) most of the time, even if a moderate number of tasks is&#13;
in the queue waiting to be run. That allows&#13;
the pool to act as a throttle (which is advantageous). If the backlog of&#13;
requests becomes too great, the pool then attempts to run more threads to&#13;
clear out the backlog (subject to a second throttle, the maximum number of&#13;
threads).</p>&#13;
&#13;
<p>If no external bottlenecks are in the system and CPU cycles are available, everything here works out: adding the new threads will&#13;
process the queue faster and likely bring it back to its desired size.&#13;
So cases where this algorithm is appropriate can certainly be constructed.</p>&#13;
&#13;
<p>On the other hand, this algorithm has no idea why the queue size has&#13;
increased. If it is caused by an external backlog, adding more&#13;
threads is the wrong thing to do. If the pool is running on&#13;
a machine that is CPU-bound, adding more threads is the wrong thing to&#13;
do. Adding threads will make sense only if the backlog occurred&#13;
because additional load came into the system (e.g., more clients started&#13;
making an HTTP request). (Yet if that is the case, why wait to add threads until the queue size has&#13;
reached a certain bound? If the additional resources are available to utilize&#13;
additional threads, adding them sooner will improve the overall&#13;
performance of the system.)</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>There are many arguments for and against each of these choices,&#13;
but when attempting to maximize performance,&#13;
this is a time to apply the KISS principle: keep it simple, stupid.&#13;
As always, the needs of the application may dictate otherwise, but&#13;
as a general recommendation, don’t use the <code>Executors</code> class to&#13;
provide default, unbounded thread pools that don’t allow you to control&#13;
the application’s memory use. Instead, construct your own&#13;
<code class="keep-together">ThreadPoolExecutor</code>&#13;
that has the same number of core and maximum threads and utilizes an&#13;
<code class="keep-together">ArrayBlockingQueue</code>&#13;
to limit the number of requests that can be held in memory waiting to be&#13;
executed.</p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Thread pools are one case where object pooling is a good thing: threads are expensive to initialize, and a thread pool allows the number of threads on a system to be easily <span class="keep-together">throttled.</span></p>&#13;
</li>&#13;
<li>&#13;
<p>Thread pools must be carefully tuned. Blindly adding new threads into a pool can, in some circumstances, degrade performance.</p>&#13;
</li>&#13;
<li>&#13;
<p>Using simpler options for a&#13;
<code class="keep-together">ThreadPoolExecutor</code>&#13;
will usually provide the best (and most predictable) performance<a data-startref="ix_ch09-asciidoc7" data-type="indexterm" id="idm45775549455752"/><a data-startref="ix_ch09-asciidoc6" data-type="indexterm" id="idm45775549454968"/>.<a data-startref="ix_ch09-asciidoc3" data-type="indexterm" id="idm45775549454168"/><a data-startref="ix_ch09-asciidoc2" data-type="indexterm" id="idm45775549453464"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The ForkJoinPool" data-type="sect1"><div class="sect1" id="idm45775549622616">&#13;
<h1>The ForkJoinPool</h1>&#13;
&#13;
<p><a data-primary="ForkJoinPool class" data-type="indexterm" id="ix_ch09-asciidoc8"/><a data-primary="threading" data-secondary="ForkJoinPool" data-type="indexterm" id="ix_ch09-asciidoc9"/>In addition to the general-purpose <code>ThreadPoolExecutors</code>, Java provides a&#13;
somewhat special-purpose pool: the&#13;
<span class="keep-together"><code>ForkJoinPool</code></span>&#13;
class. This class looks just like any other thread pool; like the&#13;
<span class="keep-together"><code>ThreadPoolExecutor</code></span>&#13;
class, it implements the&#13;
<span class="keep-together"><code>Executor</code></span>&#13;
and&#13;
<span class="keep-together"><code>ExecutorService</code></span>&#13;
interfaces. When those interfaces are used, <span class="keep-together"><code>ForkJoinPool</code></span>&#13;
uses an internal unbounded list of tasks that will be run by the&#13;
number of threads specified in its constructor. If no argument is passed&#13;
to the constructor, the pool will size itself based on the number of CPUs&#13;
available on the machine (or based on the CPUs available&#13;
to the Docker container, if&#13;
applicable).</p>&#13;
&#13;
<p>The&#13;
<span class="keep-together"><code>ForkJoinPool</code></span>&#13;
class is designed to work with divide-and-conquer algorithms: those where a&#13;
task can be&#13;
recursively broken into subsets. The subsets can be processed in parallel,&#13;
and then the results from each subset are merged into a single result.&#13;
The classic example of this is the quicksort sorting&#13;
algorithm.</p>&#13;
&#13;
<p>The important point about divide-and-conquer algorithms is that they&#13;
create a lot of tasks&#13;
that must be managed by relatively few threads. Say that we want to&#13;
sort an array of 10 million elements. We start by creating separate tasks to&#13;
perform three operations: sort the subarray containing the first 5&#13;
million elements, sort the subarray containing the&#13;
second 5 million elements, and then merge the two subarrays.</p>&#13;
&#13;
<p>The sorting of the 5-million-element arrays is similarly accomplished by&#13;
sorting subarrays of 2.5 million elements and merging those arrays. This&#13;
recursion continues until at some point (e.g., when the subarray has 47&#13;
elements), it is more efficient to use insertion sort on the array and sort&#13;
it directly. <a data-type="xref" href="#FigureForkJoin">Figure 9-1</a> shows how that all works out.</p>&#13;
&#13;
<p>In the end, we will have 262,144 tasks to sort the&#13;
leaf arrays, each of which will have 47 (or fewer) elements. (That number—47—is&#13;
algorithm-dependent and the subject of a lot of analysis, but it is the&#13;
number Java uses for quicksort.)</p>&#13;
&#13;
<figure><div class="figure" id="FigureForkJoin">&#13;
<img alt="jp2e 0901" src="assets/jp2e_0901.png"/>&#13;
<h6><span class="label">Figure 9-1. </span>Tasks in a recursive quicksort</h6>&#13;
</div></figure>&#13;
&#13;
<p>An additional 131,072 tasks are needed to merge those sorted arrays,&#13;
65,536 additional tasks to merge the next set of sorted arrays, and so on. In the end, there will be 524,287 tasks.</p>&#13;
&#13;
<p>The larger point here is that none of the tasks can complete until the&#13;
tasks that they have spawned have also completed. The tasks directly sorting&#13;
arrays of fewer than 47 elements must be completed first, and then&#13;
tasks can merge the&#13;
two small arrays that they created, and so on: everything is merged&#13;
up the chain until the entire array is merged into its final, sorted value.</p>&#13;
&#13;
<p><a data-primary="ThreadPoolExecutor class" data-secondary="ForkJoinPool as alternative to" data-type="indexterm" id="ix_ch09-asciidoc10"/>It isn’t possible to perform that algorithm efficiently using <span class="keep-together"><code>ThreadPoolExecutor</code></span>,&#13;
because a parent task must wait for its child tasks to complete. A thread&#13;
inside a thread-pool executor cannot add another task to the queue and then wait for it to finish: once&#13;
the thread is waiting, it cannot be used to execute one of the&#13;
subtasks. <span class="keep-together"><code>ForkJoinPool</code></span>,&#13;
on the other hand, allows its threads to create new tasks and then suspend&#13;
their current task. While the task is suspended, the thread can execute other pending&#13;
tasks.</p>&#13;
&#13;
<p>Let’s take a simple example: say that we have an array of doubles, and&#13;
the goal is to count the number of values in the array that are less than&#13;
0.5. It’s trivial simply to scan the array sequentially (and possibly&#13;
advantageous, as you’ll see later in this section)—but for now, it is&#13;
instructive to divide the array into subarrays and scan them in parallel&#13;
(emulating the more complex quicksort and other divide-and-conquer algorithms).&#13;
Here’s an outline of the code to achieve that with a&#13;
<span class="keep-together"><code>ForkJoinPool</code></span>:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">private</code> <code class="kd">class</code> <code class="nc">ForkJoinTask</code> <code class="kd">extends</code> <code class="n">RecursiveTask</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">&gt;</code> <code class="o">{</code>&#13;
    <code class="kd">private</code> <code class="kt">int</code> <code class="n">first</code><code class="o">;</code>&#13;
    <code class="kd">private</code> <code class="kt">int</code> <code class="n">last</code><code class="o">;</code>&#13;
&#13;
    <code class="kd">public</code> <code class="nf">ForkJoinTask</code><code class="o">(</code><code class="kt">int</code> <code class="n">first</code><code class="o">,</code> <code class="kt">int</code> <code class="n">last</code><code class="o">)</code> <code class="o">{</code>&#13;
        <code class="k">this</code><code class="o">.</code><code class="na">first</code> <code class="o">=</code> <code class="n">first</code><code class="o">;</code>&#13;
        <code class="k">this</code><code class="o">.</code><code class="na">last</code> <code class="o">=</code> <code class="n">last</code><code class="o">;</code>&#13;
    <code class="o">}</code>&#13;
&#13;
    <code class="kd">protected</code> <code class="n">Integer</code> <code class="nf">compute</code><code class="o">()</code> <code class="o">{</code>&#13;
        <code class="kt">int</code> <code class="n">subCount</code><code class="o">;</code>&#13;
        <code class="k">if</code> <code class="o">(</code><code class="n">last</code> <code class="o">-</code> <code class="n">first</code> <code class="o">&lt;</code> <code class="mi">10</code><code class="o">)</code> <code class="o">{</code>&#13;
            <code class="n">subCount</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code>&#13;
            <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="n">first</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;=</code> <code class="n">last</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
                <code class="k">if</code> <code class="o">(</code><code class="n">d</code><code class="o">[</code><code class="n">i</code><code class="o">]</code> <code class="o">&lt;</code> <code class="mf">0.5</code><code class="o">)</code>&#13;
                    <code class="n">subCount</code><code class="o">++;</code>&#13;
            <code class="o">}</code>&#13;
        <code class="o">}</code>&#13;
        <code class="k">else</code> <code class="o">{</code>&#13;
            <code class="kt">int</code> <code class="n">mid</code> <code class="o">=</code> <code class="o">(</code><code class="n">first</code> <code class="o">+</code> <code class="n">last</code><code class="o">)</code> <code class="o">&gt;&gt;&gt;</code> <code class="mi">1</code><code class="o">;</code>&#13;
            <code class="n">ForkJoinTask</code> <code class="n">left</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ForkJoinTask</code><code class="o">(</code><code class="n">first</code><code class="o">,</code> <code class="n">mid</code><code class="o">);</code>&#13;
            <code class="n">left</code><code class="o">.</code><code class="na">fork</code><code class="o">();</code>&#13;
            <code class="n">ForkJoinTask</code> <code class="n">right</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ForkJoinTask</code><code class="o">(</code><code class="n">mid</code> <code class="o">+</code> <code class="mi">1</code><code class="o">,</code> <code class="n">last</code><code class="o">);</code>&#13;
            <code class="n">right</code><code class="o">.</code><code class="na">fork</code><code class="o">();</code>&#13;
            <code class="n">subCount</code> <code class="o">=</code> <code class="n">left</code><code class="o">.</code><code class="na">join</code><code class="o">();</code>&#13;
            <code class="n">subCount</code> <code class="o">+=</code> <code class="n">right</code><code class="o">.</code><code class="na">join</code><code class="o">();</code>&#13;
        <code class="o">}</code>&#13;
        <code class="k">return</code> <code class="n">subCount</code><code class="o">;</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>The <code>fork()</code> and <code>join()</code> methods here are the key: we’d be hard-pressed to&#13;
implement this sort of recursion without those methods (which are not available&#13;
in the tasks executed by <span class="keep-together"><code>ThreadPoolExecutor</code></span>). Those methods use a series of internal, per-thread queues to manipulate&#13;
the tasks and switch threads from executing one task to executing another.&#13;
The details are transparent to the developer, though if you’re&#13;
interested in algorithms, the code makes fascinating reading.&#13;
Our focus here is on the&#13;
performance: what trade-offs exist between the&#13;
<span class="keep-together"><code>ForkJoinPool</code></span>&#13;
and&#13;
<span class="keep-together"><code>ThreadPoolExecutor</code></span>&#13;
classes?</p>&#13;
&#13;
<p>First and foremost is that the suspension implemented by the fork/join paradigm allows all the tasks to be executed by only a few threads. Counting the double values&#13;
in an array of 2 million elements using this example code creates more than 4 million tasks, but those tasks are easily executed by only a few threads (even one, if that makes sense for the machine running the test). Running&#13;
a similar algorithm using <code>ThreadPoolExecutor</code> would require more than 4 million threads, since each thread would have to wait for its subtasks to complete, and those subtasks could complete only if additional threads were available in the pool. So the fork/join suspension allows us to use algorithms that we otherwise could not, which is a performance win.<a data-startref="ix_ch09-asciidoc10" data-type="indexterm" id="idm45775549274568"/></p>&#13;
&#13;
<p>On the other hand, a simple algorithm like this isn’t particularly well-suited&#13;
for a real-world use of the fork-join pool. This pool is ideally suited for the following cases:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The merge part of the algorithm performs some interesting work (rather than simply adding two numbers as in this example).</p>&#13;
</li>&#13;
<li>&#13;
<p>The leaf calculation of the algorithm performs enough work to offset creating the task.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Absent these two criteria, it is easy enough to&#13;
partition the array into chunks and use <span class="keep-together"><code>ThreadPoolExecutor</code></span>&#13;
to have multiple threads scan the array:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code> <code class="kd">class</code> <code class="nc">ThreadPoolTest</code> <code class="o">{</code>&#13;
    <code class="kd">private</code> <code class="kt">double</code><code class="o">[]</code> <code class="n">d</code><code class="o">;</code>&#13;
&#13;
    <code class="kd">private</code> <code class="kd">class</code> <code class="nc">ThreadPoolExecutorTask</code> <code class="kd">implements</code> <code class="n">Callable</code><code class="o">&lt;</code><code class="n">Integer</code><code class="o">&gt;</code> <code class="o">{</code>&#13;
        <code class="kd">private</code> <code class="kt">int</code> <code class="n">first</code><code class="o">;</code>&#13;
        <code class="kd">private</code> <code class="kt">int</code> <code class="n">last</code><code class="o">;</code>&#13;
&#13;
        <code class="kd">public</code> <code class="nf">ThreadPoolExecutorTask</code><code class="o">(</code><code class="kt">int</code> <code class="n">first</code><code class="o">,</code> <code class="kt">int</code> <code class="n">last</code><code class="o">)</code> <code class="o">{</code>&#13;
            <code class="k">this</code><code class="o">.</code><code class="na">first</code> <code class="o">=</code> <code class="n">first</code><code class="o">;</code>&#13;
            <code class="k">this</code><code class="o">.</code><code class="na">last</code> <code class="o">=</code> <code class="n">last</code><code class="o">;</code>&#13;
        <code class="o">}</code>&#13;
&#13;
        <code class="kd">public</code> <code class="n">Integer</code> <code class="nf">call</code><code class="o">()</code> <code class="o">{</code>&#13;
            <code class="kt">int</code> <code class="n">subCount</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code>&#13;
            <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="n">first</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;=</code> <code class="n">last</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
                <code class="k">if</code> <code class="o">(</code><code class="n">d</code><code class="o">[</code><code class="n">i</code><code class="o">]</code> <code class="o">&lt;</code> <code class="mf">0.5</code><code class="o">)</code> <code class="o">{</code>&#13;
                    <code class="n">subCount</code><code class="o">++;</code>&#13;
                <code class="o">}</code>&#13;
            <code class="o">}</code>&#13;
            <code class="k">return</code> <code class="n">subCount</code><code class="o">;</code>&#13;
        <code class="o">}</code>&#13;
    <code class="o">}</code>&#13;
&#13;
    <code class="kd">public</code> <code class="kd">static</code> <code class="kt">void</code> <code class="nf">main</code><code class="o">(</code><code class="n">String</code><code class="o">[]</code> <code class="n">args</code><code class="o">)</code> <code class="o">{</code>&#13;
        <code class="n">d</code> <code class="o">=</code> <code class="n">createArrayOfRandomDoubles</code><code class="o">();</code>&#13;
        <code class="n">ThreadPoolExecutor</code> <code class="n">tpe</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ThreadPoolExecutor</code><code class="o">(</code><code class="mi">4</code><code class="o">,</code> <code class="mi">4</code><code class="o">,</code>&#13;
                                        <code class="n">Long</code><code class="o">.</code><code class="na">MAX_VALUE</code><code class="o">,</code>&#13;
                                        <code class="n">TimeUnit</code><code class="o">.</code><code class="na">SECONDS</code><code class="o">,</code>&#13;
                                	<code class="k">new</code> <code class="nf">LinkedBlockingQueue</code><code class="o">());</code>&#13;
        <code class="n">Future</code><code class="o">[]</code> <code class="n">f</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Future</code><code class="o">[</code><code class="mi">4</code><code class="o">];</code>&#13;
        <code class="kt">int</code> <code class="n">size</code> <code class="o">=</code> <code class="n">d</code><code class="o">.</code><code class="na">length</code> <code class="o">/</code> <code class="mi">4</code><code class="o">;</code>&#13;
        <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="mi">3</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
            <code class="n">f</code><code class="o">[</code><code class="n">i</code><code class="o">]</code> <code class="o">=</code> <code class="n">tpe</code><code class="o">.</code><code class="na">submit</code><code class="o">(</code>&#13;
                       <code class="k">new</code> <code class="nf">ThreadPoolExecutorTask</code><code class="o">(</code><code class="n">i</code> <code class="o">*</code> <code class="n">size</code><code class="o">,</code> <code class="o">(</code><code class="n">i</code> <code class="o">+</code> <code class="mi">1</code><code class="o">)</code> <code class="o">*</code> <code class="n">size</code> <code class="o">-</code> <code class="mi">1</code><code class="o">);</code>&#13;
        <code class="o">}</code>&#13;
        <code class="n">f</code><code class="o">[</code><code class="mi">3</code><code class="o">]</code> <code class="o">=</code> <code class="n">tpe</code><code class="o">.</code><code class="na">submit</code><code class="o">(</code><code class="k">new</code> <code class="n">ThreadPoolExecutorTask</code><code class="o">(</code><code class="mi">3</code> <code class="o">*</code> <code class="n">size</code><code class="o">,</code> <code class="n">d</code><code class="o">.</code><code class="na">length</code> <code class="o">-</code> <code class="mi">1</code><code class="o">);</code>&#13;
        <code class="kt">int</code> <code class="n">n</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code>&#13;
        <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="mi">4</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
            <code class="n">n</code> <code class="o">+=</code> <code class="n">f</code><code class="o">.</code><code class="na">get</code><code class="o">();</code>&#13;
        <code class="o">}</code>&#13;
        <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="s">"Found "</code> <code class="o">+</code> <code class="n">n</code> <code class="o">+</code> <code class="s">" values"</code><code class="o">);</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>On a four-CPU machine, this code will fully utilize all available CPUs,&#13;
processing the array in parallel while avoiding creating and queuing the&#13;
4 million tasks used by the fork/join example. The performance is&#13;
predictably faster, as <a data-type="xref" href="#TableForkJoin">Table 9-5</a> shows.</p>&#13;
<table id="TableForkJoin">&#13;
<caption><span class="label">Table 9-5. </span>Time to count an array of 2 million elements</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Number of threads</th>&#13;
<th><code>ForkJoinPool</code></th>&#13;
<th><code>ThreadPoolExecutor</code></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>1</p></td>&#13;
<td><p>125 ± 1 ms</p></td>&#13;
<td><p>1.731 ± 0.001 ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>4</p></td>&#13;
<td><p>37.7 ± 1 ms</p></td>&#13;
<td><p>0.55 ± 0.002 ms</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>The two tests differ in GC time, but the real difference comes from the divide-and-conquer, particularly with a leaf value of 10.&#13;
The overhead&#13;
of creating and managing the 4 million task objects hampers the performance of <span class="keep-together"><code>ForkJoinPool</code></span>.&#13;
When a similar alternative is available, it is likely to be faster—at&#13;
least in this simple case.</p>&#13;
&#13;
<p>Alternatively, we could have required far fewer tasks by ending the recursion&#13;
earlier. At one extreme, we could end the recursion when the subarray has&#13;
500,000 elements, which neatly partitions the work into four tasks, which is&#13;
the same as the thread pool example does. At that point, the performance of&#13;
this test would be the same (though if the work partitions that easily, there’s&#13;
no reason to use a divide-and-conquer algorithm in the first place).</p>&#13;
&#13;
<p>For illustrative purposes, we can easily enough mitigate the second point&#13;
 in our criteria by adding work&#13;
to the leaf calculation phase of our task:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="n">first</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;=</code> <code class="n">last</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
    <code class="k">if</code> <code class="o">(</code><code class="n">d</code><code class="o">[</code><code class="n">i</code><code class="o">]</code> <code class="o">&lt;</code> <code class="mf">0.5</code><code class="o">)</code> <code class="o">{</code>&#13;
        <code class="n">subCount</code><code class="o">++;</code>&#13;
    <code class="o">}</code>&#13;
    <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">j</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">j</code> <code class="o">&lt;</code> <code class="mi">500</code><code class="o">;</code> <code class="n">j</code><code class="o">++)</code> <code class="o">{</code>&#13;
        <code class="n">d</code><code class="o">[</code><code class="n">i</code><code class="o">]</code> <code class="o">*=</code> <code class="n">d</code><code class="o">[</code><code class="n">i</code><code class="o">];</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>Now the test will be dominated by the calculation of <code>d[i]</code>. But because the&#13;
merge portion of the algorithm isn’t doing any significant work, creating all the tasks still carries a penalty, as we see in <a data-type="xref" href="#TableForkJoinWork">Table 9-6</a>.</p>&#13;
<table id="TableForkJoinWork">&#13;
<caption><span class="label">Table 9-6. </span>Time to count an array of 2 million elements with added work</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Number of threads</th>&#13;
<th><code>ForkJoinPool</code></th>&#13;
<th><code>ThreadPoolExecutor</code></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>4</p></td>&#13;
<td><p>271 ± 3 ms</p></td>&#13;
<td><p>258 ± 1 ms</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Now that the time is dominated by actual calculation in the test, the fork-join&#13;
pool isn’t quite as bad compared to a partition. Still, the time to create&#13;
the tasks is significant, and&#13;
when the tasks could simply be partitioned&#13;
(i.e., when no significant work is in the merge stage), a simple thread pool&#13;
will be faster.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Work Stealing" data-type="sect2"><div class="sect2" id="idm45775548940248">&#13;
<h2>Work Stealing</h2>&#13;
&#13;
<p><a data-primary="ForkJoinPool class" data-secondary="work stealing" data-type="indexterm" id="ix_ch09-asciidoc11"/><a data-primary="work stealing" data-type="indexterm" id="ix_ch09-asciidoc12"/>One rule about using this pool is to make sure splitting the tasks makes&#13;
sense. But a second feature of the&#13;
<span class="keep-together"><code>ForkJoinPool</code></span>&#13;
makes it even more powerful:&#13;
it implements work stealing. That’s basically an implementation detail;&#13;
it means that each thread in the pool has its own queue of tasks it has&#13;
forked. Threads will preferentially work on tasks from their own queue, but&#13;
if that queue is empty, they will steal tasks from the queues of other threads.&#13;
The upshot is that even if one of the 4 million tasks takes a long time&#13;
to execute, other threads in the&#13;
<span class="keep-together"><code>ForkJoinPool</code></span>&#13;
can complete any and all of the remaining tasks. The same is not true of&#13;
the&#13;
<span class="keep-together"><code>ThreadPoolExecutor</code></span>:&#13;
if one of its tasks requires a long time, the other threads cannot pick up&#13;
additional work.</p>&#13;
&#13;
<p>When we added work to the original example, the amount of work per value was&#13;
constant. What if that work varied depending on the position of the item&#13;
in the array?</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="n">first</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;=</code> <code class="n">last</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
    <code class="k">if</code> <code class="o">(</code><code class="n">d</code><code class="o">[</code><code class="n">i</code><code class="o">]</code> <code class="o">&lt;</code> <code class="mf">0.5</code><code class="o">)</code> <code class="o">{</code>&#13;
        <code class="n">subCount</code><code class="o">++;</code>&#13;
    <code class="o">}</code>&#13;
    <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">j</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">j</code> <code class="o">&lt;</code> <code class="n">i</code><code class="o">;</code> <code class="n">j</code><code class="o">++)</code> <code class="o">{</code>&#13;
        <code class="n">d</code><code class="o">[</code><code class="n">i</code><code class="o">]</code> <code class="o">+=</code> <code class="n">j</code><code class="o">;</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>Because the outer loop (indexed by <code>j</code>) is based on the position of&#13;
the element in the array, the calculation requires a length of time&#13;
proportional to the element position: calculating the value for <code>d[0]</code>&#13;
will be very fast, while calculating the value for <code>d[d.length - 1]</code>&#13;
will take more time.</p>&#13;
&#13;
<p>Now the simple partitioning of the&#13;
<span class="keep-together"><code>ThreadPoolExecutor</code></span>&#13;
test will be at a disadvantage. The thread calculating the first partition&#13;
of the array will take a very long time to complete, much longer than the&#13;
time spent by the fourth thread operating on the last partition. Once that&#13;
fourth thread is finished, it will remain idle: everything must wait for&#13;
the first thread to complete its long task.</p>&#13;
&#13;
<p>The granularity of&#13;
the 4 million tasks in the&#13;
<span class="keep-together"><code>ForkJoinPool</code></span>&#13;
means that although one thread will get stuck doing the very long calculations&#13;
on the first 10 elements in the array, the remaining threads will still have&#13;
work to perform, and the CPU will be kept busy during most of the test.&#13;
That difference is shown in <a data-type="xref" href="#TableForkJoinBalance">Table 9-7</a>.</p>&#13;
<table id="TableForkJoinBalance">&#13;
<caption><span class="label">Table 9-7. </span>Time to process an array of 2,000,000 elements with an unbalanced workload</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Number of threads</th>&#13;
<th><code>ForkJoinPool</code></th>&#13;
<th><code>ThreadPoolExecutor</code></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>1</p></td>&#13;
<td><p>22.0 ± 0.01 seconds</p></td>&#13;
<td><p>21.7 ± 0.1 seconds</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>4</p></td>&#13;
<td><p>5.6 ± 0.01 seconds</p></td>&#13;
<td><p>9.7 ± 0.1 seconds</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>When the pool has a single thread, the computation takes essentially&#13;
the same amount of time. That makes sense: the number of calculations is&#13;
the same regardless of the pool implementation, and since those&#13;
calculations are never done in&#13;
parallel, they can be expected to take the same amount of time (though some small overhead exists for creating the 4 million tasks). But when the&#13;
 pool&#13;
contains four threads, the granularity of the tasks in the&#13;
<span class="keep-together"><code>ForkJoinPool</code></span>&#13;
gives it a decided advantage: it is able to keep the CPUs busy for almost&#13;
the entire duration of the test.</p>&#13;
&#13;
<p>This situation is called <em>unbalanced</em>, because some tasks&#13;
take longer than others (and hence the tasks in the previous example&#13;
are called <em>balanced</em>).&#13;
In general, this leads to the recommendation that using <span class="keep-together"><code>ThreadPoolExecutor</code></span>&#13;
with partitioning will give better performance when the tasks can be&#13;
easily partitioned into a balanced set,&#13;
and <span class="keep-together"><code>ForkJoinPool</code></span>&#13;
will give better performance when the tasks are unbalanced.</p>&#13;
&#13;
<p>There is a more subtle performance recommendation here as well: carefully&#13;
consider the point at which the recursion for the fork/join&#13;
paradigm should end. In this example, we’ve arbitrarily chosen it to end when&#13;
the array size is less than 10. In the balanced case, we’ve already discussed&#13;
that ending the recursion at 500,000 would be optimal.</p>&#13;
&#13;
<p>On the other hand, the recursion in the unbalanced case&#13;
gives even better performance for smaller leaf values.&#13;
Representative data points are shown in <a data-type="xref" href="#TableForkJoinRecurse">Table 9-8</a>.</p>&#13;
<table id="TableForkJoinRecurse">&#13;
<caption><span class="label">Table 9-8. </span>Time to process an array of 2,000,000 elements with varying leaf values</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Target size of leaf array</th>&#13;
<th><code>ForkJoinPool</code></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>500,000</p></td>&#13;
<td><p>9,842 ± 5 ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>50,000</p></td>&#13;
<td><p>6,029 ± 100 ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>10,000</p></td>&#13;
<td><p>5,764 ± 55 ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>1,000</p></td>&#13;
<td><p>5,657 ± 56 ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>100</p></td>&#13;
<td><p>5,598 ± 20 ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>10</p></td>&#13;
<td><p>5,601 ± 15 ms</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>With a leaf size of 500,000, we’ve duplicated the thread pool executor case.&#13;
As the leaf size drops, we benefit from the unbalanced nature of the&#13;
test, until between 1,000 and 10,000, where the performance levels off.</p>&#13;
&#13;
<p>This tuning of the leaf value is routinely done in these&#13;
kind of algorithms. As you saw earlier in this section, Java uses 47 for the leaf value in its implementation of the quicksort algorithm: that’s&#13;
the point (for that algorithm) at which the overhead of creating the tasks&#13;
outweighs the benefits of the divide-and-conquer approach.<a data-startref="ix_ch09-asciidoc12" data-type="indexterm" id="idm45775548637256"/><a data-startref="ix_ch09-asciidoc11" data-type="indexterm" id="idm45775548636552"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Automatic Parallelization" data-type="sect2"><div class="sect2" id="idm45775548939624">&#13;
<h2>Automatic Parallelization</h2>&#13;
&#13;
<p><a data-primary="automatic parallelization" data-type="indexterm" id="ix_ch09-asciidoc13"/><a data-primary="ForkJoinPool class" data-secondary="automatic parallelization" data-type="indexterm" id="ix_ch09-asciidoc14"/><a data-primary="parallelization, automatic" data-type="indexterm" id="ix_ch09-asciidoc15"/>Java has the ability to automatically parallelize certain&#13;
kinds of code. This parallelization relies on the use of the&#13;
<span class="keep-together"><code>ForkJoinPool</code></span>&#13;
class. The JVM will create a common fork-join pool for this&#13;
purpose; it is a static element of the&#13;
<span class="keep-together"><code>ForkJoinPoolClass</code></span>&#13;
that is sized by default to the number of processors on the target&#13;
machine.</p>&#13;
&#13;
<p>This parallelization occurs in many methods of the <code>Arrays</code> class:&#13;
methods to sort an array using parallel quicksorting, methods to operate&#13;
on each individual element of an array, and so on. It is also used within&#13;
the streams feature, which allows for operations (either sequential&#13;
or parallel) to be performed on each element in a collection.&#13;
Basic performance implications of streams are discussed in <a data-type="xref" href="ch12.html#Misc">Chapter 12</a>;&#13;
in this section, we’ll look at how streams can automatically be processed&#13;
in parallel.</p>&#13;
&#13;
<p>Given a collection containing a series of integers, the following code&#13;
will calculate the stock price history for the symbol corresponding to the&#13;
given integer:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="n">List</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">symbolList</code> <code class="o">=</code> <code class="o">...;</code>&#13;
<code class="n">Stream</code><code class="o">&lt;</code><code class="n">String</code><code class="o">&gt;</code> <code class="n">stream</code> <code class="o">=</code> <code class="n">symbolList</code><code class="o">.</code><code class="na">parallelStream</code><code class="o">();</code>&#13;
<code class="n">stream</code><code class="o">.</code><code class="na">forEach</code><code class="o">(</code><code class="n">s</code> <code class="o">-&gt;</code> <code class="o">{</code>&#13;
    <code class="n">StockPriceHistory</code> <code class="n">sph</code> <code class="o">=</code> <code class="k">new</code> <code class="n">StockPriceHistoryImpl</code><code class="o">(</code><code class="n">s</code><code class="o">,</code> <code class="n">startDate</code><code class="o">,</code>&#13;
                                     <code class="n">endDate</code><code class="o">,</code> <code class="n">entityManager</code><code class="o">);</code>&#13;
    <code class="n">blackhole</code><code class="o">.</code><code class="na">consume</code><code class="o">(</code><code class="n">sph</code><code class="o">);</code>&#13;
<code class="o">});</code></pre>&#13;
&#13;
<p>This code will calculate the mock price histories in parallel: the&#13;
<code>forEach()</code>  method will create a task for each element in the array list,&#13;
and each task&#13;
will be processed by the common&#13;
<span class="keep-together"><code>ForkJoinTask</code></span>&#13;
pool. That is essentially equivalent to the test at the beginning of this&#13;
chapter, which used a thread pool to calculate the histories in&#13;
parallel—though this code is much easier to write than dealing with the&#13;
thread pool explicitly.</p>&#13;
&#13;
<p>Sizing the common&#13;
<span class="keep-together"><code>ForkJoinTask</code></span>&#13;
pool is as important as sizing any other thread pool. By default, the common&#13;
pool will have as many threads as the target machines has CPUs. If you are&#13;
running multiple JVMs on the same machine, limiting that number makes sense&#13;
so that the JVMs do not compete for CPU against each other. Similarly, if&#13;
a server will execute other requests in parallel&#13;
and you want to make sure that CPU&#13;
is available for those other tasks, consider lowering the size of the&#13;
common pool.&#13;
On the other hand, if tasks in the common pool will block waiting for I/O&#13;
or other data,&#13;
the common pool size might need to be increased.</p>&#13;
&#13;
<p><span class="keep-together">The size&#13;
can be set by specifying the system property&#13;
<code>-Djava.util.concurrent</code></span><code>.ForkJoinPool.common.parallelism=</code><em><code>N</code></em>. <a data-primary="Docker container" data-type="indexterm" id="idm45775548549304"/>As usual for Docker containers, it should&#13;
be set manually in Java 8 versions prior to update 192.</p>&#13;
&#13;
<p>Earlier in this chapter, <a data-type="xref" href="#TablePool1">Table 9-1</a> showed the effect on the performance&#13;
of the parallel stock history calculations when the pool had various sizes.&#13;
<a data-type="xref" href="#TablePoolStockJoin">Table 9-9</a> compares that data to the&#13;
<span class="keep-together"><code>forEach()</code></span>&#13;
construct using the common&#13;
<span class="keep-together"><code>ForkJoinPool</code></span>&#13;
(with the&#13;
<span class="keep-together"><code>parallelism</code></span>&#13;
system property set to the given value).</p>&#13;
<table id="TablePoolStockJoin">&#13;
<caption><span class="label">Table 9-9. </span>Time required to calculate 10,000 mock price histories</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Number of threads</th>&#13;
<th><code>ThreadPoolExecutor</code></th>&#13;
<th><code>ForkJoinPool</code></th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>1</p></td>&#13;
<td><p>40 ± 0.1 seconds</p></td>&#13;
<td><p>20.2 ± 0.2 seconds</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>2</p></td>&#13;
<td><p>20.1 ± 0.07 seconds</p></td>&#13;
<td><p>15.1 ± 0.05 seconds</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>4</p></td>&#13;
<td><p>10.1 ± 0.02 seconds</p></td>&#13;
<td><p>11.7 ± 0.1 seconds</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>8</p></td>&#13;
<td><p>10.2 ± 0.3 seconds</p></td>&#13;
<td><p>10.5 ± 0.1 seconds</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>16</p></td>&#13;
<td><p>10.3 ± 0.03 seconds</p></td>&#13;
<td><p>10.3 ± 0.7 seconds</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>By default, the common pool will have four threads (on our usual&#13;
four-CPU machine),&#13;
so the third line in the table is the common case. The results for a common&#13;
pool size of one and two are exactly the sort of result that should give a&#13;
performance engineer fits: they seem to be completely out of line because&#13;
<code>ForkJoinPool</code> is performing far better than might be expected.</p>&#13;
&#13;
<p>When a test is out of line like that, the most common reason is a testing error.&#13;
In this case, however, it turns out that the&#13;
<span class="keep-together"><code>forEach()</code></span>&#13;
method does something&#13;
tricky: it uses both the thread executing the statement and the threads in&#13;
the common pool to process the data coming from the stream. Even though the&#13;
common pool in the first test is configured to have a single thread, two&#13;
threads are used to calculate the result. Consequently, the time&#13;
for a <code>ThreadPoolExecutor</code> with two threads and a <code>ForkJoinPool</code> with one&#13;
thread is essentially the same.</p>&#13;
&#13;
<p>If you need to tune the size of the common pool when using parallel&#13;
stream constructs and other autoparallel features, consider decreasing the&#13;
desired value by one.</p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>The&#13;
<span class="keep-together"><code>ForkJoinPool</code></span>&#13;
class should be used for recursive, divide-and-conquer algorithms. This class in not suited for cases that can be handled via simple partitioning.</p>&#13;
</li>&#13;
<li>&#13;
<p>Make the effort to determine the best point at which the recursion of tasks in the algorithm should cease. Creating too many tasks can hurt performance, but too few tasks will also hurt performance if the tasks do not take the same amount of time.</p>&#13;
</li>&#13;
<li>&#13;
<p>Features that use automatic parallelization will use a common instance of the&#13;
<span class="keep-together"><code>ForkJoinPool</code></span> class. You may need to adjust the default size of that common instance<a data-startref="ix_ch09-asciidoc15" data-type="indexterm" id="idm45775548518248"/><a data-startref="ix_ch09-asciidoc14" data-type="indexterm" id="idm45775548517576"/><a data-startref="ix_ch09-asciidoc13" data-type="indexterm" id="idm45775548516904"/>.<a data-startref="ix_ch09-asciidoc9" data-type="indexterm" id="idm45775548516104"/><a data-startref="ix_ch09-asciidoc8" data-type="indexterm" id="idm45775548515400"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Thread Synchronization" data-type="sect1"><div class="sect1" id="idm45775549451624">&#13;
<h1>Thread Synchronization</h1>&#13;
&#13;
<p><a data-primary="thread synchronization" data-type="indexterm" id="ix_ch09-asciidoc16"/>In a perfect world—or in examples for a book—it is relatively easy for&#13;
threads to avoid the need for synchronization. In the real world, things are&#13;
not necessarily so easy.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775548512024">&#13;
<h5>Synchronization and Java Concurrent Utilities</h5>&#13;
<p><a data-primary="thread synchronization" data-secondary="Java concurrent utilities and" data-type="indexterm" id="idm45775548510888"/>In this section, <em>synchronization</em> refers to code that is&#13;
within a block where access to a set of variables appears serialized: only a&#13;
single thread at a time can access the memory.&#13;
This includes code blocks protected by the&#13;
<code class="keep-together">synchronized</code>&#13;
keyword. It also means code that is protected by an instance of the  <span class="keep-together"><code>java.util.concurrent.lock.Lock</code></span>&#13;
class, and code&#13;
within the&#13;
<code>java.util.concurrent</code>&#13;
and&#13;
<code>java.util.concurrent.atomic</code>&#13;
packages.</p>&#13;
&#13;
<p>Strictly speaking, the atomic classes do not use synchronization, at least&#13;
in CPU programming terms. <a data-primary="CAS (Compare and Swap)" data-type="indexterm" id="idm45775548470600"/><a data-primary="Compare and Swap (CAS)" data-type="indexterm" id="idm45775548469896"/>Atomic classes utilize a <em>Compare and Swap</em> (CAS)&#13;
CPU instruction, while synchronization requires exclusive access to a resource.&#13;
Threads that utilize CAS instructions&#13;
will not block when simultaneously accessing the same resource,&#13;
while a thread that needs a synchronization lock will block if another thread&#13;
holds that resource.</p>&#13;
&#13;
<p>The two approaches have diifferent performance (discussed later in this section). However, even though CAS instructions are lockless&#13;
and nonblocking, they still exhibit most of the behavior of blocking&#13;
constructs: their end result makes it appear to the developer that threads can&#13;
access the protected memory only serially.</p>&#13;
</div></aside>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Costs of Synchronization" data-type="sect2"><div class="sect2" id="idm45775548467400">&#13;
<h2>Costs of Synchronization</h2>&#13;
&#13;
<p><a data-primary="thread synchronization" data-secondary="costs of" data-type="indexterm" id="ix_ch09-asciidoc17"/>Synchronized areas of code affect performance in two ways.&#13;
First, the amount of time an application spends in a synchronized&#13;
block affects the scalability of an application.&#13;
Second, obtaining&#13;
the synchronization lock requires CPU cycles and hence affects&#13;
performance.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Synchronization and scalability" data-type="sect3"><div class="sect3" id="idm45775548464392">&#13;
<h3>Synchronization and scalability</h3>&#13;
&#13;
<p><a data-primary="thread synchronization" data-secondary="synchronization and scalability" data-type="indexterm" id="idm45775548462968"/>First things first: when an application is split up to run on multiple threads,&#13;
the <a data-primary="Amdahl's law" data-type="indexterm" id="idm45775548461800"/>speedup it sees is&#13;
defined by an equation known as <em>Amdahl’s law</em>:</p>&#13;
<div data-type="equation">&#13;
<math alttext="upper S p e e d u p equals StartStartFraction 1 OverOver left-parenthesis 1 minus upper P right-parenthesis plus StartFraction upper P Over upper N EndFraction EndEndFraction" display="block">&#13;
  <mrow>&#13;
    <mi>S</mi>&#13;
    <mi>p</mi>&#13;
    <mi>e</mi>&#13;
    <mi>e</mi>&#13;
    <mi>d</mi>&#13;
    <mi>u</mi>&#13;
    <mi>p</mi>&#13;
    <mo>=</mo>&#13;
    <mfrac><mn>1</mn> <mrow><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>P</mi><mo>)</mo></mrow><mo>+</mo><mfrac><mi>P</mi> <mi>N</mi></mfrac></mrow></mfrac>&#13;
  </mrow>&#13;
</math>&#13;
</div>&#13;
&#13;
<p><em>P</em> is the amount of the program that is run in parallel, and <em>N</em> is the&#13;
number of threads utilized (assuming that each thread always has available&#13;
CPU). So if 20% of the code exists in serialized blocks (meaning that <em>P</em> is&#13;
80%), the code can be expected to run (only)&#13;
3.33 times faster with eight available CPUs.</p>&#13;
&#13;
<p>One key fact about this equation is that as <em>P</em> decreases—that is, as&#13;
more code is located within serialized blocks—the performance benefit&#13;
of having multiple threads also decreases.&#13;
That is why limiting the amount of code that lies in the serialized&#13;
block is so important. In this example, with eight CPUs available, we might have&#13;
hoped for an eight times increase in speed. When only 20% of the code is&#13;
within a serialized block, the benefit of having multiple threads was reduced&#13;
by more than 50% (i.e., the increase was only 3.3 times).</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Costs of locking objects" data-type="sect3"><div class="sect3" id="idm45775548447560">&#13;
<h3>Costs of locking objects</h3>&#13;
&#13;
<p><a data-primary="thread synchronization" data-secondary="costs of locking objects" data-type="indexterm" id="ix_ch09-asciidoc18"/>Aside from its impact on scalability, the operation of synchronization carries two basic costs.</p>&#13;
&#13;
<p><a data-primary="synchronization lock" data-type="indexterm" id="idm45775548444184"/>First, we have the cost of&#13;
obtaining the synchronization lock. If a lock is uncontended—meaning that&#13;
two threads are not attempting to access the lock at the same time—this cost is&#13;
minimal. There is a slight difference here between the&#13;
<code class="keep-together">synchronized</code>&#13;
keyword and CAS-based constructs. <a data-primary="uninflated locks" data-type="indexterm" id="idm45775548442280"/>Uncontended&#13;
<code class="keep-together">synchronized</code>&#13;
locks are known as&#13;
<em>uninflated locks</em>, and the cost of obtaining an uninflated lock is on the&#13;
order of a few hundred nanoseconds. Uncontended CAS&#13;
code will see an even smaller performance penalty. (See <a data-type="xref" href="ch12.html#Misc">Chapter 12</a> for&#13;
an example of the difference.)</p>&#13;
&#13;
<p>Contended constructs are more expensive. When a second thread attempts to&#13;
access a&#13;
<code class="keep-together">synchronized</code>&#13;
lock, the lock becomes (predictably) inflated. This slightly increases the&#13;
time to acquire the lock, but the real impact here is that the second thread&#13;
must wait for the first thread to release the lock. That waiting time is&#13;
application-dependent, of course.</p>&#13;
&#13;
<p>The cost for a contended operation in code using CAS instructions is&#13;
unpredictable. The classes that use CAS&#13;
primitives are based on an optimistic strategy: the thread sets a value,&#13;
executes code, and then makes sure that the initial value has not changed.&#13;
If it has, the CAS-based code must execute the code again. In the worst&#13;
case, two threads could run into an infinite loop as each&#13;
modifies the CAS-protected value, only to see that the other thread has&#13;
modified it simultaneously. In practice, two threads are not going to get into&#13;
an infinite loop like that, but as the number of threads contending for the&#13;
CAS-based value increases, the number of retries <span class="keep-together">increases.</span></p>&#13;
&#13;
<p class="pagebreak-before">The second cost of synchronization is specific to Java and depends on the&#13;
Java Memory Model. Java, unlike languages such as C++ and C,&#13;
has a&#13;
strict guarantee about the memory semantics around synchronization, and&#13;
the guarantee applies to CAS-based protection, to traditional synchronization,&#13;
and to the&#13;
<span class="keep-together"><code>volatile</code></span>&#13;
keyword.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775548434456">&#13;
<h5>Uses of volatile in Examples</h5>&#13;
<p><a data-primary="volatile keyword" data-type="indexterm" id="idm45775548433048"/>The double-checked locking example in <a data-type="xref" href="ch07.html#Memory">Chapter 7</a> required the use&#13;
of a&#13;
<code class="keep-together">volatile</code>&#13;
variable:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">private</code> <code class="kd">volatile</code> <code class="n">ConcurrentHashMap</code> <code class="n">instanceChm</code><code class="o">;</code>&#13;
<code class="o">...</code>&#13;
<code class="kd">public</code> <code class="kt">void</code> <code class="nf">doOperation</code><code class="o">()</code> <code class="o">{</code>&#13;
    <code class="n">ConcurrentHashMap</code> <code class="n">chm</code> <code class="o">=</code> <code class="n">instanceChm</code><code class="o">;</code>&#13;
    <code class="k">if</code> <code class="o">(</code><code class="n">chm</code> <code class="o">==</code> <code class="kc">null</code><code class="o">)</code> <code class="o">{</code>&#13;
        <code class="kd">synchronized</code><code class="o">(</code><code class="k">this</code><code class="o">)</code> <code class="o">{</code>&#13;
            <code class="n">chm</code> <code class="o">=</code> <code class="n">instanceChm</code><code class="o">;</code>&#13;
            <code class="k">if</code> <code class="o">(</code><code class="n">chm</code> <code class="o">==</code> <code class="kc">null</code><code class="o">)</code> <code class="o">{</code>&#13;
                <code class="n">chm</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ConcurrentHashMap</code><code class="o">();</code>&#13;
                <code class="o">...</code> <code class="n">code</code> <code class="n">to</code> <code class="n">populate</code> <code class="n">the</code> <code class="n">map</code>&#13;
                <code class="n">instanceChm</code> <code class="o">=</code> <code class="n">chm</code><code class="o">;</code>&#13;
            <code class="o">}</code>&#13;
        <code class="o">}</code>&#13;
    <code class="o">}</code>&#13;
    <code class="o">...</code><code class="na">use</code> <code class="n">the</code> <code class="n">chm</code><code class="o">...</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>The&#13;
<code class="keep-together">volatile</code>&#13;
keyword accomplishes two things in this example. First,&#13;
note that the hash map is initialized using a local variable first, and only&#13;
the final (fully initialized) value is assigned to the&#13;
<code class="keep-together">instanceChm</code>&#13;
variable.&#13;
If the code populating the hash map were using the instance variable&#13;
directly, a second thread could see a partially populated map. And second,&#13;
it ensures that when the map is completely initialized, other threads will&#13;
immediately see that value stored into the&#13;
<code class="keep-together">instanceChm</code>&#13;
variable.</p>&#13;
&#13;
<p>Similarly, <a data-type="xref" href="ch02.html#SampleApplications">Chapter 2</a> pointed out that in microbenchmarks,&#13;
one way to make sure a result is used is to store it into a <code>volatile</code> variable.&#13;
That store cannot be optimized out by the compiler, so it ensures that&#13;
the code that produced the result held in that variable cannot be optimized&#13;
out either. The <code>Blackhole</code> class in <code>jmh</code> uses volatile stores for the same&#13;
reason (though that class also prevents against some other microbenchmark&#13;
effects and is another reason that using <code>jmh</code> is better than writing your own&#13;
microbenchmarks from scratch).</p>&#13;
</div></aside>&#13;
&#13;
<p>The purpose of synchronization is to protect access to values (or variables)&#13;
in memory.&#13;
As discussed in <a data-type="xref" href="ch04.html#JustInTimeCompilation">Chapter 4</a>, variables may be&#13;
temporarily stored in registers, which is much more efficient than directly&#13;
accessing them in main memory. Register&#13;
values are not visible to other threads; the thread that modifies a value&#13;
in a register must at some point flush that register to main memory so that&#13;
other threads can see the value. The point when the register values&#13;
must be flushed is dictated by thread&#13;
synchronization.</p>&#13;
&#13;
<p>The semantics can get fairly complicated, but the easiest way to think&#13;
of this is that when a thread leaves a synchronized block,&#13;
it must flush any modified variables to main memory. That means&#13;
other threads that enter the synchronized block will see the&#13;
most recently updated values. Similarly, CAS-based constructs ensure that&#13;
variables modified during their operation are flushed to main memory, and&#13;
a variable marked&#13;
<code class="keep-together">volatile</code>&#13;
is always consistently updated in main memory&#13;
whenever it is changed.</p>&#13;
&#13;
<p>In <a data-type="xref" href="ch01.html#Introduction">Chapter 1</a>, I mentioned that you should learn to avoid&#13;
nonperformant code constructs in Java, even if it seems like that might&#13;
be “prematurely optimizing” your code (it isn’t). An interesting case of&#13;
that—and a real-world example—comes from this loop:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="n">Vector</code> <code class="n">v</code><code class="o">;</code>&#13;
<code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">v</code><code class="o">.</code><code class="na">size</code><code class="o">();</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
    <code class="n">process</code><code class="o">(</code><code class="n">v</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="n">i</code><code class="o">));</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>In production, this loop was found to be taking a surprising amount of time,&#13;
and the logical assumption was that the&#13;
<code class="keep-together">process()</code>&#13;
method was the&#13;
culprit. But it wasn’t that, nor was the issue the&#13;
<code class="keep-together">size()</code>&#13;
and&#13;
<code class="keep-together">get()</code>&#13;
method&#13;
calls themselves (which had&#13;
been inlined by the compiler). The&#13;
<code class="keep-together">get()</code>&#13;
and&#13;
<code class="keep-together">size()</code>&#13;
methods of the&#13;
<code class="keep-together">Vector</code>&#13;
class are synchronized, and it turned out that the register flushing&#13;
required by all those calls&#13;
was a huge performance problem.<sup><a data-type="noteref" href="ch09.html#idm45775548329048" id="idm45775548329048-marker">1</a></sup></p>&#13;
&#13;
<p>This isn’t ideal code for other reasons. In particular, the state of the&#13;
vector can change between the time a thread calls the <code>size()</code> method and&#13;
the time it calls the <code>get()</code> method. If&#13;
a second thread removes the last element from the vector in between the <span class="keep-together">two calls made</span> by the first thread, the <code>get()</code> method will throw an <span class="keep-together"><code>ArrayIndexOutOf</code></span><span class="keep-together"><code>BoundsException</code></span>. Quite apart from the semantic&#13;
issues in the code, the fine-grained synchronization was a bad choice here.</p>&#13;
&#13;
<p>One way to avoid that is to wrap&#13;
lots of successive, fine-grained&#13;
synchronization calls within a synchronized block:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">synchronized</code><code class="o">(</code><code class="n">v</code><code class="o">)</code> <code class="o">{</code>&#13;
    <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">v</code><code class="o">.</code><code class="na">size</code><code class="o">();</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
        <code class="n">process</code><code class="o">(</code><code class="n">v</code><code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="n">i</code><code class="o">));</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>That doesn’t work well if the&#13;
<code class="keep-together">process()</code>&#13;
method takes a long time to execute, since the vector can no longer&#13;
be processed in parallel. Alternately, it may be necessary to copy and&#13;
partition the vector so that its elements can be processed in&#13;
parallel within the copies, while other threads can still modify the&#13;
original vector.</p>&#13;
&#13;
<p>The effect of register flushing is also dependent on the kind of&#13;
processor the program is running on; processors that have a lot of registers&#13;
for threads will require more flushing than simpler processors. In fact,&#13;
this code executed for a long time without problems in thousands of&#13;
environments. It became&#13;
an issue only when it was tried on a large SPARC-based machine with many registers per thread.</p>&#13;
&#13;
<p>Does that mean you are unlikely to see issues around&#13;
register flushing in smaller environments? Perhaps. But just as multicore&#13;
CPUs have become the norm for simple laptops, more complex CPUs with more&#13;
caching and registers are also becoming more commonplace, which will expose&#13;
hidden performance issues like this.</p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Thread synchronization has two performance costs: it limits the scalability of an application, and it requires obtaining locks.<a data-startref="ix_ch09-asciidoc18" data-type="indexterm" id="idm45775548208616"/></p>&#13;
</li>&#13;
<li>&#13;
<p>The memory semantics of synchronization, CAS-based utilities, and the <code class="keep-together">volatile</code> keyword can negatively impact performance, particularly on large machines with many registers.<a data-startref="ix_ch09-asciidoc17" data-type="indexterm" id="idm45775548206280"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Avoiding Synchronization" data-type="sect2"><div class="sect2" id="idm45775548205064">&#13;
<h2>Avoiding Synchronization</h2>&#13;
&#13;
<p><a data-primary="thread synchronization" data-secondary="avoiding" data-type="indexterm" id="ix_ch09-asciidoc19"/>If synchronization can be avoided altogether, locking penalties will not affect the application’s performance.&#13;
Two general approaches can be used to achieve that.</p>&#13;
&#13;
<p>The first approach is to use different objects in each thread so that&#13;
access to the objects will be uncontended.&#13;
Many Java objects are synchronized to make them thread-safe but don’t&#13;
necessarily need to be shared. The&#13;
<code class="keep-together">Random</code> class falls into&#13;
that category; <a data-type="xref" href="ch12.html#Misc">Chapter 12</a> shows an example within the JDK where the thread-local&#13;
technique was used to develop a new class to avoid the synchronization&#13;
in that class.</p>&#13;
&#13;
<p>On the flip side, many Java objects are expensive to create or use a&#13;
substantial amount of memory.&#13;
Take, for example, the&#13;
<code class="keep-together">NumberFormat</code>&#13;
class: instances of that class are not thread-safe, and the&#13;
internationalization&#13;
required to create an instance makes constructing new objects expensive. A program could get by with a single, shared global&#13;
<code class="keep-together">NumberFormat</code>&#13;
instance, but access to that shared object would need to be <span class="keep-together">synchronized.</span></p>&#13;
&#13;
<p>Instead, a better pattern is to use a <code>ThreadLocal</code> object:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code> <code class="kd">class</code> <code class="nc">Thermometer</code> <code class="o">{</code>&#13;
    <code class="kd">private</code> <code class="kd">static</code> <code class="n">ThreadLocal</code><code class="o">&lt;</code><code class="n">NumberFormat</code><code class="o">&gt;</code> <code class="n">nfLocal</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ThreadLocal</code><code class="o">&lt;&gt;()</code> <code class="o">{</code>&#13;
        <code class="kd">public</code> <code class="n">NumberFormat</code> <code class="nf">initialValue</code><code class="o">()</code> <code class="o">{</code>&#13;
            <code class="n">NumberFormat</code> <code class="n">nf</code> <code class="o">=</code> <code class="n">NumberFormat</code><code class="o">.</code><code class="na">getInstance</code><code class="o">();</code>&#13;
            <code class="n">nf</code><code class="o">.</code><code class="na">setMinumumIntegerDigits</code><code class="o">(</code><code class="mi">2</code><code class="o">);</code>&#13;
            <code class="k">return</code> <code class="n">nf</code><code class="o">;</code>&#13;
        <code class="o">}</code>&#13;
    <code class="o">}</code>&#13;
    <code class="kd">public</code> <code class="n">String</code> <code class="nf">toString</code><code class="o">()</code> <code class="o">{</code>&#13;
        <code class="n">NumberFormat</code> <code class="n">nf</code> <code class="o">=</code> <code class="n">nfLocal</code><code class="o">.</code><code class="na">get</code><code class="o">();</code>&#13;
        <code class="n">nf</code><code class="o">.</code><code class="na">format</code><code class="o">(...);</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>By using a thread-local variable, the total number of objects is limited&#13;
(minimizing the effect on GC), and&#13;
each object will never be subject to thread contention.</p>&#13;
&#13;
<p><a data-primary="CAS (Compare and Swap)" data-type="indexterm" id="ix_ch09-asciidoc20"/><a data-primary="Compare and Swap (CAS)" data-type="indexterm" id="ix_ch09-asciidoc21"/>The second way to avoid synchronization is to use CAS-based alternatives.&#13;
In some sense, this isn’t avoiding synchronization as much&#13;
as solving the problem differently. But in this context, by reducing the&#13;
penalty for synchronization, it works out to have the same effect.</p>&#13;
&#13;
<p>The difference in performance between CAS-based protections and traditional&#13;
synchronization seems like the ideal case to employ a microbenchmark: it&#13;
should be trivial to write code that compares a CAS-based operation with&#13;
a traditional synchronized method. For example, the JDK provides a simple&#13;
way to keep a counter using CAS-based protection: the <code>AtomicLong</code> and&#13;
similar classes. A microbenchmark could then compare code that uses&#13;
CAS-based protection to traditional synchronization. For example,&#13;
say a thread needs to get a global index and increment it&#13;
atomically (so that the next thread gets the next index). Using CAS-based&#13;
operations, that’s done like this:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="n">AtomicLong</code> <code class="n">al</code> <code class="o">=</code> <code class="k">new</code> <code class="n">AtomicLong</code><code class="o">(</code><code class="mi">0</code><code class="o">);</code>&#13;
<code class="kd">public</code> <code class="kt">long</code> <code class="nf">doOperation</code><code class="o">()</code> <code class="o">{</code>&#13;
    <code class="k">return</code> <code class="n">al</code><code class="o">.</code><code class="na">getAndIncrement</code><code class="o">();</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>The traditional synchronized version of that operation looks like this:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">private</code> <code class="kt">long</code> <code class="n">al</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code>&#13;
<code class="kd">public</code> <code class="kd">synchronized</code> <code class="nf">doOperation</code><code class="o">()</code> <code class="o">{</code>&#13;
    <code class="k">return</code> <code class="n">al</code><code class="o">++;</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>The difference between these two implementations turns out to be impossible&#13;
to measure with a microbenchmark. If there&#13;
is a single thread (so there is no possibility of contention), the&#13;
microbenchmark using this code can produce a reasonable estimate of the cost&#13;
of using the two approaches in an uncontended environment (and the result&#13;
of that test is cited in <a data-type="xref" href="ch12.html#Misc">Chapter 12</a>). But that doesn’t provide any information&#13;
about what happens in a contended environment (and if the code won’t ever&#13;
be contended, it doesn’t need to be thread-safe in the first place).</p>&#13;
&#13;
<p>In a microbenchmark built around these code snippets that is run&#13;
with only two threads, an enormous amount of contention will exist on the shared resource.&#13;
That isn’t realistic either: in a real application, it is unlikely that two threads will always be accessing the shared resource simultaneously. Adding more threads simply adds more unrealistic contention to the equation.</p>&#13;
&#13;
<p>As discussed in <a data-type="xref" href="ch02.html#SampleApplications">Chapter 2</a>,&#13;
microbenchmarks tend to greatly overstate the effect of&#13;
synchronization bottlenecks on the test in question.&#13;
This discussion ideally elucidates that&#13;
point. A much more realistic picture of the&#13;
trade-off will be obtained if the code in this section is used in&#13;
an actual application.</p>&#13;
&#13;
<p>In the general case, the following guidelines apply to the performance of CAS-based&#13;
utilities compared to traditional synchronization:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>If access to a resource is uncontended, CAS-based protection will be slightly faster than traditional synchronization. If the access is always uncontended, no protection at all will be slightly faster still and will avoid corner-cases like the one you just saw with the register flushing from the <code>Vector</code> class.</p>&#13;
</li>&#13;
<li>&#13;
<p>If access to a resource is lightly or moderately contended, CAS-based protection will be faster (often much faster) than traditional synchronization.</p>&#13;
</li>&#13;
<li>&#13;
<p>As access to the resource becomes heavily contended, traditional synchronization will at some point become the more efficient choice. In practice, this occurs only on very large machines running many threads.</p>&#13;
</li>&#13;
<li>&#13;
<p>CAS-based protection is not subject to contention when values are read and not written.</p>&#13;
</li>&#13;
</ul>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775547995816">&#13;
<h5>Contended Atomic Classes</h5>&#13;
<p><a data-primary="atomic classes" data-type="indexterm" id="idm45775547994648"/><a data-primary="classes" data-secondary="contended atomic classes" data-type="indexterm" id="idm45775547993944"/><a data-primary="contended atomic classes" data-type="indexterm" id="idm45775547993032"/>Classes in the&#13;
<span class="keep-together"><code>java.util.concurrent.atomic</code></span>&#13;
package use CAS-based primitives instead of traditional synchronization. As a result,&#13;
performance of those classes (for example, the&#13;
<span class="keep-together"><code>AtomicLong</code></span> class)&#13;
tends to be faster than writing a synchronized method to increment a long variable—at least until the contention for the CAS primitive becomes too high.</p>&#13;
&#13;
<p>Java has classes to address the situation when too many threads&#13;
contend for access to primitive atomic values: atomic adders and accumulators&#13;
(for example, the&#13;
<span class="keep-together"><code>LongAdder</code></span>&#13;
class). These classes are more scalable than the traditional atomic classes. When&#13;
<span class="keep-together">multiple</span> threads update a&#13;
<span class="keep-together"><code>LongAdder</code></span>, the&#13;
class can hold the updates separately for each thread. The threads&#13;
needn’t wait for each other to complete their operation; instead, the values are&#13;
stored in (essentially) an array, and each thread can return quickly. Later, the values&#13;
will be added or accumulated when a thread attempts to retrieve the current value.</p>&#13;
&#13;
<p>Under little or no contention, the value is accumulated as the program runs, and the&#13;
behavior of the adder will be the same as that of the traditional atomic class. Under&#13;
severe contention, the updates will be much faster, though the instance will start to&#13;
use more memory to store the array of values. Retrieval of a value in that case will&#13;
also be slightly slower since it must process all the pending updates in the array.&#13;
Still, under very contended conditions, these new classes will perform even better than&#13;
their atomic counterparts.<a data-startref="ix_ch09-asciidoc21" data-type="indexterm" id="idm45775547986088"/><a data-startref="ix_ch09-asciidoc20" data-type="indexterm" id="idm45775547985384"/></p>&#13;
</div></aside>&#13;
&#13;
<p>In the end, there is no substitute for extensive testing under the actual&#13;
production conditions where the code will run: only then can a definite&#13;
statement be made as to which implementation of a particular method is better.&#13;
Even in that case, the definite statement applies only to those conditions.</p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Avoiding contention for synchronized objects is a useful way to mitigate their performance impact.</p>&#13;
</li>&#13;
<li>&#13;
<p>Thread-local variables are never subject to contention; they are ideal for holding synchronized objects that don’t actually need to be shared between threads.</p>&#13;
</li>&#13;
<li>&#13;
<p>CAS-based utilities are a way to avoid traditional synchronization for objects that do need to be shared.<a data-startref="ix_ch09-asciidoc19" data-type="indexterm" id="idm45775547962600"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="False Sharing" data-type="sect2"><div class="sect2" id="idm45775548204152">&#13;
<h2>False Sharing</h2>&#13;
&#13;
<p><a data-primary="cache line (false) sharing" data-type="indexterm" id="ix_ch09-asciidoc22"/><a data-primary="false sharing" data-type="indexterm" id="ix_ch09-asciidoc23"/><a data-primary="thread synchronization" data-secondary="false sharing" data-type="indexterm" id="ix_ch09-asciidoc24"/>One little-discussed performance implication of synchronization involves <em>false&#13;
sharing</em> (also known as <em>cache line sharing</em>). It used to be a fairly obscure artifact of threaded programs, but as multicore machines become the norm—and as other, more obvious, synchronization performance issues are addressed—false sharing is an increasingly important issue.</p>&#13;
&#13;
<p>False sharing occurs because of the way CPUs handle their cache. Consider&#13;
the data in this simple class:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code> <code class="kd">class</code> <code class="nc">DataHolder</code> <code class="o">{</code>&#13;
    <code class="kd">public</code> <code class="kd">volatile</code> <code class="kt">long</code> <code class="n">l1</code><code class="o">;</code>&#13;
    <code class="kd">public</code> <code class="kd">volatile</code> <code class="kt">long</code> <code class="n">l2</code><code class="o">;</code>&#13;
    <code class="kd">public</code> <code class="kd">volatile</code> <code class="kt">long</code> <code class="n">l3</code><code class="o">;</code>&#13;
    <code class="kd">public</code> <code class="kd">volatile</code> <code class="kt">long</code> <code class="n">l4</code><code class="o">;</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>Each <code>long</code> value is stored in memory adjacent to one another; for&#13;
example, <code>l1</code> could be stored at memory location 0xF20. Then <code>l2</code> would be stored&#13;
in memory at 0xF28, <code>l3</code> at 0xF2C, and so on. When it comes time for the program&#13;
to operate on <code>l2</code>, it will load a relatively large amount of memory—for&#13;
example, 128&#13;
bytes from location 0xF00 to 0xF80—into a cache line on one of the cores of&#13;
one of the CPUs. A second thread that wants to operate on <code>l3</code> will load&#13;
that same chunk of memory into a cache line on a different core.</p>&#13;
&#13;
<p>Loading nearby values like that makes sense in most cases: if the application&#13;
accesses one particular instance variable in an object, it is likely to access&#13;
nearby instance variables. If they are already loaded into the core’s cache, that memory access is very, very fast—a big performance win.</p>&#13;
&#13;
<p>The downside to this scheme is that&#13;
whenever the program updates a value in its local cache, that core must&#13;
notify all the&#13;
other cores that the memory in question has been changed. Those other cores must&#13;
invalidate their cache lines and reload that data from memory.</p>&#13;
&#13;
<p>Let’s see what happens if the&#13;
<span class="keep-together"><code>DataHolder</code></span>&#13;
class is heavily used by multiple threads:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="nd">@State</code><code class="o">(</code><code class="n">Scope</code><code class="o">.</code><code class="na">Benchmark</code><code class="o">)</code>&#13;
<code class="nd">@BenchmarkMode</code><code class="o">(</code><code class="n">Mode</code><code class="o">.</code><code class="na">AverageTime</code><code class="o">)</code>&#13;
<code class="kd">public</code> <code class="kd">class</code> <code class="nc">ContendedTest</code> <code class="o">{</code>&#13;
    <code class="kd">private</code> <code class="kd">static</code> <code class="kd">class</code> <code class="nc">DataHolder</code> <code class="o">{</code>&#13;
	<code class="kd">private</code> <code class="kd">volatile</code> <code class="kt">long</code> <code class="n">l1</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code>&#13;
	<code class="kd">private</code> <code class="kd">volatile</code> <code class="kt">long</code> <code class="n">l2</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code>&#13;
	<code class="kd">private</code> <code class="kd">volatile</code> <code class="kt">long</code> <code class="n">l3</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code>&#13;
	<code class="kd">private</code> <code class="kd">volatile</code> <code class="kt">long</code> <code class="n">l4</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code>&#13;
    <code class="o">}</code>&#13;
    <code class="kd">private</code> <code class="kd">static</code> <code class="n">DataHolder</code> <code class="n">dh</code> <code class="o">=</code> <code class="k">new</code> <code class="n">DataHolder</code><code class="o">();</code>&#13;
&#13;
    <code class="n">Thread</code><code class="o">[]</code> <code class="n">threads</code><code class="o">;</code>&#13;
&#13;
    <code class="nd">@Setup</code><code class="o">(</code><code class="n">Level</code><code class="o">.</code><code class="na">Invocation</code><code class="o">)</code>&#13;
    <code class="kd">public</code> <code class="kt">void</code> <code class="nf">setup</code><code class="o">()</code> <code class="o">{</code>&#13;
	<code class="n">threads</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Thread</code><code class="o">[</code><code class="mi">4</code><code class="o">];</code>&#13;
	<code class="n">threads</code><code class="o">[</code><code class="mi">0</code><code class="o">]</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Thread</code><code class="o">(()</code> <code class="o">-&gt;</code> <code class="o">{</code>&#13;
		<code class="k">for</code> <code class="o">(</code><code class="kt">long</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">nLoops</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
			<code class="n">dh</code><code class="o">.</code><code class="na">l1</code> <code class="o">+=</code> <code class="n">i</code><code class="o">;</code>&#13;
		<code class="o">}</code>&#13;
	<code class="o">});</code>&#13;
	<code class="n">threads</code><code class="o">[</code><code class="mi">1</code><code class="o">]</code> <code class="o">=</code> <code class="k">new</code> <code class="n">Thread</code><code class="o">(()</code> <code class="o">-&gt;</code> <code class="o">{</code>&#13;
		<code class="k">for</code> <code class="o">(</code><code class="kt">long</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="n">nLoops</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
			<code class="n">dh</code><code class="o">.</code><code class="na">l2</code> <code class="o">+=</code> <code class="n">i</code><code class="o">;</code>&#13;
		<code class="o">}</code>&#13;
	<code class="o">});</code>&#13;
	<code class="c1">//...similar for 2 and 3...</code>&#13;
    <code class="o">}</code>&#13;
&#13;
    <code class="nd">@Benchmark</code>&#13;
    <code class="kd">public</code> <code class="kt">void</code> <code class="nf">test</code><code class="o">(</code><code class="n">Blackhole</code> <code class="n">bh</code><code class="o">)</code> <code class="kd">throws</code> <code class="n">InterruptedException</code> <code class="o">{</code>&#13;
        <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="mi">4</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
	    <code class="n">threads</code><code class="o">[</code><code class="n">i</code><code class="o">].</code><code class="na">start</code><code class="o">();</code>&#13;
	<code class="o">}</code>&#13;
        <code class="k">for</code> <code class="o">(</code><code class="kt">int</code> <code class="n">i</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code> <code class="n">i</code> <code class="o">&lt;</code> <code class="mi">4</code><code class="o">;</code> <code class="n">i</code><code class="o">++)</code> <code class="o">{</code>&#13;
	    <code class="n">threads</code><code class="o">[</code><code class="n">i</code><code class="o">].</code><code class="na">join</code><code class="o">();</code>&#13;
	<code class="o">}</code>&#13;
   <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>We have four separate threads, and they are not sharing any&#13;
variables: each is accessing only a single member of the&#13;
<span class="keep-together"><code>DataHolder</code></span>&#13;
class. From a synchronization standpoint, there is no contention, and we&#13;
might reasonably expect that this code would execute (on our&#13;
four-core machine) in the same amount of&#13;
time regardless of whether it runs one thread or four threads.</p>&#13;
&#13;
<p>It doesn’t turn out that way. When one particular thread writes the&#13;
<span class="keep-together"><code>volatile</code></span>&#13;
value in its loop, the cache line for every other thread will get invalidated,&#13;
and the memory values must be reloaded. <a data-type="xref" href="#TableFalseSharing">Table 9-10</a> shows the&#13;
result: performance gets worse as more threads&#13;
are added.</p>&#13;
<table id="TableFalseSharing">&#13;
<caption><span class="label">Table 9-10. </span>Time to sum 100,000 values with false sharing</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Number of threads</th>&#13;
<th>Elapsed time</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>1</p></td>&#13;
<td><p>0.8 ± 0.001 ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>2</p></td>&#13;
<td><p>5.7 ± 0.3 ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>3</p></td>&#13;
<td><p>10.4 ± 0.6 ms</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>4</p></td>&#13;
<td><p>15.5 ± 0.8 ms</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>This test case is constructed to show the most severe penalty for false&#13;
sharing: essentially every write invalidates all the other cache lines, and&#13;
performance is serial.</p>&#13;
&#13;
<p>Strictly speaking, false sharing does not have to involve synchronized (or&#13;
<span class="keep-together"><code>volatile</code>)</span>&#13;
variables: whenever any data value in the CPU cache is written, other caches&#13;
that hold the same data range must be invalidated. However, remember that the&#13;
Java memory model requires that the data must be written to main memory only&#13;
at the end of a synchronization primitive (including CAS and&#13;
<span class="keep-together"><code>volatile</code></span>&#13;
constructs). So that is the situation where it will be encountered most&#13;
frequently. If, in this example, the <code>long</code> variables are not&#13;
<span class="keep-together"><code>volatile</code>,</span>&#13;
the compiler will hold the values in registers, and the test will execute&#13;
in about 0.7 milliseconds regardless of the number of threads involved.</p>&#13;
&#13;
<p>This is obviously an extreme example, but it brings up the question of how&#13;
false sharing can be detected and corrected. Unfortunately, the answer is&#13;
murky and <span class="keep-together">incomplete.</span> Nothing in the standard set of tools discussed in&#13;
<a data-type="xref" href="ch03.html#Tools">Chapter 3</a> addresses false sharing, since it requires specific knowledge&#13;
about the architecture of a <span class="keep-together">processor.</span></p>&#13;
&#13;
<p>If you are lucky, the vendor of the target processor for your application will&#13;
have a tool that can be used to diagnose false sharing. Intel, for example, has&#13;
a program called VTune Amplifier that can be used to help detect false sharing by inspecting&#13;
cache miss events. Certain native profilers can provide information about the number&#13;
of clock cycles per instruction (CPI) for a given line of code; a high CPI for&#13;
a simple instruction within a loop can indicate that the code is waiting to reload&#13;
the target memory into the CPU cache.</p>&#13;
&#13;
<p>Otherwise, detecting false sharing requires intuition and experimentation.&#13;
If an ordinary profile indicates that a particular loop is taking a surprising amount&#13;
of time, check whether multiple threads may be accessing&#13;
unshared variables within the loop. (In the realm of performance tuning&#13;
as an art rather than a science, even the Intel VTune Amplifier manual says that the “primary&#13;
means of avoiding false sharing is through code inspection.”)</p>&#13;
&#13;
<p>Preventing false sharing requires code changes. An ideal situation is when&#13;
the variables involved can be written less frequently. In the preceding example, the&#13;
calculation could take place using local variables, and only the end result is written&#13;
back to the&#13;
<span class="keep-together"><code>DataHolder</code></span>&#13;
variable. The very small number of writes that ensues is unlikely to create&#13;
contention for the cache lines, and they won’t have a performance impact even if&#13;
all four threads update their results at the same time at the end of the loop.</p>&#13;
&#13;
<p>A second possibility involves padding the variables so that they won’t be loaded&#13;
on the same cache line. If the target CPU has 128-byte cache lines, padding&#13;
like this may work (but also, it may not):</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code> <code class="kd">class</code> <code class="nc">DataHolder</code> <code class="o">{</code>&#13;
    <code class="kd">public</code> <code class="kd">volatile</code> <code class="kt">long</code> <code class="n">l1</code><code class="o">;</code>&#13;
    <code class="n">pubilc</code> <code class="kt">long</code><code class="o">[]</code> <code class="n">dummy1</code> <code class="o">=</code> <code class="k">new</code> <code class="kt">long</code><code class="o">[</code><code class="mi">128</code> <code class="o">/</code> <code class="mi">8</code><code class="o">];</code>&#13;
    <code class="kd">public</code> <code class="kd">volatile</code> <code class="kt">long</code> <code class="n">l2</code><code class="o">;</code>&#13;
    <code class="n">pubilc</code> <code class="kt">long</code><code class="o">[]</code> <code class="n">dummy2</code> <code class="o">=</code> <code class="k">new</code> <code class="kt">long</code><code class="o">[</code><code class="mi">128</code> <code class="o">/</code> <code class="mi">8</code><code class="o">];</code>&#13;
    <code class="kd">public</code> <code class="kd">volatile</code> <code class="kt">long</code> <code class="n">l3</code><code class="o">;</code>&#13;
    <code class="n">pubilc</code> <code class="kt">long</code><code class="o">[]</code> <code class="n">dummy3</code> <code class="o">=</code> <code class="k">new</code> <code class="kt">long</code><code class="o">[</code><code class="mi">128</code> <code class="o">/</code> <code class="mi">8</code><code class="o">];</code>&#13;
    <code class="kd">public</code> <code class="kd">volatile</code> <code class="kt">long</code> <code class="n">l4</code><code class="o">;</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>Using arrays like that is unlikely to work, because the JVM will probably&#13;
rearrange the layout of those instance variables so that all the arrays are&#13;
next to each other, and then all the <code>long</code> variables will&#13;
still be next to each other.&#13;
Using primitive values to pad the structure is more likely to work, though it&#13;
can be impractical because of the number of variables required.</p>&#13;
&#13;
<p>We need to consider other issues when using padding to prevent false sharing.&#13;
The size of the padding is hard to predict, since different CPUs will have&#13;
different cache sizes. And the padding obviously adds significant size to the&#13;
instances in question, which will have an impact on the garbage collector&#13;
(depending, of course, on the number of instances required). Still, absent&#13;
an algorithmic solution, padding of the data can sometimes offer significant&#13;
advantages.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="ContendedAnnotation">&#13;
<h5>The @Contended Annotation</h5>&#13;
<p><a data-primary="@Contended annotation" data-type="indexterm" id="idm45775547554760"/>A feature within the private classes of the JDK can reduce&#13;
cache contention on specified fields (JEP 142). This is achieved by using&#13;
the&#13;
<span class="keep-together"><code>@sun.misc.Contended</code></span>&#13;
to mark variables that should be automatically padded by the JVM.</p>&#13;
&#13;
<p>This annotation is meant to be private. In Java 8, it belongs to the&#13;
<code>sun.misc</code> package, though nothing prevents you from using that package in&#13;
your own code.&#13;
In Java 11, it belongs to the&#13;
<code>jdk.internal.vm.annotation</code> package, and with the module system that Java 11&#13;
uses, you cannot compile classes using that package without using the&#13;
<span class="keep-together"><code>-add-exports</code></span> flag to add the package to the set of classes exported by&#13;
the <code>java.base</code> module.</p>&#13;
&#13;
<p>By default, the JVM ignores this annotation except within classes in the JDK.&#13;
To enable application code to use the annotation, include the&#13;
<span class="keep-together"><code>-XX:-RestrictContended</code></span>&#13;
flag, which by default is <code>true</code> (meaning that the annotation is restricted to&#13;
JDK classes).</p>&#13;
&#13;
<p><a data-primary="-XX:-EnableContended" data-type="indexterm" id="idm45775547548136"/>On the other hand, to disable the automatic padding that occurs&#13;
in the JDK, set the&#13;
<span class="keep-together"><code>-XX:-EnableContended</code></span>&#13;
flag, which by default is <code>true</code>. This will lead to reductions in the size&#13;
of the <code>Thread</code> and&#13;
<span class="keep-together"><code>ConcurrentHashMap</code></span>&#13;
classes, which both use this annotation to pad their implementations in order&#13;
to guard agaist false sharing.</p>&#13;
</div></aside>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>False sharing can significantly slow down performance code that frequently modifies <code>volatile</code> variables or exits synchronized blocks.</p>&#13;
</li>&#13;
<li>&#13;
<p>False sharing is difficult to detect. When a loop seems to be taking too long to occur, inspect the code to see if it matches the pattern where false sharing can occur.</p>&#13;
</li>&#13;
<li>&#13;
<p>False sharing is best avoided by moving the data to local variables and storing them later. Alternately, padding can sometimes be used to move the conflicting variables to different cache lines<a data-startref="ix_ch09-asciidoc24" data-type="indexterm" id="idm45775547540104"/><a data-startref="ix_ch09-asciidoc23" data-type="indexterm" id="idm45775547539400"/><a data-startref="ix_ch09-asciidoc22" data-type="indexterm" id="idm45775547538728"/>.<a data-startref="ix_ch09-asciidoc16" data-type="indexterm" id="idm45775547537928"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="JVM Thread Tunings" data-type="sect1"><div class="sect1" id="idm45775547960600">&#13;
<h1>JVM Thread Tunings</h1>&#13;
&#13;
<p><a data-primary="threads" data-secondary="JVM thread tunings" data-type="indexterm" id="ix_ch09-asciidoc26"/>The JVM has a few miscellaneous tunings that affect the performance of threads&#13;
and synchronization. These tunings will have a minor impact on the&#13;
performance of applications.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tuning Thread Stack Sizes" data-type="sect2"><div class="sect2" id="StackSize">&#13;
<h2>Tuning Thread Stack Sizes</h2>&#13;
&#13;
<p>When space is at a premium, the memory used by threads can be adjusted.&#13;
Each&#13;
thread has a native stack, which is where the OS stores the call stack&#13;
information of the thread (e.g., the fact that the&#13;
<code class="keep-together">main()</code>&#13;
method has&#13;
called the&#13;
<code class="keep-together">calculate()</code>&#13;
method, which has called the&#13;
<code class="keep-together">add()</code>&#13;
method).</p>&#13;
&#13;
<p>The size of this native stack is 1 MB (except for 32-bit Windows JVMs, where&#13;
it is 320 KB).&#13;
In&#13;
a 64-bit JVM, there is usually no reason to set this value unless the machine&#13;
is quite strained for physical memory and the smaller stack size will prevent&#13;
applications from running out of native memory. <a data-primary="Docker container" data-type="indexterm" id="idm45775547529240"/>This is especially true when&#13;
running inside a Docker container in which memory is limited.</p>&#13;
&#13;
<p>As a practical rule, many programs can run with a stack size of 256 KB, and&#13;
few need the full 1 MB. The&#13;
potential downside to setting this value too small is that a thread with&#13;
an extremely large call stack will throw a&#13;
<span class="keep-together"><code>StackOverflowError</code></span>.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775547526696">&#13;
<h5>Out of Native Memory</h5>&#13;
<p><a data-primary="OutOfMemoryError" data-type="indexterm" id="idm45775547525496"/><a data-primary="threads" data-secondary="OutOfMemoryError and" data-type="indexterm" id="idm45775547524792"/>An&#13;
<span class="keep-together"><code>OutOfMemoryError</code></span>&#13;
can occur when there isn’t enough native memory to create the thread. This&#13;
can indicate one of three things:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>In a 32-bit JVM, the process is at its 4 GB (or less, depending on the OS) maximum size.</p>&#13;
</li>&#13;
<li>&#13;
<p>The system has actually run out of virtual memory.</p>&#13;
</li>&#13;
<li>&#13;
<p>On Unix-style systems, the user has already created (between all programs they are running) the maximum number of processes configured for their login. Individual threads are considered a process in that regard.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Reducing the stack size can overcome the first two issues, but it will have no effect on&#13;
the third. Unfortunately, there is no way to tell from the JVM error which of these&#13;
three cases applies, but consider any of these causes when this error is encountered.</p>&#13;
</div></aside>&#13;
&#13;
<p>To change the stack size for a thread, use the&#13;
<span class="keep-together"><code>-Xss=</code><em><code>N</code></em></span>&#13;
flag (e.g.,&#13;
<span class="keep-together"><code>-Xss=256k</code>).</span></p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Thread stack sizes can be reduced on machines where memory is scarce.</p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Biased Locking" data-type="sect2"><div class="sect2" id="BiasedLocking">&#13;
<h2>Biased Locking</h2>&#13;
&#13;
<p><a data-primary="biased locking" data-type="indexterm" id="idm45775547511960"/><a data-primary="locks" data-secondary="biased locking" data-type="indexterm" id="idm45775547511256"/><a data-primary="threads" data-secondary="biased locking" data-type="indexterm" id="idm45775547510312"/>When locks are contended, the JVM (and operating system) have choices about&#13;
how the lock should be allocated. The lock can be granted fairly, meaning that&#13;
each thread will be given the lock in a round-robin fashion. Alternately,&#13;
the lock can be biased toward the thread that most recently accessed the&#13;
lock.</p>&#13;
&#13;
<p>The theory behind biased locking is that if a thread recently used a lock,&#13;
the processor’s cache is more likely to still contain data the thread will&#13;
need the next time it executes code protected by that same lock.&#13;
If the thread&#13;
is given priority for reobtaining the lock, the probability of cache hits&#13;
increases.&#13;
When this works out, performance is improved. But because&#13;
biased locking requires bookkeeping, it can sometimes be worse for&#13;
performance.</p>&#13;
&#13;
<p>In particular, applications that use a thread pool—including&#13;
some application and REST servers—often perform worse when biased locking is in&#13;
effect. In that programming model, different threads are equally likely&#13;
to access the contended locks. <a data-primary="-XX:-UseBiasedLocking" data-type="indexterm" id="idm45775547425272"/>For these kinds of applications,&#13;
a small performance improvement can be obtained by disabling biased locking&#13;
via the&#13;
<code class="keep-together">-XX:-UseBiasedLocking</code>&#13;
option. Biased locking is enabled by default.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Thread Priorities" data-type="sect2"><div class="sect2" id="idm45775547423464">&#13;
<h2>Thread Priorities</h2>&#13;
&#13;
<p><a data-primary="priority, thread" data-type="indexterm" id="idm45775547422024"/><a data-primary="threads" data-secondary="priorities" data-type="indexterm" id="idm45775547421320"/>Each Java thread has a developer-defined <em>priority</em>, which is a hint&#13;
to the operating&#13;
system about how important the program thinks the particular thread is. If you&#13;
have different threads doing different tasks, you might think you could use&#13;
the thread priority to improve the performance of certain tasks at the&#13;
expense of other tasks running on a lower-priority thread. Unfortunately,&#13;
it doesn’t quite work like that.</p>&#13;
&#13;
<p><a data-primary="current priority" data-type="indexterm" id="idm45775547419192"/>Operating systems calculate a <em>current priority</em> for every thread running&#13;
on a machine. The current priority takes into account the Java-assigned&#13;
priority but also includes many other factors, the most important&#13;
of which is how long it has been since the thread last ran. This ensures&#13;
that all threads will have an opportunity to run at some point.&#13;
Regardless of its priority,&#13;
no thread will “starve” waiting for access to the CPU.</p>&#13;
&#13;
<p>The balance between these two factors varies among operating systems.&#13;
On Unix-based systems, the calculation of the overall priority is dominated&#13;
by the amount of time since the thread has last run—the&#13;
Java-level priority of a thread has little effect.&#13;
On Windows, threads with&#13;
a higher Java priority tend to run more than threads with a lower priority,&#13;
but even low-priority threads get a fair amount of CPU time.</p>&#13;
&#13;
<p>In either case, you cannot depend on the priority of a thread&#13;
to affect&#13;
how frequently it runs. If some tasks are more important than other&#13;
tasks, application logic must be used to prioritize them.<a data-startref="ix_ch09-asciidoc26" data-type="indexterm" id="idm45775547416104"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Monitoring Threads and Locks" data-type="sect1"><div class="sect1" id="idm45775547415272">&#13;
<h1>Monitoring Threads and Locks</h1>&#13;
&#13;
<p><a data-primary="locks" data-secondary="monitoring" data-type="indexterm" id="ix_ch09-asciidoc27"/><a data-primary="monitoring" data-secondary="threads/locks" data-type="indexterm" id="ix_ch09-asciidoc28"/><a data-primary="threads" data-secondary="monitoring" data-type="indexterm" id="ix_ch09-asciidoc29"/>When analyzing an application’s performance for the efficiency of threading&#13;
and synchronization, we should look for two things: the overall number of threads (to make sure it is neither too high&#13;
nor too low) and the amount of time threads spend waiting for a lock&#13;
or other resource.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Thread Visibility" data-type="sect2"><div class="sect2" id="idm45775547409896">&#13;
<h2>Thread Visibility</h2>&#13;
&#13;
<p><a data-primary="monitoring" data-secondary="thread visibility" data-type="indexterm" id="idm45775547408728"/>Virtually every JVM monitoring tool provides information about the number of&#13;
threads (and what they are doing).&#13;
<a data-primary="jconsole" data-type="indexterm" id="idm45775547407496"/>Interactive tools like&#13;
<code class="keep-together">jconsole</code>&#13;
show the state of threads within the&#13;
JVM. On the&#13;
<code class="keep-together">jconsole</code>&#13;
Threads panel, you can watch in real time as the number&#13;
of threads increases and decreases during the execution of your program.&#13;
<a data-type="xref" href="#FigureJConsoleThread">Figure 9-2</a> shows an example.</p>&#13;
&#13;
<figure><div class="figure" id="FigureJConsoleThread">&#13;
<img alt="jp2e 0902" src="assets/jp2e_0902.png"/>&#13;
<h6><span class="label">Figure 9-2. </span>View of live threads in <code>jconsole</code></h6>&#13;
</div></figure>&#13;
&#13;
<p>At one point, the application (NetBeans) was using a maximum&#13;
of 45 threads. At the beginning of the graph, we can see a burst where the application was using up to 38, but it settled on using between 30 and 31.&#13;
<code class="keep-together">jconsole</code> can also print an individual&#13;
thread stack; as the figure shows, the Java2D Disposer thread is presently&#13;
waiting on a reference queue lock.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Blocked Thread Visibility" data-type="sect2"><div class="sect2" id="idm45775547400280">&#13;
<h2>Blocked Thread Visibility</h2>&#13;
&#13;
<p><a data-primary="monitoring" data-secondary="blocked thread visibility" data-type="indexterm" id="ix_ch09-asciidoc30"/>Real-time thread monitoring is useful for a very high-level picture of what&#13;
threads are running in the application, but it doesn’t really provide any&#13;
data on what those threads are doing. Determining where the threads are&#13;
spending CPU cycles requires the use of a profiler, as discussed in&#13;
<a data-type="xref" href="ch03.html#Tools">Chapter 3</a>. Profilers provide great visibility into what threads are&#13;
executing, and they are generally sophisticated enough to guide you to&#13;
areas in the code where better algorithms and code choices can speed up&#13;
overall execution.</p>&#13;
&#13;
<p>It is more difficult to diagnose threads that are blocked, although&#13;
that information is often more important in the overall execution of&#13;
an application—particularly if that code is running on a multi-CPU system&#13;
and is not utilizing all the available CPU. Three approaches can be used to&#13;
perform this diagnosis. One approach is again to use a profiler, since&#13;
most profiling tools will provide a timeline of thread execution that&#13;
allows you to see the points when a thread was blocked. An&#13;
example was given in <a data-type="xref" href="ch03.html#Tools">Chapter 3</a>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Blocked threads and JFR" data-type="sect3"><div class="sect3" id="idm45775547394264">&#13;
<h3>Blocked threads and JFR</h3>&#13;
&#13;
<p><a data-primary="Java Flight Recorder (JFR)" data-secondary="blocked threads and" data-type="indexterm" id="idm45775547392824"/>By far, the best way to know when threads are blocked is to use&#13;
tools that can look into the JVM and know at a low level when the threads&#13;
are blocked. One such tool is the Java Flight Recorder, introduced&#13;
in <a data-type="xref" href="ch03.html#Tools">Chapter 3</a>. We can drill into the events&#13;
that JFR captures and look for those that are causing a thread to block. The&#13;
usual event to look for is threads that are waiting to acquire a monitor,&#13;
but if we observe threads with long reads (and rarely, long writes) to a&#13;
socket, they are likely blocked as well.</p>&#13;
&#13;
<p>These events can be easily viewed on the Histogram panel of Java&#13;
Mission Control, as shown in <a data-type="xref" href="#FigureJFRMonitor">Figure 9-3</a>.</p>&#13;
&#13;
<p>In this sample, the lock associated with the&#13;
<code class="keep-together">HashMap</code> in the&#13;
<code>sun.awt.AppCon⁠text.get()</code>&#13;
method was contended 163 times (over 66 seconds), causing an average 31 ms increase in the response time of the request being measured. The stack trace points out that the contention stems from the way the JSP is writing a&#13;
<code class="keep-together">java.util.Date</code> object. To improve the scalability of this code, a thread-local date formatter could be used instead of simply calling the date’s&#13;
<code class="keep-together">toString()</code> method.</p>&#13;
&#13;
<figure><div class="figure" id="FigureJFRMonitor">&#13;
<img alt="jp2e 0903" src="assets/jp2e_0903.png"/>&#13;
<h6><span class="label">Figure 9-3. </span>Threads blocked by a monitor in JFR</h6>&#13;
</div></figure>&#13;
&#13;
<p>This process—choosing the blocking event from the histogram and examining&#13;
the calling code—works for any kind of blocking event; it is made possible&#13;
by the tight integration of the tool with the JVM.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Blocked threads and JStack" data-type="sect3"><div class="sect3" id="idm45775547382952">&#13;
<h3>Blocked threads and JStack</h3>&#13;
&#13;
<p><a data-primary="jstack" data-type="indexterm" id="ix_ch09-asciidoc31"/>If a JFR recording of the program is not available,&#13;
an alternative is to take a lot of thread stacks from the program and examine&#13;
those.&#13;
<code class="keep-together">jstack</code>,&#13;
<code class="keep-together">jcmd</code>,&#13;
and other tools can provide information&#13;
about the state&#13;
of every thread in a VM, including whether the thread is running, waiting&#13;
for a lock, waiting for I/O, and so on. This can be quite useful for&#13;
determining what’s going on in an application, as long as&#13;
too much is not expected from the output.</p>&#13;
&#13;
<p>The first caveat in looking at thread stacks is that the JVM can dump a thread’s stack only at safepoints. Second, stacks are&#13;
dumped for each thread one at a time, so it is possible to get conflicting&#13;
information from them: two threads can show up holding the same lock, or&#13;
a thread can show up waiting for a lock that no other thread holds.</p>&#13;
&#13;
<p>Thread stacks can show how significantly threads are&#13;
blocked (since a thread that is blocked is already at a safepoint). If&#13;
successive thread dumps show many threads blocked&#13;
on a lock, you can conclude that the lock in question has significant&#13;
contention. If successive thread dumps show many threads blocked&#13;
waiting for I/O, you can conclude that whatever I/O they are reading&#13;
needs to be tuned (e.g., if they are <span class="keep-together">making</span> a database call, the SQL they&#13;
are executing needs to be tuned, or the database itself needs to be tuned).</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775547375992">&#13;
<h5>A JStack Profiler</h5>&#13;
<p>It is tempting to think you can take multiple stack dumps in rapid&#13;
succession and use that data as a quick-and-dirty profiler. After all,&#13;
sampling profilers work in essentially the same way: they periodically probe&#13;
the stack a thread is executing and extrapolate how much time is spent&#13;
in methods based on that. But between safepoints and inconsistent snapshots,&#13;
this doesn’t work out too well; you can sometimes get a very high-level&#13;
overview of the expensive methods in your application by looking at thread&#13;
stacks, but a real profiler will give far more accurate information.</p>&#13;
</div></aside>&#13;
&#13;
<p>The online examples for this book have a rudimentary parser for&#13;
<code class="keep-together">jstack</code>&#13;
output&#13;
that can summarize the state of all threads from one or more thread dumps.&#13;
A problem with&#13;
<code class="keep-together">jstack</code>&#13;
output is that it can change from release to release, so&#13;
developing a robust parser can be difficult. There is no guarantee that the&#13;
parser in the online examples won’t need to be tweaked for your particular&#13;
JVM.</p>&#13;
&#13;
<p>The basic output of the&#13;
<code class="keep-together">jstack</code>&#13;
parser looks like this:</p>&#13;
<pre data-type="programlisting">&#13;
% <strong>jstack pid &gt; jstack.out</strong>&#13;
% <strong>java ParseJStack jstack.out</strong>&#13;
[Partial output...]&#13;
Threads in start Running&#13;
    8 threads in java.lang.Throwable.getStackTraceElement(Native&#13;
Total Running Threads: 8&#13;
&#13;
Threads in state Blocked by Locks&#13;
    41 threads running in&#13;
    	com.sun.enterprise.loader.EJBClassLoader.getResourceAsStream&#13;
	(EJBClassLoader.java:801)&#13;
Total Blocked by Locks Threads: 41&#13;
&#13;
Threads in state Waiting for notify&#13;
    39 threads running in&#13;
    	com.sun.enterprise.web.connector.grizzly.LinkedListPipeline.getTask&#13;
	(LinkedListPipeline.java:294)&#13;
    18 threads running in System Thread&#13;
Total Waiting for notify Threads: 74&#13;
&#13;
Threads in state Waiting for I/O read&#13;
    14 threads running in com.acme.MyServlet.doGet(MyServlet.java:603)&#13;
Total Waiting for I/O read Threads: 14&#13;
</pre>&#13;
&#13;
<p>The parser aggregates all the threads and shows how many are in various&#13;
states. Eight threads are currently running (they happen to be doing a stack&#13;
trace—an expensive operation that is better to avoid).</p>&#13;
&#13;
<p>Forty-one threads are blocked by a lock. The method reported is the first&#13;
non-JDK method in the stack trace, which in this example is the&#13;
GlassFish method&#13;
<code class="keep-together">EJBClassLoader.getResourceAsStream()</code>.&#13;
The next step&#13;
would be to consult the stack trace, search for that method, and see&#13;
what resource the thread is blocked on.</p>&#13;
&#13;
<p>In this example, all the threads&#13;
were blocked waiting to read the same JAR file, and&#13;
the stack trace for those threads showed that all the calls came from&#13;
<a data-primary="SAX (Simple API for XML) parser" data-type="indexterm" id="idm45775547365608"/><a data-primary="Simple API for XML (SAX) parser" data-type="indexterm" id="idm45775547364840"/>instantiating a new Simple API for XML (SAX) parser. It turns out that&#13;
the SAX parser can be defined dynamically by listing the resource in&#13;
the manifest file of the application’s JAR files, which means that the JDK&#13;
must search the entire classpath for those entries until it finds the one&#13;
the application wants to use (or until it doesn’t find anything and&#13;
falls back to the system parser). Because reading the JAR file requires a&#13;
synchronization lock, all those threads trying to create a parser end up&#13;
contending for the same lock, which is greatly hampering the application’s&#13;
throughput. (To overcome this case,&#13;
set the <span class="keep-together"><code>-Djavax.xml</code></span><code>.parsers.SAXParserFactory</code> property to avoid those lookups.)</p>&#13;
&#13;
<p>The larger point is that having a lot of blocked threads&#13;
diminishes performance.&#13;
Whatever the cause of the blocking, changes need to be made to the&#13;
configuration or application to avoid it.</p>&#13;
&#13;
<p>What about the threads that are waiting for notification? Those threads are&#13;
waiting for something else to happen.&#13;
Often they are in a pool waiting for notification that a task&#13;
is ready (e.g., the&#13;
<code class="keep-together">getTask()</code>&#13;
method in the preceding output is waiting for a request).&#13;
System threads are doing things like RMI distributed GC or JMX monitoring—they appear in the <code>jstack</code> output as threads that have only&#13;
JDK classes in their stack. These conditions do not necessarily indicate&#13;
a performance problem; it is normal for them to be waiting for a notification.</p>&#13;
&#13;
<p>Another problem creeps up in the threads waiting for I/O read: these&#13;
are doing a blocking I/O call (usually the&#13;
<code class="keep-together">socketRead0()</code>&#13;
method). This is also hampering throughput: the&#13;
thread is waiting for a backend resource to answer its request. That’s the&#13;
time to start looking into the performance of the database or other&#13;
backend resource.<a data-startref="ix_ch09-asciidoc31" data-type="indexterm" id="idm45775547358104"/></p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Basic visibility into the threads of a system provides an overview of the number of threads running.</p>&#13;
</li>&#13;
<li>&#13;
<p>Thread visibility allows us to determine why threads are blocked: whether because they are waiting for a resource or for I/O.</p>&#13;
</li>&#13;
<li>&#13;
<p>Java Flight Recorder provides an easy way to examine the events that caused a thread to block.</p>&#13;
</li>&#13;
<li>&#13;
<p><code class="keep-together">jstack</code> provides a level of visibility into the resources threads are blocked on<a data-startref="ix_ch09-asciidoc30" data-type="indexterm" id="idm45775547351880"/>.<a data-startref="ix_ch09-asciidoc29" data-type="indexterm" id="idm45775547351048"/><a data-startref="ix_ch09-asciidoc28" data-type="indexterm" id="idm45775547350344"/><a data-startref="ix_ch09-asciidoc27" data-type="indexterm" id="idm45775547349672"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45775547348488">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Understanding how threads operate can yield important performance benefits.&#13;
Thread performance, though, is not so much about tuning—there are&#13;
relatively few JVM flags to tweak, and those few flags have limited effects.</p>&#13;
&#13;
<p>Instead, good thread performance is about following best-practice guidelines&#13;
for managing the number of threads and for limiting the effects of&#13;
synchronization. With the help of appropriate profiling and lock analysis&#13;
tools, applications can be examined and modified so that threading and&#13;
locking issues do not negatively affect <span class="keep-together">performance.</span><a data-startref="ix_ch09-asciidoc1" data-type="indexterm" id="idm45775547345624"/><a data-startref="ix_ch09-asciidoc0" data-type="indexterm" id="idm45775547344920"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45775548329048"><sup><a href="ch09.html#idm45775548329048-marker">1</a></sup> Although modern code would use a difference collection class, the example would be the same with a collection wrapped via the <code>synchronizedCollection()</code> method, or any other loop with excessive register flushing.</p></div></div></section></body></html>