<html><head></head><body>
<div id="sbo-rt-content"><section class="pagenumrestart" data-pdf-bookmark="Chapter 1. Optimization and Performance Defined" data-type="chapter" epub:type="chapter"><div class="chapter" id="pracjavaperf-CHP-1">
<h1><span class="label">Chapter 1. </span>Optimization and Performance Defined</h1>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id92">
<h1>A Note for Early Release Readers</h1>
<p>With Early Release ebooks, you get books in their earliest form—the author’s raw and unedited content as they write—so you can take advantage of these technologies long before the official release of these titles.</p>
<p>This will be the 1st chapter of the final book.</p>
<p>If you have comments about how we might improve the content and/or examples in this book, or if you notice missing material within this chapter, please reach out to the editor at <em>rfernando@oreilly.com</em>.</p>
</div></aside>
<p>Optimizing the performance of Java (or any other sort of code) is often seen as a Dark Art.
There’s a mystique about performance analysis—it’s commonly viewed as a craft practiced by the “lone hacker, who is tortured and deep thinking” (one of Hollywood’s favorite tropes about computers and the people who operate them).
The image is one of a single individual who can see deeply into a system and come up with a magic solution that makes the system work faster.</p>
<p>This image is often coupled with the unfortunate (but all-too-common) situation where performance is a second-class concern of the software teams.
This sets up a scenario where analysis is only done once the system is already in trouble, and so needs a performance “hero” to save it.
The reality, however, is a little different.</p>
<p>The truth is that performance analysis is a weird blend of hard empiricism and squishy human psychology.
What matters is, at one and the same time, the absolute numbers of observable metrics and how the end users and stakeholders <em>feel</em> about them.
The resolution of this apparent paradox is the subject of the rest of this book.</p>
<p>Since the publication of the First Edition, this situation has only sharpened.
As more and more workloads move into the cloud, and as systems become ever-more complicated, the strange brew that combines very different factors has become even more important and prevalent.
The “domain of concern” that an engineer who cares about performance needs to operate in has continued to broaden.</p>
<p>This is because production systems have become even more complicated.
More of them now have aspects of distributed systems to consider in addition to the performance of individual application processes.
As system architectures become larger and more complex, the number of engineers who must concern themselves with performance has also increased.</p>
<p>The new edition of this book responds to these changes in our industry by providing four things:</p>
<ul>
<li>
<p>A necessary deep-dive on the performance of application code running within a single-JVM</p>
</li>
<li>
<p>A discussion of JVM internals</p>
</li>
<li>
<p>Details of how the modern cloud stack interacts with Java / JVM applications</p>
</li>
<li>
<p>A first look at the behavior of Java applications running on a cluster in a cloud environment</p>
</li>
</ul>
<p>In this chapter, we will get going by setting the stage with some definitions and establishing a framework for <em>how</em> we talk about performance—​starting with some problems and pitfalls that plague many discussions of Java performance.</p>
<section data-pdf-bookmark="Java Performance—The Wrong Way" data-type="sect1"><div class="sect1" id="pracjavaperf-CHP-1-SECT-1">
<h1>Java Performance—The Wrong Way</h1>
<p>For many years, one of the top three hits on Google for “Java performance tuning” was an article from 1997–8, which had been ingested into the index very early in Google’s history.
The page had presumably stayed close to the top because its initial ranking served to actively drive traffic to it, creating a feedback loop.</p>
<p>The page housed advice that was completely out of date, no longer true, and in many cases detrimental to applications.
However, its favored position in the search engine results caused many, many developers to be exposed to terrible advice.</p>
<p>For example, very early versions of Java had terrible method dispatch performance.
As a workaround, some Java developers advocated avoiding small methods and instead writing monolithic methods.
Of course, over time, the performance of virtual dispatch greatly improved.</p>
<p>Not only that, but with modern JVM technologies (especially automatic managed inlining), virtual dispatch has now been eliminated at a large number—​perhaps even the majority—​of call sites.
Code that followed the “lump everything into one method” advice is now at a substantial disadvantage, as it is very unfriendly to modern Just-in-Time (JIT) compilers.</p>
<p>There’s no way of knowing how much damage was done to the performance of applications that were subjected to the bad advice, but this case neatly demonstrates the dangers of not using a quantitative and verifiable approach to performance.
It also provides yet another excellent example of why you shouldn’t believe everything you read on the internet.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The execution speed of Java code is highly dynamic and fundamentally depends on the underlying Java Virtual Machine. An old piece of Java code may well execute faster on a more recent JVM, even without recompiling the Java source code.</p>
</div>
<p>As you might imagine, for this reason (and others we will discuss later) this book is not a cookbook of performance tips to apply to your code. Instead, we focus on a range of aspects that come together to produce good performance engineering:</p>
<ul>
<li>
<p>Performance methodology within the overall software lifecycle</p>
</li>
<li>
<p>Theory of testing as applied to performance</p>
</li>
<li>
<p>Measurement, statistics, and tooling</p>
</li>
<li>
<p>Analysis skills (both systems and data)</p>
</li>
<li>
<p>Underlying technology and mechanisms</p>
</li>
</ul>
<p>By bringing these aspects together, the intention is to help you build an understanding that can be broadly applied to whatever performance circumstances that you may face.</p>
<p>Later in the book, we will introduce some heuristics and code-level techniques for optimization, but these all come with caveats and tradeoffs that the developer should be aware of before using them.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Please do not skip ahead to those sections and start applying the techniques detailed without properly understanding the context in which the advice is given.
All of these techniques are capable of doing more harm than good if you lack a proper understanding of how they should be applied.</p>
</div>
<p>In general, there are:</p>
<ul>
<li>
<p>No magic “go faster” switches for the JVM</p>
</li>
<li>
<p>No “tips and tricks” to make Java run faster</p>
</li>
<li>
<p>No secret algorithms that have been hidden from you</p>
</li>
</ul>
<p>As we explore our subject, we will discuss these misconceptions in more detail, along with some other common mistakes that developers often make when approaching Java performance analysis and related issues.</p>
<p>Our “No Tips and Tricks” approach extends to our coverage of cloud techniques.
You will not find virtually any discussion of the vendor-specific techniques present on the cloud hyperscalars (AWS, Azure, GCP, OpenShift, and so on).
This is for two main reasons:</p>
<ul>
<li>
<p>It would expand the scope of the book and make it unmanageably long</p>
</li>
<li>
<p>It is impossible to stay current with such a large topic area</p>
</li>
</ul>
<p>The progress made by teams working on those products would make any detailed information about them out-of-date by the time the book is published.
So, instead, in the cloud chapters, we focus on fundamentals and patterns, which remain effective regardless of which hyperscalar your applications are deployed upon.</p>
<p>Still here? Good. Then let’s talk about performance.</p>
</div></section>
<section data-pdf-bookmark="Java Performance Overview" data-type="sect1"><div class="sect1" id="pracjavaperf-CHP-1-SECT-2">
<h1>Java Performance Overview</h1>
<p>To understand why Java performance is the way that it is, let’s start by considering a classic quote from James Gosling, the creator of Java:</p>
<blockquote>
<p>Java is a blue collar language. It’s not PhD thesis material but a language for a job.<sup><a data-type="noteref" href="ch01.xhtml#id93" id="id93-marker">1</a></sup></p>
<p data-type="attribution">James Gosling</p>
</blockquote>
<p>That is, Java has always been an extremely practical language. Its attitude to performance was initially that as long as the environment was <em>fast enough</em>, then raw performance could be sacrificed if developer productivity benefited. It was therefore not until relatively recently, with the increasing maturity and sophistication of JVMs such as HotSpot, that the Java environment became suitable for high-performance computing applications.</p>
<p>This practicality manifests itself in many ways in the Java platform, but one of the most obvious is the use of <em>managed subsystems</em>.
The idea is that the developer gives up some aspects of low-level control in exchange for not having to worry about some of the details of the capability under management.</p>
<p>The most obvious example of this is, of course, memory management.
The JVM provides automatic memory management in the form of a pluggable <em>garbage collection</em> subsystem (usually referred to as GC), so that memory does not have to be manually tracked by the programmer.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Managed subsystems occur throughout the JVM and their existence introduces extra complexity into the runtime behavior of JVM applications.</p>
</div>
<p>As we will discuss in the next section, the complex runtime behavior of JVM applications requires us to treat our applications as experiments under test.
This leads us to think about the statistics of observed measurements, and here we make an unfortunate discovery.</p>
<p>The observed performance measurements of JVM applications are very often not normally distributed.
This means that elementary statistical techniques (especially <em>standard deviation</em> and <em>variance</em> for example) are ill-suited for handling results from JVM applications.
This is because many basic statistics methods contain an implicit assumption about the normality of results distributions.</p>
<p>One way to understand this is that for JVM applications outliers can be very significant—for a low-latency trading application, for example.
This means that sampling of measurements is also problematic, as it can easily miss the exact events that have the most importance.</p>
<p>Finally, a word of caution.
It is very easy to be misled by Java performance measurements.
The complexity of the environment means that it is very hard to isolate individual aspects of the system.</p>
<p>Measurement also has an overhead, and frequent sampling (or recording every result) can have an observable impact on the performance numbers being recorded.
The nature of Java performance numbers requires a certain amount of statistical sophistication, and naive techniques frequently produce incorrect results when applied to Java/JVM applications.</p>
<p>These concerns also resonate into the domain of cloud native applications.
Automatic management of applications has very much become part of the cloud native experience—​especially with the rise of technologies such as Kubernetes.
The need to balance the cost of collecting data with the need to collect enough to make conclusions is also an important architectural concern for cloud native apps—​we will have more to say about that in Chapter 10.</p>
</div></section>
<section data-pdf-bookmark="Performance as an Experimental Science" data-type="sect1"><div class="sect1" id="pracjavaperf-CHP-1-SECT-3">
<h1>Performance as an Experimental Science</h1>
<p>Java/JVM software stacks are, like most modern software systems, very complex.
In fact, due to the highly optimizing and adaptive nature of the JVM, production systems built on top of the JVM can have some subtle and intricate performance behavior.
This complexity has been made possible by Moore’s Law and the unprecedented growth in hardware capability that it represents.</p>
<blockquote>
<p>The most amazing achievement of the computer software industry is its continuing cancellation of the steady and staggering gains made by the computer hardware industry.</p>
<p data-type="attribution">Henry Petroski (attr)</p>
</blockquote>
<p>While some software systems have squandered the historical gains of the industry, the JVM represents something of an engineering triumph.
Since its inception in the late 1990s the JVM has developed into a very high-performance, general-purpose execution environment that puts those gains to very good use.</p>
<p>The tradeoff, however, is that like any complex, high-performance system, the JVM requires a measure of skill and experience to get the absolute best out of it.</p>
<blockquote>
<p>A measurement not clearly defined is worse than useless.<sup><a data-type="noteref" href="ch01.xhtml#id94" id="id94-marker">2</a></sup></p>
<p data-type="attribution">Eli Goldratt</p>
</blockquote>
<p>JVM performance tuning is therefore a synthesis between technology, methodology, measurable quantities, and tools.
Its aim is to effect measurable outputs in a manner desired by the owners or users of a system.
In other words, performance is an experimental science—it achieves a desired result by:</p>
<ul>
<li>
<p>Defining the desired outcome</p>
</li>
<li>
<p>Measuring the existing system</p>
</li>
<li>
<p>Determining what is to be done to achieve the requirement</p>
</li>
<li>
<p>Undertaking an improvement exercise</p>
</li>
<li>
<p>Retesting</p>
</li>
<li>
<p>Determining whether the goal has been achieved</p>
</li>
</ul>
<p>The process of defining and determining desired performance outcomes builds a set of quantitative objectives.
It is important to establish what should be measured and record  the objectives, which then form part of the project’s artifacts and deliverables.
From this, we can see that performance analysis is based upon defining, and then achieving, nonfunctional requirements.</p>
<p>This process is, as has been previewed, not one of reading chicken entrails or another divination method.
Instead, we rely upon statistics and an appropriate handling (and interpretation) of results.</p>
<p>In this chapter, we discuss these techniques as they apply to a single JVM.
In <a data-type="xref" href="ch02.xhtml#pracjavaperf-CHP-2">Chapter 2</a> we will introduce a primer on the basic statistical techniques that are required for accurate handling of data generated from a JVM performance analysis project.
Later on, primarily in Chapter 10, we will discuss how these techniques generalize to a clustered application and give rise to the notion of Observability.</p>
<p>It is important to recognize that, for many real-world projects, a more sophisticated understanding of data and statistics will undoubtedly be required.
You are therefore encouraged to view the statistical techniques found in this book as a starting point, rather than a definitive statement.</p>
</div></section>
<section data-pdf-bookmark="A Taxonomy for Performance" data-type="sect1"><div class="sect1" id="pracjavaperf-CHP-1-SECT-4">
<h1>A Taxonomy for Performance</h1>
<p>In this section, we introduce some basic observable quantities for performance analysis.
These provide a vocabulary for performance analysis and will allow you to frame the objectives of a tuning project in quantitative terms.
These objectives are the nonfunctional requirements that define performance goals.
Note that these quantities are not necessarily directly available in all cases, and some may require some work to obtain from the raw numbers obtained from our system.</p>
<p>One common basic set of performance <span class="keep-together">observables is:</span></p>
<ul>
<li>
<p>Throughput</p>
</li>
<li>
<p>Latency</p>
</li>
<li>
<p>Capacity</p>
</li>
<li>
<p>Utilization</p>
</li>
<li>
<p>Efficiency</p>
</li>
<li>
<p>Scalability</p>
</li>
<li>
<p>Degradation</p>
</li>
</ul>
<p>We will briefly discuss each in turn.
Note that for most performance projects, not every metric will be optimized simultaneously.
The case of only a few metrics being improved in a single performance iteration is far more common, and this may be as many as can be tuned at once.
In real-world projects, it may well be the case that optimizing one metric comes at the detriment of another metric or group of metrics.</p>
<section data-pdf-bookmark="Throughput" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-1-SECT-4.1">
<h2>Throughput</h2>
<p>Throughput is a metric that represents the rate of work a system or subsystem can perform.
This is usually expressed as number of units of work in some time period.
For example, we might be interested in how many transactions per second a system can execute.</p>
<p>For the throughput number to be meaningful in a real performance exercise, it should include a description of the reference platform it was obtained on.
For example, the hardware spec, OS, and software stack are all relevant to throughput, as is whether the system under test is a single server or a cluster.
In addition, transactions (or units of work) should be the same between tests.
Essentially, we should seek to ensure that the workload for throughput tests is kept consistent between runs.</p>
<p>Performance metrics are sometimes explained via metaphors that evoke plumbing.
If we adopt this view point then, if a water pipe can produce 100 liters per second, then the volume produced in 1 second (100 liters) is the throughput.
Note that this value is a function of the speed of the water and the cross-sectional area of the pipe.</p>
</div></section>
<section data-pdf-bookmark="Latency" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-1-SECT-4.2">
<h2>Latency</h2>
<p>To continue the metaphor of the previous section—​latency is how long it takes a given liter to traverse the pipe.
This is a function of both the length of the pipe and how quickly the water is moving through it.
It is not, however, a function of the diameter of the pipe.</p>
<p>In software, latency is normally quoted as an end-to-end time—​the time taken to process a single transaction and see a result.
It is dependent on workload, so a common approach is to produce a graph showing latency as a function of increasing workload.
We will see an example of this type of graph in <a data-type="xref" href="#pracjavaperf-CHP-1-SECT-5">“Reading Performance Graphs”</a>.</p>
</div></section>
<section data-pdf-bookmark="Capacity" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-1-SECT-4.3">
<h2>Capacity</h2>
<p>The capacity is the amount of work parallelism a system possesses—that is, the
number of units of work (e.g., transactions) that can be simultaneously ongoing in the
system.</p>
<p>Capacity is obviously related to throughput, and we should expect that as the
concurrent load on a system increases, throughput (and latency) will be
affected. For this reason, capacity is usually quoted as the processing available at
a given value of latency or throughput.</p>
</div></section>
<section data-pdf-bookmark="Utilization" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-1-SECT-4.4">
<h2>Utilization</h2>
<p>One of the most common performance analysis tasks is to achieve efficient use of a system’s resources.
Ideally, CPUs should be used for handling units of work, rather than being idle (or spending time handling OS or other housekeeping tasks).</p>
<p>Depending on the workload, there can be a huge difference between the utilization levels of different resources.
For example, a computation-intensive workload (such as graphics processing or encryption) may be running at close to 100% CPU but only be using a small percentage of available memory.</p>
<p>As well as CPU, other resources types—​such as network, memory, and (sometimes) the storage I/O subsystem—​are becoming important resources to manage in cloud-native applications.
For many applications, more memory than CPU is “wasted”, and for many microservices network traffic has become the real bottleneck.</p>
</div></section>
<section data-pdf-bookmark="Efficiency" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-1-SECT-4.5">
<h2>Efficiency</h2>
<p>Dividing the throughput of a system by the utilized resources gives a measure of the overall efficiency of the system. Intuitively, this makes sense, as requiring more resources to produce the same throughput is one useful definition of being less efficient.</p>
<p>It is also possible, when one is dealing with larger systems, to use a form of cost accounting to measure efficiency. If solution A has a total cost of ownership (TCO) twice that of solution B for the same throughput then it is, clearly, half as efficient.</p>
</div></section>
<section data-pdf-bookmark="Scalability" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-1-SECT-4.6">
<h2>Scalability</h2>
<p>The throughput or capacity of a system of course depends upon the resources available for processing.
The scalability of a system or application can be defined in several ways—​but one useful one is as the change in throughput as resources are added.
The holy grail of system scalability is to have throughput change exactly in step with resources.</p>
<p>Consider a system based on a cluster of servers. If the cluster is expanded, for example, by doubling in size, then what throughput can be achieved? If the new cluster can handle twice the volume of transactions, then the system is exhibiting  “perfect linear scaling.” This is very difficult to achieve in practice, especially over a  wide range of possible loads.</p>
<p>System scalability is dependent upon a number of factors, and is not normally a simple linear relationship.
It is very common for a system to scale close to linearly for some range of resources, but then at higher loads to encounter some limitation that prevents perfect scaling.</p>
</div></section>
<section data-pdf-bookmark="Degradation" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-1-SECT-4.7">
<h2>Degradation</h2>
<p>If we increase the load on a system, either by increasing the rate at which requests arrive or the size of the individual requests, then we may see a change in the observed latency and/or throughput.</p>
<p>Note that this change is dependent on utilization. If the system is underutilized,
then there should be some slack before observables change, but if resources are fully
utilized then we would expect to see throughput stop increasing, or latency increase.
These changes are usually called the degradation of the system under additional load.</p>
</div></section>
<section data-pdf-bookmark="Correlations Between the Observables" data-type="sect2"><div class="sect2" id="pracjavaperf-CHP-1-SECT-4.8">
<h2>Correlations Between the Observables</h2>
<p>The behavior of the various performance observables is usually connected in some manner.
The details of this connection will depend upon whether the system is running at peak utility. For example, in general, the utilization will change as the load on a system increases. However, if the system is underutilized, then increasing load may not appreciably increase utilization. Conversely, if the system is already stressed, then the effect of increasing load may be felt in another observable.</p>
<p>As another example, scalability and degradation both represent the change in behavior of a system as more load is added.
For scalability, as the load is increased, so are available resources, and the central question is whether the system can make use of them. On the other hand, if load is added but additional resources are not provided, degradation of some performance observable (e.g., latency) is the expected outcome.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>In rare cases, additional load can cause counterintuitive results. For example, if the change in load causes some part of the system to switch to a more resource-intensive but higher-performance mode, then the overall effect can be to reduce latency, even though more requests are being received.</p>
</div>
<p>To take one example, in Chapter 6 we will discuss HotSpot’s JIT compiler in detail. To be considered eligible for JIT compilation, a method has to be executed in interpreted mode “sufficiently frequently.” So it is possible at low load to have key methods stuck in interpreted mode, but for those to become eligible for compilation at higher loads due to increased calling frequency on the methods. This causes later calls to the same method to run much, much faster than earlier executions.</p>
<p>Different workloads can have very different characteristics.
For example, a trade on the financial markets, viewed end to end, may have an execution time (i.e., latency) of hours or even days.
However, millions of them may be in progress at a major bank at any given time. Thus, the capacity of the system is very large, but the latency is also large.</p>
<p>However, let’s consider only a single subsystem within the bank.
The matching of a buyer and a seller (which is essentially the parties agreeing on a price) is known as <em>order matching</em>.
This individual subsystem may have only hundreds of pending orders at any given time, but the latency from order acceptance to completed match may be as little as 1 millisecond (or even less in the case of “low-latency” trading).</p>
<p>In this section we have met the most frequently encountered performance observables.
Occasionally slightly different definitions, or even different metrics, are used, but
in most cases these will be the basic system numbers that will normally be used
to guide performance tuning, and act as a taxonomy for discussing the performance of
systems of interest.</p>
</div></section>
</div></section>
<section data-pdf-bookmark="Reading Performance Graphs" data-type="sect1"><div class="sect1" id="pracjavaperf-CHP-1-SECT-5">
<h1>Reading Performance Graphs</h1>
<p>To conclude this chapter, let’s look at some common patterns of behavior
that occur in performance tests. We will explore these by looking at graphs of real
observables, and we will encounter many other examples of graphs of our data as we
proceed.</p>
<p>The graph in <a data-type="xref" href="#pracjavaperf-CHP-1-FIG-1">Figure 1-1</a> shows sudden, unexpected degradation of performance (in this case, latency) under increasing load—commonly called a <em>performance elbow</em>.</p>
<figure class="width_set_50"><div class="figure" id="pracjavaperf-CHP-1-FIG-1">
<img alt="ocnj2 0101" height="897" src="assets/ocnj2_0101.png" width="941"/>
<h6><span class="label">Figure 1-1. </span>A performance elbow</h6>
</div></figure>
<p>By contrast, <a data-type="xref" href="#pracjavaperf-CHP-1-FIG-2">Figure 1-2</a> shows the much happier case of
throughput scaling almost linearly as machines are added to a cluster. This is close to ideal behavior, and is only likely to be achieved in extremely favorable circumstances—e.g., scaling a stateless  protocol with no need for session affinity with a single server.</p>
<figure class="width_set_50"><div class="figure" id="pracjavaperf-CHP-1-FIG-2">
<img alt="ocnj2 0102" height="954" src="assets/ocnj2_0102.png" width="954"/>
<h6><span class="label">Figure 1-2. </span>Near-linear scaling</h6>
</div></figure>
<p>In Chapter 13 we will meet Amdahl’s Law, named for the famous computer scientist (and “father of the mainframe”) Gene Amdahl of IBM. <a data-type="xref" href="#pracjavaperf-CHP-1-FIG-3">Figure 1-3</a> shows a graphical representation of his fundamental constraint on scalability; it shows the maximum possible speedup as a function of the number of processors devoted to the task.</p>
<figure><div class="figure" id="pracjavaperf-CHP-1-FIG-3">
<img alt="ocnj2 0103" height="900" src="assets/ocnj2_0103.png" width="1440"/>
<h6><span class="label">Figure 1-3. </span>Amdahl’s Law</h6>
</div></figure>
<p>We display three cases: where the underlying task is 75%, 90%, and 95% parallelizable.
This clearly shows that whenever the workload has any piece at all that must be performed serially, linear scalability is impossible, and there are strict limits on how much scalability can be achieved.
This justifies the commentary around <a data-type="xref" href="#pracjavaperf-CHP-1-FIG-2">Figure 1-2</a>—even in the best cases linear scalability is all but impossible to achieve.</p>
<p>The limits imposed by Amdahl’s Law are surprisingly restrictive.
Note in particular that the x-axis of the graph is logarithmic, and so even with an algorithm that is 95% parallelizable (and thus only 5% serial), 32 processors are needed for a factor-of-12 speedup.
Even worse, no matter how many cores are used, the maximum speedup is only a factor of 20 for that algorithm.
In practice, many algorithms are far more than 5% serial, and so have a more constrained maximum possible speedup.</p>
<p>Another common source of performance graphs in software systems is memory utilization.
As we will see in Chapter 4, the underlying technology in the JVM’s garbage collection subsystem naturally gives rise to a “sawtooth” pattern of memory used for healthy applications that aren’t under stress.
We can see an example in <a data-type="xref" href="#pracjavaperf-CHP-1-FIG-4">Figure 1-4</a> --which is a close-up of a screenshot from the Mission Control tool (JMC) provided by Eclipse Adoptium.</p>
<figure><div class="figure" id="pracjavaperf-CHP-1-FIG-4">
<img alt="ocnj2 0104" height="229" src="assets/ocnj2_0104.png" width="1065"/>
<h6><span class="label">Figure 1-4. </span>Healthy memory usage</h6>
</div></figure>
<p>One key performance metric for JVM is the allocation rate—​effectively how quickly it can create new objects (in bytes per second).
We will have a great deal to say about this aspect of JVM performance in Chapter 4 and Chapter 5.</p>
<p>In <a data-type="xref" href="#pracjavaperf-CHP-1-FIG-5">Figure 1-5</a>, we can see a zoomed-in view of allocation rate, also captured from JMC.
This has been generated from a benchmark program that is deliberately stressing the JVM’s memory subsystem—​we have tried to make the JVM achieve 8GiB/s of allocation, but as we can see, this is beyond the capability of the hardware, and instead the maximum allocation rate of the system is between 4 and 5GiB/s.</p>
<figure><div class="figure" id="pracjavaperf-CHP-1-FIG-5">
<img alt="ocnj2 0105" height="225" src="assets/ocnj2_0105.png" width="1071"/>
<h6><span class="label">Figure 1-5. </span>Sample problematic allocation rate</h6>
</div></figure>
<p>Note that tapped-out allocation is a different problem to the system having a resource leak.
In that case, it is common for it to manifest in a manner like that shown in <a data-type="xref" href="#pracjavaperf-CHP-1-FIG-6">Figure 1-6</a>,
where an observable (in this case latency) slowly degrades as the load is ramped up, before hitting an inflection point where the system rapidly degrades.</p>
<figure class="width_set_50"><div class="figure" id="pracjavaperf-CHP-1-FIG-6">
<img alt="ocnj2 0106" height="1051" src="assets/ocnj2_0106.png" width="1175"/>
<h6><span class="label">Figure 1-6. </span>Degrading latency under higher load</h6>
</div></figure>
<p>Let’s move on to discuss some extra things to consider when working with cloud systems.</p>
</div></section>
<section data-pdf-bookmark="Performance in Cloud Systems" data-type="sect1"><div class="sect1" id="pracjavaperf-CHP-1-SECT-6">
<h1>Performance in Cloud Systems</h1>
<p>Modern cloud systems are nearly always distributed systems in that they are comprised of a cluster of nodes (JVM instances) interoperating via shared network resources.
This means that, in addition to all the complexity of single-node systems, there is another level of complexity that must be addressed.</p>
<p>Operators of distributed systems have to think about things such as:</p>
<ul>
<li>
<p>How is work shared out among the nodes in the cluster?</p>
</li>
<li>
<p>How do we roll out a new version of the software to the cluster (or new config)?</p>
</li>
<li>
<p>What happens when a node leaves the cluster?</p>
</li>
<li>
<p>What happens when a new node joins the cluster?</p>
</li>
<li>
<p>What happens if the new node is misconfigured in some way?</p>
</li>
<li>
<p>What happens if the new node behaves differently in some way, compared to the rest of the cluster?</p>
</li>
<li>
<p>What happens if there is a problem with the code that controls the cluster itself?</p>
</li>
<li>
<p>What happens if there is a catastrophic failure of the entire cluster, or some infrastructure that it depends upon?</p>
</li>
<li>
<p>What happens if a component in the infrastructure the cluster depends upon is a limited resource and becomes a bottleneck to scalability?</p>
</li>
</ul>
<p>These concerns, which we will explore fully later in the book, have a major impact on how cloud systems behave.
They affect the key performance observables such as throughput, latency, efficiency and utilization.</p>
<p>Not only that, but there are two very important aspects—​which differ from the single-JVM case—​that may not be obvious at first sight to newcomers to cloud systems.
First is that many possible impacts are caused by the internal behavior of a cluster, which may be opaque to the performance engineer.</p>
<p>We will discuss this in detail in Chapter 10 when we tackle the topic of Observability in modern systems, and how to implement solutions to this visibility problem.</p>
<p>The second is that the efficiency and utilization of how a service uses cloud providers has a direct effect on the cost of running that service.
Inefficiencies and misconfigurations can show up in the cost base of a service in a far more direct way.
In fact, this is one way to think about the rise of cloud.</p>
<p>In the old days, teams would often own actual physical servers in dedicated areas (usually called <em>cages</em>) in datacenters.
Purchasing these servers represented <em>capital expenditure</em>, and the servers were tracked as an asset.
When we use cloud providers, such as AWS or Azure, we are renting time on machines actually owned by companies such as Amazon or Microsoft.
This is <em>operational expenditure</em>, and it is a cost (or liability).
This shift means that the computational requirements of our systems are now much more open to scrutiny by the financial folks.</p>
<p>Overall, it is important to recognize that cloud systems fundamentally consist clusters of processes (in our case, JVMs) that dynamically change over time.
The clusters can grow or shrink in size, but even if they do not, over time the participating processes will change.
This stands in sharp contrast to traditional host-based systems where the processes forming a cluster are usually much more long-lived and belong to a known, and stable, collection of hosts.</p>
</div></section>
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="pracjavaperf-CHP-1-SECT-7">
<h1>Summary</h1>
<p>In this chapter we have started to discuss what Java performance is and is not. We have introduced the fundamental topics of empirical science and measurement, and the basic vocabulary and observables that a good performance exercise will use. We have introduced some common cases that are often seen within the results obtained from performance tests. Finally, we have introduced the very basics of the sorts of additional issues that can arise in cloud systems.</p>
<p>Let’s move on and begin discussing some major aspects of performance testing, as well as how to handle the numbers that are generated by those tests.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id93"><sup><a href="ch01.xhtml#id93-marker">1</a></sup> J. Gosling, “The feel of Java,” <em>Computer</em>, vol. 30, no. 6 (June 1997): 53-57</p><p data-type="footnote" id="id94"><sup><a href="ch01.xhtml#id94-marker">2</a></sup> E. Goldratt and J. Cox, “The Goal,”  (Gower Publishing, 1984)</p></div></div></section></div></body></html>