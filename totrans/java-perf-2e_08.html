<html><head></head><body><section data-pdf-bookmark="Chapter 8. Native Memory Best Practices" data-type="chapter" epub:type="chapter"><div class="chapter" id="NativeMemory">&#13;
<h1><span class="label">Chapter 8. </span>Native Memory Best Practices</h1>&#13;
&#13;
&#13;
<p><a data-primary="native memory" data-type="indexterm" id="ix_ch08-asciidoc0"/>The heap is the largest consumer of memory in a Java application, but&#13;
the JVM will allocate and use a large amount of native memory. And while&#13;
<a data-type="xref" href="ch07.html#Memory">Chapter 7</a> discussed ways to efficiently manage the heap from a programmatic&#13;
point of view, the configuration of the heap and how it interacts with the&#13;
native memory of the operating system is another important factor in the&#13;
overall performance of an application. There’s a terminology conflict here,&#13;
since C programmers tend to refer to portions of their native memory as the C&#13;
heap.&#13;
In keeping with a Java-centric worldview, we’ll continue to use <em>heap</em> to&#13;
refer to the Java heap, and <em>native memory</em> to refer to the non-heap memory&#13;
of the JVM, including the C heap.</p>&#13;
&#13;
<p>This chapter discusses these aspects of native (or operating system)&#13;
memory. We start with a discussion of the entire memory use of the JVM,&#13;
with a goal of understanding how to monitor that usage for performance&#13;
issues. Then we’ll discuss various ways to tune the JVM and operating&#13;
system for optimal memory use.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Footprint" data-type="sect1"><div class="sect1" id="Footprint">&#13;
<h1>Footprint</h1>&#13;
&#13;
<p><a data-primary="footprint" data-type="indexterm" id="ix_ch08-asciidoc1"/><a data-primary="native memory" data-secondary="footprint" data-type="indexterm" id="ix_ch08-asciidoc2"/>The heap (usually) accounts for the largest amount of memory used by the&#13;
JVM, but the JVM also uses memory for its internal operations.&#13;
This nonheap memory is&#13;
native memory. Native memory can also be allocated in applications&#13;
(via&#13;
JNI calls to&#13;
<span class="keep-together"><code>malloc()</code></span>&#13;
and similar methods, or when using New I/O, or NIO). The total of native and heap&#13;
memory used by the JVM yields the total <em>footprint</em> of an application.</p>&#13;
&#13;
<p>From the point of view of the operating system, this total footprint is&#13;
the key to performance. If enough physical memory to contain&#13;
the entire total footprint of an application is not available,&#13;
performance may begin to suffer. The operative word here is <em>may</em>. Parts of native&#13;
memory are used only during startup (for instance, the memory&#13;
associated with loading the JAR files in the classpath), and if that memory is&#13;
swapped out, it won’t necessarily be noticed. Some of the native memory&#13;
used by one Java process is shared with other Java processes on the system,&#13;
and some smaller part is shared with other kinds of processes on the system.&#13;
For the most part, though, for optimal performance you want to be sure that&#13;
the total footprint of all Java processes does not exceed the physical memory of&#13;
the machine (plus you want to leave some memory available for other&#13;
applications).</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Measuring Footprint" data-type="sect2"><div class="sect2" id="idm45775549963720">&#13;
<h2>Measuring Footprint</h2>&#13;
&#13;
<p><a data-primary="footprint" data-secondary="measuring" data-type="indexterm" id="idm45775549962120"/>To measure the total footprint of a process, you need to use an&#13;
operating-system-specific tool. In Unix-based systems, programs like&#13;
<code>top</code> and <code>ps</code> can show you that data at a basic level; on Windows, you&#13;
can use&#13;
<span class="keep-together"><code>perfmon</code></span>&#13;
or&#13;
<span class="keep-together"><code>VMMap</code></span>.&#13;
No matter which tool and platform are used,&#13;
you need to look&#13;
at the actual allocated memory (as opposed to the reserved memory) of the&#13;
process.</p>&#13;
&#13;
<p>The distinction between allocated and reserved memory comes about as&#13;
a result of the way the JVM (and all programs) manage memory.&#13;
Consider a heap that is specified&#13;
with the parameters&#13;
<span class="keep-together"><code>-Xms512m</code></span>&#13;
<span class="keep-together"><code>-Xmx2048m</code></span>.&#13;
The heap starts by using 512 MB, and it&#13;
will be resized as needed to meet the GC goals of the application.</p>&#13;
&#13;
<p>That concept is the essential difference between committed (or allocated) memory&#13;
and reserved memory (sometimes called the <em>virtual size</em> of a process).&#13;
The JVM must tell the operating system that it might need as much as 2 GB of&#13;
memory for the heap, so that memory is <em>reserved</em>: the operating system&#13;
promises that when the JVM attempts to allocate additional memory when it&#13;
increases the size of the heap, that&#13;
memory will be available.</p>&#13;
&#13;
<p>Still, only 512 MB of that memory is allocated initially, and that 512 MB is all of the memory that is being used (for the heap). That (actually <em>allocated</em>) memory is known as the <em>committed memory</em>. The&#13;
amount of committed memory will fluctuate as the heap resizes; in particular,&#13;
as the heap size increases, the committed memory correspondingly&#13;
increases.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775549952328">&#13;
<h5>Is Over-Reserving a Problem?</h5>&#13;
<p><a data-primary="committed memory" data-type="indexterm" id="idm45775549950872"/><a data-primary="reserved memory" data-type="indexterm" id="idm45775549950168"/>When we look at performance, only committed memory really matters: a performance problem never results from reserving too much memory.</p>&#13;
&#13;
<p>However, sometimes you want to make sure that the JVM does not reserve&#13;
too much memory. This is particularly true for 32-bit JVMs. Since the&#13;
maximum process size of a 32-bit application is 4 GB (or less, depending on&#13;
the operating system), over-reserving memory can be an issue. A JVM that&#13;
reserves 3.5 GB of memory for the heap is left with only 0.5 GB of native&#13;
memory for its stacks, code cache, and so on. It doesn’t matter if the&#13;
heap expands to commit only 1 GB of memory: because of the 3.5 GB reservation,&#13;
the amount of memory for other operations is limited to 0.5 GB.</p>&#13;
&#13;
<p>64-bit JVMs aren’t limited that way by the process size, but they are&#13;
limited by the total amount of virtual memory on the machine. Say you&#13;
have a small server with 4 GB of physical memory and 10 GB of virtual memory,&#13;
and start a JVM with a maximum heap size of 6 GB. That will reserve 6 GB&#13;
of virtual memory (plus more for nonheap memory sections). Regardless of&#13;
how large that heap grows (and the memory that is committed),&#13;
a second JVM will be able to reserve only less than 4 GB of memory on that&#13;
machine.</p>&#13;
&#13;
<p>All things being equal, it’s convenient to oversize JVM structures and let the&#13;
JVM optimally use that memory. But it isn’t always feasible.</p>&#13;
</div></aside>&#13;
&#13;
<p>This difference applies to almost all significant memory that the JVM allocates.&#13;
The code cache grows from an initial to a maximum value&#13;
as more code gets compiled. Metaspace&#13;
is allocated separately from the heap and grows between&#13;
its initial (committed) size and its maximum (reserved) size.</p>&#13;
&#13;
<p>Thread stacks are an exception to this.&#13;
Every&#13;
time the JVM creates a thread, the OS allocates some native memory to hold&#13;
that thread’s stack, committing more memory to the process (until the thread&#13;
exits at least). Thread stacks, though, are fully allocated when they are&#13;
created.</p>&#13;
&#13;
<p><a data-primary="resident set size (RSS)" data-type="indexterm" id="idm45775549945000"/><a data-primary="RSS (resident set size)" data-type="indexterm" id="idm45775549944408"/>In Unix systems, the footprint of an application can be estimated&#13;
by the <em>resident set size (RSS)</em> of the process as reported by various OS tools. That value is&#13;
a good estimate of the amount of committed memory a process is using, though it is inexact&#13;
in two ways. First, the few pages that are shared at the OS level between JVM&#13;
and other processes (that is, the text portions of shared libraries) are&#13;
counted in the RSS of each process. Second, a process may have committed more&#13;
memory than it paged in at any moment. Still, tracking the RSS of a&#13;
process is a good first-pass way to monitor the total memory use. On more&#13;
recent Linux kernels, the PSS is a refinement of the RSS that removes&#13;
the data shared by other programs.</p>&#13;
&#13;
<p>On Windows systems, the equivalent idea is called the <em>working set</em> of an&#13;
application, which is what is reported by the task manager.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Minimizing Footprint" data-type="sect2"><div class="sect2" id="idm45775549963160">&#13;
<h2>Minimizing Footprint</h2>&#13;
&#13;
<p><a data-primary="footprint" data-secondary="minimizing" data-type="indexterm" id="idm45775549940504"/>To minimize the footprint used by the JVM, limit the amount of memory used&#13;
by the following:</p>&#13;
<dl>&#13;
<dt>Heap</dt>&#13;
<dd>&#13;
<p>The heap is the biggest chunk of memory, though surprisingly it may take up only 50% to 60% of the total footprint. Using a smaller maximum heap (or setting the GC tuning parameters such that the heap never fully expands) limits the program’s footprint.</p>&#13;
</dd>&#13;
<dt>Thread stacks</dt>&#13;
<dd>&#13;
<p>Thread stacks are quite large, particularly for a 64-bit JVM. See <a data-type="xref" href="ch09.html#ThreadPerformance">Chapter 9</a> for ways to limit the amount of memory consumed by thread stacks.</p>&#13;
</dd>&#13;
<dt>Code cache</dt>&#13;
<dd>&#13;
<p>The code cache uses native memory to hold compiled code. As discussed in <a data-type="xref" href="ch04.html#JustInTimeCompilation">Chapter 4</a>, this can be tuned (though performance will suffer if all the code cannot be compiled because of space limitations).</p>&#13;
</dd>&#13;
<dt>Native library allocations</dt>&#13;
<dd>&#13;
<p>Native libraries can allocate their own memory, which can sometimes be&#13;
<span class="keep-together">significant.</span></p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>The next few sections discuss how to monitor and reduce these areas.</p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>The total footprint of the JVM has a significant effect on its performance, particularly if physical memory on the machine is constrained. Footprint is another aspect of performance tests that should be commonly monitored.</p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Native Memory Tracking" data-type="sect2"><div class="sect2" id="NMT">&#13;
<h2>Native Memory Tracking</h2>&#13;
&#13;
<p><a data-primary="footprint" data-secondary="Native Memory Tracking" data-type="indexterm" id="ix_ch08-asciidoc3"/><a data-primary="native memory" data-secondary="NMT" data-type="indexterm" id="ix_ch08-asciidoc4"/><a data-primary="Native Memory Tracking (NMT)" data-type="indexterm" id="ix_ch08-asciidoc5"/>The JVM provides bounded visibility into how it allocates native memory.&#13;
It is important to realize that this tracking applies to the memory allocated&#13;
by the code JVM itself, but it does not include any memory allocated by a&#13;
native library used by the application. This includes both third-party&#13;
native libraries and the native libraries (e.g., <em>libsocket.so</em>) that ship with&#13;
the JDK itself.</p>&#13;
&#13;
<p><a data-primary="-XX:NativeMemoryTracking=X" data-type="indexterm" id="idm45775549860904"/>Using the option <span class="keep-together"><code>-XX:NativeMemoryTracking=<em>off|summary|detail</em></code></span> enables this visibility. By&#13;
default, Native Memory Tracking (NMT) is off. If the summary or&#13;
detail mode is enabled, you can get the native memory&#13;
information at any time from <code>jcmd</code>:</p>&#13;
<pre data-type="programlisting">&#13;
% <strong>jcmd process_id VM.native_memory summary</strong>&#13;
</pre>&#13;
&#13;
<p><a data-primary="-XX:+PrintNMTStatistics" data-type="indexterm" id="idm45775549856872"/>If the JVM is started with the argument&#13;
<span class="keep-together"><code>-XX:+PrintNMTStatistics</code></span>&#13;
(by default, <code>false</code>), the JVM will&#13;
print out information about the allocation when the program exits.</p>&#13;
&#13;
<p>Here is the summary output from a JVM running with a 512 MB initial heap size&#13;
and a 4 GB maximum heap size:</p>&#13;
&#13;
<pre data-type="programlisting">Native Memory Tracking:&#13;
&#13;
 Total: reserved=5947420KB, committed=620432KB</pre>&#13;
&#13;
<p>Although the JVM has made memory reservations totaling 5.9 GB, it has&#13;
used much less than&#13;
that: only 620 MB. This is fairly typical (and one reason not to pay&#13;
particular attention to the virtual size of the process displayed in OS&#13;
tools, since that reflects only the memory reservations).</p>&#13;
&#13;
<p>This memory usage breaks down as follows. The heap itself is (unsurprisingly) the largest part of the reserved&#13;
memory at 4 GB. But the dynamic sizing of the heap meant it grew only to&#13;
268 MB (in this case, the heap sizing was <code>-Xms256m -Xmx4g</code>, so the actual&#13;
heap usage has expanded only a small amount):</p>&#13;
&#13;
<pre data-type="programlisting">-                 Java Heap (reserved=4194304KB, committed=268288KB)&#13;
                            (mmap: reserved=4194304KB, committed=268288KB)</pre>&#13;
&#13;
<p>Next is the native memory used to hold class metadata.&#13;
Again, note that the JVM has reserved&#13;
more memory than it used to hold the 24,316 classes in the program.&#13;
The committed size here will start at the value of the&#13;
<span class="keep-together"><code>MetaspaceSize</code></span>&#13;
flag and grow as needed until it reaches the value of the&#13;
<span class="keep-together"><code>MaxMetaspaceSize</code></span>&#13;
flag:</p>&#13;
&#13;
<pre data-type="programlisting">-                     Class (reserved=1182305KB, committed=150497KB)&#13;
                            (classes #24316)&#13;
                            (malloc=2657KB #35368)&#13;
                            (mmap: reserved=1179648KB, committed=147840KB)</pre>&#13;
&#13;
<p>Seventy-seven thread stacks were allocated at about 1 MB each:</p>&#13;
&#13;
<pre data-type="programlisting">-                    Thread (reserved=84455KB, committed=84455KB)&#13;
                            (thread #77)&#13;
                            (stack: reserved=79156KB, committed=79156KB)&#13;
                            (malloc=243KB, #314)&#13;
                            (arena=5056KB, #154)</pre>&#13;
&#13;
<p>Then comes the JIT code cache: 24,316 classes is not&#13;
very many, so just a small section of the code cache is committed:</p>&#13;
&#13;
<pre data-type="programlisting">-                      Code (reserved=102581KB, committed=15221KB)&#13;
                            (malloc=2741KB, #4520)&#13;
                            (mmap: reserved=99840KB, committed=12480KB)</pre>&#13;
&#13;
<p>Next is the area outside the heap that GC algorithm uses for its&#13;
processing. The size of this area depends on the GC algorithm in use:&#13;
the (simple) serial collector will reserve far less than the more complex&#13;
G1 GC algorithm (though, in general, the amount here will never be very large):</p>&#13;
&#13;
<pre data-type="programlisting">-                        GC (reserved=199509KB, committed=53817KB)&#13;
                            (malloc=11093KB #18170)&#13;
                            (mmap: reserved=188416KB, committed=42724KB)</pre>&#13;
&#13;
<p>Similarly, this area is used by the compiler for its operations, apart&#13;
from the resulting code placed in the code cache:</p>&#13;
&#13;
<pre data-type="programlisting">-                  Compiler (reserved=162KB, committed=162KB)&#13;
                            (malloc=63KB, #229)&#13;
                            (arena=99KB, #3)</pre>&#13;
&#13;
<p>Internal operations of the JVM are represented here. Most of them tend to&#13;
be small, but one important exception is direct byte buffers, which are&#13;
allocated here:</p>&#13;
&#13;
<pre data-type="programlisting">-                  Internal (reserved=10584KB, committed=10584KB)&#13;
                            (malloc=10552KB #32851)&#13;
                            (mmap: reserved=32KB, committed=32KB)</pre>&#13;
&#13;
<p>Symbol table references (constants from class files) are held here:</p>&#13;
&#13;
<pre data-type="programlisting">-                    Symbol (reserved=12093KB, committed=12093KB)&#13;
                            (malloc=10039KB, #110773)&#13;
                            (arena=2054KB, #1)</pre>&#13;
&#13;
<p>NMT itself needs some space for its operation (which is one reason it is&#13;
not enabled by default):</p>&#13;
&#13;
<pre data-type="programlisting">-    Native Memory Tracking (reserved=7195KB, committed=7195KB)&#13;
                            (malloc=16KB #199)&#13;
                            (tracking overhead=7179KB)</pre>&#13;
&#13;
<p>Finally, here are some minor bookkeeping sections of the JVM:</p>&#13;
&#13;
<pre data-type="programlisting">-               Arena Chunk (reserved=188KB, committed=188KB)&#13;
                            (malloc=188KB)&#13;
-                   Unknown (reserved=8192KB, committed=0KB)&#13;
                            (mmap: reserved=8192KB, committed=0KB)</pre>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775549835768">&#13;
<h5>Detailed Memory Tracking Information</h5>&#13;
<p><a data-primary="-XX:NativeMemoryTracking=X" data-type="indexterm" id="idm45775549834312"/><a data-primary="Native Memory Tracking (NMT)" data-secondary="detailed tracking information" data-type="indexterm" id="idm45775549833544"/>If the JVM is started with&#13;
<span class="keep-together"><code>-XX:NativeMemoryTracking=detail</code></span>,&#13;
then <code>jcmd</code> (with a final <code>detail</code> argument)&#13;
will provide detailed information about the native memory&#13;
allocation. That includes a map of the entire memory space, which includes&#13;
lines like this:</p>&#13;
&#13;
<pre data-type="programlisting">0x00000006c0000000 - 0x00000007c0000000] reserved 4194304KB for Java Heap&#13;
        from [ReservedSpace::initialize(unsigned long, unsigned long,&#13;
                            bool, char*, unsigned long, bool)+0xc2]&#13;
        [0x00000006c0000000 - 0x00000006fb100000] committed 967680KB&#13;
            from [PSVirtualSpace::expand_by(unsigned long)+0x53]&#13;
        [0x000000076ab00000 - 0x00000007c0000000] committed 1397760KB&#13;
            from [PSVirtualSpace::expand_by(unsigned long)+0x53]</pre>&#13;
&#13;
<p>The 4 GB of heap space was reserved in the&#13;
<span class="keep-together"><code>initialize()</code></span>&#13;
function, with two allocations from that made in the&#13;
<span class="keep-together"><code>expand_by()</code></span>&#13;
function.</p>&#13;
&#13;
<p>That kind of information is repeated for the entire process space. It provides&#13;
interesting clues if you are a JVM engineer, but for the rest of us,&#13;
the summary information is useful enough.</p>&#13;
</div></aside>&#13;
&#13;
<p>Overall, NMT provides two key pieces of information:</p>&#13;
<dl>&#13;
<dt>Total committed size</dt>&#13;
<dd>&#13;
<p>The total committed size of the JVM is (ideally) close to the amount&#13;
of physical memory that the process will consume. This, in turn, should be close to the RSS (or working set) of the application, but those OS-provided measurements don’t include any memory that has been committed but paged out of the process. In fact, if the RSS of the process is less than the committed memory, that is often an indication that the OS is having difficulty fitting all of the JVM in physical <span class="keep-together">memory.</span></p>&#13;
</dd>&#13;
<dt>Individual committed sizes</dt>&#13;
<dd>&#13;
<p>When it is time to tune maximum values—of the heap, the code cache,&#13;
and the metaspace—it is helpful to know how much of that memory the JVM is&#13;
using. Overallocating those areas usually leads only to harmless memory&#13;
reservations, though when reserved memory is important,&#13;
NMT can help track down where those maximum sizes can be trimmed.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>On the other hand, as I noted at the beginning of this section, NMT does&#13;
not provide visibility into the native memory use of shared libraries, so&#13;
in some cases the total process size will be larger than the committed size&#13;
of the JVM data structures.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="NMT over time" data-type="sect3"><div class="sect3" id="idm45775549820776">&#13;
<h3>NMT over time</h3>&#13;
&#13;
<p><a data-primary="Native Memory Tracking (NMT)" data-secondary="memory allocations over time" data-type="indexterm" id="idm45775549819336"/>NMT also allows you to track how memory allocations occur over time. After&#13;
the JVM is started with NMT enabled, you can establish a baseline for&#13;
memory usage with this command:</p>&#13;
<pre data-type="programlisting">&#13;
% <strong>jcmd process_id VM.native_memory baseline</strong>&#13;
</pre>&#13;
&#13;
<p>That causes the JVM to mark its current memory allocations. Later, you can&#13;
compare the current memory usage to that mark:</p>&#13;
<pre data-type="programlisting">&#13;
% <strong>jcmd process_id VM.native_memory summary.diff</strong>&#13;
Native Memory Tracking:&#13;
&#13;
Total:  reserved=5896078KB  -3655KB, committed=2358357KB -448047KB&#13;
&#13;
-             Java Heap (reserved=4194304KB, committed=1920512KB -444927KB)&#13;
                        (mmap: reserved=4194304KB, committed=1920512KB -444927KB)&#13;
....&#13;
</pre>&#13;
&#13;
<p>In this case, the JVM has reserved 5.8 GB of memory and is presently using&#13;
2.3 GB. That committed size is 448 MB less than when the baseline was&#13;
established. Similarly, the committed memory used by the heap has declined by&#13;
444 MB (and the rest of the output could be inspected to see where else the&#13;
memory use declined to account for the remaining 4 MB).</p>&#13;
&#13;
<p>This is a useful technique to examine the footprint of the JVM over time.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775549813528">&#13;
<h5>NMT Auto Disabling</h5>&#13;
<p><a data-primary="Native Memory Tracking (NMT)" data-secondary="auto disabling" data-type="indexterm" id="idm45775549812328"/>In the NMT output, we saw that NMT itself requires native memory.&#13;
In addition, enabling NMT will create background threads that assist with&#13;
memory tracking.</p>&#13;
&#13;
<p>If the JVM becomes severly stressed for memory or&#13;
CPU resources, NMT will automatically turn itself off to save resources.&#13;
This is usually a good thing—unless the stressed situation is what you need&#13;
to diagnose. In that case, you can make sure that NMT keeps running by&#13;
disabling the&#13;
<span class="keep-together"><code>-XX:-AutoShutdownNMT</code></span>&#13;
flag (by default, <code>true</code>).</p>&#13;
</div></aside>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Native Memory Tracking (NMT) provides details about the native memory usage of the JVM. From an operating system perspective, that includes the JVM heap (which to the OS is just a section of native memory).</p>&#13;
</li>&#13;
<li>&#13;
<p>The summary mode of NMT is sufficient for most analysis and allows you to determine how much memory the JVM has committed (and what that memory is used for).<a data-startref="ix_ch08-asciidoc5" data-type="indexterm" id="idm45775549805624"/><a data-startref="ix_ch08-asciidoc4" data-type="indexterm" id="idm45775549804920"/><a data-startref="ix_ch08-asciidoc3" data-type="indexterm" id="idm45775549804248"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Shared Library Native Memory" data-type="sect2"><div class="sect2" id="idm45775549866632">&#13;
<h2>Shared Library Native Memory</h2>&#13;
&#13;
<p><a data-primary="native memory" data-secondary="shared library native memory" data-type="indexterm" id="ix_ch08-asciidoc6"/><a data-primary="shared library native memory" data-type="indexterm" id="ix_ch08-asciidoc7"/>From an architectural perspective, NMT is part of HotSpot: the C++ engine that&#13;
runs the Java bytecode of your application. That is underneath the JDK itself,&#13;
so it does not track allocations by anything at the JDK level. Those&#13;
allocations come from shared libraries (those loaded by the&#13;
<code>System.loadLibrary()</code> call).</p>&#13;
&#13;
<p>Shared libraries are often thought of as third-party extensions of Java: for&#13;
instance, the Oracle WebLogic Server has several native libraries that&#13;
it uses to handle I/O more efficiently than the JDK.<sup><a data-type="noteref" href="ch08.html#idm45775549798408" id="idm45775549798408-marker">1</a></sup> But the JDK itself has several native libraries,&#13;
and like all shared libraries, these are outside the view of NMT.</p>&#13;
&#13;
<p><a data-primary="memory leaks" data-secondary="NMT and" data-type="indexterm" id="idm45775549797192"/>Hence, native memory leaks—where the RSS or working set of an application&#13;
continually grows overtime—are usually not detected by NMT. The memory pools&#13;
that NMT monitors all generally have an upper bound (e.g., the maximum heap&#13;
size). NMT is useful in telling us which of those pools is using a lot&#13;
of memory (and hence which need to be tuned to use less memory), but an&#13;
application that is leaking native memory without bound is&#13;
typically doing so because of issues in a native library.</p>&#13;
&#13;
<p>No Java-level tools can really help us detect where an&#13;
application is using native memory from shared libraries. OS-level tools can&#13;
tell us that the working set of the process is continually growing, and if&#13;
a process grows to have a working set of 10 GB and NMT tells us that the&#13;
JVM has committed only 6 GB of memory, we know that the other 4 GB of memory&#13;
must come from native library allocations.</p>&#13;
&#13;
<p>Figuring out which native library is responsible requires OS-level tools&#13;
rather than tools from the JDK. Various debugging versions of&#13;
<code>malloc</code> can be used for this purpose. These are useful to a point, though&#13;
often native memory is allocated via an <code>mmap</code> call, and most libraries to&#13;
track <code>malloc</code> calls will miss those.</p>&#13;
&#13;
<p>A good alternative is a profiler that can profile native code as well&#13;
as Java code. For example, in <a data-type="xref" href="ch03.html#Tools">Chapter 3</a> we discussed the Oracle Studio&#13;
Profiler, which is a mixed-language profiler. That profiler has an option to&#13;
trace memory allocations as well—one caveat is that it can track only&#13;
the memory allocations of the native code and not the Java code, but that’s&#13;
what we’re after in this case.</p>&#13;
&#13;
<p><a data-type="xref" href="#FigureNativeMemory">Figure 8-1</a> shows the native allocation view within the Studio&#13;
Profiler.</p>&#13;
&#13;
<figure><div class="figure" id="FigureNativeMemory">&#13;
<img alt="A profiler snapshot of native memory allocation." src="assets/jp2e_0801.png"/>&#13;
<h6><span class="label">Figure 8-1. </span>Profiling of native memory</h6>&#13;
</div></figure>&#13;
&#13;
<p>This call graph shows us that the WebLogic native function <code>mapFile</code> has&#13;
used <code>mmap</code> to allocate about 150 GB of native memory into our process.&#13;
This is a little misleading: multiple mappings to that file exist, and&#13;
the profiler isn’t quite smart enough to realize that they are sharing the&#13;
actual memory: if there were, for example, 100 mappings of that 15 GB file,&#13;
the memory usage increases by only 15 GB. (And, to be frank, I purposely&#13;
corrupted that file to make it that large; this is in no way reflective of&#13;
actual usage.) Still, the native profiler has pointed to the location of&#13;
the issue.</p>&#13;
&#13;
<p>Within the JDK itself, two common operations can lead to large&#13;
amounts of native memory usage: the use of <code>Inflater</code> and <code>Deflater</code> objects,&#13;
and the use of NIO buffers. Even without profiling, there are ways to detect&#13;
if these operations are causing your native memory growth.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Native memory and inflaters/deflaters" data-type="sect3"><div class="sect3" id="idm45775549784312">&#13;
<h3>Native memory and inflaters/deflaters</h3>&#13;
&#13;
<p><a data-primary="Deflater class" data-type="indexterm" id="idm45775549782856"/><a data-primary="Inflater class" data-type="indexterm" id="idm45775549782152"/><a data-primary="shared library native memory" data-secondary="inflaters/deflaters" data-type="indexterm" id="idm45775549781480"/>The <code>Inflater</code> and <code>Deflater</code> classes perform various kinds of&#13;
compression: zip, gzip, and so on. They can be used directly or implicitly&#13;
via various input streams. These various algorithms use platform-specific&#13;
native libraries to carry out their operations. Those libraries can allocate&#13;
a significant amount of native memory.</p>&#13;
&#13;
<p>When you use one of these classes, you are&#13;
supposed to—as the documentation says—call the <code>end()</code> method when the&#13;
operation is complete. Among other things, that frees the native memory used&#13;
by the object. If you are using a stream, you are supposed to close the&#13;
stream (and the stream class will call the <code>end()</code> method on its internal&#13;
object).</p>&#13;
&#13;
<p>If you forget to call the <code>end()</code> method, all is not lost. Recall from&#13;
<a data-type="xref" href="ch07.html#Memory">Chapter 7</a> that all objects have a cleanup mechanism exactly for this&#13;
situation:&#13;
the <code>finalize()</code> method (in JDK 8) or the <code>Cleaner</code> associated with the&#13;
object (in JDK 11) can call the <code>end()</code> method when the <code>Inflater</code>&#13;
object is collected. So you’re not going to leak native memory&#13;
here; eventually, the objects will be collected and finalized, and the native&#13;
memory will be freed.</p>&#13;
&#13;
<p>Still, this can take a long time. The size of the <code>Inflater</code> object is&#13;
relatively small, and in an application with a large heap that rarely&#13;
performs a full GC, it’s easy enough for these objects to get promoted into&#13;
the old generation and stick around for hours. So even if there is technically&#13;
not a leak—the native memory will eventually get freed when the application&#13;
performs a full GC—failure to call the <code>end()</code> operation here can have all&#13;
the appearances of a native memory leak.</p>&#13;
&#13;
<p>For that matter, if the <code>Inflater</code> object itself is leaking in the Java code,&#13;
then the native memory will be actually leaking.</p>&#13;
&#13;
<p>So when a lot of native memory is leaking,&#13;
it can be helpful to take a heap dump of the application and look for these&#13;
<code>Inflater</code> and <code>Deflater</code> objects. Those objects likely won’t be causing&#13;
an issue in the heap itself (they are too small for that), but a large number&#13;
of them will indicate that there is significant usage of native memory.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Native NIO buffers" data-type="sect3"><div class="sect3" id="NIOBuffers">&#13;
<h3>Native NIO buffers</h3>&#13;
&#13;
<p><a data-primary="NIO buffers" data-type="indexterm" id="idm45775549767624"/><a data-primary="nonblocking I/O (NIO)" data-secondary="native NIO buffers" data-type="indexterm" id="idm45775549766696"/><a data-primary="shared library native memory" data-secondary="native NIO buffers" data-type="indexterm" id="idm45775549765752"/>NIO byte buffers&#13;
allocate native (off-heap) memory if they are created via the&#13;
<span class="keep-together"><code>allocateDirect()</code></span>&#13;
method of the <code>ByteBuffer</code> class&#13;
or the&#13;
<span class="keep-together"><code>map()</code></span>&#13;
method of the <code>FileChannel</code> class.</p>&#13;
&#13;
<p>Native byte buffers are important from&#13;
a performance perspective, since they allow native code and Java code to&#13;
share data without copying it. Buffers used for filesystem and socket operations are the most common example. Writing data to a native&#13;
NIO buffer and then sending that data to the channel (e.g., the file or socket)&#13;
requires no copying of data between the JVM and the C library used to&#13;
transmit the data. If a heap byte buffer is used instead,&#13;
contents of the buffer must be copied by the JVM.</p>&#13;
&#13;
<p>The&#13;
<span class="keep-together"><code>allocateDirect()</code></span>&#13;
method call is expensive; direct byte buffers&#13;
should be reused as much as possible. The ideal situation occurs when threads are&#13;
independent and each can keep a direct byte buffer as a thread-local variable.&#13;
That can sometimes use too much native memory if many threads&#13;
need buffers of variable sizes, since eventually each thread will&#13;
end up with a buffer at the maximum possible size. For that kind of situation—or when thread-local buffers don’t fit the application design—an object pool of direct byte buffers may be more useful.</p>&#13;
&#13;
<p>Byte buffers can also be managed by slicing them. The&#13;
application can allocate one very large direct byte buffer, and individual&#13;
requests can allocate a portion out of that buffer by using the&#13;
<span class="keep-together"><code>slice()</code></span>&#13;
method&#13;
of the&#13;
<span class="keep-together"><code>ByteBuffer</code></span>&#13;
class. This solution can become unwieldy when the slices&#13;
are not always the same size: the original byte buffer can then become&#13;
fragmented in the same way the heap becomes fragmented when allocating and&#13;
freeing objects of different sizes. Unlike the heap, however, the individual&#13;
slices of a byte buffer cannot be compacted, so this solution works&#13;
well only when all the slices are a uniform size.</p>&#13;
&#13;
<p>From a tuning perspective, one thing to realize with any of these&#13;
programming models is that the amount of direct byte buffer space that an&#13;
application can allocate can be limited by the JVM. <a data-primary="-XX:MaxDirectMemorySize=N" data-type="indexterm" id="idm45775549756088"/>The total amount of&#13;
memory that can be allocated for direct byte buffers is specified by&#13;
setting the&#13;
<span class="keep-together"><code>-XX:MaxDirectMemorySize</code>=<em><code>N</code></em></span>&#13;
flag. The default value for this flag in current JVMs is 0. The meaning&#13;
of that limit has been the subject of frequent changes, but in later versions&#13;
of Java 8 (and all versions of Java 11), the maximum limit&#13;
is equal to the maximum heap size: if the maximum heap size is 4 GB, you&#13;
can also create 4 GB of off-heap memory in direct and/or mapped byte buffers.&#13;
You can increase the value past the maximum heap value if you need to.</p>&#13;
&#13;
<p>The memory allocated for direct byte buffers is included in the <code>Internal</code>&#13;
section of the NMT report; if that number is large, it is almost always&#13;
because of these buffers. If you want to know exactly how much the buffers&#13;
themselves are consuming, mbeans keep track of that.&#13;
Inspecting the mbean&#13;
<span class="keep-together"><code>java.nio.BufferPool.direct.Attributes</code></span>&#13;
or&#13;
<span class="keep-together"><code>java.nio.BufferPool.mapped.Attributes</code></span>&#13;
will show you the amount of memory each type has allocated.&#13;
<a data-type="xref" href="#FigureNativeByteBuffer">Figure 8-2</a> shows a case where we’ve mapped 10 buffers&#13;
totaling 10 MB of space.</p>&#13;
&#13;
<figure><div class="figure" id="FigureNativeByteBuffer">&#13;
<img alt="Amount of native memory consumed by mmaped buffers." src="assets/jp2e_0802.png"/>&#13;
<h6><span class="label">Figure 8-2. </span>Inspecting byte buffer native memory</h6>&#13;
</div></figure>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775549747336">&#13;
<h5>Linux System Memory Leaking</h5>&#13;
<p><a data-primary="Linux" data-secondary="memory leaking" data-type="indexterm" id="idm45775549745880"/><a data-primary="memory leaks" data-secondary="Linux" data-type="indexterm" id="idm45775549744904"/><a data-primary="native memory" data-secondary="Linux system memory leaking" data-type="indexterm" id="idm45775549743960"/>Large Linux systems can sometimes exhibit a native memory leak due to the&#13;
design of their memory allocation libraries. These libraries partition&#13;
the native memory into allocation segments, which benefits allocation by&#13;
multiple threads (since it limits lock contention).</p>&#13;
&#13;
<p>Native memory, though, is not managed like the Java heap: in particular,&#13;
native memory is never compacted. Hence, allocation patterns in native memory&#13;
can lead to the same fragmentation as described in <a data-type="xref" href="ch05.html#GC">Chapter 5</a>.</p>&#13;
&#13;
<p>It is possible to run out of native memory in Java because of native&#13;
memory fragmentation; this occurs most often on larger systems (e.g., those&#13;
with more than eight cores) since the number of memory partitions in Linux&#13;
is a function of the number of cores in the system.</p>&#13;
&#13;
<p>Two things can help diagnose this problem: first, the application will&#13;
thrown an <span class="keep-together"><code>OutOfMemoryError</code></span>&#13;
saying it is out of native memory. Second, if&#13;
you look at the <em>smaps</em> file of the process, it will show many small (usually 64 KB) allocations. <a data-primary="MALLOC_ARENA_MAX" data-type="indexterm" id="idm45775549738664"/><a data-primary="native memory" data-secondary="MALLOC_ARENA_MAX" data-type="indexterm" id="idm45775549737960"/>In this case, the remedy is to&#13;
set the environment variable <code>MALLOC_ARENA_MAX</code> to a small number like 2&#13;
or 4. The default value of that variable is the number of cores on the&#13;
system multiplied by 8 (which is why the problem is more frequently seen&#13;
on large systems). Native memory will still be fragmented in this case, but&#13;
the fragmentation should be less severe.</p>&#13;
</div></aside>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>If an application seems to be using too much native memory, it is likely from&#13;
native libraries rather than the JVM itself.</p>&#13;
</li>&#13;
<li>&#13;
<p>Native profiles can be effective in pinpointing the source of these allocations.</p>&#13;
</li>&#13;
<li>&#13;
<p>A few common JDK classes can often contribute to native memory usage; make sure to use these classes correctly<a data-startref="ix_ch08-asciidoc7" data-type="indexterm" id="idm45775549732008"/><a data-startref="ix_ch08-asciidoc6" data-type="indexterm" id="idm45775549731304"/>.<a data-startref="ix_ch08-asciidoc2" data-type="indexterm" id="idm45775549730504"/><a data-startref="ix_ch08-asciidoc1" data-type="indexterm" id="idm45775549729800"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="JVM Tunings for the Operating System" data-type="sect1"><div class="sect1" id="idm45775549803064">&#13;
<h1>JVM Tunings for the Operating System</h1>&#13;
&#13;
<p><a data-primary="Java Virtual Machine (JVM)" data-secondary="tunings for OS" data-type="indexterm" id="ix_ch08-asciidoc8"/><a data-primary="native memory" data-secondary="JVM tunings for OS" data-type="indexterm" id="ix_ch08-asciidoc9"/><a data-primary="tuning" data-secondary="JVM tunings for OS" data-type="indexterm" id="ix_ch08-asciidoc10"/>The JVM can use several tunings to improve the way it uses OS memory.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Large Pages" data-type="sect2"><div class="sect2" id="LargePages">&#13;
<h2>Large Pages</h2>&#13;
&#13;
<p><a data-primary="Java Virtual Machine (JVM)" data-secondary="tuning large pages" data-type="indexterm" id="ix_ch08-asciidoc11"/>Discussions about memory allocation and swapping occur in terms of&#13;
pages. A <em>page</em> is a unit of memory used by operating systems to manage physical&#13;
memory. It is the minimum unit of allocation for the operating system:&#13;
when 1 byte is allocated, the operating system&#13;
must allocate an entire page. Further allocations for that program come from&#13;
that same page until it is filled, at which point a new page is&#13;
allocated.</p>&#13;
&#13;
<p>The operating system allocates many more pages than can fit in physical&#13;
memory, which is why there is paging: pages of the address space are moved&#13;
to and from swap space (or other storage depending on what the page contains).&#13;
This means there must be some mapping between these pages and where they&#13;
are currently stored in the computer’s RAM. Those mappings are handled in&#13;
two ways. All page mappings are held in a global page table (which&#13;
the OS can scan to find a particular mapping), and <a data-primary="TLBs (translation lookaside buffers)" data-type="indexterm" id="idm45775549718872"/><a data-primary="translation lookaside buffers (TLBs)" data-type="indexterm" id="idm45775549718152"/>the most frequently used&#13;
mappings are held in translation lookaside buffers (TLBs). TLBs are held in&#13;
a fast cache, so accessing pages through a TLB entry is much faster than&#13;
accessing it through the page table.</p>&#13;
&#13;
<p>Machines have a limited number of TLB entries, so it becomes&#13;
important to maximize the hit rate on TLB entries (it functions as a&#13;
least recently used cache).&#13;
Since each entry&#13;
represents a page of memory, increasing the page&#13;
size used by an application is often advantageous. If each page represents more memory,&#13;
fewer TLB entries are required to encompass the entire program, and it&#13;
is more likely&#13;
that a page will be found in the TLB when required. This is true in general for&#13;
any program, and so is also true specifically for&#13;
things like Java application servers or other Java programs with even a&#13;
moderately sized heap.</p>&#13;
&#13;
<p>Large pages must be enabled at both the Java and OS levels. <a data-primary="-XX:+UseLargePages" data-type="indexterm" id="idm45775549715672"/>At the Java&#13;
level, the&#13;
<span class="keep-together"><code>-XX:+UseLargePages</code></span>&#13;
flag enables large page use; by default, this flag is <code>false</code>. Not all operating&#13;
systems support large pages, and the way to enable them obviously varies.</p>&#13;
&#13;
<p>If the&#13;
<span class="keep-together"><code>UseLargePages</code></span>&#13;
flag is enabled on a system that does not&#13;
support large pages, no warning is given, and the JVM uses regular pages.&#13;
If the&#13;
<span class="keep-together"><code>UseLargePages</code></span>&#13;
flag is enabled on a system that does support&#13;
large pages, but for which no large pages are available (either because&#13;
they are already all in use or because the operating system is misconfigured),&#13;
the JVM will print a warning.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Linux huge (large) pages" data-type="sect3"><div class="sect3" id="idm45775549710792">&#13;
<h3>Linux huge (large) pages</h3>&#13;
&#13;
<p><a data-primary="huge pages" data-type="indexterm" id="idm45775549709416"/><a data-primary="large pages" data-secondary="Linux huge pages" data-type="indexterm" id="idm45775549708712"/><a data-primary="Linux" data-secondary="huge pages" data-type="indexterm" id="idm45775549707768"/>Linux refers to large pages as <em>huge pages</em>.&#13;
The configuration of huge pages on Linux varies somewhat from release&#13;
to release; for the most accurate instructions, consult the documentation&#13;
for your release. But the general procedure is this:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Determine which huge page sizes the kernel supports. The size is based on the computer’s processor and the boot parameters given when the kernel has started, but the most common value is 2 MB:</p>&#13;
<pre data-type="programlisting">&#13;
# <strong>grep Hugepagesize /proc/meminfo</strong>&#13;
Hugepagesize:       2048 kB&#13;
</pre>&#13;
</li>&#13;
<li>&#13;
<p>Figure out how many huge pages are needed. If a JVM will allocate a 4 GB heap and the system has 2 MB huge pages, 2,048 huge pages will be needed for that heap. The number of huge pages that can be used is defined globally in the Linux kernel, so repeat this process for all the JVMs that will run (plus any other programs that will use huge pages). You should overestimate this value by 10% to account for other nonheap uses of huge pages (so the example here uses 2,200 huge pages).</p>&#13;
</li>&#13;
<li>&#13;
<p>Write out that value to the operating system (so it takes effect immediately):</p>&#13;
<pre data-type="programlisting">&#13;
# <strong>echo 2200 &gt; /proc/sys/vm/nr_hugepages</strong>&#13;
</pre>&#13;
</li>&#13;
<li>&#13;
<p>Save that value in <em>/etc/sysctl.conf</em> so that it is preserved after rebooting:</p>&#13;
&#13;
<pre data-type="programlisting">sys.nr_hugepages=2200</pre>&#13;
</li>&#13;
<li>&#13;
<p>On many versions of Linux, the amount of huge page memory that a user can allocate is limited. Edit the <em>/etc/security/limits.conf</em> file and add <code>memlock</code> entries for the user running your JVMs (e.g., in the example, the user <code>appuser</code>):</p>&#13;
&#13;
<pre data-type="programlisting">appuser soft    memlock        4613734400&#13;
appuser hard    memlock        4613734400</pre>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>If the <em>limits.conf</em> file is modified, the user must log in again for the value to take effect. At this point, the JVM should be able to allocate the necessary huge pages.&#13;
To verify that it works, run the following command:</p>&#13;
<pre data-type="programlisting">&#13;
# <strong>java -Xms4G -Xmx4G -XX:+UseLargePages -version</strong>&#13;
java version "1.8.0_201"&#13;
Java(TM) SE Runtime Environment (build 1.8.0_201-b09)&#13;
Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)&#13;
</pre>&#13;
&#13;
<p>Successful completion of that command indicates that the huge pages are&#13;
configured correctly.&#13;
If the huge page memory configuration is not correct, a warning will be given:</p>&#13;
&#13;
<pre data-type="programlisting">Java HotSpot(TM) 64-Bit Server VM warning:&#13;
Failed to reserve shared memory (errno = 22).</pre>&#13;
&#13;
<p>The program runs in that case; it just uses regular instead of large pages.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Linux transparent huge pages" data-type="sect3"><div class="sect3" id="idm45775549690344">&#13;
<h3>Linux transparent huge pages</h3>&#13;
&#13;
<p><a data-primary="large pages" data-secondary="Linux transparent huge pages" data-type="indexterm" id="idm45775549688776"/><a data-primary="Linux" data-secondary="transparent huge pages" data-type="indexterm" id="idm45775549687784"/><a data-primary="transparent huge pages" data-type="indexterm" id="idm45775549686840"/>Linux kernels starting with version 2.6.32 support <em>transparent huge pages</em>.&#13;
These offer (in theory) the same performance benefit as traditional huge&#13;
pages, but they have&#13;
some differences from traditional huge pages.</p>&#13;
&#13;
<p>First, traditional huge pages are locked into memory; they can never be swapped.&#13;
For Java, this is an advantage, since as we’ve discussed, swapping portions&#13;
of the heap is bad for GC performance. Transparent huge pages can be&#13;
swapped to disk, which is bad for performance.</p>&#13;
&#13;
<p>Second, allocation of a transparent huge page is also significantly&#13;
different from&#13;
a traditional huge page. Traditional huge pages are set aside at kernel&#13;
boot time; they are always available. Transparent huge pages are allocated&#13;
on demand: when the application requests a 2 MB page, the kernel will attempt&#13;
to find 2 MB of contiguous space in physical memory for the page. If&#13;
physical memory is fragmented, the kernel may decide to take time to&#13;
rearrange pages in a process similar to the by-now-familiar compacting of&#13;
memory in the Java heap. This means that the time to allocate a page may&#13;
be significantly longer as it waits for the kernel to finish making room&#13;
for the memory.</p>&#13;
&#13;
<p>This affects all programs, but for Java it can lead to very long GC pauses.&#13;
During GC, the JVM may decide to expand the heap and request new pages. If&#13;
that page allocation takes a few hundred milliseconds or even a second, the&#13;
GC time is significantly affected.</p>&#13;
&#13;
<p>Third, transparent huge pages are configured differently at both the OS&#13;
and Java levels. The details of that follow.</p>&#13;
&#13;
<p>At the operating system level, transparent huge pages are configured by&#13;
changing&#13;
the contents of <em>/sys/kernel/mm/transparent_hugepage/enabled</em>:</p>&#13;
<pre data-type="programlisting">&#13;
# <strong>cat /sys/kernel/mm/transparent_hugepage/enabled</strong>&#13;
always [madvise] never&#13;
# <strong>echo always &gt; /sys/kernel/mm/transparent_hugepage/enabled</strong>&#13;
# <strong>cat /sys/kernel/mm/transparent_hugepage/enabled</strong>&#13;
[always] madvise never&#13;
</pre>&#13;
&#13;
<p>The three choices here are as follows:</p>&#13;
<dl>&#13;
<dt><code>always</code></dt>&#13;
<dd>&#13;
<p>All programs are given huge pages when possible.</p>&#13;
</dd>&#13;
<dt><code>madvise</code></dt>&#13;
<dd>&#13;
<p>Programs that request huge pages are given them; other programs get regular <span class="keep-together">(4 KB)</span> pages.</p>&#13;
</dd>&#13;
<dt><code>never</code></dt>&#13;
<dd>&#13;
<p>No program gets huge pages, even when they request them.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>Different versions of Linux have a different default value for that&#13;
setting (and it is subject to change in future releases). Ubuntu 18.04 LTS,&#13;
for example, sets the default to <code>madvise</code>, but CentOS 7 (and vendor&#13;
releases like Red Hat and Oracle Enterprise Linux based on that) sets it&#13;
to <code>always</code>.  Be aware also that on&#13;
cloud machines, the supplier of the OS&#13;
image may have changed that value; I’ve seen Ubuntu images that also set the&#13;
value to <code>always</code>.</p>&#13;
&#13;
<p>If the value is set to <code>always</code>, no configuration is needed at the&#13;
Java level: the JVM will be given huge pages. In fact, all programs that&#13;
run on the system will run in huge pages.</p>&#13;
&#13;
<p><a data-primary="-XX:+UseTransparentHugePages" data-type="indexterm" id="idm45775549669304"/>If the value is set to <code>madvise</code> and you want the JVM to use huge pages,&#13;
specify the&#13;
<span class="keep-together"><code>UseTransparentHugePages</code></span>&#13;
flag (by default, <code>false</code>). Then the JVM will make the appropriate request when it&#13;
allocates pages&#13;
and be given huge pages.</p>&#13;
&#13;
<p>Predictably, if the value is set to <code>never</code>, no Java-level argument will&#13;
allow the JVM to get huge pages. Unlike traditional huge pages, though, no&#13;
warning is given if you specify the&#13;
<span class="keep-together"><code>UseTransparentHugePages</code></span>&#13;
flag and the system cannot provide them.</p>&#13;
&#13;
<p>Because of the differences in swapping and allocation of transparent huge&#13;
pages, they are often not recommended for use with Java; certainly their use&#13;
can lead to unpredictable spikes in pause times. On the other hand,&#13;
particularly on systems where they are enabled by default, you&#13;
will—transparently, as advertised—see performance benefits most of the time&#13;
when using them. If you want to be sure to get the smoothest performance&#13;
with huge pages, though, you are better off setting the system to use&#13;
transparent huge pages only when requested and configuring traditional huge&#13;
pages for use by your JVM.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Windows large pages" data-type="sect3"><div class="sect3" id="idm45775549689720">&#13;
<h3>Windows large pages</h3>&#13;
&#13;
<p><a data-primary="large pages" data-secondary="Windows" data-type="indexterm" id="idm45775549662232"/><a data-primary="Windows large pages" data-type="indexterm" id="idm45775549661256"/>Windows <em>large pages</em> can be enabled on only server-based Windows versions.&#13;
Exact instructions for Windows 10 are given here; variations exist among <span class="keep-together">releases:</span></p>&#13;
<ol>&#13;
<li>&#13;
<p>Start the Microsoft Management Center. Click the Start button, and in the Search box, type <strong><code>mmc</code></strong>.</p>&#13;
</li>&#13;
<li>&#13;
<p>If the left panel does not display a Local Computer Policy icon, select Add/Remove Snap-in from the File menu and add the Group Policy Object Editor. If that option is not available, the version of Windows in use does not support large pages.</p>&#13;
</li>&#13;
<li>&#13;
<p>In the left panel, expand Local Computer Policy → Computer Configuration → Windows Settings → Security Settings → Local Policies and click the User Rights Assignment folder.</p>&#13;
</li>&#13;
<li>&#13;
<p>In the right panel, double-click “Lock pages in memory.”</p>&#13;
</li>&#13;
<li>&#13;
<p>In the pop-up, add the user or group.</p>&#13;
</li>&#13;
<li>&#13;
<p>Click OK.</p>&#13;
</li>&#13;
<li>&#13;
<p>Quit the MMC.</p>&#13;
</li>&#13;
<li>&#13;
<p>Reboot.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>At this point, the JVM should be able to allocate the necessary large pages.&#13;
To verify that it works, run the following command:</p>&#13;
<pre data-type="programlisting">&#13;
# <strong>java -Xms4G -Xmx4G -XX:+UseLargePages -version</strong>&#13;
java version "1.8.0_201"&#13;
Java(TM) SE Runtime Environment (build 1.8.0_201-b09)&#13;
Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)&#13;
</pre>&#13;
&#13;
<p>If the command completes successfully like that, large pages are set up&#13;
correctly.&#13;
If the large memory configuration is incorrect, a warning is given:</p>&#13;
&#13;
<pre data-type="programlisting">Java HotSpot(TM) Server VM warning: JVM cannot use large page memory&#13;
because it does not have enough privilege to lock pages in memory.</pre>&#13;
&#13;
<p>Remember that the command will not print an error on a Windows system (such as&#13;
“home” versions) that does&#13;
not support large pages: once the JVM finds out that large&#13;
pages are not supported on the OS, it sets the&#13;
<span class="keep-together"><code>UseLargePages</code></span>&#13;
flag&#13;
to <code>false</code>, regardless of the command-line setting.</p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Using large pages will usually measurably speed up <span class="keep-together">applications.</span></p>&#13;
</li>&#13;
<li>&#13;
<p>Large page support must be explicitly enabled in most operating systems<a data-startref="ix_ch08-asciidoc11" data-type="indexterm" id="idm45775549641928"/>.<a data-startref="ix_ch08-asciidoc10" data-type="indexterm" id="idm45775549641016"/><a data-startref="ix_ch08-asciidoc9" data-type="indexterm" id="idm45775549640312"/><a data-startref="ix_ch08-asciidoc8" data-type="indexterm" id="idm45775549639640"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45775549638456">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Although the Java heap is the memory region that gets the most attention,&#13;
the entire footprint of the JVM is crucial to its performance, particularly&#13;
in relation to the operating system. The tools discussed in this chapter allow&#13;
you to track that footprint over time (and, crucially, to focus on the&#13;
committed memory of the JVM rather than the reserved memory).</p>&#13;
&#13;
<p>Certain ways that the JVM uses OS memory—particularly large pages—can also be tuned to improve performance. Long-running JVMs will&#13;
almost always benefit from using large pages, particularly if they have&#13;
large heaps.<a data-startref="ix_ch08-asciidoc0" data-type="indexterm" id="idm45775549636120"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45775549798408"><sup><a href="ch08.html#idm45775549798408-marker">1</a></sup> This is mostly a historical artifact: these libraries were developed before NIO and largely duplicate its functionality.</p></div></div></section></body></html>