<html><head></head><body><section data-pdf-bookmark="Chapter 10. Java Servers" data-type="chapter" epub:type="chapter"><div class="chapter" id="JavaServers">&#13;
<h1><span class="label">Chapter 10. </span>Java Servers</h1>&#13;
&#13;
&#13;
<p><a data-primary="Java servers" data-type="indexterm" id="ix_ch10-asciidoc0"/>This chapter explores topics around Java server technologies.&#13;
At their core, these technologies are all about how to transmit data, usually&#13;
over HTTP, between clients and servers. Hence, this chapter’s primary focus is on topics common to general server technology: how to scale servers using&#13;
different thread models, asynchronous responses, asynchronous requests, and&#13;
efficient handling of JSON data.</p>&#13;
&#13;
<p>Scaling servers is mostly about effective use of threads, and that use&#13;
requires event-driven, nonblocking I/O. Traditional Java/Jakarta&#13;
EE servers like Apache Tomcat, IBM WebSphere Application Server, and Oracle WebLogic Server have used Java NIO APIs to&#13;
do that for quite some time. Current server frameworks like Netty and Eclipse Vert.x&#13;
isolate the complexity of the Java NIO APIs to provide easy-to-use building&#13;
blocks for building smaller-footprint servers, and servers like Spring WebFlux&#13;
and Helidon are built on those frameworks (both use the Netty framework)&#13;
to provide scalable Java servers.</p>&#13;
&#13;
<p><a data-primary="reactive programming" data-type="indexterm" id="idm45775547339464"/>These newer frameworks offer programming models based on reactive&#13;
programming. At its core, <em>reactive programming</em> is based on handling&#13;
asynchronous data streams using an event-based paradigm. Though reactive&#13;
programming is a different way of looking at the events, for our purposes&#13;
both reactive programming and asynchronous programming offer the same&#13;
performance benefit: the ability to scale programs (and in particular,&#13;
to scale I/O) to many connections or data sources.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Java NIO Overview" data-type="sect1"><div class="sect1" id="idm45775547337624">&#13;
<h1>Java NIO Overview</h1>&#13;
&#13;
<p><a data-primary="Java servers" data-secondary="NIO overview" data-type="indexterm" id="ix_ch10-asciidoc1"/><a data-primary="nonblocking I/O (NIO)" data-secondary="overview" data-type="indexterm" id="ix_ch10-asciidoc2"/>If you’re familiar with the way nonblocking I/O works, you can skip to the next&#13;
section. If not, here’s a brief overview of how it works and why it is&#13;
important as the basis of this chapter.</p>&#13;
&#13;
<p>In early versions of Java, all I/O was blocking. A thread that attempted to&#13;
read data from a socket would wait (block) until at least some data was&#13;
available or the read timed out. More importantly, there is no way to know&#13;
if data is available on the socket without attempting to read from&#13;
the socket. So a thread that wanted to process data over a client connection&#13;
would have to issue a request to read the data, block until data is&#13;
available, process the request and send back the response, and then return&#13;
to the blocking read on the socket. This leads to the situation outlined&#13;
in <a data-type="xref" href="#FigureBlockingRead">Figure 10-1</a>.</p>&#13;
&#13;
<figure><div class="figure" id="FigureBlockingRead">&#13;
<img alt="Threads blocking on I/O reads from clients" src="assets/jp2e_1001.png"/>&#13;
<h6><span class="label">Figure 10-1. </span>Threads blocking on I/O reads from clients</h6>&#13;
</div></figure>&#13;
&#13;
<p>Blocking I/O requires that the server has a one-to-one correspondence&#13;
between client connections and server threads; each thread can handle only a single&#13;
connection. This is particularly an issue for clients that want to use HTTP&#13;
keepalive to avoid the performance impact of creating a new socket with&#13;
every request. Say that 100 clients are sending requests with an&#13;
average 30-second think time between requests, and it takes the server 500&#13;
milliseconds to process a request. In that case, an average of fewer than two requests will be in progress at any point, yet the server&#13;
will need 100 threads to process all the clients. This is highly inefficient.</p>&#13;
&#13;
<p>Hence, when Java introduced NIO APIs that were nonblocking, server&#13;
frameworks migrated to that model for their client handling. This leads&#13;
to the situation shown in <a data-type="xref" href="#FigureNonBlockingRead">Figure 10-2</a>.</p>&#13;
&#13;
<figure><div class="figure" id="FigureNonBlockingRead">&#13;
<img alt="Threads handling nonblocking I/O" src="assets/jp2e_1002.png"/>&#13;
<h6><span class="label">Figure 10-2. </span>Threads with event notification for reads</h6>&#13;
</div></figure>&#13;
&#13;
<p>Now the socket associated with each client is registered with a selector&#13;
in the server (the selector here is an instance of the&#13;
<code>Selector</code> class and handles the interface to the operating system that&#13;
provides notifications when data is available on a socket).&#13;
When the client sends a request, the selector gets an event&#13;
from the operating system and then notifies a thread in the server thread pool&#13;
that a particular client has I/O that can be read. That thread will read&#13;
the data from the client, process the request, send the response back,&#13;
and then go back to waiting for the next request.<sup><a data-type="noteref" href="ch10.html#idm45775547323624" id="idm45775547323624-marker">1</a></sup> And while we still have <em>N</em> clients in the diagram, they are&#13;
processed using <em>M</em> threads.</p>&#13;
&#13;
<p>Now that the clients are no longer coupled to a particular server thread,&#13;
the server thread pool can be tuned to handle the number of simultaneous&#13;
requests we expect the server to handle. In the example we used before, a&#13;
thread pool with a size of two would be sufficient to handle the load from&#13;
all 100 clients. If the requests could arrive nonuniformly but still within&#13;
the general parameters of a 30-second think time, we might need five or six&#13;
threads to handle the number of simultaneous requests. The use of nonblocking I/O allows us to use many fewer threads than we&#13;
have clients, which is a huge efficiency gain.<a data-startref="ix_ch10-asciidoc2" data-type="indexterm" id="idm45775547320984"/><a data-startref="ix_ch10-asciidoc1" data-type="indexterm" id="idm45775547320280"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Server Containers" data-type="sect1"><div class="sect1" id="idm45775547319480">&#13;
<h1>Server Containers</h1>&#13;
&#13;
<p><a data-primary="Java servers" data-secondary="server containers" data-type="indexterm" id="ix_ch10-asciidoc4"/><a data-primary="server containers" data-type="indexterm" id="ix_ch10-asciidoc5"/>Scaling server connections over multiple clients is the first hurdle in&#13;
server performance, which depends on the server using nonblocking I/O&#13;
for basic connection handling.&#13;
Whether servers use nonblocking APIs for other operations is also important&#13;
and is discussed later in this chapter, but for now we’ll look at&#13;
tuning the basic connection handling.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tuning Server Thread Pools" data-type="sect2"><div class="sect2" id="idm45775547315496">&#13;
<h2>Tuning Server Thread Pools</h2>&#13;
&#13;
<p><a data-primary="Java servers" data-secondary="tuning server thread pools" data-type="indexterm" id="ix_ch10-asciidoc6"/><a data-primary="REST servers" data-secondary="container" data-type="indexterm" id="ix_ch10-asciidoc7a"/><a data-primary="server containers" data-secondary="tuning server thread pools" data-type="indexterm" id="ix_ch10-asciidoc7"/><a data-primary="thread pools" data-secondary="tuning server thread pools" data-type="indexterm" id="ix_ch10-asciidoc8"/><a data-primary="tuning" data-secondary="server thread pools" data-type="indexterm" id="ix_ch10-asciidoc9"/>In current servers, then, the requests that come from clients are handled&#13;
by an arbitrary thread in the server thread pool. Tuning that thread pool&#13;
hence becomes quite important.</p>&#13;
&#13;
<p>As mentioned in the previous section, server frameworks vary in the way they manage connections and associated thread pool(s). <a data-primary="selector threads" data-type="indexterm" id="idm45775547307080"/>The basic model described there was to have&#13;
one or more threads that act as selectors: these threads notify the system&#13;
call when I/O is available and are called <em>selector threads</em>.&#13;
<a data-primary="worker threads" data-type="indexterm" id="idm45775547305688"/>Then a separate thread pool of <em>worker threads</em> handles the actual&#13;
request/response to a client after the selector notifies them that I/O is&#13;
pending for the client.</p>&#13;
&#13;
<p>The selector and worker threads can be set&#13;
up in various ways:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Selector and worker thread pools can be separate. The selectors wait for notification on all sockets and hand off requests to the worker thread pool.</p>&#13;
</li>&#13;
<li>&#13;
<p>Alternately, when the selector is notified about I/O, it reads (perhaps only part of) the I/O to determine information about the request. Then the selector forwards the request to different server thread pools, depending on the type of request.</p>&#13;
</li>&#13;
<li>&#13;
<p>A selector pool accepts new connections on a <code>ServerSocket</code>, but after the connections are made, all work is handled in the worker thread pool. A thread in the worker thread pool will sometimes use the <code>Selector</code> class to wait for pending I/O about an existing connection, and it will sometimes be handling the notification from a worker thread that I/O for a client is pending (e.g., it will perform the request/response for the client).</p>&#13;
</li>&#13;
<li>&#13;
<p>There needn’t be a distinction at all between threads that act as selectors and threads that handle requests. A thread that is notified about I/O available on a socket can process the entire request. Meanwhile, the other threads in the pool are notified about I/O on other sockets and handle the requests on those other sockets.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Despite these differences, we should keep two basic things in mind when&#13;
tuning the server thread pools. First (and most important) is that we need&#13;
sufficient worker threads to handle the number of simultaneous requests (not&#13;
simultaneous connections) that the server can handle. As discussed in&#13;
<a data-type="xref" href="ch09.html#ThreadPerformance">Chapter 9</a>, this partly depends on whether those requests will&#13;
themselves execute CPU-intensive code or will make other blocking calls.&#13;
An additional consideration in this case is what happens&#13;
if the server makes additional nonblocking calls.</p>&#13;
&#13;
<p>Consider a REST server that just performs CPU-intensive calculations. Then,&#13;
like all CPU-bound cases, there is no need to have more threads than there are&#13;
virtual CPUs on the machine or container running the server: we’ll never be&#13;
able to run more threads than that.</p>&#13;
&#13;
<p>What if the REST server, in turn, makes outbound calls to another resource—say,&#13;
another REST server or a database? Now it depends on whether those calls are&#13;
blocking or nonblocking. Assume for now that those calls are blocking.&#13;
Now we’ll need one thread for every simultaneous outbound blocking call. This&#13;
threatens to turn our server back into an inefficient&#13;
one-thread-per-client model.</p>&#13;
&#13;
<p>Imagine that in order to satisfy a particular client request, the&#13;
worker thread must spend 900 ms retrieving data from a database&#13;
and 100 ms setting up that database call and processing the&#13;
data into the response for the&#13;
client on a system with two non-hyper-threaded CPUs. That server has enough&#13;
CPU to process 20 requests per second. If a request comes from each client&#13;
every 30 seconds, the server can handle 600 clients. Because the client&#13;
connection handling is nonblocking, we don’t need 600 threads in the&#13;
worker thread pool,&#13;
but we cannot get by with only 2 threads (one per CPU) either. On average,&#13;
20 requests will be blocked at a time, so we’ll need at least that many&#13;
threads in the worker thread pool.</p>&#13;
&#13;
<p>Now let’s say that the outbound request is also nonblocking so that during&#13;
the 900 ms the database takes to return the answer, the thread&#13;
making the database call is free to handle other requests. Now we’re back to&#13;
needing only two worker threads: they can spend all their time handling the&#13;
100 ms sections it takes to deal with the database data, keeping the&#13;
CPUs fully busy and the throughput of our server at the maximum value.</p>&#13;
&#13;
<p>As usual, this simplifies the discussion somewhat: we need time to read&#13;
and set up the requests, and so on. Still, the basic rule holds: you need as&#13;
many threads in the worker pool as will be simultaneously executing code&#13;
and simultaneously blocked on other resources.</p>&#13;
&#13;
<p>Another tuning consideration here is the number of threads&#13;
that need to act as selectors at any given point. You need more&#13;
than one. A selector thread executes the <code>select()</code> call in order to find&#13;
out which of many sockets has I/O available. It must then spend time&#13;
processing that data: at the very least, notifying the other worker threads&#13;
about which clients&#13;
have a request to be processed. Then it can return and call the <code>select()</code>&#13;
method again. But during the time it processes the results from the&#13;
<code>select()</code> call, another thread should be&#13;
executing the <code>select()</code> call for other sockets to see when they have available&#13;
data.</p>&#13;
&#13;
<p>So in frameworks that have a separate pool of threads for selectors, you’ll&#13;
want to make sure the pool has at least a few threads (typically,&#13;
three is the default value for frameworks). In frameworks where the same pool&#13;
of threads handles selection and processing, you’ll want to add a few extra&#13;
threads than is required based on the worker guideline we just discussed.<a data-startref="ix_ch10-asciidoc9" data-type="indexterm" id="idm45775547288296"/><a data-startref="ix_ch10-asciidoc8" data-type="indexterm" id="idm45775547287592"/><a data-startref="ix_ch10-asciidoc7a" data-type="indexterm" id="idm45775547286920"/><a data-startref="ix_ch10-asciidoc7" data-type="indexterm" id="idm45775547286248"/><a data-startref="ix_ch10-asciidoc6" data-type="indexterm" id="idm45775547285576"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Async Rest Servers" data-type="sect2"><div class="sect2" id="AsyncRESTServer">&#13;
<h2>Async Rest Servers</h2>&#13;
&#13;
<p><a data-primary="async REST servers" data-type="indexterm" id="ix_ch10-asciidoc10"/><a data-primary="Java servers" data-secondary="async REST servers" data-type="indexterm" id="ix_ch10-asciidoc11"/><a data-primary="REST servers" data-secondary="asynchronous" data-type="indexterm" id="ix_ch10-asciidoc12"/><a data-primary="server containers" data-secondary="async REST servers" data-type="indexterm" id="ix_ch10-asciidoc13"/>An alternative to tuning the request thread pool of a server is to defer&#13;
work to another thread pool. <a data-primary="JAX-RS (Java API for RESTful Web Services)" data-type="indexterm" id="ix_ch10-asciidoc14"/>This is an approach taken by the async&#13;
server implementation of JAX-RS as well as Netty’s event executor tasks&#13;
(designed for long-running tasks) and other frameworks.</p>&#13;
&#13;
<p>Let’s look at this from the perspective of JAX-RS. In a simple REST server,&#13;
requests and responses are all handled on the same&#13;
thread. This throttles the concurrency of those servers. For instance, the&#13;
default thread pool for a Helidon server on an eight-CPU machine is 32.&#13;
Consider the following endpoint:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting">    <code class="nd">@GET</code>&#13;
    <code class="nd">@Path</code><code class="o">(</code><code class="s">"/sleep"</code><code class="o">)</code>&#13;
    <code class="nd">@Produces</code><code class="o">(</code><code class="n">MediaType</code><code class="o">.</code><code class="na">APPLICATION_JSON</code><code class="o">)</code>&#13;
    <code class="kd">public</code> <code class="n">String</code> <code class="nf">sleepEndpoint</code><code class="o">(</code>&#13;
        <code class="nd">@DefaultValue</code><code class="o">(</code><code class="s">"100"</code><code class="o">)</code> <code class="nd">@QueryParam</code><code class="o">(</code><code class="s">"delay"</code><code class="o">)</code> <code class="kt">long</code> <code class="n">delay</code>&#13;
        <code class="o">)</code> <code class="kd">throws</code> <code class="n">ParseException</code> <code class="o">{</code>&#13;
        <code class="k">try</code> <code class="o">{</code> <code class="n">Thread</code><code class="o">.</code><code class="na">sleep</code><code class="o">(</code><code class="n">delay</code><code class="o">);</code> <code class="o">}</code> <code class="k">catch</code> <code class="o">(</code><code class="n">InterruptedException</code> <code class="n">ie</code><code class="o">)</code> <code class="o">{}</code>&#13;
        <code class="k">return</code> <code class="s">"{\"sleepTime\": \""</code> <code class="o">+</code> <code class="n">delay</code> <code class="o">+</code> <code class="s">"\"}"</code><code class="o">;</code>&#13;
    <code class="o">}</code></pre>&#13;
&#13;
<p>The point of the sleep in this example is just for testing: assume that the&#13;
sleep is making a remote database call or calling another REST server, and&#13;
that remote call takes 100 ms. If I run that test in a Helidon server with&#13;
default configuration, it will handle 32 simultaneous requests. A load&#13;
generator with a concurrency of 32 will report that each request takes&#13;
100 ms (plus 1–2 ms for processing). A load generator with a concurrency of&#13;
64 will report that each request takes 200 ms, since each request will have&#13;
to wait for another request to finish before it can start processing.</p>&#13;
&#13;
<p>Other servers will have a different configuration, but the effect is the same:&#13;
there&#13;
will be some throttle based on the size of the request thread pool. Often&#13;
that is a good thing: if the 100 ms is spent as active CPU time (instead&#13;
of sleeping) in this&#13;
example, then the server won’t really be able to handle 32 requests&#13;
simultaneously unless it is running on a very large machine.</p>&#13;
&#13;
<p>In this case, though, the machine is not even close to being CPU-bound; it&#13;
may take only 20%–30% of a single core to process the load when there is no&#13;
processing to be done (and again, the same amount to process the load if those&#13;
100 ms time intervals are just a remote call to another service). So we&#13;
can increase the concurrency on this machine by changing the configuration of&#13;
the default thread pool to run more calls. The limit here would be based on&#13;
the concurrency of the remote systems; we still want to throttle the calls into&#13;
those systems so that they are not overwhelmed.</p>&#13;
&#13;
<p>JAX-RS provides a second way to increase the concurrency, and that is by&#13;
utilizing an asynchronous response. The asynchronous response allows us to&#13;
defer the business logic processing to a different thread pool:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting">    <code class="n">ThreadPoolExecutor</code> <code class="n">tpe</code> <code class="o">=</code> <code class="n">Executors</code><code class="o">.</code><code class="na">newFixedThreadPool</code><code class="o">(</code><code class="mi">64</code><code class="o">);</code>&#13;
    <code class="nd">@GET</code>&#13;
    <code class="nd">@Path</code><code class="o">(</code><code class="s">"/asyncsleep"</code><code class="o">)</code>&#13;
    <code class="nd">@Produces</code><code class="o">(</code><code class="n">MediaType</code><code class="o">.</code><code class="na">APPLICATION_JSON</code><code class="o">)</code>&#13;
    <code class="kd">public</code> <code class="kt">void</code> <code class="nf">sleepAsyncEndpoint</code><code class="o">(</code>&#13;
        <code class="nd">@DefaultValue</code><code class="o">(</code><code class="s">"100"</code><code class="o">)</code> <code class="nd">@QueryParam</code><code class="o">(</code><code class="s">"delay"</code><code class="o">)</code> <code class="kt">long</code> <code class="n">delay</code><code class="o">,</code>&#13;
        <code class="nd">@Suspended</code> <code class="kd">final</code> <code class="n">AsyncResponse</code> <code class="n">ar</code>&#13;
        <code class="o">)</code> <code class="kd">throws</code> <code class="n">ParseException</code> <code class="o">{</code>&#13;
        <code class="n">tpe</code><code class="o">.</code><code class="na">execute</code><code class="o">(()</code> <code class="o">-&gt;</code> <code class="o">{</code>&#13;
            <code class="k">try</code> <code class="o">{</code> <code class="n">Thread</code><code class="o">.</code><code class="na">sleep</code><code class="o">(</code><code class="n">delay</code><code class="o">);</code> <code class="o">}</code> <code class="k">catch</code> <code class="o">(</code><code class="n">InterruptedException</code> <code class="n">ie</code><code class="o">)</code> <code class="o">{}</code>&#13;
            <code class="n">ar</code><code class="o">.</code><code class="na">resume</code><code class="o">(</code><code class="s">"{\"sleepTime\": \""</code> <code class="o">+</code> <code class="n">delay</code> <code class="o">+</code> <code class="s">"\"}"</code><code class="o">);</code>&#13;
        <code class="o">});</code>&#13;
    <code class="o">}</code></pre>&#13;
&#13;
<p class="pagebreak-before">In this example, the initial request comes in on the server’s default&#13;
thread pool.&#13;
That request sets up a call to execute the business logic in a separate&#13;
thread pool (called the <em>async thread pool</em>), and then the <code>sleepAsyncEndpoint()</code>&#13;
 method immediately returns.&#13;
That frees the thread from the default thread pool so it can immediately&#13;
handle another request. Meanwhile, the async response (annotated with the&#13;
<code>@Suspended</code> tag) is waiting for the logic to complete; when that happens,&#13;
it is resumed with the response to be sent back to the user.</p>&#13;
&#13;
<p>This allows us to run 64 (or whatever parameter we pass to the thread pool)&#13;
simultaneous requests before the requests start to&#13;
back up. But frankly, we haven’t achieved anything different from resizing the default thread pool to 64. In fact, in this case, our response will&#13;
be slightly worse, since the request gets sent to a different thread for&#13;
processing, which will take a few milliseconds.</p>&#13;
&#13;
<p>There are three reasons you would use an async response:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>To introduce more parallelism into the business logic. Imagine that instead of sleeping for 100 ms, our code had to make&#13;
three (unrelated) JDBC calls to obtain data needed for the response. Using an&#13;
async response allows the code to process each call in parallel,&#13;
with each JDBC call using a separate thread in the async thread pool.</p>&#13;
</li>&#13;
<li>&#13;
<p>To limit the number of active threads.</p>&#13;
</li>&#13;
<li>&#13;
<p>To properly throttle the server.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In most REST servers, if we just throttle the request&#13;
thread pool, new requests will wait their turn, and the queue for the&#13;
thread pool will grow. Often, this queue is unbounded (or at least has a&#13;
very large bound) so that the total number of requests ends up being&#13;
unmanageable. Requests that spend a long time in a thread pool queue will&#13;
often be abandoned by the time they are processed, and even if they are not&#13;
abandoned, the long response times are going to kill the total throughput&#13;
of the <span class="keep-together">system.</span></p>&#13;
&#13;
<p>A better approach is to look at the async thread pool status before queueing the&#13;
response, and rejecting the request if the system is too busy.</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting">    <code class="nd">@GET</code>&#13;
    <code class="nd">@Path</code><code class="o">(</code><code class="s">"/asyncreject"</code><code class="o">)</code>&#13;
    <code class="nd">@Produces</code><code class="o">(</code><code class="n">MediaType</code><code class="o">.</code><code class="na">APPLICATION_JSON</code><code class="o">)</code>&#13;
    <code class="kd">public</code> <code class="kt">void</code> <code class="nf">sleepAsyncRejectEndpoint</code><code class="o">(</code>&#13;
        <code class="nd">@DefaultValue</code><code class="o">(</code><code class="s">"100"</code><code class="o">)</code> <code class="nd">@QueryParam</code><code class="o">(</code><code class="s">"delay"</code><code class="o">)</code> <code class="kt">long</code> <code class="n">delay</code><code class="o">,</code>&#13;
        <code class="nd">@Suspended</code> <code class="kd">final</code> <code class="n">AsyncResponse</code> <code class="n">ar</code>&#13;
        <code class="o">)</code> <code class="kd">throws</code> <code class="n">ParseException</code> <code class="o">{</code>&#13;
        <code class="k">if</code> <code class="o">(</code><code class="n">tpe</code><code class="o">.</code><code class="na">getActiveCount</code><code class="o">()</code> <code class="o">==</code> <code class="mi">64</code><code class="o">)</code> <code class="o">{</code>&#13;
            <code class="n">ar</code><code class="o">.</code><code class="na">cancel</code><code class="o">();</code>&#13;
            <code class="k">return</code><code class="o">;</code>&#13;
        <code class="o">}</code>&#13;
        <code class="n">tpe</code><code class="o">.</code><code class="na">execute</code><code class="o">(()</code> <code class="o">-&gt;</code> <code class="o">{</code>&#13;
            <code class="c1">// Simulate processing delay using sleep</code>&#13;
            <code class="k">try</code> <code class="o">{</code> <code class="n">Thread</code><code class="o">.</code><code class="na">sleep</code><code class="o">(</code><code class="n">delay</code><code class="o">);</code> <code class="o">}</code> <code class="k">catch</code> <code class="o">(</code><code class="n">InterruptedException</code> <code class="n">ie</code><code class="o">)</code> <code class="o">{}</code>&#13;
            <code class="n">ar</code><code class="o">.</code><code class="na">resume</code><code class="o">(</code><code class="s">"{\"sleepTime\": \""</code> <code class="o">+</code> <code class="n">delay</code> <code class="o">+</code> <code class="s">"\"}"</code><code class="o">);</code>&#13;
        <code class="o">});</code>&#13;
    <code class="o">}</code></pre>&#13;
&#13;
<p>That can be accomplished in many ways, but for this simple example, we’ll look at the active count running in the pool. If the count is equal to the&#13;
pool size, the response is immediately canceled. (A more sophisticated&#13;
example would set up a bounded queue for the pool and cancel the request in&#13;
the thread pool’s rejected execution handler.) The effect here is that the&#13;
caller will immediately receive an HTTP 503 Service Unavailable status,&#13;
indicating that the request cannot be processed at this time. That is the&#13;
preferred way to handle an overloaded server in the REST world, and immediately&#13;
returning that status will reduce the load on our overloaded server, which&#13;
in the end will lead to much better overall performance<a data-startref="ix_ch10-asciidoc14" data-type="indexterm" id="idm45775547087688"/>.<a data-startref="ix_ch10-asciidoc13" data-type="indexterm" id="idm45775546965000"/><a data-startref="ix_ch10-asciidoc12" data-type="indexterm" id="idm45775546964360"/><a data-startref="ix_ch10-asciidoc11" data-type="indexterm" id="idm45775546963688"/><a data-startref="ix_ch10-asciidoc10" data-type="indexterm" id="idm45775546963016"/></p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Nonblocking I/O using Java’s NIO APIs allows servers to scale by reducing the number of threads required to handle multiple clients.</p>&#13;
</li>&#13;
<li>&#13;
<p>This technique means that a server will need one or more thread pools to handle the client requests. This pool should be tuned based on the maximum number of simultaneous requests the server should handle.</p>&#13;
</li>&#13;
<li>&#13;
<p>A few extra threads are then needed for handling selectors (whether as part of the worker thread pool or a separate thread pool depending on the server framework).</p>&#13;
</li>&#13;
<li>&#13;
<p>Server frameworks often have a mechanism to defer long requests to a different thread pool, which offers more robust handling of requests on the main thread pool.<a data-startref="ix_ch10-asciidoc5" data-type="indexterm" id="idm45775546957128"/><a data-startref="ix_ch10-asciidoc4" data-type="indexterm" id="idm45775546956424"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Asynchronous Outbound Calls" data-type="sect1"><div class="sect1" id="idm45775546955240">&#13;
<h1>Asynchronous Outbound Calls</h1>&#13;
&#13;
<p><a data-primary="asynchronous outbound calls" data-type="indexterm" id="ix_ch10-asciidoc15"/><a data-primary="Java servers" data-secondary="asynchronous outbound calls" data-type="indexterm" id="ix_ch10-asciidoc16"/>The preceding section gave the example of a server with two CPUs that needed&#13;
a pool of 20 threads to obtain its maximum throughput. That was because the&#13;
threads spent 90% of their time blocked on I/O while making an outbound call&#13;
to another resource.</p>&#13;
&#13;
<p>Nonblocking I/O can help in this instance too: if those outbound HTTP or&#13;
JDBC calls are nonblocking, we needn’t dedicate a thread to the call&#13;
and can reduce the size of the thread pool accordingly.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Asynchronous HTTP" data-type="sect2"><div class="sect2" id="idm45775546950808">&#13;
<h2>Asynchronous HTTP</h2>&#13;
&#13;
<p><a data-primary="asynchronous HTTP" data-type="indexterm" id="ix_ch10-asciidoc17"/><a data-primary="HTTP, asynchronous" data-type="indexterm" id="ix_ch10-asciidoc18"/>HTTP clients are classes that (unsurprisingly) handle HTTP requests to a&#13;
server. There are many clients, and they all have different&#13;
functional as well as performance&#13;
characteristics. In this section, we’ll look into the performance&#13;
characteristics for common use cases among them.</p>&#13;
&#13;
<p>Java 8 has a basic HTTP client, the <code>java.net.HttpURLConnection</code> class (and for <span class="keep-together">secure</span> connections, the subclass <code>java.net.HttpsURLConnection</code>). Java 11 adds a <span class="keep-together">new client:</span> the <span class="keep-together"><code>java.net.http.HttpClient</code></span> class (which also handles HTTPS). <span class="keep-together">Other HTTP</span> client classes from other packages include <code>org.apache.http.cli⁠ent​.HttpClient</code> from the Apache Foundation,&#13;
<span class="keep-together"><code>org​.asynchttpclient.AsyncHttpClient</code></span> built on top of the Netty Project,&#13;
and <span class="keep-together"><code>org.eclipse.jetty.client.HttpClient</code></span> from the Eclipse Foundation.</p>&#13;
&#13;
<p>Although it is possible to perform basic operations with the&#13;
<code>HttpURLConnection</code> class, most REST calls are made using a framework such as JAX-RS. Hence, most HTTP clients directly implement those APIs&#13;
(or slight variants), but the default implementation of JAX-RS&#13;
also provides connectors for the most popular HTTP clients. Hence, you&#13;
can use JAX-RS with the underlying HTTP client that gives you the&#13;
best performance. The JAX-RS and underlying HTTP clients carry two basic performance considerations.</p>&#13;
&#13;
<p>First, the JAX-RS connectors provide a <code>Client</code> object that is used to make&#13;
the REST calls; when using the clients directly, they similarly provide a client&#13;
object with a name like <code>HttpClient</code> (the <code>HttpURLConnection</code> class is an&#13;
exception; it cannot be reused).&#13;
A typical client would be created and used like this:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">private</code> <code class="kd">static</code> <code class="n">Client</code> <code class="n">client</code><code class="o">;</code>&#13;
<code class="kd">static</code> <code class="o">{</code>&#13;
    <code class="n">ClientConfig</code> <code class="n">cc</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ClientConfig</code><code class="o">();</code>&#13;
    <code class="n">cc</code><code class="o">.</code><code class="na">connectorProvider</code><code class="o">(</code><code class="k">new</code> <code class="n">JettyConnectorProvider</code><code class="o">());</code>&#13;
    <code class="n">client</code> <code class="o">=</code> <code class="n">ClientBuilder</code><code class="o">.</code><code class="na">newClient</code><code class="o">(</code><code class="n">cc</code><code class="o">);</code>&#13;
<code class="o">}</code>&#13;
&#13;
<code class="kd">public</code> <code class="n">Message</code> <code class="nf">getMessage</code><code class="o">()</code> <code class="o">{</code>&#13;
    <code class="n">Message</code> <code class="n">m</code> <code class="o">=</code> <code class="n">client</code><code class="o">.</code><code class="na">target</code><code class="o">(</code><code class="n">URI</code><code class="o">.</code><code class="na">create</code><code class="o">(</code><code class="n">url</code><code class="o">)</code>&#13;
                  <code class="o">.</code><code class="na">request</code><code class="o">(</code><code class="n">MediaType</code><code class="o">.</code><code class="na">APPLICATION_JSON</code><code class="o">)</code>&#13;
                  <code class="o">.</code><code class="na">get</code><code class="o">(</code><code class="n">Message</code><code class="o">.</code><code class="na">class</code><code class="o">);</code>&#13;
    <code class="k">return</code> <code class="n">m</code><code class="o">;</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>The key in this example is that the <code>client</code> object is a static,&#13;
shared object.&#13;
All client objects are threadsafe, and all are expensive to instantiate, so&#13;
you want only a limited number of them (e.g., one) in your application.</p>&#13;
&#13;
<p>The second performance consideration is to make sure that the HTTP&#13;
client properly&#13;
pools connections and uses keepalive to keep connections open. Opening a&#13;
socket for HTTP communications is an expensive operation, particularly if the&#13;
protocol is HTTPS and the client and server must perform an SSL handshake.&#13;
Like JDBC connections, HTTP(S) connections should be&#13;
reused.</p>&#13;
&#13;
<p>All HTTP clients provide a mechanism to pool them, though the mechanism of&#13;
pooling within the <code>HttpURLConnection</code> class is frequently misunderstood.&#13;
By default, that class will pool five connections (per server). Unlike a&#13;
traditional pool, though, the pool in this class does not throttle connections:&#13;
if you request a sixth connection, a new connection will be created and then&#13;
destroyed when you are finished with it. That kind of transient connection&#13;
is not something you see with a traditional connection pool.&#13;
So in the default configuration of the <code>HttpURLConnection</code> class,&#13;
it’s easy to see lots of transient connections and&#13;
assume that the connections are not being pooled (and the Javadoc isn’t&#13;
helpful here either;&#13;
it never mentions the pooling functionality, though the behavior is documented&#13;
elsewhere).</p>&#13;
&#13;
<p>You can change the size of the pool by setting&#13;
the system property&#13;
<span class="keep-together"><code>-Dhttp.maxConnections=<em>N</em></code>,</span>&#13;
which defaults to 5. Despite its name, this property applies to&#13;
HTTPS connections as well. There is no way to have this class throttle&#13;
connections, though.</p>&#13;
&#13;
<p>In the new <code>HttpClient</code> class in JDK 11, the pool follows a similar idea,&#13;
but with two important differences. First, the default pool size is&#13;
unbounded, though that can be set with the&#13;
<span class="keep-together"><code>-Djdk.httpclient.connectionPoolSize=<em>N</em></code></span>&#13;
system property. That property still doesn’t act as a throttle; if you&#13;
request more connections than are configured, they will be created when needed and then destroyed when&#13;
they are complete. Second, this pool is per <code>HttpClient</code> object, so if you&#13;
are not reusing that object, you will not get any connection pooling.</p>&#13;
&#13;
<p><a data-primary="JAX-RS (Java API for RESTful Web Services)" data-type="indexterm" id="idm45775546891192"/>In JAX-RS itself, it is frequently suggested to use a different connector&#13;
than the default if you want a connection pool. Since the default connector&#13;
uses the <code>HttpURLConnection</code> class, that’s not true: unless you want to&#13;
throttle the connections, you can tune the connection size of that class&#13;
as we’ve just discussed. Other popular connectors will also pool the&#13;
connections.</p>&#13;
<table id="TableHttpClientPool">&#13;
<caption><span class="label">Table 10-1. </span>Tuning the HTTP connection pool of popular clients</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Connector</th>&#13;
<th>HTTP client class</th>&#13;
<th>Pooling mechanism</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>Default</p></td>&#13;
<td><p><code>java.net.HttpURLConnection</code></p></td>&#13;
<td><p>Setting the <code>maxConnections</code> system property</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Apache</p></td>&#13;
<td><p><code>org.apache.http.​cli⁠ent.HttpClient</code></p></td>&#13;
<td><p>Create a <code>PoolingHttpClientConnectionManager</code></p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Grizzly</p></td>&#13;
<td><p><code>com.ning.http.client.​Asyn⁠cHttpClient</code></p></td>&#13;
<td><p>Pooled by default; can modify configuration</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Jetty</p></td>&#13;
<td><p><code>org.eclipse.jetty.​cli⁠ent.HttpClient</code></p></td>&#13;
<td><p>Pooled by default; can modify configuration</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>In JAX-RS, the Grizzly connection manager uses the&#13;
<code>com.ning.http.client​.Asyn⁠cHttpClient</code> client. That client has since been renamed to&#13;
<code>org​.asyn⁠chttpclient.AsyncHttpClient</code>; it is the async client built on&#13;
top of Netty.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Async HTTP clients" data-type="sect3"><div class="sect3" id="AsyncHTTPClients">&#13;
<h3>Async HTTP clients</h3>&#13;
&#13;
<p><a data-primary="asynchronous HTTP" data-secondary="clients" data-type="indexterm" id="ix_ch10-asciidoc19"/>Asynchronous (async) HTTP clients, like async HTTP servers, allow for&#13;
better thread management&#13;
in an application. The thread that makes an async call sends the request to&#13;
the remote server, and arrangements are made for a different (background)&#13;
thread to process the request when it is available.</p>&#13;
&#13;
<p>That statement (“arrangements are made”) is purposely vague here, because the&#13;
mechanism in which that is achieved is very different between different&#13;
HTTP clients. But from a performance perspective, the point is that using&#13;
an async client increases performance because it defers the response handling&#13;
to another thread, allowing more things to run in parallel.</p>&#13;
&#13;
<p><a data-primary="JAX-RS (Java API for RESTful Web Services)" data-type="indexterm" id="idm45775546822360"/>Async HTTP clients are a feature of JAX-RS 2.0, though most standalone HTTP&#13;
clients also support async features directly. In fact, you may have noticed&#13;
that some of the clients we looked at had <em>async</em> as part of their name; they&#13;
are asynchronous by default. Although they have a synchronous mode, that&#13;
happens in the implementation of the synchronous methods: those methods make&#13;
an async call, wait for the response to be complete, and then return that&#13;
response (synchronously) to the caller.</p>&#13;
&#13;
<p>This async mode is supported by JAX-RS 2.0 implementations, including those&#13;
in the reference Jersey implementation. That implementation includes several&#13;
connectors that can be used asynchronously, though not all these connectors are truly asynchronous. In all cases, the response handling is deferred to another thread, but it can operate in two basic ways. In one case, that other thread can simply use standard, blocking Java I/O. In that case, the background&#13;
thread pool needs one thread for every request to be handled concurrently.&#13;
That’s the same as with the async server: we gain concurrency by&#13;
adding lots of other threads.</p>&#13;
&#13;
<p>In the second case, the HTTP client uses nonblocking I/O. For that kind of&#13;
processing, the background thread needs a few (at least one, but typically&#13;
more) threads to handle NIO key selection and then some threads to handle responses as they come in. In many cases, these HTTP clients&#13;
then use fewer threads overall. NIO is classic event-driven programming:&#13;
when data on a socket connection is available to be read, a thread (usually&#13;
from a pool) is notified of that event. That thread reads the data, processes&#13;
it (or passes the data to yet another thread to be processed), and then&#13;
returns to the pool.</p>&#13;
&#13;
<p>Async programming is typically thought of as being event-driven, and so in&#13;
a strict sense, the async HTTP clients that use blocking I/O (and pin a&#13;
thread for the entire request) are not asynchronous. The API gives the&#13;
illusion of asynchronous behavior, even if the thread scalability will not&#13;
be what we are expecting.</p>&#13;
&#13;
<p>From a performance perspective, the async client gives us similar&#13;
benefits as the async server: we can increase concurrency during a request&#13;
so that it executes more quickly, and we can better manage (and throttle)&#13;
requests by utilizing different thread pools.</p>&#13;
&#13;
<p>Let’s take a common case for async examples: a REST service that functions&#13;
as an aggregator of information from three other REST services. The&#13;
pseudocode outline for such a service looks like this:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kd">public</code> <code class="kd">class</code> <code class="nc">TestResource</code> <code class="o">{</code>&#13;
    <code class="kd">public</code> <code class="kd">static</code> <code class="kd">class</code> <code class="nc">MultiCallback</code> <code class="kd">extends</code> <code class="n">InvocationCallback</code><code class="o">&lt;</code><code class="n">Message</code><code class="o">&gt;</code> <code class="o">{</code>&#13;
        <code class="kd">private</code> <code class="n">AsyncResponse</code> <code class="n">ar</code><code class="o">;</code>&#13;
        <code class="kd">private</code> <code class="n">AtomicDouble</code> <code class="n">total</code> <code class="o">=</code> <code class="k">new</code> <code class="n">AtomicDouble</code><code class="o">(</code><code class="mi">0</code><code class="o">);</code>&#13;
        <code class="kd">private</code> <code class="n">AtomicInteger</code> <code class="n">pendingResponses</code><code class="o">;</code>&#13;
        <code class="kd">public</code> <code class="nf">MultiCallback</code><code class="o">(</code><code class="n">AsyncResponse</code> <code class="n">ar</code><code class="o">,</code> <code class="kt">int</code> <code class="n">targetCount</code><code class="o">)</code> <code class="o">{</code>&#13;
            <code class="k">this</code><code class="o">.</code><code class="na">ar</code> <code class="o">=</code> <code class="n">ar</code><code class="o">;</code>&#13;
            <code class="n">pendingResponse</code> <code class="o">=</code> <code class="k">new</code> <code class="n">AtomicInteger</code><code class="o">(</code><code class="n">targetCount</code><code class="o">);</code>&#13;
        <code class="o">}</code>&#13;
        <code class="kd">public</code> <code class="kt">void</code> <code class="nf">completed</code><code class="o">(</code><code class="n">Message</code> <code class="n">m</code><code class="o">)</code> <code class="o">{</code>&#13;
            <code class="kt">double</code> <code class="n">d</code> <code class="o">=</code> <code class="n">total</code><code class="o">.</code><code class="na">getAndIncrement</code><code class="o">(</code><code class="n">Message</code><code class="o">.</code><code class="na">getValue</code><code class="o">());</code>&#13;
            <code class="k">if</code> <code class="o">(</code><code class="n">targetCount</code><code class="o">.</code><code class="na">decrementAndGet</code><code class="o">()</code> <code class="o">==</code> <code class="mi">0</code><code class="o">)</code> <code class="o">{</code>&#13;
                <code class="n">ar</code><code class="o">.</code><code class="na">resume</code><code class="o">(</code><code class="s">"{\"total\": \""</code> <code class="o">+</code> <code class="n">d</code> <code class="o">+</code> <code class="s">"\"}"</code><code class="o">);</code>&#13;
            <code class="o">}</code>&#13;
        <code class="o">}</code>&#13;
    <code class="o">}</code>&#13;
&#13;
    <code class="nd">@GET</code>&#13;
    <code class="nd">@Path</code><code class="o">(</code><code class="s">"/aggregate"</code><code class="o">)</code>&#13;
    <code class="nd">@Produces</code><code class="o">(</code><code class="n">MediaType</code><code class="o">.</code><code class="na">APPLICATION_JSON</code><code class="o">)</code>&#13;
    <code class="kd">public</code> <code class="kt">void</code> <code class="nf">aggregate</code><code class="o">(</code><code class="nd">@Suspended</code> <code class="kd">final</code> <code class="n">AsyncResponse</code> <code class="n">ar</code><code class="o">)</code>&#13;
                    <code class="kd">throws</code> <code class="n">ParseException</code> <code class="o">{</code>&#13;
        <code class="n">MultiCallback</code> <code class="n">callback</code> <code class="o">=</code> <code class="k">new</code> <code class="n">MultiCallback</code><code class="o">(</code><code class="n">ar</code><code class="o">,</code> <code class="mi">3</code><code class="o">);</code>&#13;
        <code class="n">target1</code><code class="o">.</code><code class="na">request</code><code class="o">().</code><code class="na">async</code><code class="o">().</code><code class="na">get</code><code class="o">(</code><code class="n">callback</code><code class="o">);</code>&#13;
        <code class="n">target2</code><code class="o">.</code><code class="na">request</code><code class="o">().</code><code class="na">async</code><code class="o">().</code><code class="na">get</code><code class="o">(</code><code class="n">callback</code><code class="o">);</code>&#13;
        <code class="n">target3</code><code class="o">.</code><code class="na">request</code><code class="o">().</code><code class="na">async</code><code class="o">().</code><code class="na">get</code><code class="o">(</code><code class="n">callback</code><code class="o">);</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>Note that we also use an async response in this example, but we don’t need&#13;
a separate pool as before: the request will be resumed in one of the threads&#13;
that handles the response.</p>&#13;
&#13;
<p>This introduces the desired concurrency into this operation, but let’s take&#13;
a little closer look into the thread usage. <a data-type="xref" href="#FigureAsyncClient">Figure 10-3</a> shows the&#13;
significant thread usage by the Helidon server when executing this example.</p>&#13;
&#13;
<figure><div class="figure" id="FigureAsyncClient">&#13;
<img alt="Async Client Thread Usage" src="assets/jp2e_1003.png"/>&#13;
<h6><span class="label">Figure 10-3. </span>Simple thread usage of async HTTP clients</h6>&#13;
</div></figure>&#13;
&#13;
<p>At time T1, the request comes in and starts executing on a Helidon request&#13;
thread. The thread sets up the three remote calls; each call is actually&#13;
sent by a thread in the async client pool. (In the diagram, the three are&#13;
sent by the same thread, but that is timing dependent: they may execute on&#13;
three separate threads depending on how quickly the requests are made and&#13;
how long it takes them to send their data.) The three sockets associated&#13;
with those calls are also registered on the event queue being processed by&#13;
the NIO polling thread. The request thread finishes processing at time T2.</p>&#13;
&#13;
<p>At time T3, the NIO polling thread gets an event that one of the sockets has&#13;
data, so it sets up HTTP client thread #1 to read and process that data.&#13;
That processing continues until time T5. Meanwhile at time T4, the NIO&#13;
polling thread gets an event that another socket has data to read, which is&#13;
then read and processed by HTTP client thread #2 (which takes until time T7).&#13;
Then at time T5, the third socket is ready to be processed. Because HTTP&#13;
client thread #1 is idle, it can read and process that request, which finishes&#13;
at time T8 (and at that point, the <code>resume()</code> method is called on the&#13;
response object, and the response is delivered to the client).</p>&#13;
&#13;
<p>The duration of the processing in the client threads is the key here. If&#13;
the processing is very fast and the responses staggered well enough,&#13;
a single thread can handle all the responses. If the processing takes a long&#13;
time or the responses are bunched, we’ll need one thread per request. In the&#13;
example, we’re in a middle ground: we used fewer threads than a&#13;
one-thread-per-request model, but more than one thread. This is a key difference&#13;
between a REST server and something like an nginx server of static content:&#13;
ultimately, even in a completely asynchronous implementation, the CPU needs&#13;
of the business logic are going to require a fair number of threads in&#13;
order to get good concurrency.</p>&#13;
&#13;
<p>This example assumes that the HTTP client is utilizing NIO. If the client&#13;
uses traditional NIO, the figure would be slightly different. When&#13;
the first async client thread call is made, that call will last all the way&#13;
until time T7. The second call on the async client will need a new thread;&#13;
that request will last until time T8. And the third async client thread&#13;
will run until time T5 (the clients would not be expected to complete in the&#13;
same order as they were started).&#13;
<a data-type="xref" href="#FigureAsyncClientTraditional">Figure 10-4</a> shows the difference.</p>&#13;
&#13;
<p>In either case, the results are the same for the end user: the three requests&#13;
are handled in parallel, with the expected gain in performance. But the&#13;
thread usage (and hence overall system efficiency) will be quite different.<a data-startref="ix_ch10-asciidoc19" data-type="indexterm" id="idm45775546540408"/></p>&#13;
&#13;
<figure><div class="figure" id="FigureAsyncClientTraditional">&#13;
<img alt="Blocking Client Thread Usage" src="assets/jp2e_1004.png"/>&#13;
<h6><span class="label">Figure 10-4. </span>Simple thread usage of blocking HTTP clients</h6>&#13;
</div></figure>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Async HTTP clients and thread usage" data-type="sect3"><div class="sect3" id="idm45775546826680">&#13;
<h3>Async HTTP clients and thread usage</h3>&#13;
&#13;
<p><a data-primary="asynchronous HTTP" data-secondary="clients and thread usage" data-type="indexterm" id="idm45775546536904"/>These background thread pool(s) will act as throttles, of course, and they&#13;
must as usual be tuned so that they are large enough to handle the concurrency&#13;
that your application needs, but not too large so as to overwhelm requests to the&#13;
backend resource. Often the default settings are sufficient, but if you need&#13;
to look further into the different connectors with the reference implementation&#13;
of JAX-RS and their background&#13;
pool, here is some additional information on each of them.</p>&#13;
<dl>&#13;
<dt>Default connector</dt>&#13;
<dd>&#13;
<p>The default connector uses blocking I/O. A single async client thread&#13;
pool in Jersey (the reference JAX-RS implementation) will handle all&#13;
requests; the threads in this pool are named beginning with <code>jersey-client-async-executor</code>. That pool will need one thread&#13;
per simultaneous request, as <a data-type="xref" href="#FigureAsyncClientTraditional">Figure 10-4</a> showed.&#13;
By default, that pool size is unbounded; you can set a bound when the client&#13;
is configured by setting this property:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting">    <code class="n">ClientConfig</code> <code class="n">cc</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ClientConfig</code><code class="o">();</code>&#13;
    <code class="n">cc</code><code class="o">.</code><code class="na">property</code><code class="o">(</code><code class="n">ClientProperties</code><code class="o">.</code><code class="na">ASYNC_THREADPOOL_SIZE</code><code class="o">,</code> <code class="mi">128</code><code class="o">);</code>&#13;
    <code class="n">client</code> <code class="o">=</code> <code class="n">ClientBuilder</code><code class="o">.</code><code class="na">newClient</code><code class="o">(</code><code class="n">cc</code><code class="o">);</code></pre>&#13;
</dd>&#13;
<dt>Apache connector</dt>&#13;
<dd>&#13;
<p>Although the Apache libraries have a true asynchronous client (one&#13;
that uses NIO for reading the response rather than requiring a dedicated&#13;
thread), the Apache&#13;
connector in Jersey uses the traditional blocking I/O Apache client. With&#13;
respect to thread pools, it behaves and is configured&#13;
just like the default <span class="keep-together">connector.</span></p>&#13;
</dd>&#13;
<dt>Grizzly connector</dt>&#13;
<dd>&#13;
<p>The HTTP client used by the Grizzly connector is asynchronous, following&#13;
the  model in <a data-type="xref" href="#FigureAsyncClient">Figure 10-3</a>. Multiple pools are involved: a pool (<code>grizzly-ahc-kernel</code>) that writes the requests, a&#13;
pool (<code>nioEventLoopGroup</code>) that waits for NIO events, and a pool&#13;
(<code>pool-N</code>) that reads and processes the responses. That latter pool is the&#13;
important one to configure for throughput/throttling reasons, and its size is unbounded; it can be throttled by using the&#13;
<code>ASYNC_THREADPOOL_SIZE</code> property.</p>&#13;
</dd>&#13;
<dt>Jetty connector</dt>&#13;
<dd>&#13;
<p>Jetty uses an asynchronous client. Requests are sent and read from the same&#13;
thread pool (and event polling also happens in that pool). In Jersey, that&#13;
pool is also configured using the <code>ASYNC_THREADPOOL_SIZE</code> property, though&#13;
a server using Jetty has two backend thread pools: the standard pool of&#13;
<code>jersey-client-async-executor</code> threads (which handles miscellaneous&#13;
bookkeeping), and the pool of threads handling the Jetty clients (those&#13;
threads are named beginning with <code>HttpClient</code>). If that property is not&#13;
set, the size of the <code>HttpClient</code> pool will be 200.</p>&#13;
</dd>&#13;
</dl>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>Always make sure that the connection pool for HTTP clients is set correctly.</p>&#13;
</li>&#13;
<li>&#13;
<p>Asynchronous HTTP clients can improve performance by distributing work among multiple threads, increasing <span class="keep-together">concurrency.</span></p>&#13;
</li>&#13;
<li>&#13;
<p>Async HTTP clients built using NIO will require fewer threads than those built using traditional I/O, but a REST server still requires a fairly large number of threads to process asynchronous requests.</p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Asynchronous database calls" data-type="sect3"><div class="sect3" id="idm45775546496520">&#13;
<h3>Asynchronous database calls</h3>&#13;
&#13;
<p><a data-primary="asynchronous database calls" data-type="indexterm" id="idm45775546495112"/><a data-primary="databases" data-secondary="asynchronous calls" data-type="indexterm" id="idm45775546494392"/><a data-primary="JDBC (Java Database Connectivity)" data-secondary="asynchronous database calls" data-type="indexterm" id="idm45775546493448"/>If the outbound call in question is a call to a relational database, making&#13;
it truly asynchronous is hard. The standard JDBC API does not lend itself&#13;
to using nonblocking I/O, so a general solution will require a new API&#13;
or new technologies. Various proposals around such an&#13;
API have been made and rejected, and&#13;
current hopes are that a new lightweight task model known as <em>fibers</em> will&#13;
make it possible for existing synchronous APIs to scale well without the&#13;
need for asynchronous programming. Fibers are part of the OpenJDK&#13;
<a href="https://oreil.ly/npuXr">Project Loom</a>, but no target release&#13;
date has been set (as of this writing).</p>&#13;
&#13;
<p>Proposals (and implementations) of asynchronous JDBC wrappers&#13;
often defer the JDBC work to a separate thread pool. This is similar to the default Jersey <span class="keep-together">asynchronous</span> HTTP client from the preceding section:&#13;
from a programmatic viewpoint, the API looks asynchronous. But in implementation, background threads are blocked on the I/O channels, so we don’t gain any scalability by going in that direction.</p>&#13;
&#13;
<p>Various projects outside the JDK can fill the gap.&#13;
The most widely used is Spring Data <a href="https://oreil.ly/tN6oR">R2DBC</a>&#13;
from the Spring project. This requires using a different API, and drivers&#13;
are available only for certain databases. Still, for nonblocking access to&#13;
a relational database, this is the best game in town.</p>&#13;
&#13;
<p>For NoSQL databases, the story is somewhat similar. On the other hand, no Java standard exists for accessing a NoSQL database in the first place, so&#13;
your programming depends on a database-proprietary API anyway. So the Spring&#13;
projects for reactive NoSQL databases can be used for true asynchronous&#13;
access<a data-startref="ix_ch10-asciidoc18" data-type="indexterm" id="idm45775546487128"/><a data-startref="ix_ch10-asciidoc17" data-type="indexterm" id="idm45775546486424"/>.<a data-startref="ix_ch10-asciidoc16" data-type="indexterm" id="idm45775546485624"/><a data-startref="ix_ch10-asciidoc15" data-type="indexterm" id="idm45775546484920"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="JSON Processing" data-type="sect1"><div class="sect1" id="idm45775546484120">&#13;
<h1>JSON Processing</h1>&#13;
&#13;
<p><a data-primary="Java servers" data-secondary="JSON processing" data-type="indexterm" id="ix_ch10-asciidoc20"/><a data-primary="JSON processing" data-type="indexterm" id="ix_ch10-asciidoc21"/>Now that we’ve looked at the mechanics of how data is sent in Java servers,&#13;
let’s delve into the data itself. In this section, we’ll look primarily&#13;
at JSON processing. Older Java programs often use XML (and the processing&#13;
trade-offs among JSON and XML are pretty much identical); there are also&#13;
newer formats like Apache Avro and Google’s protocol buffers.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775546450568">&#13;
<h5>JSON Sample Data</h5>&#13;
<p>The examples in this section are based on the JSON returned from using the&#13;
eBay REST service that returns 100 items for sale that match a particular&#13;
keyword. A portion of that data looks like this:</p>&#13;
<pre data-type="programlisting">&#13;
{"findItemsByKeywordsResponse": [&#13;
    {"ack":["Success"],&#13;
     "version":["1.13.0"],&#13;
     "timestamp":["2019-08-26T19:48:28.830Z"],&#13;
     "searchResult":[&#13;
        {"@count":"100",&#13;
         "item":[&#13;
             {"itemId":["153452538114"],&#13;
              "title":["Apple iPhone 6"],&#13;
              ... more fields ...&#13;
             }&#13;
         ],&#13;
         ... more items ...&#13;
        }&#13;
     ]&#13;
    }&#13;
]}&#13;
</pre>&#13;
</div></aside>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="An Overview of Parsing and Marshaling" data-type="sect2"><div class="sect2" id="idm45775546447704">&#13;
<h2>An Overview of Parsing and Marshaling</h2>&#13;
&#13;
<p><a data-primary="JSON processing" data-secondary="parsing/marshaling" data-type="indexterm" id="idm45775546446296"/><a data-primary="marshalling" data-type="indexterm" id="idm45775546445160"/><a data-primary="parsing" data-secondary="JSON data" data-type="indexterm" id="idm45775546444488"/>Given a series of JSON strings, a program must convert&#13;
those strings into data suitable for processing by Java. This is called&#13;
either <em>marshaling</em>&#13;
or <em>parsing</em>, depending on the context and the resulting output. If the output&#13;
is a Java object, the process is called <em>marshaling</em>; if the data is processed&#13;
as it is read, the process is called <em>parsing</em>. The reverse—producing JSON strings from other data—is called <em>unmarshaling</em>.</p>&#13;
&#13;
<p>We can use three general techniques to handle JSON data:</p>&#13;
<dl>&#13;
<dt>Pull parsers</dt>&#13;
<dd>&#13;
<p>The input data is associated with a parser, and the program asks for (or pulls) a series of tokens from the parser.</p>&#13;
</dd>&#13;
<dt>Document models</dt>&#13;
<dd>&#13;
<p>The input data is converted to a document-style object that the application can then walk through as it looks for pieces of data. The interface here is in terms of generic document-oriented objects.</p>&#13;
</dd>&#13;
<dt>Object representations</dt>&#13;
<dd>&#13;
<p>The input data is converted to one or more Java objects by using a set of predefined classes that reflect the structure of the data (e.g., a predefined&#13;
<code class="keep-together">Person</code>&#13;
class is for data that represents an individual). These are typically known as plain old Java objects (POJOs).</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>These techniques are listed in rough order of fastest to slowest, but again&#13;
the functional differences between them are more important than their&#13;
performance differences.&#13;
Simple scanning is all a parser can do, so they&#13;
are not ideally suited for data that must be accessed&#13;
in random order or examined more than once. To handle those situations, a&#13;
program using only a simple parser would need to build an internal data&#13;
structure, which is a simple matter of programming.&#13;
But the document and Java object models already provide structured data,&#13;
which will usually be easier than defining new structures on your own.</p>&#13;
&#13;
<p>This, in fact, is the real difference between using a parser and using&#13;
a data marshaler. The first item in the list is a parser, and&#13;
it is up to the application logic to handle the data as the parser provides&#13;
it. The next two are data marshalers: they must use a parser to process&#13;
the data, but they provide a data representation that more-complex programs&#13;
can use in their logic.</p>&#13;
&#13;
<p>So the primary choice regarding which technique to use is determined by how&#13;
the application needs to be written. If a program needs to make one simple&#13;
pass through the data, simply using the fastest parser will suffice.&#13;
Directly using a parser is also appropriate if the data is to be saved in&#13;
a simple, application-defined structure; for example, the prices for the items&#13;
in the sample data could be saved to an&#13;
<code class="keep-together">ArrayList</code>, which would be&#13;
easy for other application logic to process.</p>&#13;
&#13;
<p>Using a document model is more appropriate when the format of the data is&#13;
important. If the format of the data must be preserved, a document&#13;
format is easy: the data can be read into the document format,&#13;
altered in some way, and then the document format can simply be written to&#13;
a new data stream.</p>&#13;
&#13;
<p>For ultimate flexibility, an object model provides Java-language-level&#13;
representation of the data. The data can be manipulated in the familiar terms&#13;
of objects and their attributes. The added complexity in the marshaling&#13;
is (mostly) transparent to the developer and may make that part of the&#13;
application a little slower, but the productivity improvement in working&#13;
with the code can offset that issue.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="JSON Objects" data-type="sect2"><div class="sect2" id="idm45775546429848">&#13;
<h2>JSON Objects</h2>&#13;
&#13;
<p><a data-primary="JSON processing" data-secondary="JSON objects" data-type="indexterm" id="idm45775546428376"/><a data-primary="objects" data-secondary="JSON" data-type="indexterm" id="idm45775546427400"/>JSON data has two object representations. The first is&#13;
generic: simple JSON objects. These objects are manipulated via&#13;
generic interfaces: <code>JsonObject</code>, <code>JsonArray</code>, and so on. They provide&#13;
a way to build up or inspect JSON documents without making specific class&#13;
representations of the data.</p>&#13;
&#13;
<p>The second JSON object representation binds the JSON data to a&#13;
full-fledged Java class, using JSON bindings (JSON-B) that result in POJO. For example,&#13;
the item data in our sample JSON data would be represented by an <code>Item</code>&#13;
class that has attributes for its fields.</p>&#13;
&#13;
<p>The difference between the two object representations is that the first is&#13;
generic and requires no classes. Given a <code>JsonObject</code> that represents an&#13;
item in our sample data, the title of the item would be found like this:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="n">JsonObject</code> <code class="n">jo</code><code class="o">;</code>&#13;
<code class="n">String</code> <code class="n">title</code> <code class="o">=</code> <code class="n">jo</code><code class="o">.</code><code class="na">getString</code><code class="o">(</code><code class="s">"title"</code><code class="o">);</code></pre>&#13;
&#13;
<p>In JSON-B, the title of an item would be available via more intuitive&#13;
getters and <span class="keep-together">setters:</span></p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="n">Item</code> <code class="n">i</code><code class="o">;</code>&#13;
<code class="n">String</code> <code class="n">title</code> <code class="o">=</code> <code class="n">i</code><code class="o">.</code><code class="na">getTitle</code><code class="o">();</code></pre>&#13;
&#13;
<p>In either case, the&#13;
object itself is created with an underlying parser, so it is important to&#13;
<span class="keep-together">configure</span> the parser for optimal performance. But in addition to parsing the data, the object implementations allow us to produce a JSON string from the object (i.e., to unmarshal the&#13;
object). <a data-type="xref" href="#TableJsonB">Table 10-2</a> shows the performance of those operations.</p>&#13;
<table id="TableJsonB">&#13;
<caption><span class="label">Table 10-2. </span>Performance of JSON object models</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Object model</th>&#13;
<th>Marshal performance</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>JSON object</p></td>&#13;
<td><p>2318 ± 51 μs</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>JSON-B classes</p></td>&#13;
<td><p>7071 ± 71 μs</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>Jackson mapper</p></td>&#13;
<td><p>1549 ± 40 μs</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>Producing a simple JSON object is substantially faster than producing custom&#13;
Java classes, though those Java classes will be easier to work with from a&#13;
programming perspective.</p>&#13;
&#13;
<p>The Jackson mapper in this table is an alternate approach, which at this&#13;
point has pretty much eclipsed other uses. Although Jackson provides an&#13;
implementation of the standard JSON parsing (JSON-P) API, they have an alternate&#13;
implementation that marshals and unmarshals JSON data into Java objects, but&#13;
that doesn’t follow JSON-B. That implementation is built on the&#13;
<code>ObjectMapper</code> class that Jackson provides. The JSON-B code to&#13;
marshal data into an object looks like this:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="n">Jsonb</code> <code class="n">jsonb</code> <code class="o">=</code> <code class="n">JsonbBuilder</code><code class="o">.</code><code class="na">create</code><code class="o">();</code>&#13;
<code class="n">FindItemsByKeywordsResponse</code> <code class="n">f</code> <code class="o">=</code>&#13;
    <code class="n">jsonb</code><code class="o">.</code><code class="na">fromJson</code><code class="o">(</code><code class="n">inputStream</code><code class="o">,</code> <code class="n">FindItemsByKeywordsResponse</code><code class="o">.</code><code class="na">class</code><code class="o">);</code></pre>&#13;
&#13;
<p>The <code>ObjectMapper</code> code is slightly different:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="n">ObjectMapper</code> <code class="n">mapper</code> <code class="o">=</code> <code class="k">new</code> <code class="n">ObjectMapper</code><code class="o">();</code>&#13;
<code class="n">FindItemsByKeywordsResponse</code> <code class="n">f</code> <code class="o">=</code>&#13;
    <code class="n">mapper</code><code class="o">.</code><code class="na">readValue</code><code class="o">(</code><code class="n">inputStream</code><code class="o">,</code> <code class="n">FindItemsByKeywordsResponse</code><code class="o">.</code><code class="na">class</code><code class="o">);</code></pre>&#13;
&#13;
<p>From a performance perspective, <code>ObjectMapper</code> use has some pitfalls.&#13;
As the JSON data is marshaled, the <code>mapper</code> will create a lot of proxy&#13;
classes that are used to create the resulting POJO. That in&#13;
itself is somewhat time-consuming the first time a class is used.&#13;
To overcome this, a common mistake—and&#13;
the second performance issue—is to create lots of mapper objects (e.g.,&#13;
a static one per class that performs the marshaling). This often leads&#13;
to memory pressure, excessive GC cycles, and even <code>OutOfMemory</code> errors.&#13;
There need be only a single <code>ObjectMapper</code>&#13;
object in an application, which helps both CPU and memory usage. Even&#13;
so, an object model representation of data will consume memory for those&#13;
objects.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="JSON Parsing" data-type="sect2"><div class="sect2" id="idm45775546429224">&#13;
<h2>JSON Parsing</h2>&#13;
&#13;
<p><a data-primary="JSON processing" data-secondary="direct parsing" data-type="indexterm" id="ix_ch10-asciidoc22"/><a data-primary="parsing" data-secondary="JSON data" data-type="indexterm" id="ix_ch10-asciidoc23"/>Direct parsing of JSON data has two advantages. First,&#13;
if the JSON object model is too memory-intensive for your application,&#13;
directly parsing the JSON and processing it will save that memory.&#13;
Second, if the JSON you are dealing with contains a lot of data (or&#13;
data that you want in some way to filter), parsing it directly will be&#13;
more <span class="keep-together">efficient.</span></p>&#13;
&#13;
<p>All JSON parsers are pull parsers, which&#13;
operate by retrieving data from the stream on demand.&#13;
The basic pull parser for the tests in this section has this loop as its main&#13;
logic:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="n">parser</code> <code class="o">=</code> <code class="n">factory</code><code class="o">.</code><code class="na">createParser</code><code class="o">(</code><code class="n">inputStream</code><code class="o">);</code>&#13;
<code class="kt">int</code> <code class="n">idCount</code> <code class="o">=</code> <code class="mi">0</code><code class="o">;</code>&#13;
<code class="k">while</code> <code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">hasNext</code><code class="o">())</code> <code class="o">{</code>&#13;
    <code class="n">Event</code> <code class="n">event</code> <code class="o">=</code> <code class="n">parser</code><code class="o">.</code><code class="na">next</code><code class="o">();</code>&#13;
    <code class="k">switch</code> <code class="o">(</code><code class="n">event</code><code class="o">)</code> <code class="o">{</code>&#13;
        <code class="k">case</code> <code class="nl">KEY_NAME:</code>&#13;
            <code class="n">String</code> <code class="n">s</code> <code class="o">=</code> <code class="n">parser</code><code class="o">.</code><code class="na">getString</code><code class="o">();</code>&#13;
            <code class="k">if</code> <code class="o">(</code><code class="n">ID</code><code class="o">.</code><code class="na">equals</code><code class="o">(</code><code class="n">s</code><code class="o">))</code> <code class="o">{</code>&#13;
                <code class="n">isID</code> <code class="o">=</code> <code class="kc">true</code><code class="o">;</code>&#13;
            <code class="o">}</code>&#13;
            <code class="k">break</code><code class="o">;</code>&#13;
        <code class="k">case</code> <code class="nl">VALUE_STRING:</code>&#13;
            <code class="k">if</code> <code class="o">(</code><code class="n">isID</code><code class="o">)</code> <code class="o">{</code>&#13;
                <code class="k">if</code> <code class="o">(</code><code class="n">addId</code><code class="o">(</code><code class="n">parser</code><code class="o">.</code><code class="na">getString</code><code class="o">()))</code> <code class="o">{</code>&#13;
                    <code class="n">idCount</code><code class="o">++;</code>&#13;
                    <code class="k">return</code><code class="o">;</code>&#13;
                <code class="o">}</code>&#13;
                <code class="n">isID</code> <code class="o">=</code> <code class="kc">false</code><code class="o">;</code>&#13;
            <code class="o">}</code>&#13;
            <code class="k">continue</code><code class="o">;</code>&#13;
        <code class="k">default</code><code class="o">:</code>&#13;
            <code class="k">continue</code><code class="o">;</code>&#13;
    <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>This code pulls tokens from the parser. In the code, most tokens are&#13;
just discarded. When a start token is found, the code checks to see if the&#13;
token is an item ID.&#13;
If it is, the next character token will be the ID the application&#13;
wants to save.</p>&#13;
&#13;
<p>This test also allows us to filter the data; in this case, we are filtering&#13;
to read only the first 10 items in the JSON data. That’s done when we&#13;
process the ID:&#13;
that ID is saved via the&#13;
<code class="keep-together">addItemId()</code> method, which&#13;
returns <code>true</code>&#13;
if the desired number&#13;
of IDs have been stored. When that happens, the loop can just return and not&#13;
process the remaining data in the input stream.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm45775546105368">&#13;
<h5>Reusing Factories and Parsers</h5>&#13;
<p><a data-primary="factories, reusing" data-type="indexterm" id="idm45775546104200"/><a data-primary="parsing" data-secondary="reusing" data-type="indexterm" id="idm45775546103496"/><a data-primary="reusing parsers" data-type="indexterm" id="idm45775546102552"/>JSON parser factories are expensive to create. Fortunately, the&#13;
factories are thread-safe, so it is easy to store the factory in a global&#13;
static variable and reuse the factory as needed.</p>&#13;
&#13;
<p>In general, the actual parsers cannot be reused, nor are they thread-safe.&#13;
Parsers, therefore, are usually created on demand.</p>&#13;
</div></aside>&#13;
&#13;
<p>How do these parsers actually perform?&#13;
<a data-type="xref" href="#TablePullParserJson">Table 10-3</a> shows the average time in microseconds&#13;
to parse&#13;
the sample document, assuming parsing stops after 10 items, and to process&#13;
the entire document. Predictably, parsing 90% fewer items leads to a 90%&#13;
improvement in performance.</p>&#13;
<table id="TablePullParserJson">&#13;
<caption><span class="label">Table 10-3. </span>Performance of pull parsers</caption>&#13;
<thead>&#13;
<tr>&#13;
<th>Items processed</th>&#13;
<th>Default parser</th>&#13;
<th>Jackson parser</th>&#13;
</tr>&#13;
</thead>&#13;
<tbody>&#13;
<tr>&#13;
<td><p>10</p></td>&#13;
<td><p>159 ± 2 us</p></td>&#13;
<td><p>86 ± 5 μs</p></td>&#13;
</tr>&#13;
<tr>&#13;
<td><p>100</p></td>&#13;
<td><p>1662 ± 46 us</p></td>&#13;
<td><p>770 ± 4 μs</p></td>&#13;
</tr>&#13;
</tbody>&#13;
</table>&#13;
&#13;
<p>As has been the case for a while,&#13;
the Jackson parser delivers superior performance here, but both are quite&#13;
faster than reading actual objects.<a data-startref="ix_ch10-asciidoc23" data-type="indexterm" id="idm45775546090632"/><a data-startref="ix_ch10-asciidoc22" data-type="indexterm" id="idm45775546089928"/></p>&#13;
<div data-type="tip"><h1>Quick Summary</h1>&#13;
<ul>&#13;
<li>&#13;
<p>There are two options for processing JSON: creating POJOs objects, and using direct parsing.</p>&#13;
</li>&#13;
<li>&#13;
<p>The choice is dependent on the application needs, but direct parsing offers filtering and general performance opportunities. Creating JSON objects can often lead to GC issues when the objects are large.</p>&#13;
</li>&#13;
<li>&#13;
<p>The Jackson parser is generally the fastest parser; it should be preferred over default implementations.<a data-startref="ix_ch10-asciidoc21" data-type="indexterm" id="idm45775546085160"/><a data-startref="ix_ch10-asciidoc20" data-type="indexterm" id="idm45775546084456"/></p>&#13;
</li>&#13;
</ul>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm45775546246008">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>Nonblocking I/O forms the basics of effective server scaling because it allows&#13;
servers to handle a relatively large number of connections with a relatively&#13;
small number of threads. Traditional servers utilize this for basic&#13;
client connection handling, and newer server frameworks can extend the&#13;
nonblocking nature up the stack to other applications.<a data-startref="ix_ch10-asciidoc0" data-type="indexterm" id="idm45775546081736"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<div data-type="footnotes"><p data-type="footnote" id="idm45775547323624"><sup><a href="ch10.html#idm45775547323624-marker">1</a></sup> This scheme has many slight variations; you’ll see some of those in the next section.</p></div></div></section></body></html>