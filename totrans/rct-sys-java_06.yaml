- en: Chapter 4\. Design Principles of Reactive Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 3](ch03.html#distributed-system), we looked at the challenges behind
    distributed systems. It’s now time to see what Reactive has to offer. Reactive
    can be seen as a set of principles for building distributed systems, a kind of
    checklist to verify that no major known concern was overlooked while architecting
    and building a system. These principles focus on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Responsiveness
  prefs: []
  type: TYPE_NORMAL
- en: The ability to handle requests when facing failures or peaks of load
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency
  prefs: []
  type: TYPE_NORMAL
- en: The ability to do more with fewer resources
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we cover the principles promoted by reactive systems.
  prefs: []
  type: TYPE_NORMAL
- en: Reactive Systems 101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2013, a group of distributed systems experts gathered and wrote the first
    version of “The Reactive Manifesto.” They assembled in this whitepaper their experience
    building distributed systems and cloud applications. While in 2013 the cloud was
    not precisely what it is today, the dynamic creation of ephemeral resources was
    already a well-known mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: '“The Reactive Manifesto” defines *reactive systems* as distributed systems
    having four characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Responsive
  prefs: []
  type: TYPE_NORMAL
- en: Able to handle requests in a timely fashion
  prefs: []
  type: TYPE_NORMAL
- en: Resilient
  prefs: []
  type: TYPE_NORMAL
- en: Able to manage failures gracefully
  prefs: []
  type: TYPE_NORMAL
- en: Elastic
  prefs: []
  type: TYPE_NORMAL
- en: Able to scale up and down according to the load and resources
  prefs: []
  type: TYPE_NORMAL
- en: Message driven
  prefs: []
  type: TYPE_NORMAL
- en: Using asynchronous message-based communication among the components forming
    the system
  prefs: []
  type: TYPE_NORMAL
- en: These four characteristics are represented in [Figure 4-1](#image:reactive-systems).
  prefs: []
  type: TYPE_NORMAL
- en: '![Reactive systems characteristics](assets/rsij_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. Reactive systems characteristics
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re seeing this picture for the first time, you may be confused by all
    the arrows. It can look like a well-tailored marketing campaign. It’s not, and
    let’s explain why these pillars make a lot of sense when building cloud native
    and Kubernetes-native applications. Let’s start with the bottom of the figure.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of trying to make distributed systems simpler than they are, reactive
    systems embrace their asynchronous nature. They use *asynchronous message passing*
    to establish the connective tissue among the components. Asynchronous message
    passing ensures loose coupling, isolation, and location transparency. In a reactive
    system, interactions rely on messages sent to abstract destinations. These messages
    carry everything—data as well as failures. Asynchronous message passing also improves
    resource utilization. Employing nonblocking communication (we cover that part
    later in this chapter) allows idle components to consume almost no CPU and memory.
    Asynchronous message passing enables elasticity and resilience, as depicted by
    the two bottom arrows in [Figure 4-1](#image:reactive-systems).
  prefs: []
  type: TYPE_NORMAL
- en: '*Elasticity* means that the system can adapt itself, or parts of itself, to
    handle the fluctuating load. By looking at the messages flowing among the components,
    a system can determine which parts reach their limits and create more instances
    or route the messages elsewhere. Cloud infrastructure enables creating these instances
    quickly at runtime. But elasticity is not only about scaling up; it’s also about
    scaling down. The system can decide to scale down underused parts to save resources.
    At runtime, the system adjusts itself, always meeting the current demand, avoiding
    bottlenecks, overflows, and overcommitted resources. As you can imagine, elasticity
    requires observability, replication, and routing features. Observability is covered
    in [Chapter 13](ch13.html#observability). In general, the last two are provided
    by the infrastructure such as Kubernetes or cloud providers.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Resilience* means handling failure gracefully. As explained in [Chapter 3](ch03.html#distributed-system),
    failures are inevitable in distributed systems. Instead of hiding them, reactive
    systems consider failures first-class citizens. The system should be able to handle
    them and react to them. Failures are contained within each component, isolating
    components from one another. This isolation ensures that parts of the system can
    fail and recover without jeopardizing the whole system. For instance, by replicating
    components (elasticity), the system can continue to handle incoming messages even
    if some elements are failing. The implementation of resilience is shared between
    the application (which needs to be aware of failures, contain them, and, if possible,
    handle them gracefully) and the infrastructure (which monitors the systems and
    restarts fallen components).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last characteristic is the whole purpose of reactive systems: being *responsive*.
    Your system needs to stay responsive—to respond in a timely fashion—even under
    fluctuating load (elasticity) and when facing failure (resilience). Relying on
    message passing enables these characteristics and much more, such as flow control
    by monitoring the messages in the system and applying backpressure when necessary.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, reactive systems are exactly what we want to build: distributed
    systems able to handle the uncertainty, failures, and load efficiently. Their
    characteristics meet the requirement for cloud native and Kubernetes-native applications
    perfectly. But don’t be mistaken; building a reactive system is still making a
    distributed system. It’s challenging. However, by following these principles,
    the resulting system will be more responsive, more robust, and more efficient.
    The rest of this book details how we can easily implement such systems with Quarkus
    and messaging technologies.'
  prefs: []
  type: TYPE_NORMAL
- en: Commands and Events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we’ve covered many of the foundational principles, you might be confused.
    In [Chapter 1](ch01.html#introduction), we said that being reactive is related
    to being event driven, but in the previous section, we explicitly mentioned asynchronous
    message passing. Does that mean the same thing? Not completely.
  prefs: []
  type: TYPE_NORMAL
- en: But first, we need to discuss the differences between commands and events. As
    complicated as a distributed system design can be, the concepts of commands and
    events are fundamental. Nearly all interactions between individual components
    involve one or the other.
  prefs: []
  type: TYPE_NORMAL
- en: Commands
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Every system issues commands. *Commands* are actions that a user wishes to
    perform. Most HTTP-based APIs pass commands: the client asks for an action to
    happen. It’s important to understand that the action has not yet happened. It
    may happen in the future, or not; it may complete successfully or fail. In general,
    commands are sent to a specific recipient, and a result is sent back to the client.'
  prefs: []
  type: TYPE_NORMAL
- en: Take the simple HTTP application we used in [Chapter 3](ch03.html#distributed-system).
    You emitted a simple HTTP request. As we’ve said, that was a command. The application
    receives that command, handles it, and produces a result.
  prefs: []
  type: TYPE_NORMAL
- en: Events
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Events* are actions that have successfully completed. An event represents
    a *fact*, something that happened: a keystroke, a failure, an order, anything
    important to the organization or system at hand. An event can be the result of
    work done by a command.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go back to the preceding HTTP request example. Once the response has been
    written, it becomes an event. We have seen an HTTP request and its response. That
    event can be written in a log or broadcast to interested parties so they can be
    aware of what happened.
  prefs: []
  type: TYPE_NORMAL
- en: Events are immutable. You cannot delete an event. Admittedly, you can’t change
    the past. If you want to refute a previously sent fact, you need to fire another
    event invalidating the fact. The carried facts are made irrelevant only by another
    fact establishing the current knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Messages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: But, how to publish these events? There are many ways. These days, solutions
    like Apache Kafka or Apache ActiveMQ (we cover both in [Chapter 11](ch11.html#event-bus))
    are popular. They act as brokers between the producers and consumers. Essentially,
    our events are written into *topics* or *queues*. To write these events, the application
    sends a message to the broker, targeting a specific destination (the queue or
    the topic).
  prefs: []
  type: TYPE_NORMAL
- en: A *message* is a self-contained data structure describing the event and any
    relevant details about the event, such as who emitted it, at what time it was
    emitted, and potentially its unique ID. It’s generally better to keep the event
    itself business-centric and use additional metadata for the technical aspects.
  prefs: []
  type: TYPE_NORMAL
- en: On the other side, to consume events, you subscribe to the queue or topic containing
    the events you are interested in and receive the messages. You unwrap the event
    and can also get the associated metadata (for example, when the event happened,
    where it happened, and so forth). The processing of an event can lead to the publication
    of other events (again, packaged in messages and sent to a known destination)
    or to the execution of commands.
  prefs: []
  type: TYPE_NORMAL
- en: Brokers and messages can also convey commands. In this case, the message contains
    the description of the action to execute, and another message (potentially multiple
    messages) would carry the outcome if needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Commands Versus Events: An Example'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s take a look at an example to highlight the differences between commands
    and events. Imagine an ecommerce shop, like the one depicted in [Figure 4-2](#image:shop-order-services).
    The user picks a set of products and finalizes the order (process to payment,
    get the delivery date, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: '![Simplified architecture of an ecommerce shop](assets/rsij_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. Simplified architecture of an ecommerce shop
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The user sends a command (using an HTTP request, for example) to the shop service
    with the items the user wishes to receive. In a traditional application, once
    `ShopService` receives the command, it would call `OrderService` and invoke an
    `order` method with the username, the list of items (basket), and so on. Calling
    the `order` method is a command. That makes `ShopService` dependent on `OrderService`
    and reduces the component autonomy: `ShopService` cannot operate without `OrderService`.
    We are creating a distributed monolith, a distributed application that would collapse
    as soon as one of its parts fails.^([1](ch04.html#idm45358831669616))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see the difference if, instead of using a command between `ShopService`
    and `OrderService`, we publish an event. Once the user finalizes the order, the
    application still sends a command to `ShopService`. However, this time, `ShopService`
    *transforms* that command into an event: *a new order has been placed*. The event
    contains the user, the basket, and so on. The event is a fact written in a log,
    or wrapped into a message and sent to a broker.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other side, `OrderService` observes the *a new order has been placed*
    event, by reading where these events are stored. When `ShopService` emits the
    event, it receives it and can process it.
  prefs: []
  type: TYPE_NORMAL
- en: With this architecture, `ShopService` does not depend on `OrderService`. In
    addition, `OrderService` does not depend on `ShopService`, and it would process
    any observed event, regardless of the emitter. For example, a mobile application
    can emit the same event when the user validates an order from a mobile phone.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple components can consume events ([Figure 4-3](#image:shop-messages)).
    For example, in addition to `OrderService`, `StatisticsService` keeps track of
    the most ordered items. It consumes the same event, without having to modify `ShopService`
    to receive them.
  prefs: []
  type: TYPE_NORMAL
- en: A component observing events can derive new ones from them. For instance, `StatisticsService`
    could analyze the order and compute recommendations. These recommendations could
    be seen as another fact, and so communicate as an event. `ShopService` could observe
    these events and process them to influence item selection. However, `StatisticsService`
    and `ShopService` are independent of each other. The knowledge is cumulative and
    occurs by receiving new events and deriving, as done by `StatisticsService`, new
    facts from the received events.
  prefs: []
  type: TYPE_NORMAL
- en: As depicted in [Figure 4-3](#image:shop-messages), we can use *message queues*
    to transport our events. These events are wrapped into messages, sent to known
    destinations (`orders` and `recommendations`). `OrderService` and `StatisticsService`
    consume and process the messages independently.
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture of the ecommerce shop using events and message brokers](assets/rsij_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. Architecture of the ecommerce shop with events and message queues
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s important for these destinations to persist the events as an ordered sequence.
    By keeping that sequence, the system can go back in time and reprocess the events.
    Such a *replay* mechanism, popular in the Kafka world, has multiple benefits.
    You can restart with a clean state after a disaster by reprocessing all the stored
    events. Then, if we change the recommendation algorithm from the statistic services,
    for example, it would be able to re-accumulate all the knowledge and derive new
    recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: While the event emission sounds explicit in this example, that’s not always
    the case. For instance, events can be created from database writes.^([2](ch04.html#idm45358831644992))
  prefs: []
  type: TYPE_NORMAL
- en: Commands and events are the basis of most of the interactions. While we use
    mostly commands, events come with significant benefits. Events are facts. Events
    tell a story, the story of your system, a narrative that describes your system’s
    evolution. In reactive systems, events are wrapped into messages, and these messages
    are sent to destinations, transported by message brokers such as AMQP or Kafka
    ([Figure 4-4](#image:reactive-architecture)). Such an approach solves two important
    architectural issues arising from the distributed systems. First, it naturally
    handles real-world asynchronicity. Second, it binds together services without
    relying on strong coupling. At the edge of the system, this approach uses commands
    most of the time, often relying on HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: '![Overview of a reactive system](assets/rsij_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. Overview of a reactive system
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This asynchronous message-passing aspect of reactive systems forms the connective
    tissue. It not only grants the applications forming the system more autonomy and
    independence, but also enables resilience and elasticity. You may wonder how,
    and you will get the beginning of our response in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Destinations and Space Decoupling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The reactive applications, forming a reactive system, communicate using messages.
    They subscribe to destinations and receive the messages sent by other components
    to these destinations. These messages can carry commands or events, though as
    described in the previous section, events provide interesting benefits. These
    destinations are not bound to specific components or instances. They are virtual.
    Components must know only the name (generally business related, such as `orders`)
    of the destination, not who’s producing or consuming. It enables location transparency.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using Kubernetes, you may consider location transparency as already
    managed for you. Indeed, you can use Kubernetes *services* to implement location
    transparency. You have a single endpoint delegating to a group of selected *pods*.
    But this location transparency is somewhat limited and often tied to HTTP or request/reply
    protocols. Other environments can use service discovery infrastructure such as
    [HashiCorp Consul](https://consul.io) or [Netflix Eureka](https://oreil.ly/H9Ygn).
  prefs: []
  type: TYPE_NORMAL
- en: Using messages sent to a destination allows you, as the sender, to ignore who
    precisely is going to receive the message. You don’t know if someone is currently
    available or if multiple components or instances are waiting for your message.
    This number of consumers can evolve at runtime; more instances can be created,
    moved, or destroyed, and new components deployed. But for you, as a sender, you
    don’t need to know. You just use a specified destination. Let’s illustrate the
    advantages of this *addressability* by using the example from the previous section.
    `ShopService` emits `order placed` events carried inside messages sent to the
    `orders` destination ([Figure 4-3](#image:shop-messages)). It is likely possible
    that during a quiet period, only a single instance of `OrderService` runs. If
    there are not many orders, why bother having more? We could even imagine having
    no instance, and instantiating one when we receive an order. Serverless platforms
    are offering this *scale-from-zero* ability. However, over time, your shop gets
    more customers, and a single instance may not be enough. Thanks to location transparency,
    we can start other instances of `OrderService` to share the load ([Figure 4-5](#image:shop-elasticity)).
    `ShopService` is not modified and ignores this new topology.
  prefs: []
  type: TYPE_NORMAL
- en: '![Elasticity provided by the use of message passing](assets/rsij_0405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. Elasticity provided by the use of message passing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The way the load is shared among the consumers is also irrelevant for the sender.
    It can be a round-robin, a load-based selection, or something more clever. When
    the load returns to normal, the system can reduce the number of instances and
    save resources. Note that this kind of elasticity works perfectly for stateless
    services. For stateful services, it may be harder, as the instances may have to
    share the state. However, solutions exist (though not without caveats), like the
    [Kubernetes `StatefulSet`](https://oreil.ly/kVRID) or an [in-memory data grid](https://oreil.ly/wNUIQ),
    to coordinate state among instances of the same service. Message passing also
    enables replication. Following the same principle, we can shadow the active `OrderService`
    instance and take over if the primary instance fails ([Figure 4-6](#image:shop-fail-over)).
    This approach avoids service disruption. That kind of failover may also require
    state sharing.
  prefs: []
  type: TYPE_NORMAL
- en: '![Resilience provided by the use of message passing](assets/rsij_0406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-6\. Resilience provided by the use of message passing
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By using message passing, our system becomes not only asynchronous, but also
    elastic and resilient. When you architect your system, you list the destinations
    that implement the communication pattern you want. In general, you would use one
    destination per type of event, but that’s not necessarily the case. However, avoid
    at all costs having a destination per component instance. It introduces coupling
    between the sender and the receiver, discarding the benefits. It also reduces
    the extensibility. Finally, it’s important to keep the set of destinations stable.
    Changing a destination would break the components using it or would force you
    to handle redirections.
  prefs: []
  type: TYPE_NORMAL
- en: Time Decoupling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Location transparency is not the only benefit. Asynchronous message passing
    also enables time decoupling.
  prefs: []
  type: TYPE_NORMAL
- en: Modern message backbones, such as [AMQP 1.0](https://amqp.org), [Apache Kafka](https://kafka.apache.org/),
    and even Java Message Service (JMS), enable time decoupling. With these event
    brokers, events are not lost if there are no consumers. The events are stored
    and delivered later. Each broker has its own way. For instance, AMQP 1.0 uses
    persistent messages and durable subscribers to ensure message delivery. Kafka
    stores records in a durable, fault-tolerant, ordered log. The records can be retrieved
    so long as they remain stored within the topic.
  prefs: []
  type: TYPE_NORMAL
- en: If our `ShopService` emits the finalized orders as events, it does not need
    to know whether `OrderService` is available. It knows that it’s going to be processed
    eventually. If, for example, no instances of `OrderService` are available when
    `ShopService` emits the event, it’s not lost. When an instance gets ready, it
    receives the pending orders and processes them. The user is then notified asynchronously
    with an email.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, the message broker must be available and reachable. Most message
    brokers have replication abilities preventing unavailability issues and message
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is becoming common to store events in an event log. Such ordered and append-only
    structure represents the full history of your system. Every time the state changes,
    the system appends the new state to the log.
  prefs: []
  type: TYPE_NORMAL
- en: Time decoupling increases the independence of our components. Time decoupling,
    combined with other features enabled by asynchronous message passing, achieves
    a high level of independence among our components and keeps coupling to a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: The Role of Nonblocking Input/Output
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, you may wonder what the difference is between an application
    using Kafka or AMQP and a reactive system. Message passing is the essence of reactive
    systems, and most of them rely on some sort of message broker. Message passing
    enables resilience and elasticity, which lead to responsiveness. It promotes space
    and time decoupling, making our system much more robust.
  prefs: []
  type: TYPE_NORMAL
- en: But reactive systems are not only exchanging messages. Sending and receiving
    messages must be done efficiently. To achieve this, Reactive promotes the use
    of nonblocking I/Os.
  prefs: []
  type: TYPE_NORMAL
- en: Blocking Network I/O, Threads, and Concurrency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand the benefits of nonblocking I/O, we need to know how blocking
    I/Os work. Let’s use a client/server interaction to illustrate. When a client
    sends a request to a server, the server processes it and sends back a response.
    HTTP, for instance, follows this principle. For this to happen, both the client
    and the server need to establish a connection before the interaction starts. We
    will not go into the depths of the [seven-layer model](https://oreil.ly/kcTBH)
    and the protocol stack involved in this interaction; you can find plenty of articles
    online about that topic.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Examples from this section can be run directly from your IDE. Use *chapter-4/non-blocking-io/src/main/java/org/acme/client/EchoClient.java*
    to invoke the started server. Be sure to avoid running multiple servers concurrently
    as they all use the same port (9999).
  prefs: []
  type: TYPE_NORMAL
- en: To establish that connection between the client and the server, we use `sockets`,
    as shown in [Example 4-1](#reactive-system::blocking-server).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-1\. A single-threaded echo server using blocking I/O (*chapter-4/non-blocking-io/src/main/java/org/acme/blocking/BlockingEchoServer.java*)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The client and server have to bind themselves to a socket forming the connection.
    The server listens to its socket for the client to connect. Once established,
    the client and server can both write and read data from the socket bound to that
    connection.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, because it’s simpler, applications are developed using a synchronous
    development model. Such a development model executes instructions sequentially,
    one after the other. So when such applications interact across the network, they
    expect to continue using a synchronous development model even for I/O. This model
    uses synchronous communication and blocks the execution until the operation completes.
    In [Example 4-1](#reactive-system::blocking-server), we wait for a connection
    and handle it synchronously. We read and write using synchronous APIs. It’s simpler,
    but it leads to the use of blocking I/O.
  prefs: []
  type: TYPE_NORMAL
- en: With blocking I/O, when the client sends a request to the server, the socket
    processing that connection and the corresponding thread that reads from it is
    blocked until some read data appears. The bytes are accumulated in the network
    buffer until everything is read and ready for processing. Until the operation
    is complete, the server can do nothing more but wait.
  prefs: []
  type: TYPE_NORMAL
- en: The consequence of this model is that we cannot serve more than one connection
    within a single thread. When the server receives a connection, it uses that thread
    to read the request, process it, and write the response. That thread is blocked
    until the last byte of the response is written on the wire. A single client connection
    blocks the server! Not very efficient, right?
  prefs: []
  type: TYPE_NORMAL
- en: To execute concurrent requests with this approach, the only way is to have multiple
    threads. We need to allocate a new thread for each client connection. To handle
    more clients, you need to use more threads and process each request on a different
    *worker* thread; see [Example 4-2](#multi-threaded-server).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-2\. Principles behind multithreaded server using blocking I/O
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: To implement this principle, we need a thread pool (*worker pool*). When the
    client connects, we accept the connection and offload the processing to a separate
    thread. Thus, the server thread can still accept other connections, as shown in
    [Example 4-3](#reactive-system::blocking-server-workers).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-3\. A multithreaded echo server using blocking I/O (*chapter-4/non-blocking-io/src/main/java/org/acme/blocking/BlockingWithWorkerEchoServer.java*)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_design_principles_of_reactive_systems_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a worker thread pool to handle the request.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_design_principles_of_reactive_systems_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Offload the processing of the request to a thread from the thread pool. The
    rest of the code is unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s the model used, by default, in traditional Java frameworks such as Jakarta
    EE or Spring.^([3](ch04.html#idm45358831203712)) Even if these frameworks may
    use nonblocking I/O under the hood, they use *worker* threads to handle the requests.
    But this approach has many drawbacks, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Each thread requires a stack of memory allocated to it. With the increasing
    number of connections, spawning multiple threads and switching between them will
    consume not only memory but also CPU cycles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At any given point in time, multiple threads could be waiting for the client
    requests. That’s a massive waste of resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your concurrency (the number of requests you can handle at a given time—10 in
    the previous example) is limited by the number of threads you can create.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On public clouds, the blocking I/O approach inflates your monthly bill; on private
    clouds, it reduces the deployment density. Therefore, this approach is not ideal
    if you have to handle many connections or implement applications dealing with
    a lot of I/O. In the realm of distributed systems, that’s often the case. Luckily,
    there’s an alternative.
  prefs: []
  type: TYPE_NORMAL
- en: How Does Nonblocking I/O Work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The alternative is *nonblocking I/O*. The difference is evident from its name.
    Instead of waiting for the completion of the transmission, the caller is not blocked
    and can continue its processing. The magic happens in the operating system. With
    nonblocking I/O, the operating system queues the requests. The system processes
    the actual I/O in the future. When the I/O completes, and the response is ready,
    a *continuation*, often implemented as a callback, happens and the caller receives
    the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the benefits and see how these continuations work, we
    need to look under the hood: how is nonblocking I/O implemented? We already mentioned
    a queue. The system enqueues I/O operations and returns immediately, so the caller
    is not blocked while waiting for the I/O operations to complete. When a response
    comes back, the system stores the result in a structure. When the caller needs
    the result, it interrogates the system to see whether the operation completed
    ([Example 4-4](#reactive-system::non-blocking-server-loop)).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-4\. An echo server using nonblocking I/O (*chapter-4/non-blocking-io/src/main/java/org/acme/nio/NonBlockingServer.java*)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Nonblocking I/O introduces a few new concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: We don’t use `InputStream` or `OutputStream` (which are blocking by nature),
    but `Buffer`, which is a temporary storage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Channel` can be viewed as an endpoint for an open connection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Selector` is the cornerstone of nonblocking I/O in Java.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Selector` manages multiple channels, either server or client channels. When
    you use nonblocking I/O, you create `Selector`. Each time you deal with a new
    channel, you register this channel on the selector with the events you are interested
    in (accept, ready to read, ready to write).'
  prefs: []
  type: TYPE_NORMAL
- en: Then your code polls `Selector` with only one thread to see if the channel is
    ready. When the channel is ready to read or write, you can start to read and write.
    We don’t need to have a thread for every channel at all, and a single thread can
    handle many channels.
  prefs: []
  type: TYPE_NORMAL
- en: The selector is an abstraction of the nonblocking I/O implementation provided
    by the underlying operating system. Various approaches, depending on the operating
    systems, are available.
  prefs: []
  type: TYPE_NORMAL
- en: First, `select` was implemented in the 1980s. It supports the registration of
    1,024 sockets. That was certainly enough in the ’80s, but not anymore.
  prefs: []
  type: TYPE_NORMAL
- en: '`poll` is a replacement for `select` introduced in 1997. The most significant
    difference is that `poll` no longer limits the number of sockets. However, as
    with `select`, the system tells you only how many channels are ready, not which
    ones. You need to iterate over the set of channels to check which ones are ready.
    When there are few channels, it is not a big problem. Once the number of channels
    is more than hundreds of thousands, the iteration time is considerable.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, `epoll` appeared in 2002 in the Linux Kernel 2.5.44. `Kqueue` appeared
    in FreeBSD in 2000 and `/dev/poll` in Solaris around the same time. These mechanisms
    return the set of channels that are ready to be processed—no more iteration over
    every channel! Finally, Windows systems provide IOCP, an optimized implementation
    of `select`.
  prefs: []
  type: TYPE_NORMAL
- en: What’s important to remember is that regardless of how the operating systems
    implement it, with nonblocking I/O, you need only a single thread to handle multiple
    requests. This model is much more efficient than blocking I/O, as you don’t need
    to create threads to handle concurrent requests. Eliminating these extra threads
    makes your application much more efficient in terms of memory consumption (about
    1 MB per thread) and avoids wasting CPU cycles because of context switches (1–2
    microseconds per switch).^([4](ch04.html#idm45358830663584))
  prefs: []
  type: TYPE_NORMAL
- en: Reactive systems recommend the use of nonblocking I/O to receive and send messages.
    Thus, your application can handle more messages with fewer resources. Another
    advantage is that an idle application would consume almost no memory or CPUs.
    You don’t have to reserve resources up front.
  prefs: []
  type: TYPE_NORMAL
- en: Reactor Pattern and Event Loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Nonblocking I/O gives us the possibility to handle multiple concurrent requests
    or messages with a single thread. How could we handle these concurrent requests?
    How do we structure our code when using nonblocking I/O? The examples given in
    the previous section are not scaling well; we can quickly see that implementing
    a REST API with such a model will be a nightmare. Besides, we would like to avoid
    using worker threads, as it would discard the advantages of nonblocking I/O. We
    need something different: the reactor pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: The *reactor pattern*, illustrated in [Figure 4-7](#image:event-loop), allows
    associating I/O events with *event handlers*. The *reactor*, the cornerstone of
    this mechanism, invokes the event handlers when the expected event is received.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of the reactor pattern is to avoid creating a thread for each message,
    request, and connection. This pattern receives events from multiple channels and
    sequentially distributes them to the corresponding event handlers.
  prefs: []
  type: TYPE_NORMAL
- en: '![The reactor pattern](assets/rsij_0407.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-7\. The reactor pattern
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Implementation of the reactor pattern uses an *event loop* ([Figure 4-7](#image:event-loop)).
    It’s a thread iterating over the set of channels, and when data is ready to be
    consumed, the event loop invokes the associated event handler sequentially, in
    a single-threaded manner.
  prefs: []
  type: TYPE_NORMAL
- en: When you combine nonblocking I/O and the reactor pattern, you organize your
    code as a set of event handlers. That approach works wonderfully with reactive
    code as it exposes the notion of events, the essence of Reactive.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reactor pattern has two variants:'
  prefs: []
  type: TYPE_NORMAL
- en: The *multireactor* pattern uses multiple event loops (generally one or two per
    CPU core), which increase the concurrency of the application. Multireactor pattern
    implementations, such as Eclipse Vert.x, call the event handlers in a single-threaded
    manner to avoid deadlock or state visibility issues.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *proactor* pattern can be seen as an asynchronous version of the reactor
    pattern. Long-running event handlers invoke a continuation when they complete.
    Such mechanisms allow mixing nonblocking and blocking I/O ([Figure 4-8](#image:proactor)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![the proactor pattern](assets/rsij_0408.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-8\. The proactor pattern
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can integrate nonblocking event handlers, as well as blocking ones, by offloading
    their execution to separate threads when it’s inevitable. When their execution
    completes, the proactor pattern invokes the continuation. As you will see in [Chapter 6](ch06.html#quarkus-reactive),
    this is the pattern used by Quarkus.
  prefs: []
  type: TYPE_NORMAL
- en: Anatomy of Reactive Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last few years, many frameworks have popped up, offering reactive application
    support. Their goal is to simplify the implementation of reactive applications.
    They achieve this by providing higher-level primitives and APIs to handle events
    and abstract nonblocking I/O.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, and you may have recognized this already, using nonblocking I/O is not
    that simple. Combining this with a reactor pattern (or a variant) can be convoluted.
    Fortunately, alongside frameworks, libraries and toolkits are doing the heavy
    lifting. Netty is an asynchronous event-driven network application framework leveraging
    nonblocking I/O to build highly concurrent applications. It’s the most used library
    to handle nonblocking I/O in the Java world. But Netty can be challenging. [Example 4-5](#reactive-system::netty-echo)
    implements the *echo* TCP server using Netty.
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-5\. An echo server using Netty (*chapter-4/non-blocking-io/src/main/java/org/acme/netty/NettyEchoServer.java*)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The Vert.x toolkit, based on top of Netty, provides higher-level features to
    build reactive applications such as HTTP clients and servers, messaging clients,
    etc. Typically, the same *echo* TCP server using Vert.x looks like [Example 4-6](#reactive-system::vertx-echo).
  prefs: []
  type: TYPE_NORMAL
- en: Example 4-6\. An echo server using Vert.x (*chapter-4/non-blocking-io/src/main/java/org/acme/vertx/VertxEchoServer.java*)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Most Java frameworks offering Reactive capabilities are based on Netty or Vert.x.
    As shown in [Figure 4-9](#image:reactive-framework), they all follow the same
    type of blueprint.
  prefs: []
  type: TYPE_NORMAL
- en: '![The common architecture of reactive frameworks](assets/rsij_0409.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-9\. The common architecture of reactive frameworks
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At the bottom, you have the nonblocking I/O. Generally, frameworks use Netty
    or Vert.x. This layer handles client connections, outbound requests, and response
    writing. In other words, it manages the I/O part. Most of the time, this layer
    implements the reactor pattern (or a variant), and so provides an event-loop-based
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in the second layer, you have the *reactive framework* per se. The role
    of this layer is to provide high-level APIs that are easy to use. You use these
    APIs to write your application code. Instead of having to handle nonblocking I/O
    channels, this layer provides high-level objects such as HTTP requests, responses,
    Kafka messages, and so on. Much easier!
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in the top layer, you have your application. Your code does not need
    to touch nonblocking I/O concepts, thanks to the reactive framework. It can focus
    on incoming events and handle them. Your code is *just* a collection of event
    handlers. It can use the features provided by the reactive framework to interact
    with other services or middleware.
  prefs: []
  type: TYPE_NORMAL
- en: 'But there is a catch. The event handler from your code is invoked using the
    *event loop* thread (an I/O thread). If your code blocks this thread, no other
    concurrent events can be processed. It would be a disaster in terms of responsiveness
    and concurrency. The consequence of such an architecture is clear: your code must
    be nonblocking. It must never block the I/O threads, as they are rare and are
    used to handle multiple concurrent requests. To achieve this, you could offload
    the processing of some events to a worker thread (using the proactor pattern).
    While it can discard some of the benefits of nonblocking I/O, it is sometimes
    the most rational choice ([Figure 4-10](#image:event-loop-worker)). Nevertheless,
    we should not abuse this as it would discard the reactive benefits and make the
    application slow. The multiple context switches required to handle an event on
    a worker thread penalizes the response time.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running some event handlers on worker threads](assets/rsij_0410.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-10\. Running some event handlers on worker threads
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Typically, our applications from [Chapter 2](ch02.html#quarkus) and [Chapter 3](ch03.html#distributed-system)
    rely on such a mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Another possibility is to rely only on nonblocking code, relying on asynchronous
    APIs provided by the reactive framework. These APIs would be nonblocking, and
    if the business logic involved I/O, it uses nonblocking I/O. Every time an *event
    handler* executes an asynchronous operation, another handler (the continuation)
    is registered, and when the expected event arrives, the event loop invokes it.
    Thus, the processing is divided into smaller handlers running asynchronously.
    That model is the most efficient and embraces the concepts entirely behind Reactive.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reactive systems are about building better distributed systems. They don’t aim
    to hide the nature of distributed systems but, on the contrary, embrace it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you learned the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The four pillars of reactive systems (asynchronous message passing, elasticity,
    resilience, and responsiveness)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How asynchronous message passing enables elasticity and resilience, and increases
    the autonomy of each individual component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The role of commands and events in a distributed system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How nonblocking I/O improves resource utilization in reactive applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But this last point has a significant drawback, as we need to write nonblocking
    code. What a coincidence! The next chapter is precisely about that!
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch04.html#idm45358831669616-marker)) [“Don’t Build a Distributed Monolith”](https://oreil.ly/CtY3x)
    by Ben Christensen is an interesting talk about distributed monoliths and why
    you should avoid them.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch04.html#idm45358831644992-marker)) This pattern is called [Change Data
    Capture](https://oreil.ly/Umhs9). Frameworks such as [Debezium](https://debezium.io)
    are a key element of reactive systems when using databases, as the events are
    emitted without any impact on the application code.
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch04.html#idm45358831203712-marker)) We are referring to the traditional
    Spring Framework. Reactive Spring is based on nonblocking I/O.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch04.html#idm45358830663584-marker)) [“Measuring Context Switching and
    Memory Overheads for Linux Threads”](https://oreil.ly/hv2Uy) by Eli Bendersky
    provides interesting data about the cost of threads on Linux.
  prefs: []
  type: TYPE_NORMAL
