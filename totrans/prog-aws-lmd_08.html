<html><head></head><body><section data-pdf-bookmark="Chapter 8. Advanced AWS Lambda" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch08">&#13;
<h1><span class="label">Chapter 8. </span>Advanced AWS Lambda</h1>&#13;
&#13;
&#13;
<p>As we start getting towards the end of the book, it’s time to learn some of the aspects of Lambda that are important as you start to build production-ready applications—error handling, scaling, plus a few capabilities of Lambda that we don’t use all the time, but are there—and important—when you need them.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Error Handling" data-type="sect1"><div class="sect1" id="error-handling">&#13;
<h1>Error Handling</h1>&#13;
&#13;
<p>All of our examples so far have lived in the wonderful world of rainbows and unicorns where no systems fail and no one makes a mistake in writing code.&#13;
Of course, back in the real world, Things Go Wrong, and any useful production application and architecture needs to handle the times when errors occur, whether those be errors in our code or in the systems we rely on.</p>&#13;
&#13;
<p>Since AWS Lambda is a “platform,” it has certain constraints and behavior when it comes to errors, and in this section we’ll dig into what kind of errors can happen, for which contexts, and how we can handle them.&#13;
As<a data-primary="error handling" data-secondary="errors versus exceptions" data-type="indexterm" id="idm46222413382808"/><a data-primary="exceptions" data-see="error handling" data-type="indexterm" id="idm46222413381864"/> a language note, we use the words <em>error</em> and <em>exception</em> interchangeably, without the nuance that comes between the two terms in the Java world.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Classes of Error" data-type="sect2"><div class="sect2" id="idm46222413379720">&#13;
<h2>Classes of Error</h2>&#13;
&#13;
<p>When<a data-primary="error handling" data-secondary="classes of errors" data-type="indexterm" id="idm46222413378152"/> using Lambda, there are several different classes of error that can occur.&#13;
The primary ones are as follows, in order roughly of the time in which they can occur through the processing of an event:</p>&#13;
<ol>&#13;
<li>&#13;
<p>Error initializing the Lambda function (a problem loading our code, locating the handler, or with the function signature)</p>&#13;
</li>&#13;
<li>&#13;
<p>Error parsing input into specified function parameters</p>&#13;
</li>&#13;
<li>&#13;
<p>Error communicating with an external downstream service (database, etc).</p>&#13;
</li>&#13;
<li>&#13;
<p>Error generated within the Lambda function (either within its code or within the immediate environment, like an out-of-memory problem)</p>&#13;
</li>&#13;
<li>&#13;
<p>Error caused by function timeout</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>Another way we can break up errors is into <em>handled</em> errors and <em>unhandled</em> errors.</p>&#13;
&#13;
<p>For example, let’s consider the case where we communicate with a downstream microservice over HTTP, and it throws an error.&#13;
In this case, we may choose to catch the error within the Lambda function and process it there (a handled error), or we may let the error propagate out to the environment (an unhandled error).</p>&#13;
&#13;
<p>Alternatively, say we specified an incorrect method name in our Lambda configuration.&#13;
In this case, we are unable to catch the error in the Lambda function code, so this is always an unhandled error.</p>&#13;
&#13;
<p>If we handle an error ourselves, within code, then Lambda really has nothing to do with our particular error handling strategy.&#13;
We can log to standard error if like, but as we saw in <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>, standard error is treated identically to standard output as far as Lambda as concerned, and no alarms are raised if content is sent to it.</p>&#13;
&#13;
<p>Therefore, the nuances that come with handling errors in Lambda are all about unhandled errors—those that bubble out of our code to the Lambda runtime via an uncaught exception or that happen externally to our code.&#13;
What happens to these errors?&#13;
Interestingly, this depends significantly on the type of event source that triggers our Lambda function in the first place, as we will now examine.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="The Various Behaviors of Lambda Error Processing" data-type="sect2"><div class="sect2" id="lambda-error-behaviors">&#13;
<h2>The Various Behaviors of Lambda Error Processing</h2>&#13;
&#13;
<p>Lambda<a data-primary="error handling" data-secondary="Lambda error processing" data-type="indexterm" id="idm46222413342104"/><a data-primary="Simple Que Service (SQS)" data-secondary="error handling" data-type="indexterm" id="idm46222413341096"/> divides what it does with errors according to the event source that triggers invocation.&#13;
Every event source is placed into one of the event source types we listed in <a data-type="xref" href="ch05.html#ch05">Chapter 5</a> (<a data-type="xref" href="ch05.html#lambda-event-source-types">Table 5-1</a>):</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Synchronous event sources (e.g., API Gateway)</p>&#13;
</li>&#13;
<li>&#13;
<p>Asynchronous event sources (e.g., S3 and SNS)</p>&#13;
</li>&#13;
<li>&#13;
<p>Stream/queue event sources (e.g., Kinesis Data Streams and SQS)</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Each of these categories has a different model for processing errors thrown by a Lambda function, as follows.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Synchronous event sources" data-type="sect3"><div class="sect3" id="idm46222413334440">&#13;
<h3>Synchronous event sources</h3>&#13;
&#13;
<p>This<a data-primary="event sources" data-secondary="synchronous event sources" data-type="indexterm" id="idm46222413332856"/><a data-primary="synchronous event sources" data-secondary="error processing" data-type="indexterm" id="idm46222413331784"/> is the simplest model.&#13;
For Lambda functions invoked in this way, the error is propagated back up to the caller, and no automatic retry is performed.&#13;
How the error is exposed to the upstream client depends on the precise nature of how the Lambda function was called, so you should try forcing errors within your code to see how such problems are exposed.</p>&#13;
&#13;
<p>For example, if API Gateway is the event source, then errors thrown by a Lambda function will result in an error being sent back to API Gateway.&#13;
API Gateway in turn  returns a 500 HTTP response to the original requestor.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Asynchronous event sources" data-type="sect3"><div class="sect3" id="idm46222413329448">&#13;
<h3>Asynchronous event sources</h3>&#13;
&#13;
<p>Since<a data-primary="event sources" data-secondary="asynchronous event sources" data-type="indexterm" id="ESasynch08"/><a data-primary="asynchronous event sources" data-secondary="error processing" data-type="indexterm" id="AESerror08"/> this model of invocation is asynchronous, or event oriented, there is no upstream caller that can do anything useful with an error, so Lambda has a more sophisticated error handling model.</p>&#13;
&#13;
<p>First, if an error is detected in this model of invocation, then Lambda will (by default) retry processing the event up to twice further (for a total of three attempts), with a delay between such retries (the precise delay is not documented, but we’ll see an example a little later).</p>&#13;
&#13;
<p>If the Lambda function fails for all retry attempts, then the event will be posted to the function’s error destination and/or dead letter queue if either is configured (more on this later); otherwise, the event is discarded and lost.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Stream/queue event sources" data-type="sect3"><div class="sect3" id="idm46222413323624">&#13;
<h3>Stream/queue event sources</h3>&#13;
&#13;
<p>In<a data-primary="event sources" data-secondary="stream/queue event sources" data-type="indexterm" id="idm46222413322328"/><a data-primary="stream/queue event sources" data-secondary="error processing" data-type="indexterm" id="idm46222413321256"/> the absence of a configured error-handling strategy (see <a data-type="xref" href="#failure-handling-features">“Handling Kinesis and DynamoDB Stream Errors”</a>), if an error bubbles up to the Lambda runtime when processing an event from a stream/queue event source, then Lambda will keep retrying the event until either (a) the failing event expires in the upstream source or (b) the problem is resolved.&#13;
This means that the processing of the stream or queue is effectively blocked until the error is resolved.&#13;
Note that there are particular nuances here when using streams that are scaled to multiple shards, which we recommend you research if this applies to you.</p>&#13;
&#13;
<p>The<a data-primary="error handling" data-secondary="documentation pages" data-type="indexterm" id="idm46222413318440"/> following documentation pages are useful when you are considering error handling with Lambda:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p><a href="https://oreil.ly/4wxMf">Error Handling and Automatic Retries in AWS Lambda</a></p>&#13;
</li>&#13;
<li>&#13;
<p><a href="https://oreil.ly/ag0cu">AWS Lambda Function Errors in Java</a></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Deep Dive into Asynchronous Event Source Errors" data-type="sect2"><div class="sect2" id="idm46222413313704">&#13;
<h2>Deep Dive into Asynchronous Event Source Errors</h2>&#13;
&#13;
<p>Asynchronous<a data-primary="error handling" data-secondary="asynchronous event source errors" data-type="indexterm" id="EHasync08"/> event sources are a popular use of Lambda and have a complicated error processing model, so let’s look at this topic a little deeper by way of an example.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Retries" data-type="sect3"><div class="sect3" id="asynchronous-retries">&#13;
<h3>Retries</h3>&#13;
&#13;
<p>We<a data-primary="retries" data-type="indexterm" id="retries08"/> start with the following code:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kn">package</code> <code class="n">book</code><code class="o">;</code>&#13;
&#13;
<code class="kn">import</code> <code class="nn">com.amazonaws.services.lambda.runtime.events.S3Event</code><code class="o">;</code>&#13;
&#13;
<code class="kd">public</code> <code class="kd">class</code> <code class="nc">S3ErroringLambda</code> <code class="o">{</code>&#13;
  <code class="kd">public</code> <code class="kt">void</code> <code class="nf">handler</code><code class="o">(</code><code class="n">S3Event</code> <code class="n">event</code><code class="o">)</code> <code class="o">{</code>&#13;
    <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="s">"Received new S3 event"</code><code class="o">);</code>&#13;
    <code class="k">throw</code> <code class="k">new</code> <code class="nf">RuntimeException</code><code class="o">(</code><code class="s">"This function unable to process S3 Events"</code><code class="o">);</code>&#13;
  <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>We wire this up to an S3 bucket in the same way that we did for the <code>BatchEvents Lambda</code> function in <a data-type="xref" href="ch05.html#ch05">Chapter 5</a>, and we’ll see the SAM template for that a little later.</p>&#13;
&#13;
<p>If we upload a file to the S3 bucket attached to this function, we see <a data-type="xref" href="#s3-error-logs">Figure 8-1</a> in our logs.</p>&#13;
&#13;
<p>Notice that Lambda tries to process the S3 event three times—once at 20:44:00, then about a minute later, and then about two minutes after that.&#13;
These are the three total attempts to process an event that Lambda promises for an asynchronous event source.</p>&#13;
&#13;
<p>We are able configure the number of retries that Lambda will perform—0, 1, or 2—using a separate CloudFormation resource.&#13;
For example, let’s configure Lambda not to attempt any retries for the <code>SingleEventLambda</code> function from <a data-type="xref" href="ch05.html#serverless-data-pipeline-example">“Example: Building a Serverless Data Pipeline”</a>.&#13;
We can add the following resource to the application template:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting">  <code class="nt">SingleEventInvokeConfig</code><code class="p">:</code>&#13;
    <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">AWS::Lambda::EventInvokeConfig</code>&#13;
    <code class="nt">Properties</code><code class="p">:</code>&#13;
      <code class="nt">FunctionName</code><code class="p">:</code> <code class="kt">!Ref</code> <code class="l-Scalar-Plain">SingleEventLambda</code>&#13;
      <code class="nt">Qualifier</code><code class="p">:</code> <code class="s">"$LATEST"</code>&#13;
      <code class="nt">MaximumRetryAttempts</code><code class="p">:</code> <code class="l-Scalar-Plain">0</code></pre>&#13;
&#13;
<figure><div class="figure" id="s3-error-logs">&#13;
<img alt="images/ch08_image01.png" src="assets/awsl_0801.png"/>&#13;
<h6><span class="label">Figure 8-1. </span>Lambda logs during S3 error</h6>&#13;
</div></figure>&#13;
&#13;
<p>If we don’t make any further changes, Lambda won’t do anything more after all the retries (if any) are complete—brief data about the original event will be logged, but eventually it will be discarded.&#13;
For something like S3 this isn’t too bad—we can always list all of the objects in S3 later.&#13;
But for other event sources, this might be a problem if we can’t go and regenerate the events once the cause of the error is fixed.&#13;
There are two solutions to this problem—DLQs and destinations.&#13;
DLQs have been around longer, so we’ll describe them first, but destinations have more capabilities.<a data-primary="" data-startref="retries08" data-type="indexterm" id="idm46222413256168"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Dead letter queues" data-type="sect3"><div class="sect3" id="idm46222413309576">&#13;
<h3>Dead letter queues</h3>&#13;
&#13;
<p>Lambda<a data-primary="dead letter queues (DLQs)" data-type="indexterm" id="dlqs08"/><a data-primary="Simple Que Service (SQS)" data-secondary="dead letter ques (DLQs)" data-type="indexterm" id="idm46222413200952"/> provides the capability of automatically forwarding events (for asynchronous sources) that fail all of their retries to a dead letter queue (DLQ).&#13;
This DLQ can be either an SNS topic or an SQS queue.&#13;
Once the event is in SNS or SQS, you can do whatever you want with it either immediately, or manually later, in the case of SQS.&#13;
For example, you may register a separate Lambda function as an SNS topic listener that posts a copy of the failing event to an operations Slack channel for manual <span class="keep-together">processing</span>.</p>&#13;
&#13;
<p>DLQs can be configured along with all the other properties of a Lambda function.&#13;
For example, we can add a DLQ to our example app, and also add a DLQ processing function, with the SAM template.</p>&#13;
<div data-type="example" id="EX8-1">&#13;
<h5><span class="label">Example 8-1. </span>SAM template with DLQ and DLQ listener</h5>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">AWSTemplateFormatVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">2010-09-09</code>&#13;
<code class="nt">Transform</code><code class="p">:</code> <code class="l-Scalar-Plain">AWS::Serverless-2016-10-31</code>&#13;
<code class="nt">Description</code><code class="p">:</code> <code class="l-Scalar-Plain">chapter8-s3-errors</code>&#13;
&#13;
<code class="nt">Resources</code><code class="p">:</code>&#13;
  <code class="nt">DLQ</code><code class="p">:</code>&#13;
    <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">AWS::SNS::Topic</code>&#13;
&#13;
  <code class="nt">ErrorTriggeringBucket</code><code class="p">:</code>&#13;
    <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">AWS::S3::Bucket</code>&#13;
    <code class="nt">Properties</code><code class="p">:</code>&#13;
      <code class="nt">BucketName</code><code class="p">:</code> <code class="kt">!Sub</code> <code class="l-Scalar-Plain">${AWS::AccountId}-${AWS::Region}-errortrigger</code>&#13;
&#13;
  <code class="nt">S3ErroringLambda</code><code class="p">:</code>&#13;
    <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">AWS::Serverless::Function</code>&#13;
    <code class="nt">Properties</code><code class="p">:</code>&#13;
      <code class="nt">Runtime</code><code class="p">:</code> <code class="l-Scalar-Plain">java8</code>&#13;
      <code class="nt">MemorySize</code><code class="p">:</code> <code class="l-Scalar-Plain">512</code>&#13;
      <code class="nt">Handler</code><code class="p">:</code> <code class="l-Scalar-Plain">book.S3ErroringLambda::handler</code>&#13;
      <code class="nt">CodeUri</code><code class="p">:</code> <code class="l-Scalar-Plain">target/lambda.zip</code>&#13;
      <code class="nt">DeadLetterQueue</code><code class="p">:</code>&#13;
        <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">SNS</code>&#13;
        <code class="nt">TargetArn</code><code class="p">:</code> <code class="kt">!Ref</code> <code class="l-Scalar-Plain">DLQ</code>&#13;
      <code class="nt">Events</code><code class="p">:</code>&#13;
        <code class="nt">S3Event</code><code class="p">:</code>&#13;
          <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">S3</code>&#13;
          <code class="nt">Properties</code><code class="p">:</code>&#13;
            <code class="nt">Bucket</code><code class="p">:</code> <code class="kt">!Ref</code> <code class="l-Scalar-Plain">ErrorTriggeringBucket</code>&#13;
            <code class="nt">Events</code><code class="p">:</code> <code class="l-Scalar-Plain">s3:ObjectCreated:*</code>&#13;
&#13;
  <code class="nt">DLQProcessingLambda</code><code class="p">:</code>&#13;
    <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">AWS::Serverless::Function</code>&#13;
    <code class="nt">Properties</code><code class="p">:</code>&#13;
      <code class="nt">Runtime</code><code class="p">:</code> <code class="l-Scalar-Plain">java8</code>&#13;
      <code class="nt">MemorySize</code><code class="p">:</code> <code class="l-Scalar-Plain">512</code>&#13;
      <code class="nt">Handler</code><code class="p">:</code> <code class="l-Scalar-Plain">book.DLQProcessingLambda::handler</code>&#13;
      <code class="nt">CodeUri</code><code class="p">:</code> <code class="l-Scalar-Plain">target/lambda.zip</code>&#13;
      <code class="nt">Events</code><code class="p">:</code>&#13;
        <code class="nt">SnsEvent</code><code class="p">:</code>&#13;
          <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">SNS</code>&#13;
          <code class="nt">Properties</code><code class="p">:</code>&#13;
            <code class="nt">Topic</code><code class="p">:</code> <code class="kt">!Ref</code> <code class="l-Scalar-Plain">DLQ</code></pre></div>&#13;
&#13;
<p>The important elements to observe here are as follows:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>We define our own SNS topic to act as a DLQ.</p>&#13;
</li>&#13;
<li>&#13;
<p>Within the application function (<code>S3ErroringLambda</code>), we tell Lambda that we want a DLQ for the function, that it’s of type SNS, and that DLQ messages should be sent to the topic we created in this template.</p>&#13;
</li>&#13;
<li>&#13;
<p>We also define a separate function (<code>DLQProcessingLambda</code>) that is triggered by events sent to the DLQ.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Our code for <code>DLQProcessingLambda</code> is as follows:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kn">package</code> <code class="n">book</code><code class="o">;</code>&#13;
&#13;
<code class="kn">import</code> <code class="nn">com.amazonaws.services.lambda.runtime.events.SNSEvent</code><code class="o">;</code>&#13;
&#13;
<code class="kd">public</code> <code class="kd">class</code> <code class="nc">DLQProcessingLambda</code> <code class="o">{</code>&#13;
  <code class="kd">public</code> <code class="kt">void</code> <code class="nf">handler</code><code class="o">(</code><code class="n">SNSEvent</code> <code class="n">event</code><code class="o">)</code> <code class="o">{</code>&#13;
    <code class="n">event</code><code class="o">.</code><code class="na">getRecords</code><code class="o">().</code><code class="na">forEach</code><code class="o">(</code><code class="n">snsRecord</code> <code class="o">-&gt;</code>&#13;
        <code class="n">System</code><code class="o">.</code><code class="na">out</code><code class="o">.</code><code class="na">println</code><code class="o">(</code><code class="s">"Received DLQ event: "</code> <code class="o">+</code> <code class="n">snsRecord</code><code class="o">.</code><code class="na">toString</code><code class="o">())</code>&#13;
    <code class="o">);</code>&#13;
  <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>Now if we upload a file to S3, we see the following in the logs for <code>DLQProcessing Lambda</code> after the final delivery attempt to <code>S3ErroringLambda</code>:</p>&#13;
&#13;
<pre data-type="programlisting">Received DLQ event: {sns: {messageAttributes:&#13;
    {RequestID={type: String,value: ff294606-e377-4bad-8f2a-4c5f88042656},&#13;
     ErrorCode={type: String,value: 200}, ...</pre>&#13;
&#13;
<p>The event sent to the DLQ processing function includes the full original event that failed, allowing you to save this off and process later.&#13;
It also includes the <code>RequestID</code> of the original event, which allows you to search within the application Lambda function’s log for clues as to what went wrong.</p>&#13;
&#13;
<p>While in this example we included all of the DLQ resources within the same template as the application itself, you may choose to use resources outside of the application and therefore share those DLQ elements across applications.<a data-primary="" data-startref="dlqs08" data-type="indexterm" id="idm46222413024216"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Destinations" data-type="sect3"><div class="sect3" id="idm46222413254600">&#13;
<h3>Destinations</h3>&#13;
&#13;
<p>At<a data-primary="destinations" data-type="indexterm" id="idm46222413021704"/> the end of 2019 AWS introduced an alternative to DLQs for capturing failed events: <a href="https://oreil.ly/XT6Ds"><em>destinations</em></a>.&#13;
Destinations are actually a more powerful feature than DLQ since you can capture both errors <em>and</em> successfully processed asynchronous events.</p>&#13;
&#13;
<p>Further, destinations support more types of target than DLQs.&#13;
SNS and SQS are supported, just as they are with DLQs, but you can also route directly to another Lambda function (skipping the message bus part) or EventBridge.</p>&#13;
&#13;
<p>To configure a Destination, we use the same type of <code>AWS::Lambda::EventInvokeConfig</code> resource we created earlier when configuring retry counts (see <a data-type="xref" href="#asynchronous-retries">“Retries”</a>).&#13;
For example, let’s replace the DLQ in the previous example with a Destination:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">AWSTemplateFormatVersion</code><code class="p">:</code> <code class="l-Scalar-Plain">2010-09-09</code>&#13;
<code class="nt">Transform</code><code class="p">:</code> <code class="l-Scalar-Plain">AWS::Serverless-2016-10-31</code>&#13;
<code class="nt">Description</code><code class="p">:</code> <code class="l-Scalar-Plain">chapter8-s3-errors</code>&#13;
&#13;
<code class="nt">Resources</code><code class="p">:</code>&#13;
  <code class="nt">ErrorTriggeringBucket</code><code class="p">:</code>&#13;
    <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">AWS::S3::Bucket</code>&#13;
    <code class="nt">Properties</code><code class="p">:</code>&#13;
      <code class="nt">BucketName</code><code class="p">:</code> <code class="kt">!Sub</code> <code class="l-Scalar-Plain">${AWS::AccountId}-${AWS::Region}-errortrigger</code>&#13;
&#13;
  <code class="nt">S3ErroringLambda</code><code class="p">:</code>&#13;
    <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">AWS::Serverless::Function</code>&#13;
    <code class="nt">Properties</code><code class="p">:</code>&#13;
      <code class="nt">Runtime</code><code class="p">:</code> <code class="l-Scalar-Plain">java8</code>&#13;
      <code class="nt">MemorySize</code><code class="p">:</code> <code class="l-Scalar-Plain">512</code>&#13;
      <code class="nt">Handler</code><code class="p">:</code> <code class="l-Scalar-Plain">book.S3ErroringLambda::handler</code>&#13;
      <code class="nt">CodeUri</code><code class="p">:</code> <code class="l-Scalar-Plain">target/lambda.zip</code>&#13;
      <code class="nt">Events</code><code class="p">:</code>&#13;
        <code class="nt">S3Event</code><code class="p">:</code>&#13;
          <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">S3</code>&#13;
          <code class="nt">Properties</code><code class="p">:</code>&#13;
            <code class="nt">Bucket</code><code class="p">:</code> <code class="kt">!Ref</code> <code class="l-Scalar-Plain">ErrorTriggeringBucket</code>&#13;
            <code class="nt">Events</code><code class="p">:</code> <code class="l-Scalar-Plain">s3:ObjectCreated:*</code>&#13;
      <code class="nt">Policies</code><code class="p">:</code>&#13;
        <code class="nt">— LambdaInvokePolicy</code><code class="p">:</code>&#13;
            <code class="nt">FunctionName</code><code class="p">:</code> <code class="kt">!Ref</code> <code class="l-Scalar-Plain">ErrorProcessingLambda</code>&#13;
&#13;
  <code class="nt">ErrorProcessingLambda</code><code class="p">:</code>&#13;
    <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">AWS::Serverless::Function</code>&#13;
    <code class="nt">Properties</code><code class="p">:</code>&#13;
      <code class="nt">Runtime</code><code class="p">:</code> <code class="l-Scalar-Plain">java8</code>&#13;
      <code class="nt">MemorySize</code><code class="p">:</code> <code class="l-Scalar-Plain">512</code>&#13;
      <code class="nt">Handler</code><code class="p">:</code> <code class="l-Scalar-Plain">book.ErrorProcessingLambda::handler</code>&#13;
      <code class="nt">CodeUri</code><code class="p">:</code> <code class="l-Scalar-Plain">target/lambda.zip</code>&#13;
&#13;
  <code class="nt">S3ErroringLambdaInvokeConfig</code><code class="p">:</code>&#13;
    <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">AWS::Lambda::EventInvokeConfig</code>&#13;
    <code class="nt">Properties</code><code class="p">:</code>&#13;
      <code class="nt">FunctionName</code><code class="p">:</code> <code class="kt">!Ref</code> <code class="l-Scalar-Plain">S3ErroringLambda</code>&#13;
      <code class="nt">Qualifier</code><code class="p">:</code> <code class="s">"$LATEST"</code>&#13;
      <code class="nt">DestinationConfig</code><code class="p">:</code>&#13;
        <code class="nt">OnFailure</code><code class="p">:</code>&#13;
          <code class="nt">Destination</code><code class="p">:</code> <code class="kt">!GetAtt</code> <code class="l-Scalar-Plain">ErrorProcessingLambda.Arn</code></pre>&#13;
&#13;
<p>There are a few aspects to notice from this example:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>There are no explicit queues or topics.</p>&#13;
</li>&#13;
<li>&#13;
<p>The Destination at the end defines that when <code>S3ErroringLambda</code> fails, we want events to be sent to <code>ErrorProcessingLambda</code>.</p>&#13;
</li>&#13;
<li>&#13;
<p>The application function needs to be given permission to invoke the error handling function, which we enable via the <code>Policies</code> property on the <code>S3Erroring Lambda</code> resource.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The event that is sent to <code>ErrorProcessingLambda</code> is <em>not</em> the same type as that sent to a DLQ.&#13;
At time of writing, the <code>aws-lambda-java-events</code> library has not been updated to include the Destination types, and deserializing these types is tricky due to some unfortunate naming of fields within the sent objects.&#13;
Ideally by the time you read this book, this will have been fixed!</p>&#13;
&#13;
<p>Destinations will likely replace most usages of DLQ, and we’re also interested to see how people use the <code>OnSuccess</code> version of destinations to build interesting solutions.<a data-primary="" data-startref="ESasynch08" data-type="indexterm" id="idm46222412782456"/><a data-primary="" data-startref="AESerror08" data-type="indexterm" id="idm46222412781592"/><a data-primary="" data-startref="EHasync08" data-type="indexterm" id="idm46222412780648"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Handling Kinesis and DynamoDB Stream Errors" data-type="sect2"><div class="sect2" id="failure-handling-features">&#13;
<h2>Handling Kinesis and DynamoDB Stream Errors</h2>&#13;
&#13;
<p>In<a data-primary="stream/queue event sources" data-secondary="error processing" data-type="indexterm" id="idm46222412777832"/><a data-primary="Kinesis" data-type="indexterm" id="idm46222412776808"/><a data-primary="DynamoDB" data-secondary="error handling" data-type="indexterm" id="idm46222412776136"/> late 2019, AWS added a number of <a href="https://oreil.ly/gWKX-">failure-handling features</a> to the Kinesis and DynamoDB stream event sources.&#13;
These new features make it possible<a data-primary="poison pill scenarios" data-type="indexterm" id="idm46222412774360"/> to avoid “poison pill” scenarios, where a single bad record could block stream (or shard) processing for up to a week (depending on how long the stream retains records).</p>&#13;
&#13;
<p>The failure-handling features can be configured via SAM (or CloudFormation), and are applied when a Lambda function fails to process a batch of records from either a Kinesis or DynamoDB stream.&#13;
The new features are as follows:</p>&#13;
<dl>&#13;
<dt>Bisect on Function Error</dt>&#13;
<dd>&#13;
<p>Instead of simply retrying the entire batch of records for a failed Lambda invocation, this feature splits the batch into two. These smaller batches are retried <span class="keep-together">separately</span>. This approach can automatically narrow failures down to whichever individual records are causing a problem, and those records can be dealt with via the other error-handling features.</p>&#13;
</dd>&#13;
<dt>Maximum Record Age</dt>&#13;
<dd>&#13;
<p>This instructs the Lambda function to skip records older than a specified Maximum Record Age (which can be from 60 seconds to 7 days).</p>&#13;
</dd>&#13;
<dt>Maximum Retry Attempts</dt>&#13;
<dd>&#13;
<p>This feature retries failed batches for a configurable number of times and then sends information about the batch of records to the configured <em>on-failure destination</em> (the next feature in this list).</p>&#13;
</dd>&#13;
<dt>Destination on Failure</dt>&#13;
<dd>&#13;
<p>This is an SNS topic or SQS queue that will receive information about failed batches. Note that it doesn’t receive the actual failed records—those have to be extracted from the stream before they expire.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>A comprehensive error-handling approach can (and should) combine all of these features.&#13;
For example, a failed batch of records can be split (perhaps several times) until there is a single-record batch causing a failure.&#13;
That single-record batch might be retried 10 times or until the record is 15 minutes old, at which point the details of the batch (with its single failed record) will be sent to an SNS topic.&#13;
A separate Lambda could be subscribed to that SNS topic, automatically retrieve the failed record from the stream, and store it in S3 for later investigation.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Tracing Errors with X-Ray" data-type="sect2"><div class="sect2" id="idm46222412763784">&#13;
<h2>Tracing Errors with X-Ray</h2>&#13;
&#13;
<p>If<a data-primary="X-Ray" data-type="indexterm" id="idm46222412762488"/> you are using AWS X-Ray (discussed in <a data-type="xref" href="ch07.html#distributed-tracing">“Distributed Tracing”</a>), then it will be able to show where errors are occurring in your graph of components.&#13;
For more details, see <a data-type="xref" href="ch07.html#finding-errors">“Finding Errors”</a>, and the X-Ray documentation.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Error Handling Strategies" data-type="sect2"><div class="sect2" id="idm46222412759592">&#13;
<h2>Error Handling Strategies</h2>&#13;
&#13;
<p>So<a data-primary="error handling" data-secondary="strategies for" data-type="indexterm" id="idm46222412758040"/> given everything we now know about errors, and Lambda’s capabilities and behaviors regarding them, how should we choose to deal with errors?</p>&#13;
&#13;
<p>For unhandled errors, we should set up monitoring (see <a data-type="xref" href="ch07.html#cloudwatch-alarms">“Alarms”</a>), and when errors occur, we will likely need some kind of manual intervention.&#13;
The urgency of this will depend on the context, and also the type of the event source—remember in the case of stream/queue event sources that processing is blocked until the error is cleared.</p>&#13;
&#13;
<p>For handled errors, though, we have an interesting choice.&#13;
Should we process the error and rethrow, or should we capture the error and exit the function cleanly?&#13;
Again, this will depend on the context and invocation type, but here are some thoughts.</p>&#13;
&#13;
<p>For<a data-primary="synchronous event sources" data-secondary="error handling strategies" data-type="indexterm" id="idm46222412754168"/> synchronous event sources, you will likely want to return some kind of error to the original caller.&#13;
Typically you’ll want to do that explicitly within the Lambda code and return a well-formatted error.&#13;
A problem here, though, is that Lambda won’t know if this is an error, so you’ll need to track this metric manually.&#13;
The problem with letting unhandled errors bubble out from synchronously called Lambdas is that you have no control over the error returned to the upstream client.</p>&#13;
&#13;
<p>For<a data-primary="asynchronous event sources" data-secondary="error handling strategies" data-type="indexterm" id="idm46222412752056"/> asynchronous event sources, what you do will largely depend on whether you want to use a DLQ or Destination.&#13;
If you do, then there’s often no harm in either letting an error bubble out or throwing a custom error and then handling the error in whatever is processing messages from the DLQ/Destination.&#13;
If you don’t use a DLQ/Destination then you may want to at least log the failing input event if the error occurs within your code.</p>&#13;
&#13;
<p>For<a data-primary="Kinesis" data-type="indexterm" id="idm46222412750056"/><a data-primary="DynamoDB" data-secondary="error handling" data-type="indexterm" id="idm46222412749320"/><a data-primary="stream/queue event sources" data-secondary="error handling strategies" data-type="indexterm" id="idm46222412748376"/> Kinesis and DynamoDB stream event sources, using one of the failure-handling features described earlier allows processing to continue even if some records cause errors.&#13;
With a properly configured <em>Destination on Failure</em>, this is an effective error-handling strategy, although it assumes that it is safe for your application to potentially process records out of order.&#13;
If that isn’t the case, then consider omitting the failure-handling features and relying on the platform’s automatic retry behavior (which in this case would block processing until the error is resolved or the records expire).</p>&#13;
&#13;
<p>For SQS you’ll typically want to handle errors within your code, since otherwise further processing is blocked.&#13;
An effective way to do this is to put a top-level <code>try-catch</code> block in your handler function.&#13;
Within this block, you can set up your own retry strategy or log the failing event and exit the function cleanly.&#13;
In certain situations, you really will want to block further event processing until the problem causing the error is resolved, in which case you can throw a new error from the top-level try-catch block and use the platform’s automatic retry behavior.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Scaling" data-type="sect1"><div class="sect1" id="lambda-scaling">&#13;
<h1>Scaling</h1>&#13;
&#13;
<p>In <a data-type="xref" href="ch05.html#ch05">Chapter 5</a> we<a data-primary="auto-scaling" data-secondary="fan-out pattern" data-type="indexterm" id="idm46222412742024"/><a data-primary="scaling" data-secondary="auto-scaling feature" data-type="indexterm" id="idm46222412741048"/><a data-primary="fan-out pattern" data-type="indexterm" id="idm46222412740104"/> touched on one of the most valuable aspects of Lambda—its ability to auto-scale without any effort (see <a data-type="xref" href="ch05.html#data-pipeline-fanout">Figure 5-10</a>).&#13;
In the data pipeline example we used this auto-scaling ability to implement a “fan-out” pattern—processing many small events in parallel.</p>&#13;
&#13;
<p>This is the key to Lambda’s scaling model—if all current instances of a function are currently in use when a new event occurs, then Lambda will automatically create a new instance, <em>scaling out</em> the function, to handle the new event.</p>&#13;
&#13;
<p>Eventually, after a period of inactivity, function instances will be <em>reaped</em>, <em>scaling in</em> the function.</p>&#13;
&#13;
<p>From<a data-primary="costs" data-secondary="benefits of auto-scaling feature" data-type="indexterm" id="idm46222412735480"/> a cost perspective, Lambda guarantees that we are only charged while our function is processing an event, so it costs the same to process one hundred Lambda events serially in one function instance as it does to process them in parallel in one hundred instances (subject to any extra time costs involved in cold start, which we describe later in this <span class="keep-together">chapter</span>).</p>&#13;
&#13;
<p>Lambda scaling has limits, of course, which we’ll examine in a moment, but first let’s take a look at Lambda’s magical auto-scaling.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Observing Lambda Scaling" data-type="sect2"><div class="sect2" id="idm46222412732616">&#13;
<h2>Observing Lambda Scaling</h2>&#13;
&#13;
<p>Let’s<a data-primary="scaling" data-secondary="observing Lambda scaling" data-type="indexterm" id="idm46222412730920"/> start with the following code:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kn">package</code> <code class="n">book</code><code class="o">;</code>&#13;
&#13;
<code class="kd">public</code> <code class="kd">class</code> <code class="nc">MyLambda</code> <code class="o">{</code>&#13;
  <code class="kd">private</code> <code class="kd">static</code> <code class="kd">final</code> <code class="n">String</code> <code class="n">instanceID</code> <code class="o">=</code>&#13;
    <code class="n">java</code><code class="o">.</code><code class="na">util</code><code class="o">.</code><code class="na">UUID</code><code class="o">.</code><code class="na">randomUUID</code><code class="o">().</code><code class="na">toString</code><code class="o">();</code>&#13;
&#13;
  <code class="kd">public</code> <code class="n">String</code> <code class="nf">handler</code><code class="o">(</code><code class="n">String</code> <code class="n">input</code><code class="o">)</code> <code class="o">{</code>&#13;
    <code class="k">return</code> <code class="s">"This is function instance "</code> <code class="o">+</code> <code class="n">instanceID</code><code class="o">;</code>&#13;
  <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>Static and instance members of a function handler’s class are instantiated once per instance of a function.&#13;
We discuss this further later, in the section about cold starts.&#13;
Therefore, if we invoke the previous code five times in succession, it will always return the same value for the <code>instanceID</code> member.</p>&#13;
&#13;
<p>Now let’s change the code a little, adding a <code>sleep</code> statement:</p>&#13;
&#13;
<pre data-code-language="java" data-type="programlisting"><code class="kn">package</code> <code class="n">book</code><code class="o">;</code>&#13;
&#13;
<code class="kd">public</code> <code class="kd">class</code> <code class="nc">MyLambda</code> <code class="o">{</code>&#13;
  <code class="kd">private</code> <code class="kd">static</code> <code class="kd">final</code> <code class="n">String</code> <code class="n">instanceID</code> <code class="o">=</code>&#13;
    <code class="n">java</code><code class="o">.</code><code class="na">util</code><code class="o">.</code><code class="na">UUID</code><code class="o">.</code><code class="na">randomUUID</code><code class="o">().</code><code class="na">toString</code><code class="o">();</code>&#13;
&#13;
  <code class="kd">public</code> <code class="n">String</code> <code class="nf">handler</code><code class="o">(</code><code class="n">String</code> <code class="n">input</code><code class="o">)</code> <code class="kd">throws</code> <code class="n">Exception</code> <code class="o">{</code>&#13;
    <code class="n">Thread</code><code class="o">.</code><code class="na">sleep</code><code class="o">(</code><code class="mi">5000</code><code class="o">);</code>&#13;
    <code class="k">return</code> <code class="s">"This is function instance "</code> <code class="o">+</code> <code class="n">instanceID</code><code class="o">;</code>&#13;
  <code class="o">}</code>&#13;
<code class="o">}</code></pre>&#13;
&#13;
<p>Make sure if you’re deploying this code to include a <code>Timeout</code> configuration of at least six seconds; otherwise, you’ll see a good example of a timeout error!</p>&#13;
&#13;
<p>Now invoke the function several times in parallel.&#13;
One way to do this is by running the same <code>aws lambda invoke</code> command from multiple terminal tabs.&#13;
Depending on how quick on the draw you are for navigating terminal sessions, you’ll now see that different container IDs are returned for different invocations.</p>&#13;
&#13;
<p>This behavior is visible because when Lambda receives the second request to invoke your function, the previous container that was used for the first request is still processing that request, so Lambda creates a new instance, automatically scaling out, to handle the second request.&#13;
This creation of a new instance happens for the third and fourth requests too, if you’re fast enough.</p>&#13;
&#13;
<p>This is an example of invoking the Lambda function directly, but this is the same scaling behavior we see when Lambda is invoked by most event sources, including API Gateway, S3, or SNS, whenever one instance of a Lambda function is not sufficient to keep up with the event load.&#13;
Magical auto-scaling, without any effort!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Scaling Limits and Throttling" data-type="sect2"><div class="sect2" id="idm46222412731992">&#13;
<h2>Scaling Limits and Throttling</h2>&#13;
&#13;
<p>AWS<a data-primary="scaling" data-secondary="scaling limits and throttling" data-type="indexterm" id="Slimits08"/><a data-primary="throttling" data-type="indexterm" id="throttling08"/> is not an infinite computer, and there are limits to Lambda’s scaling.&#13;
Amazon limits the number of concurrent executions across all functions per AWS account, per region.&#13;
By default, at the time of writing, this limit is one thousand, but you can make a support request to have this increased.&#13;
Partly this limit exists because of the physical constraints of living in a material universe and partly so that your AWS bill doesn’t explode to astronomical proportions!</p>&#13;
&#13;
<p>If<a data-primary="metrics" data-secondary="Throttles" data-type="indexterm" id="idm46222412587000"/> you reach this limit, you’ll start to experience <em>throttling</em>, and you’ll know this because the account-wide <code>Throttles</code> CloudWatch metric for your Lambda functions will suddenly have an amount greater than zero.&#13;
This makes it a great metric to set a Cloudwatch alarm for (we talked about built-in metrics and alarms in <a data-type="xref" href="ch07.html#metrics">“Metrics”</a>).</p>&#13;
&#13;
<p>When your function is throttled, the behavior exhibited by AWS is similar to the behavior that occurs when your function throws an error (which we talked about earlier in this chapter—<a data-type="xref" href="#lambda-error-behaviors">“The Various Behaviors of Lambda Error Processing”</a>)—in other words, it depends on the type of event source.&#13;
In summary:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>For<a data-primary="synchronous event sources" data-secondary="scaling limits and throttling" data-type="indexterm" id="idm46222412581528"/> synchronous event sources (e.g., API Gateway), throttling is treated as an error and passed back up to the caller as an HTTP status code 500 error.</p>&#13;
</li>&#13;
<li>&#13;
<p>For<a data-primary="asynchronous event sources" data-secondary="scaling limits and throttling" data-type="indexterm" id="idm46222412579464"/> asynchronous event sources (e.g., S3), Lambda will retry calling your Lambda function for up to six hours, by default.&#13;
This is configurable, for example, by using the <code>MaximumEventAgeInSeconds</code> property of the <a href="https://oreil.ly/by8cO"><code>AWS::Lambda::EventInvokeConfig</code> CloudFormation resource</a> that we introduced in <a data-type="xref" href="#asynchronous-retries">“Retries”</a>.</p>&#13;
</li>&#13;
<li>&#13;
<p>For<a data-primary="stream/queue event sources" data-secondary="scaling limits and throttling" data-type="indexterm" id="idm46222412575032"/> stream/queue event sources (e.g., Kinesis), Lambda will block and retry until successful or the data expires.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Stream-based sources may also have other scaling restrictions, for example, based on the number of shards of your stream and the configured <a href="https://oreil.ly/4RSoj"><code>ParallelizationFactor</code></a>.</p>&#13;
&#13;
<p>Since the Lambda concurrency limit is account-wide, one particularly important aspect to be aware of is that one Lambda function that has scaled particularly wide can impact every other Lambda function in the same AWS account + region pair.&#13;
Because of this, it is strongly recommended that, at the very least, you use separate AWS accounts for production and testing—deliberately DoS’ing (denial-of-servicing) your production application because of a load test against a staging environment is a particularly embarrassing situation to explain!</p>&#13;
&#13;
<p>But beyond the production versus test account separation, we also recommend using different AWS “subaccounts” within one AWS “organization” for different “services” within your ecosystem to further isolate yourself from the problems of account-wide limits.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Burst limits" data-type="sect3"><div class="sect3" id="burst-limits">&#13;
<h3>Burst limits</h3>&#13;
&#13;
<p>The<a data-primary="burst limits" data-type="indexterm" id="idm46222412568632"/> limits and throttling mentioned refer to the total capacity available to your Lambda functions.&#13;
However, there’s another limit to be occasionally aware of—the <em>burst limit</em>.&#13;
This refers to <em>how quickly</em> (as opposed to <em>how wide</em>) your Lambda function can scale.&#13;
By default Lambda can scale out a function by up to 500 instances every minute, with perhaps a small boost at the beginning.&#13;
If your workload can burst faster than this (and we’ve seen some that can), then you’ll need to be aware of burst limits and may want to consider asking AWS to increase your burst limit.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Reserved concurrency" data-type="sect3"><div class="sect3" id="reserved-concurrency">&#13;
<h3>Reserved concurrency</h3>&#13;
&#13;
<p>We<a data-primary="reserved concurrency" data-type="indexterm" id="idm46222412563848"/> just mentioned earlier that one Lambda function that has scaled particularly wide can impact the rest of the account by using all of the available concurrency.&#13;
Lambda has a tool to help with this—the optional <em>reserved concurrency</em> configuration that can be applied to a function’s configuration.</p>&#13;
&#13;
<p>Setting a reserved concurrency value does two things:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>It guarantees that the particular function will always have up to that available amount of concurrency, no matter what any other functions are doing in the account.</p>&#13;
</li>&#13;
<li>&#13;
<p>It limits that function to scale <em>no wider</em> than that amount of concurrency.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>This second feature has some useful benefits that we discuss in <a data-type="xref" href="ch09.html#manage-scaling-with-reserved-concurrency">“Solution: Manage scaling with reserved concurrency”</a>.</p>&#13;
&#13;
<p>If you are using SAM to define your application’s infrastructure, you can use the <code>ReservedConcurrentExecutions</code> property of the <code>AWS::Serverless::Function</code> resource type to declare a reserved concurrency setting.<a data-primary="" data-startref="Slimits08" data-type="indexterm" id="idm46222412556520"/><a data-primary="" data-startref="throttling08" data-type="indexterm" id="idm46222412555576"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Thread Safety" data-type="sect2"><div class="sect2" id="idm46222412591256">&#13;
<h2>Thread Safety</h2>&#13;
&#13;
<p>Because<a data-primary="scaling" data-secondary="thread safety" data-type="indexterm" id="idm46222412553336"/><a data-primary="thread safety" data-type="indexterm" id="idm46222412552328"/> of Lambda’s scaling model, we are guaranteed that at most one event will be processed per function instance at any one time.&#13;
In other words, you never need to be concerned about multiple events being processed at the same time within a function’s runtime, let alone within a function object instance.&#13;
Therefore, unless you create any of your own threads, Lambda programming is entirely thread safe.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="lambda-and-threading">&#13;
<h5>Lambda and Threading</h5>&#13;
<p>Applications<a data-primary="AWS Lambda" data-secondary="threading and" data-type="indexterm" id="idm46222412517384"/> spawn threads for a few reasons, typically to:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Provide scaling by enabling an application to handle multiple requests at one time in the same process</p>&#13;
</li>&#13;
<li>&#13;
<p>Perform parallel computation across a number of CPU cores</p>&#13;
</li>&#13;
<li>&#13;
<p>Perform nonblocking I/O against an external resource so that work can continue while the I/O request completes</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>Of these uses the first—spawning threads to scale to handle multiple requests—is unnecessary in Lambda.&#13;
As we’ve just described, the Lambda platform uses a process-based scaling model, invoking a different instance of the Lambda runtime per request.</p>&#13;
&#13;
<p>The second use is rare in Lambda development.&#13;
However, if you do need this capability, then Lambda will provide two execution cores if you specify more than 1792MB for your memory size.&#13;
Typically, however, if you need to perform parallel computation, you would “fan out” processing, like we did in <a data-type="xref" href="ch05.html#serverless-data-pipeline-example">“Example: Building a Serverless Data Pipeline”</a>.</p>&#13;
&#13;
<p>The final case is a common usage pattern, though, even in Lambda development, and one you may well come across.&#13;
As such, it’s important that you understand how Lambda interacts with threads that are spawned from your own code.</p>&#13;
&#13;
<p>The key is this section from the <a href="https://oreil.ly/K5Ukb">AWS Lambda Execution Context documentation</a>:</p>&#13;
<blockquote>&#13;
<p>Background processes or callbacks initiated by your Lambda function that did not complete when the function ended resume if AWS Lambda chooses to reuse the <span class="keep-together">execution</span> context.&#13;
You should make sure any background processes or callbacks in your code are complete before the code exits.</p></blockquote>&#13;
&#13;
<p>What this means is that you are free to create your own threads, but you should know two things:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>When you return from your handler function, those threads will be “frozen.”</p>&#13;
</li>&#13;
<li>&#13;
<p>If the Lambda runtime where you spawned threads is reused, then those threads will continue where they left off <em>for the previously processed event</em>.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>You’ll typically want to make sure that all spawned threads have completed processing before you return from your handler.&#13;
In the context of nonblocking external requests, this means that you’ll want to wait until either those requests have completed, or have timed out, before continuing processing.</p>&#13;
&#13;
<p>As a final note on this topic, remember that many Java libraries will create threads on your behalf, so be aware when using any libraries that may do so.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Vertical Scaling" data-type="sect2"><div class="sect2" id="idm46222412502312">&#13;
<h2>Vertical Scaling</h2>&#13;
&#13;
<p>Almost<a data-primary="scaling" data-secondary="vertical scaling" data-type="indexterm" id="idm46222412501048"/><a data-primary="vertical scaling" data-type="indexterm" id="idm46222412500040"/> all of Lambda’s scaling capability is “horizontal”—that is, its ability to scale wider to handle multiple events in parallel.&#13;
This is in contrast to “vertical” scaling—the ability to handle more load by increasing the computational capability of an individual node.</p>&#13;
&#13;
<p>Lambda also has a rudimentary vertical scaling option, however, in its memory <span class="keep-together">configuration</span>.&#13;
We discussed this in <a data-type="xref" href="ch03.html#memory-and-cpu">“Memory and CPU”</a>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Versions and Aliases, Traffic Shifting" data-type="sect1"><div class="sect1" id="versions-and-aliases">&#13;
<h1>Versions and Aliases, Traffic Shifting</h1>&#13;
&#13;
<p>In your experiments with Lambda so far, you may have occasionally seen the string "<code>$LATEST</code>" appear.&#13;
This is a reference to a Lambda function’s <em>version</em>.&#13;
There’s more to versions than just <code>$LATEST</code> though, so let’s take a look.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Lambda Versions" data-type="sect2"><div class="sect2" id="idm46222412493608">&#13;
<h2>Lambda Versions</h2>&#13;
&#13;
<p>Whenever<a data-primary="versions" data-secondary="invoking specific versions" data-type="indexterm" id="idm46222412492104"/><a data-primary="Lambda functions" data-secondary="invoking specific versions" data-type="indexterm" id="idm46222412491032"/><a data-primary="$LATEST" data-type="indexterm" id="idm46222412490072"/> we’ve deployed a new configuration, or new code, for our Lambda functions, we’ve always overridden what came before.&#13;
The old function was dead, long live the new function.</p>&#13;
&#13;
<p>However, Lambda supports keeping those old functions around if you want it to, by way of a capability named Lambda Function Versioning.</p>&#13;
&#13;
<p>Without using versioning explicitly, Lambda has exactly one version of your function at any one time.&#13;
Its name is <code>$LATEST</code>, which you can reference explicitly; alternatively, if you don’t specify a version (or alias, which we’ll see in a moment), you are also referring implicitly to <code>$LATEST</code>.</p>&#13;
&#13;
<p>When you create or update a function, however, you are able at the time, or some time later, to snapshot that function to a version.&#13;
The identifier of the version is a linear counter, starting at 1.&#13;
You can’t edit a version, which means that it only ever makes sense to create a versioned snapshot from the current <code>$LATEST</code> version.</p>&#13;
&#13;
<p>You invoke a version of a function when calling it explicitly by adding a <code>:VERSION-IDENTIFIER</code> to its ARN, or if using the AWS CLI, you can add a <code>--qualifier</code> <em><code>VERSION-IDENTIFIER</code></em> parameter to the <code>aws lambda invoke</code> command.</p>&#13;
&#13;
<p>You can create a version using various AWS CLI commands or the web console.&#13;
You can’t create a version explicitly using SAM, but you can do so implicitly when you use <em>aliases</em>, which we’ll explain next.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Lambda Aliases" data-type="sect2"><div class="sect2" id="idm46222412482152">&#13;
<h2>Lambda Aliases</h2>&#13;
&#13;
<p>While<a data-primary="Lambda functions" data-secondary="invoking aliases" data-type="indexterm" id="idm46222412480648"/><a data-primary="aliases" data-secondary="invoking" data-type="indexterm" id="idm46222412479640"/> you are able to explicitly reference a numbered version of a Lambda function, when using versions, it’s more typical to use an <em>alias</em>.&#13;
An alias is a named pointer to a Lambda version—either <code>$LATEST</code>, or a numeric, snapshotted version.&#13;
An alias can be updated at any time to point to a different version.&#13;
For example, you may start off pointing to <code>$LATEST</code>, but then point to a specific version when you want to add stability to the alias.</p>&#13;
&#13;
<p>You invoke an alias of a function in precisely the same way as you do with a function version—by specifying it in an ARN or in the <code>--qualifier</code> argument of the CLI.&#13;
An event source can be configured to point to a specific alias, and if the underlying alias is updated to point to a new version, then events from the source will flow to that new version.</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm46222412475640">&#13;
<h5>Be Careful If Introspecting the Invoked Alias</h5>&#13;
<p>One<a data-primary="aliases" data-secondary="introspecting" data-type="indexterm" id="idm46222412474056"/> useful thing to know if you’re using aliases and versions is that a Lambda function is able to know which alias or version was used to invoke the function, if any, by calling the <code>getInvokedFunctionArn()</code> method on the handler <code>Context</code> object.&#13;
For example, you may use this in your code to switch between different databases for two aliases named <code>DEV</code> or <code>PROD</code>.</p>&#13;
&#13;
<p>However, if both your <code>DEV</code> <em>alias</em> and your <code>PROD</code> <em>alias</em> are pointing to the same function <em>version</em>, then one function instance can handle events for both aliases—this is because the Lambda platform will reuse instances for a version no matter the alias that was involved.&#13;
Because of this, it’s imperative that any alias-specific logic you may have in your Lambda function is sensitive to this scenario. For example, you might choose to reset connections for each event invocation, or keep multiple cross-invocation state objects for different aliases.</p>&#13;
</div></aside>&#13;
&#13;
<p>When you deploy a Lambda function with SAM, you can define an alias that is automatically updated to point to the latest, published version.&#13;
You do this by adding the <code>AutoPublishAlias</code> property, and giving an alias name as a value.</p>&#13;
&#13;
<p>However, there’s a much more powerful way of using aliases with SAM.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Traffic Shifting" data-type="sect2"><div class="sect2" id="idm46222412466056">&#13;
<h2>Traffic Shifting</h2>&#13;
&#13;
<p>If<a data-primary="traffic shifting" data-type="indexterm" id="idm46222412464312"/><a data-primary="aliases" data-secondary="traffic shifting" data-type="indexterm" id="idm46222412463576"/> you use the <code>AutoPublishAlias</code> property of a Lambda function with SAM, all events from an event source immediately get routed to the new version of the function.&#13;
If something goes wrong, you can manually update the alias to point to the previous version.</p>&#13;
&#13;
<p>Lambda and SAM also have functionality to improve this process first by giving the opportunity to split traffic, sending some to the new version and some to the old version.&#13;
This means that if a problem occurs, and a rollback is required, not all traffic has been impacted by the problem.</p>&#13;
&#13;
<p>The second improvement is that a rollback can automatically be performed if an error is detected, where you have the opportunity to define how the error is calculated in a couple of different ways.</p>&#13;
&#13;
<p>There<a data-primary="CodeDeploy service" data-type="indexterm" id="idm46222412460088"/> are a number of moving pieces involved in getting this working—Lambda aliases, Lambda alias update policies, and use of the <a href="https://oreil.ly/t2gIB">AWS CodeDeploy</a> service.&#13;
Fortunately, SAM does a good job of wrapping all of this up for you so that you don’t need to worry about all of the gory details.&#13;
The main thing you need to do is add a <code><span class="keep-together">DeploymentPreference</span></code> property to your Lambda function in your SAM template, which is <a href="https://oreil.ly/EhJaS">thoroughly documented</a>.</p>&#13;
&#13;
<p>A choice you need to make when using traffic shifting is how you want your traffic to be shifted to the new alias. This breaks down into four options:</p>&#13;
<dl>&#13;
<dt>All at once</dt>&#13;
<dd>&#13;
<p>While this may sound the same at first glance as <code>AutoPublishAlias</code> it’s actually a lot more powerful, since you have the opportunity to automatically roll back deployment through<a data-primary="hooks" data-type="indexterm" id="hook08"/> “hooks,” as we’ll describe in a moment.&#13;
This is a fully automated implementation of <a href="https://oreil.ly/qowK1"><em>Blue Green Deployment</em></a> for Lambda.</p>&#13;
</dd>&#13;
<dt>Canary</dt>&#13;
<dd>&#13;
<p>Send a small percentage of traffic to the new version, and if it works, then send the remaining traffic; otherwise, roll back.</p>&#13;
</dd>&#13;
<dt>Linear</dt>&#13;
<dd>&#13;
<p>Similar to Canary, but send increasing percentages of traffic to the new version, still allowing for rollback.</p>&#13;
</dd>&#13;
<dt>Custom</dt>&#13;
<dd>&#13;
<p>Decide for yourself how you want traffic to split across the old and new aliases.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>As<a data-primary="rollbacks" data-type="indexterm" id="idm46222412446728"/><a data-primary="versions" data-secondary="rolling back" data-type="indexterm" id="idm46222412445992"/><a data-primary="aliases" data-secondary="rolling back" data-type="indexterm" id="idm46222412445048"/><a data-primary="alarms, building for metrics" data-type="indexterm" id="idm46222412444104"/> we mentioned already, a powerful element to this feature is that automatic rollback can be implemented via two different mechanisms—<em>hooks</em> and <em>alarms</em>.</p>&#13;
&#13;
<p><em>Hook</em>-triggered rollback is available to any of the previous schemes.&#13;
You can define <em>pretraffic hooks</em> and/or <em>posttraffic hooks</em>.&#13;
These hooks are simply other Lambda functions that will run whatever logic they need to decide whether deployment has been successful—either before any traffic is routed to the new alias or after all traffic has been shifted.</p>&#13;
&#13;
<p><em>Alarms</em> are available with schemes that offer gradual traffic shifting.&#13;
You can define any number of <em>CloudWatch Alarms</em> (which we discussed in <a data-type="xref" href="ch07.html#cloudwatch-alarms">“Alarms”</a>), and if any of those alarms transition to their <em>alarm</em> state, then a rollback to the original alias will be performed.</p>&#13;
&#13;
<p>For more details on Lambda traffic shifting, see the <a href="https://oreil.ly/SXGLS">SAM documentation</a>.<a data-primary="" data-startref="hook08" data-type="indexterm" id="idm46222412436760"/></p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="When (Not) to Use Versions and Aliases" data-type="sect2"><div class="sect2" id="idm46222412465384">&#13;
<h2>When (Not) to Use Versions and Aliases</h2>&#13;
&#13;
<p>Lambda’s<a data-primary="versions" data-secondary="when not to use" data-type="indexterm" id="idm46222412434344"/><a data-primary="aliases" data-secondary="when not to use" data-type="indexterm" id="idm46222412433336"/> traffic shifting capability is very powerful, and if you don’t already have a canary release scheme upstream of your Lambda code, then it may well be useful for you.</p>&#13;
&#13;
<p>However, apart from traffic shifting, we try to steer away from versions and aliases.&#13;
We find that they typically add unnecessary complexity, and instead we prefer to use alternative techniques.&#13;
For example, for separating development and production versions of code, we prefer to use different deployed stacks.&#13;
For “rolling back” code, our preference is to use a fast-running deployment pipeline, and roll back at the source repository, triggering a new commit through the pipeline.</p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Very occasionally you’ll see that some event sources use, and recommend, using Lambda aliases.&#13;
One example of this is when integrating Lambda with <a href="https://oreil.ly/4U1ZD">AWS Application Load Balancer (ALB)</a>.</p>&#13;
</div>&#13;
&#13;
<p>If you do use versions and aliases, be aware of a couple of “gotchas,” beyond the function instance warning earlier:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Versions do not automatically clean up after themselves, so periodically you’ll want to delete old versions. Otherwise, you may find you hit your account-level “function and layer storage” limit of 75GB.</p>&#13;
</li>&#13;
<li>&#13;
<p>The default CloudWatch metrics views in the AWS Web Console for Lambda are a little odd when you’re using aliases and versions.&#13;
Make sure you’re being explicit about which version(s) or alias(es) you want to view data for when you’re using CloudWatch metrics in this way.</p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cold Starts" data-type="sect1"><div class="sect1" id="cold-starts">&#13;
<h1>Cold Starts</h1>&#13;
&#13;
<p>Now<a data-primary="cold starts" data-secondary="defined" data-type="indexterm" id="idm46222412423800"/> we move on to the thorny subject of <em>cold starts</em>.&#13;
Depending on who you talk to, cold starts may be a minor footnote in the life of a Lambda developer, or it may be a complete blocker to Lambda even being considered a valid computation platform.&#13;
We find how best to approach cold starts is somewhere between these two points—worth understanding and treating with rigor, but not a deal-breaker in most situations.</p>&#13;
&#13;
<p>But what are cold starts, when do they happen, what impact do they have, and how can we mitigate them?&#13;
There’s a lot of fear, uncertainty, and doubt (FUD) surrounding cold starts, and we hope to remove some of that FUD for you here.&#13;
Let’s dive in.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What Is a Cold Start?" data-type="sect2"><div class="sect2" id="idm46222412421080">&#13;
<h2>What Is a Cold Start?</h2>&#13;
&#13;
<p>Back in <a data-type="xref" href="ch03.html#ch03">Chapter 3</a>, we explored the chain of activity (<a data-type="xref" href="ch03.html#lambda-execution-environment">Figure 3-1</a>) that occurs when a Lambda function is invoked for the first time—from starting a host Linux environment through to calling our handler function.&#13;
In between those two activities the JVM will be started, the Lambda Java Runtime will be started, our code will be loaded, and depending on the precise nature of our Lambda function, more may happen besides.&#13;
We collectively group this chain into something we call a <em>cold start</em>, and it results in a new <em>instance</em> (an execution environment, a runtime, and our code) of our Lambda function being available to process events.</p>&#13;
&#13;
<p>An important point here is that all of this activity occurs <em>when our Lambda function is invoked</em>, not before.&#13;
In other words, Lambda doesn’t create function instances solely when Lambda code is deployed—it creates them <em>on demand</em>.</p>&#13;
&#13;
<p>However, cold<a data-primary="freezing and thawing instances" data-type="indexterm" id="idm46222412414872"/><a data-primary="instances, freezing and thawing" data-type="indexterm" id="idm46222412414120"/><a data-primary="Lambda functions" data-secondary="freezing and thawing instances" data-type="indexterm" id="idm46222412413432"/> starts are special occurrences, rather than something that happens on every invocation, because typically Lambda won’t perform a cold start for every event that triggers our function.&#13;
This is because once our function has finished executing, Lambda can <a href="https://oreil.ly/YrC-W"><em>freeze</em></a> the instance and keep it around for a little while in case another event happens soon.&#13;
If an event does happen soon, then Lambda will <em>thaw</em> the instance and call it with the event.&#13;
For many Lambda functions, cold starts in fact occur less than 1% of the time, but it’s still useful to know when they do occur.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="When Does a Cold Start Occur?" data-type="sect2"><div class="sect2" id="idm46222412410184">&#13;
<h2>When Does a Cold Start Occur?</h2>&#13;
&#13;
<p>A<a data-primary="cold starts" data-secondary="occurrences of" data-type="indexterm" id="idm46222412408712"/><a data-primary="Lambda functions" data-see="also cold starts" data-type="indexterm" id="idm46222412407704"/> cold start is necessary whenever there is no existing function instance available to process an event. This situation happens at the following times:</p>&#13;
<ol>&#13;
<li>&#13;
<p>When a Lambda function’s code or configuration changes (including when the first version of a function is deployed)</p>&#13;
</li>&#13;
<li>&#13;
<p>When all previous instances have been expired due to inactivity</p>&#13;
</li>&#13;
<li>&#13;
<p>When all previous instances have been “reaped” due to age</p>&#13;
</li>&#13;
<li>&#13;
<p>When Lambda needs to scale out because all current instances for the required function are already processing events</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
&#13;
<p>Let’s look at these four types of occurrence in a little more detail.</p>&#13;
<ol>&#13;
<li>&#13;
<p>When we deploy our function for the first time, Lambda will create an instance of our function, as we’ve already seen.&#13;
However, Lambda will also create a new instance whenever a function is invoked after we deploy a new version of the function code, or when we change the Lambda configuration of our functions.&#13;
Such configuration doesn’t just cover environment variables—it also covers runtime aspects like timeouts, memory settings, DLQ, etc.</p>&#13;
&#13;
<p>A corollary of this is that one instance of a Lambda function is guaranteed to have the same code and configuration no matter how many times it is called.</p>&#13;
</li>&#13;
<li>&#13;
<p>Lambda will keep function instances around for a little while in case another event happens “soon.”&#13;
The precise definition of <em>soon</em> is not documented, but it can be anywhere between a few minutes and a few hours (and is not necessarily constant).&#13;
In other words, if your function processes an event, and then a minute later another event occurs, there’s a very good chance the second event will be processed using the same instance of your function that was used to process the first event.&#13;
However, if there’s a day or more between events, your function will likely experience a cold start for every event.&#13;
In the past, some people have used a “ping hack” to work around this and keep their function “alive,” but in late 2019 AWS introduced Provisioned Concurrency (see <a data-type="xref" href="#provisioned-concurrency">“Provisioned Concurrency”</a>) to solve this kind of concern.</p>&#13;
</li>&#13;
<li>&#13;
<p>Even if your Lambda event is fairly active, Amazon doesn’t keep instances around forever, even if they’re being used every few seconds.&#13;
How long AWS will keep instances around is, again, undocumented, but at time of writing we see instances lasting five to six hours, and after that they’re killed off.</p>&#13;
</li>&#13;
<li>&#13;
<p>Finally, a cold start will occur if all current instances of a function are already busy processing events and Lambda “scales out,” as we described this earlier in this chapter.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Identifying Cold Starts" data-type="sect2"><div class="sect2" id="idm46222412393992">&#13;
<h2>Identifying Cold Starts</h2>&#13;
&#13;
<p>How<a data-primary="cold starts" data-secondary="identifying" data-type="indexterm" id="idm46222412392488"/> can you tell when a cold start has occurred? There are many ways of doing so, but here are a few.</p>&#13;
&#13;
<p>First, you’ll notice a latency spike.&#13;
Cold starts typically add anywhere from 100 milliseconds to 10 seconds to the latency of your function, depending on the makeup of your function.&#13;
Therefore, if your function typically takes less than that, a cold start will be easy to see in the function’s latency metrics.</p>&#13;
&#13;
<p>Next you’ll be able to tell when a cold start has occurred due to a way that Lambda’s logging works.&#13;
As we discussed in <a data-type="xref" href="ch07.html#lambda-and-cloudwatch-logs">“Lambda and CloudWatch Logs”</a>, when Lambda functions log, the output is captured in CloudWatch Logs.&#13;
All of the log output for one function is available in one CloudWatch Log <em>group</em>, but each instance of a function will write to a separate log <em>stream</em>, within the log group.&#13;
Therefore if you see the number of log streams within a log group increase then you know a cold start has occurred.</p>&#13;
&#13;
<p>Also, you can track cold starts yourself within code.&#13;
Since the Java object encapsulating your handler is instantiated only once per instance of the actual function runtime, any instance member or static member initialization will happen at cold start, and never again for the lifetime of the function instance.&#13;
Therefore, if you add a constructor, or static initializer, to your code, it will be called only when the function is experiencing a cold start.&#13;
You can add explicit logging to your handler class constructor to see a cold start occurring in your function logs. Alternatively, we saw examples of identifying cold starts earlier in this chapter.</p>&#13;
&#13;
<p>You can also identify cold starts using X-Ray and some third-party Lambda monitoring tools.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Impact of Cold Starts" data-type="sect2"><div class="sect2" id="idm46222412386168">&#13;
<h2>Impact of Cold Starts</h2>&#13;
&#13;
<p>So<a data-primary="cold starts" data-secondary="impact of" data-type="indexterm" id="idm46222412384664"/> far we’ve described what cold starts are, when they happen, and how you can identify them.&#13;
But why should you care about cold starts?</p>&#13;
&#13;
<p>As we just mentioned in the previous section, one way to identify a cold start is that you’ll typically see a latency spike in your event processing when one occurs, and this is most often why people are concerned about them.&#13;
While end-to-end latency of a small Lambda function might be 50 ms in a usual case, a cold start could add <em>at least</em> 200 ms to this amount, and, depending on various factors, may add seconds, or even tens of seconds.&#13;
The reasons that cold starts add latency are because of all the steps that need to occur during creation of a function instance.</p>&#13;
&#13;
<p>Does this mean that we <em>always</em> need to care about cold starts?&#13;
That depends a lot on what your Lambda function is doing.</p>&#13;
&#13;
<p>For instance, say your function is asynchronously processing objects created in S3, and you are ambivalent as to whether it takes minutes to process such objects.&#13;
Do you care about cold starts in this situation?&#13;
Probably not.&#13;
Especially when you consider that S3 has no guaranteed subsecond delivery of events anyway.</p>&#13;
&#13;
<p>Here’s another example of where you likely won’t care too much about cold starts: say that you have a function that is processing messages from Kinesis, that each event takes about 100 ms to process, and that there’s typically always enough data to keep your Lambda functions busy.&#13;
In this case, one instance of your Lambda function may process 200,000 events before it gets “reaped.” In other words <em>cold starts might only affect 0.0005% of Lambda invocations</em>.&#13;
Even if a cold start added 10 seconds to your startup latency, it’s highly likely that you’ll be OK with such an impact in this scenario, when you consider amortizing that time over the lifetime of an instance.</p>&#13;
&#13;
<p>On the other hand, say you’re building a web application, and there’s a particular element that calls a Lambda function, but that function gets called in AWS only once per hour.&#13;
This might mean you’re getting a cold start every time the function is invoked.&#13;
Further, let’s say for this particular function that the cold start overhead is five seconds.&#13;
Is this a problem?&#13;
It might be.&#13;
If so, can this overhead be reduced?&#13;
Perhaps, and we’ll talk about that in the next section.</p>&#13;
&#13;
<p>Although the concern with cold starts is almost always about latency overhead, it’s also important to note that if your function loads data from a downstream resource at startup, it will be doing that every time a cold start occurs.&#13;
You may want to consider this when you’re thinking about the impact your Lambda functions have on downstream resources, especially when all of your instances cold start after a deployment.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Mitigating Cold Starts" data-type="sect2"><div class="sect2" id="idm46222412376792">&#13;
<h2>Mitigating Cold Starts</h2>&#13;
&#13;
<p>Cold starts<a data-primary="cold starts" data-secondary="mitigating" data-type="indexterm" id="CSmitiga08"/> will always occur with Lambda, and unless we use Provisioned Concurrency (described in the next section), such cold starts will always, occasionally, affect our function’s performance.&#13;
If cold starts are causing you a problem, there are various techniques you can use to mitigate their impact.&#13;
Just make sure that they really are causing you a problem, though—like other forms of performance optimization, you want to make sure you do this work only if it’s truly necessary.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Reduce artifact size" data-type="sect3"><div class="sect3" id="idm46222412373256">&#13;
<h3>Reduce artifact size</h3>&#13;
&#13;
<p>Often<a data-primary="artifacts" data-secondary="reducing size of" data-type="indexterm" id="idm46222412371784"/> the most effective tool in reducing cold start impact is to reduce the size of our code artifact. We can do that in two main ways:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Reduce the amount of our own code in the artifact to just that needed by the Lambda function (where “amount” means both size and number of classes).</p>&#13;
</li>&#13;
<li>&#13;
<p>Prune dependencies so that only libraries that our Lambda function needs are stored in the artifact.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>There are a couple of follow-on techniques here. First, create a different artifact for each of your Lambda functions, and execute the tasks for each artifact.&#13;
This was the point of the effort we went to in <a data-type="xref" href="ch05.html#ch05">Chapter 5</a> when we created the multimodule Maven project.</p>&#13;
&#13;
<p>Second, if you want to optimize library dependencies further, then consider <em>breaking depended-upon libraries apart to just the code you need</em>.&#13;
And perhaps even re-implement library functionality in your own code.&#13;
Obviously there’s some work necessary here to do this correctly and safely, but it might be a useful technique for you.</p>&#13;
&#13;
<p>These techniques reduce cold starts in two ways.&#13;
First, there’s simply a smaller artifact to copy and unpack before the runtime starts.&#13;
But furthermore, there’s less code for your runtime to load and initialize.</p>&#13;
&#13;
<p>All of these techniques are somewhat unusual in modern server-side software development.&#13;
We’ve become used to being able to add dependencies willy-nilly to our projects, creating multi-hundred-megabyte deployment artifacts while Maven or NPM “download the internet.”&#13;
This is typically sufficient in traditional server-side development since disk space is cheap, networks are fast, and most importantly, we don’t care too much about startup time for our servers, at least not on the order of a few seconds here and there.</p>&#13;
&#13;
<p>But with functions as a service (FaaS), and Lambda in particular, we care about startup time to a much more significant extent, so we need to be more judicious with how we build and package our software.</p>&#13;
&#13;
<p>To prune dependencies in JVM projects, you may want to consider using the <a href="https://oreil.ly/RZYMF">Apache Maven Dependency plug-in</a>, which will report on how dependencies in your project are used, or a similar tool.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Use a more load-speed-efficient packaging format" data-type="sect3"><div class="sect3" id="idm46222412362008">&#13;
<h3>Use a more load-speed-efficient packaging format</h3>&#13;
&#13;
<p>As<a data-primary="building and packaging Lambda functions" data-secondary="load-speed-efficient formats" data-type="indexterm" id="idm46222412360616"/> we called out in <a data-type="xref" href="ch04.html#ch04">Chapter 4</a>, <a href="https://oreil.ly/_S6Bb">AWS recommends</a> the ZIP file approach to packaging a Lambda function, over the uberjar<a data-primary="uberjars" data-type="indexterm" id="idm46222412357880"/> approach, because it decreases the time Lambda needs to unpack your deployment artifact.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Reduce startup logic" data-type="sect3"><div class="sect3" id="idm46222412356600">&#13;
<h3>Reduce startup logic</h3>&#13;
&#13;
<p>Later<a data-primary="startup logic, reducing" data-type="indexterm" id="idm46222412355128"/><a data-primary="Lambda functions" data-secondary="state and" data-type="indexterm" id="idm46222412354392"/> in this chapter, we’ll look at state in Lambda functions.&#13;
Despite what you may have heard, Lambda functions aren’t stateless; they just have an unusual model when it comes to thinking about state.</p>&#13;
&#13;
<p>A fairly common thing to do with Lambda functions is to create or load various resources when the function is first invoked.&#13;
We saw this to a small extent in the examples in <a data-type="xref" href="ch05.html#ch05">Chapter 5</a> when we initialized our serialization libraries and SDKs.&#13;
However for some functions, it makes sense to grab this idea by the horns and create a large local cache, loaded from some other resources, in the name of more quickly handling events during the lifetime of the instance.</p>&#13;
&#13;
<p>Such startup logic doesn’t happen for free though, and will increase cold start time.&#13;
If you are loading initial resources at cold start, you may find that you have a trade-off to make between how much you improve the performance of subsequent invocations versus how long the initial invocation takes.&#13;
If possible, you may want to consider if you can gradually “warm” your function’s local cache over a series of initial <span class="keep-together">invocations</span>.</p>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>One big cause of slow startup is the use of application frameworks like Spring.&#13;
As we discuss later (see <a data-type="xref" href="#java-application-frameworks">“Lambda and Java Application Frameworks”</a>), we strongly discourage the use of such frameworks with Lambda.&#13;
If cold starts are causing you a problem, and you’re using an application framework, then we recommend your first course of action should be to investigate whether you can remove the framework from your Lambda function.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Language choice" data-type="sect3"><div class="sect3" id="idm46222412347304">&#13;
<h3>Language choice</h3>&#13;
&#13;
<p>Another<a data-primary="AWS Lambda" data-secondary="language selection" data-type="indexterm" id="idm46222412345864"/><a data-primary="language selection" data-type="indexterm" id="idm46222412344856"/> area that can impact cold start time is the choice of language runtime.&#13;
JavaScript, Python, and Go simply take less time to start up than the JVM or .NET runtime.&#13;
Therefore, if you’re writing a small function that isn’t called often, and you care about reducing cold start impact as much as possible, you may want to use either JavaScript, Python, or Go over Java, all other development aspects being equal.</p>&#13;
&#13;
<p>Because of this difference in startup time, we often hear people dismiss the JVM and .NET runtimes as Lambda runtimes in general, but this is a short-sighted opinion.&#13;
For instance, in the situation we described earlier with the Kinesis processing function, what if, on average, the JVM function took 80 ms to process an event, but a JavaScript equivalent took 120 ms?&#13;
In this case, you would literally be paying twice as much for the JavaScript version of your code to run (since billable Lambda time is rounded up to the next 100 ms).&#13;
In this situation, JavaScript may be the wrong choice of runtime.</p>&#13;
&#13;
<p>It’s perfectly possible to use alternative (non-Java) JVM languages within Lambda (which we talk about more at the end of this chapter).&#13;
One important aspect to remember, though, is that typically these languages come with their own “language runtimes” and libraries, and both of these will increase cold start time.</p>&#13;
&#13;
<p>Finally, on the topic of language choice, it’s worth keeping some perspective when it comes to impact of language on cold start, or event-processing, performance.&#13;
The most important factor in language choice is how effectively you can build and maintain your code—the human element of software development.&#13;
The cost of runtime performance differences between Lambda language runtimes may pale in comparison with this.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Memory and CPU" data-type="sect3"><div class="sect3" id="idm46222412340888">&#13;
<h3>Memory and CPU</h3>&#13;
&#13;
<p>Certain<a data-primary="Lambda functions" data-secondary="memory and CPU" data-type="indexterm" id="idm46222412339560"/><a data-primary="memory-size setting" data-type="indexterm" id="idm46222412338552"/> aspects of your function’s configuration can also affect cold start time.&#13;
One of the primary examples of this is the <code>MemorySize</code> setting you choose.&#13;
A larger memory setting also gives more CPU resources, and therefore a larger memory setting may speed up the time it takes your JVM code to JIT&#13;
compile.<a data-primary="" data-startref="CSmitiga08" data-type="indexterm" id="idm46222412337032"/></p>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>Until late 2019, another configuration setting of a Lambda function that could&#13;
significantly increase cold start time was whether you were using a <em>virtual private cloud (VPC)</em>.&#13;
We discuss VPCs in general later in this chapter, but for now all you need to know is that if you see&#13;
any documentation anywhere warning of awful Lamdba startup times because of VPCs, then you can sit happy&#13;
in the knowledge that this has now been resolved.&#13;
For more details on what AWS did to improve this,&#13;
see&#13;
<a href="https://oreil.ly/UnES6">this&#13;
article</a>.</p>&#13;
</div>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Provisioned Concurrency" data-type="sect2"><div class="sect2" id="provisioned-concurrency">&#13;
<h2>Provisioned Concurrency</h2>&#13;
&#13;
<p>In<a data-primary="cold starts" data-secondary="Provisioned Concurrency" data-type="indexterm" id="CSprocon08"/><a data-primary="Provisioned Concurrency" data-type="indexterm" id="provcon08"/><a data-primary="costs" data-secondary="Provisioned Concurrency" data-type="indexterm" id="Cprocon08"/> late 2019 AWS announced a new Lambda feature—<em>Provisioned Concurrency</em>.&#13;
Provisioned Concurrency (PC) allows an engineer to effectively “pre-warm” Lambda functions, thereby removing (almost) all of the impact of cold starts.&#13;
Before we describe how to use this feature, here are some important caveats:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>PC breaks the request-based cost model of Lambda.&#13;
With PC you pay whether your functions are invoked or not.&#13;
Using Lambda with PC therefore negates one of the main benefits of serverless: costs that scale to zero (see <a data-type="xref" href="ch01.html#lambda-as-faas">“FaaS as Implemented by Lambda”</a>).</p>&#13;
</li>&#13;
<li>&#13;
<p>To avoid paying for costs related to peak usage, you need to manually configure AWS Auto Scaling with PC (see <a href="https://oreil.ly/9x0D6">this AWS blog article on how to implement this</a>).&#13;
This is extra operational overhead on your part.</p>&#13;
</li>&#13;
<li>&#13;
<p>PC adds significant deployment time overhead.&#13;
In our experiments, at the time of writing, deploying a Lambda function with a PC setting of 1 (see below as to what this means) has an overhead of about four minutes.&#13;
Using a setting of 10 or 100 is about seven minutes.</p>&#13;
</li>&#13;
<li>&#13;
<p>PC requires using either versions or aliases, which we described earlier in this chapter (see <a data-type="xref" href="#versions-and-aliases">“Versions and Aliases, Traffic Shifting”</a>).&#13;
As we mentioned in that section, we do not recommend using versions or aliases in most cases, due to the extra complexity they bring.</p>&#13;
</li>&#13;
</ul>&#13;
<div data-type="warning" epub:type="warning"><h6>Warning</h6>&#13;
<p>Given these significant caveats, our recommendation is that you only reach for Provisioned Concurrency if you <em>absolutely need to</em>.&#13;
As we mention in the summary of this section, we find that most teams that are concerned initially about cold starts find that they are of no effective consequence once they start using Lambda at scale in production, especially if the teams follow the other advice we give in this chapter about cold start mitigation.</p>&#13;
</div>&#13;
&#13;
<p>Now, we’ve told you why you almost certainly shouldn’t use Provisioned Concurrency, let’s talk about what it is!</p>&#13;
&#13;
<p>PC, at its simplest, is a numerical value (<em>n</em>) that tells the Lambda platform to always have <em>at least</em> <em>n</em> execution environments of your function in a “warm” state.&#13;
“Warm” here means that the execution environment has been created, and your Lambda function handler code has been instantiated.&#13;
In fact, the entire execution chain (see <a data-type="xref" href="ch03.html#lambda-execution-environment">Figure 3-1</a>) is performed during warming, apart from actually calling your handler method.</p>&#13;
&#13;
<p>Since under a PC context Lambda won’t call a nonwarmed function (apart from one caveat about scaling, which we’ll describe in a moment), this guarantees that you won’t have any performance-impacting cold starts at all!&#13;
In other words, <em>all</em> of your function invocations will respond in their regular “warm” time.</p>&#13;
&#13;
<p>Another nice aspect to PC is that it is defined solely in deployment configuration—no change to your code is required to use it (although you may want to change your code, as we will describe about code instantiation in a moment).</p>&#13;
&#13;
<p>Let’s look at an example. Say that we have the following function configured in our SAM template:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">HelloWorldLambda</code><code class="p">:</code>&#13;
<code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">AWS::Serverless::Function</code>&#13;
<code class="nt">Properties</code><code class="p">:</code>&#13;
  <code class="nt">Runtime</code><code class="p">:</code> <code class="l-Scalar-Plain">java8</code>&#13;
  <code class="nt">MemorySize</code><code class="p">:</code> <code class="l-Scalar-Plain">512</code>&#13;
  <code class="nt">Handler</code><code class="p">:</code> <code class="l-Scalar-Plain">book.HelloWorld::handler</code>&#13;
  <code class="nt">CodeUri</code><code class="p">:</code> <code class="l-Scalar-Plain">target/lambda.zip</code>&#13;
  <code class="nt">AutoPublishAlias</code><code class="p">:</code> <code class="l-Scalar-Plain">live</code>&#13;
  <code class="nt">ProvisionedConcurrencyConfig</code><code class="p">:</code>&#13;
    <code class="nt">ProvisionedConcurrentExecutions</code><code class="p">:</code> <code class="l-Scalar-Plain">1</code></pre>&#13;
&#13;
<p>The new lines here are those last three.&#13;
First you’ll see that we’re using an alias—PC requires configuring a <code>ProvisionedConcurrentExecutions</code> value for each version or alias that we want PC for.&#13;
We can’t configure a <code>ProvisionedConcurrentExecutions</code> value for <code>$LATEST</code>—the default version.</p>&#13;
&#13;
<p>In this example, we then specify that we want to always have one instance of our Lambda function pre-warmed.</p>&#13;
&#13;
<p>When we deploy this function for the first time, Lambda will instantiate the Java class <code>HelloWorld</code>, which contains our handler, even before any invocations occur.&#13;
Then, when an event is received for the function, Lambda calls this pre-warmed function.&#13;
When we <em>redeploy</em> the function, Lambda will keep routing requests to the old (warm) version and start using the new version only once all the provisioned instances for that version have been created.&#13;
Again, this makes sure that function invocation isn’t impacted by cold starts.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>In other third-party Lambda documentation, you may see recommendations to use a secondary, scheduled, “ping” function that calls the application function, to avoid cold starts.&#13;
PC, with a setting of 1, in almost any case is a more effective replacement of such a mechanism.</p>&#13;
</div>&#13;
&#13;
<p>Now, let’s cover a few details you should be aware of.</p>&#13;
&#13;
<p>First, pricing.&#13;
As mentioned, PC has (at the time of writing) a different cost model to regular “on-demand” Lambda.&#13;
As described in <a data-type="xref" href="ch03.html#how-expensive-is-lambda">“How Expensive Is Lambda?”</a>, on-demand Lambda costs are based on how many requests your Lambda function receives and how long your Lambda function is executing (duration).&#13;
For PC you still pay the request cost, and a (smaller) amount for duration, but you <em>also</em> pay a charge for the entire time your function is deployed, not just when it is processing requests.</p>&#13;
&#13;
<p>Let’s build on  <a data-type="xref" href="ch03.html#how-expensive-is-lambda">“How Expensive Is Lambda?”</a>, specifically the example for the web API.&#13;
Our cost estimate for just on-demand Lambda was $21.60/month.&#13;
How much does it cost using Provisioned Concurrency?</p>&#13;
&#13;
<p>Again, we’ll assume 512-MB RAM, less than 100 ms to process a request and 864,000 requests/day.&#13;
Let’s start with using a PC value of 10, since that’s what we expect to peak up to.&#13;
In this scenario, our Lambda costs are as follows:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The request cost is unchanged at $5.18/month.</p>&#13;
</li>&#13;
<li>&#13;
<p>The duration cost is 0.1 × 864000 × 0.5 × $0.000009722 = $0.42/day, or $12.60/month.</p>&#13;
</li>&#13;
<li>&#13;
<p>The Provisioned Concurrency cost is 10 × 0.000004167 × 0.5 × 86400 = $1.80/day, or $54/month.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>The total cost therefore has increased by a little over three times from approximately $22/month to $72/month. Yikes!</p>&#13;
&#13;
<p>Now, this is likely a “worst case” since we are setting PC at peak.&#13;
One option we have is to manually configure auto-scaling for PC.&#13;
This is described on the <a href="https://oreil.ly/8p8K6">AWS blog introducing PC</a>.&#13;
Let’s say that doing this means our PC configuration averages around 2.&#13;
In this case, our total costs are $29/month.&#13;
This is still 30% more expensive than on-demand, plus now we have the added complexity of managing PC auto-scaling.</p>&#13;
&#13;
<p>There are some scenarios where if you have a very consistent usage model, then PC works out cheaper than on-demand, but in most cases you should expect to pay a significant overhead to use PC.</p>&#13;
&#13;
<p>Another issue related to costs is that you probably want to have different configuration for development versus production to avoid paying “always-on” costs for <span class="keep-together">development</span> <span class="keep-together">environments</span>.&#13;
You can do this using CloudFormation techniques, but again this is extra mental overhead.<a data-primary="" data-startref="Cprocon08" data-type="indexterm" id="idm46222412268920"/></p>&#13;
&#13;
<p>That’s enough about costs. Let’s move on to a different subject!</p>&#13;
&#13;
<p>What happens if at a certain point in time you have more invocations than your PC configuration?&#13;
As we looked at earlier in this chapter, we know that Lambda always increases the number of active execution environments to satisfy load.&#13;
For example, say that Lambda needs to use an 11th execution environment for your function, but you have a PC setting of 10—what happens now?&#13;
In this case, Lambda will spin up a new execution environment in the “traditional” on-demand model to cover the extra load.&#13;
You will be charged for this extra capacity in the usual on-demand fashion, but be warned—the first event using that new extra environment will also incur cold-start latency in the normal way!</p>&#13;
&#13;
<p>Finally, a quick note on making the most of PC.&#13;
AWS has been doing a great job over the last few years in reducing the <em>platform</em> overhead of cold starts, so the main point of PC is mostly to mitigate <em>application</em> overhead—the time taken to instantiate your language runtime, code, and handler class.&#13;
This last element—class instantiation—is important since your handler class constructor is called during pre-warming.&#13;
Therefore, you’ll want to move as much application setup as possible to class and object instantiation time and not do this in the handler method itself.&#13;
We’ve used this pattern throughout the book, but it’s especially important if you’re using PC.</p>&#13;
&#13;
<p>Given all of our dire warnings about using PC, when do we recommend using it? Here are a few scenarios where we can imagine PC being useful:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>When you have a Lambda function called very infrequently (say once per hour, or longer) that you always want to return quickly (subsecond), and you are willing to pay the cost overhead.</p>&#13;
</li>&#13;
<li>&#13;
<p>If your application has extreme “burst” scale scenarios (see <a data-type="xref" href="#burst-limits">“Burst limits”</a>) that Lambda can’t handle by default, then you can pre-warm sufficient capacity.</p>&#13;
</li>&#13;
<li>&#13;
<p>If your function itself has significant code-level cold-start time (e.g., several seconds) that is not sufficient for application performance, and you have no other way to mitigate this.&#13;
This is typical if you’re using a heavyweight application framework within your Lambda code.<a data-primary="" data-startref="CSprocon08" data-type="indexterm" id="idm46222412259800"/><a data-primary="" data-startref="provcon08" data-type="indexterm" id="idm46222412258824"/></p>&#13;
</li>&#13;
</ul>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Cold Start Summary" data-type="sect2"><div class="sect2" id="idm46222412332088">&#13;
<h2>Cold Start Summary</h2>&#13;
&#13;
<p>Cold starts<a data-primary="cold starts" data-secondary="overview of" data-type="indexterm" id="idm46222412256440"/> might be nothing you need to ever spend too much effort on, depending on what you use Lambda for, but it’s certainly a topic that you should be aware of, since how cold starts are mitigated often runs counter to how we typically build and package systems.</p>&#13;
&#13;
<p>We mentioned <em>FUD</em> around cold starts earlier, and cold starts are also often “thrown under the bus” for latency problems that turn out to actually have nothing to do with cold starts at all.&#13;
Remember to perform proper latency analysis if you’re having latency concerns—make sure your actual problem isn’t, for example, how your code is interacting with a downstream system.</p>&#13;
&#13;
<p>Also make sure to continue to test latency over time, especially if you rule out a certain use of Lambda because of cold starts.&#13;
AWS has made, and continues to make, significant improvements in this part of the Lambda platform.</p>&#13;
&#13;
<p>In our experience, cold starts concern teams when they first use Lambda, especially under spiky development loads, but once they see how Lambda performs under production loads, they often never worry about cold starts again.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="State" data-type="sect1"><div class="sect1" id="idm46222412425144">&#13;
<h1>State</h1>&#13;
&#13;
<p>Almost any application needs to consider state.&#13;
Such state may be <em>persistent</em>—in other words, it captures data that is required to fulfill subsequent requests.&#13;
Alternatively, it may be <em>cached</em> state—a copy of data that is used to improve performance, where the persisted version is stored elsewhere.</p>&#13;
&#13;
<p>Despite how it’s occasionally perceived, Lambda is <em>not</em> stateless—data can be stored in memory and on disk both during and across requests.</p>&#13;
&#13;
<p>In-memory state is available via a handler method’s object and class members—any data loaded into such members is available the next time that function instance is invoked again, and a Lambda function can have up to a total of 3GB RAM (some of that will be used by the Lambda runtime).</p>&#13;
&#13;
<p>Lambda function instances also have access to 512MB of local disk storage in <em>/tmp</em>.&#13;
While this state is not automatically shared across function instances, it will, again, be available for subsequent invocations of the same function instance.</p>&#13;
&#13;
<p>However, the nature of Lambda’s runtime model significantly impacts how such state can be used.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Persistent Application State" data-type="sect2"><div class="sect2" id="idm46222412215368">&#13;
<h2>Persistent Application State</h2>&#13;
&#13;
<p>The way that Lambda creates function instances, especially in the way that it scales, has significant implications on architecture.&#13;
For example, we have absolutely no guarantee that sequential requests, for the same upstream client, will be handled by the same function instance.&#13;
There is no “client affinity” for Lambda functions.</p>&#13;
&#13;
<p>This means that we <em>cannot assume</em> that any state that was available locally (in-memory, or on local disk) in a Lambda function for one request will be available for a subsequent request.&#13;
This is true whether our function scales or not—scaling just underlines the point.</p>&#13;
&#13;
<p>Therefore, all persistent application state that we want to keep across Lambda function invocations must be <em>externalized</em>.&#13;
In other words, this means that any state we want to keep beyond an individual invocation has to be either stored downstream of our Lambda function—in a database, external file storage, or other downstream service—or it must be returned to the caller in the case of a synchronously called <span class="keep-together">function</span>.</p>&#13;
&#13;
<p>This might sound like a massive restriction, but in fact this way of building server-side software is not new.&#13;
Many people have been espousing the virtues of the <a href="https://12factor.net/"><em>12-factor architecture</em></a> for years, and this aspect of externalizing state is expressed within the sixth factor of that paradigm.</p>&#13;
&#13;
<p>That being said, this definitely is a constraint of Lambda, and may require you to significantly re-architect existing applications that you want to move to Lambda.&#13;
It may also mean that some applications that require particularly low latency to state (for example, gaming servers) are not good candidate applications for Lambda, nor are those that require a large data set in memory in order to perform adequately.</p>&#13;
&#13;
<p>There are various common services that people use to externalize their application state with Lambda:</p>&#13;
<dl>&#13;
<dt>DynamoDB</dt>&#13;
<dd>&#13;
<p>DynamoDB<a data-primary="NoSQL databases" data-type="indexterm" id="idm46222412205544"/> is the NoSQL database of AWS.&#13;
We used DynamoDB in the API example in <a data-type="xref" href="ch05.html#serverless-api-example">“Example: Building a Serverless API”</a>.&#13;
The benefits of DynamoDB are that it is fast, fairly easy to operate and configure, and has very similar scaling properties to Lambda.&#13;
The chief drawback to DynamoDB is that modeling data can get tricky.</p>&#13;
</dd>&#13;
<dt>RDS</dt>&#13;
<dd>&#13;
<p>AWS has various relational databases that it groups in the Relational/SQL Database Service (RDS) family, and all of these are available for use from Lambda.&#13;
One fairly new option within this family is <a href="https://oreil.ly/2Kc4E"><em>Aurora Serverless</em></a>—an auto-scaling version of Amazon’s own <em>Aurora</em> MySQL and Postgres engines, made for serverless applications.&#13;
The benefits of using a SQL database over a NoSQL one are decades of experience building such applications. The drawbacks, versus DynamoDB at least, typically are higher latencies and more operational overhead (with nonserverless RDS).</p>&#13;
</dd>&#13;
<dt>S3</dt>&#13;
<dd>&#13;
<p>Simple Storage Service (S3)—which we’ve used several times throughout this book—can be used as a data store for Lambda.&#13;
It’s simple to use, but isn’t particularly low latency, and also has limited querying capabilities in comparison with one of the database services, unless you also use <a href="https://aws.amazon.com/athena">Amazon Athena</a>.</p>&#13;
</dd>&#13;
<dt>ElastiCache</dt>&#13;
<dd>&#13;
<p>AWS offers a managed version of the Redis persistent cache application as part of its <a href="https://aws.amazon.com/elasticache">ElastiCache</a> family.&#13;
Of these four options, ElastiCache typically offers the fastest performance, but since it isn’t a true serverless service, it does require some operational overhead.</p>&#13;
</dd>&#13;
<dt>Custom downstream service</dt>&#13;
<dd>&#13;
<p>Alternatively, you may choose to implement your own in-memory persistence in a downstream service, built using traditional designs.</p>&#13;
</dd>&#13;
</dl>&#13;
&#13;
<p>AWS continues to make interesting developments in this area, and we recommend that you investigate all recently announced advances whenever you pick a persistence solution.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Caching" data-type="sect2"><div class="sect2" id="caching">&#13;
<h2>Caching</h2>&#13;
&#13;
<p>While we can’t rely on Lambda’s state capabilities for persistent application state, we absolutely can use them for caching data that is also stored elsewhere.&#13;
Put another way, while it’s true that we have no guarantee that one Lambda function instance will be called multiple times, we do know that it <em>probably will be</em>, depending on invocation frequency.&#13;
Because of this, cache state is a candidate for Lambda’s local storage.</p>&#13;
&#13;
<p>We can use either or both of Lambda’s in-memory or on-disk locations for cached data.&#13;
For example, say that we always need a set of fairly up-to-date reference data from a downstream service to process an event, but “fairly up-to-date” is on order of “valid within the last day.”&#13;
In this case, we can load the reference data once, for the first invocation of the function instance, and then store that data locally in a static or instance member variable.&#13;
Remember—our handler function instance object will be instantiated only once per runtime environment.</p>&#13;
&#13;
<p>As another example, say that we want to call an external program or library as part of our execution—Lambda gives us a full Linux environment with which to do this.&#13;
That program/library may be too big to fit in either a Lambda code artifact (which is restricted to at most 250MB when uncompressed) or even a Lambda layer (see later in this chapter about layers).&#13;
Instead, we can copy the external code from S3 to <em>/tmp</em> the first time we need it for a function instance, and then for subsequent requests for that instance the code will be available locally already.</p>&#13;
&#13;
<p>Both of these examples relate to state that consists of chunks of data—application data, or libraries and executables.&#13;
Another form of state in our Lambda applications are the runtime structures of our code itself, including those that represent connections to external services.&#13;
These runtime structures either may take some amount of time to create when the function is invoked, or in the case of connections to services may take time to initialize, e.g., for authentication procedures.&#13;
In either case, in Lambda, we will very often store these structures in program elements that live longer than the call to the method itself—in Java this means storing them in instance or static members.</p>&#13;
&#13;
<p>We showed examples of this earlier in the book. For example in <a data-type="xref" href="ch05.html#ch05">Chapter 5</a> at <a data-type="xref" href="ch05.html#EX5-3">Example 5-3</a> we store the following in instance members:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The <code>ObjectMapper</code> instance, because that is a program structure that takes some time to instantiate</p>&#13;
</li>&#13;
<li>&#13;
<p>The DynamoDB client, which is a connection to the external DynamoDB service</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>While we typically use this form of object caching for performance reasons in certain situations, it can also significantly improve the cost effectiveness of our overall system—see <a data-type="xref" href="ch09.html#cache-to-improve-costs">“Lambda Runtime Model and Cost Impact on Downstream Systems”</a> for more detail on this.</p>&#13;
&#13;
<p>Sometimes Lambda’s own state capabilities are insufficient—for example, our total cache state might be too large to fit in memory, too slow to load up during a cold start, or update frequently (updating a locally cached version in a Lambda function is a tricky thing to manage, although it can be done).&#13;
In such a case, you may choose to use one of the persistence services mentioned in the previous section as a caching <span class="keep-together">solution</span>.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Lambda and Java Application Frameworks" data-type="sect1"><div class="sect1" id="java-application-frameworks">&#13;
<h1>Lambda and Java Application Frameworks</h1>&#13;
<div data-type="note" epub:type="note"><h6>Note</h6>&#13;
<p>So far in this book most of our guidance has been how to use AWS Lambda, with a few warnings along the way.&#13;
We’re now going to take a brief tangent and talk about something we <em>don’t</em> recommend doing.</p>&#13;
</div>&#13;
&#13;
<p>Over the last two decades it’s been very common to build server-side Java applications using some kind of container and/or framework.&#13;
Back in the early 2000s, “Java Enterprise Edition” (J2EE) was all the rage, with application servers like WebLogic, WebSphere, and JBoss allowing you to build your apps with the Enterprise JavaBeans (EJB) or Servlet framework.&#13;
For those of you not around then we can promise you, from personal experience, that this was not a whole bunch of fun.</p>&#13;
&#13;
<p>People realized that these big servers were often unwieldy and/or expensive, and so they have been largely replaced by more “lightweight” equivalents, of which Spring is the most common.&#13;
Spring itself has evolved along the way, of course, into Spring Boot, and people also use various Java web frameworks to build applications.</p>&#13;
&#13;
<p>Because there is so much institutional knowledge in our industry on how to build “Java applications” with these tools, there’s a very large temptation to carry on using them, and just port the runtime from a running process to a Lambda function.&#13;
AWS has even put significant effort into supporting precisely this way of thinking, via the <a href="https://oreil.ly/T_ruW">serverless Java Container</a> project.</p>&#13;
&#13;
<p>While we admire AWS’s desire to “meet people where they are” in this way, we <em>strongly discourage</em> the use of most Java frameworks when building applications with Lambda, for the following reasons.</p>&#13;
&#13;
<p>First, building a complete app in a single Lambda function misses the fundamental point of Lambda.&#13;
Lambda functions are meant to be small, individual, short-lived functions that are event-driven, and programmed to accept a specific input event.&#13;
“Java applications,” on the other hand, are literally servers that have a lifecycle and state, and are typically designed to handle multiple types of request.&#13;
If you’re building miniservers, you’re not thinking serverlessly.</p>&#13;
&#13;
<p>Next, most application servers assume that there is some amount of shared state from request to request.&#13;
While it’s possible not to work this way, it’s not a natural-feeling way of working in these environments.</p>&#13;
&#13;
<p>Another reason we think this is a bad idea is that it detracts from the value provided by other AWS serverless services.&#13;
For example, with the AWS project mentioned earlier, API Gateway is used, but in a “full proxy” mode.&#13;
Here’s a snippet from the SAM template from the <a href="https://oreil.ly/KZYj3">Spring Boot example</a>:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">Resources</code><code class="p">:</code>&#13;
  <code class="nt">PetStoreFunction</code><code class="p">:</code>&#13;
    <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">AWS::Serverless::Function</code>&#13;
    <code class="nt">Properties</code><code class="p">:</code>&#13;
      <code class="nt">Events</code><code class="p">:</code>&#13;
        <code class="nt">GetResource</code><code class="p">:</code>&#13;
          <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">Api</code>&#13;
          <code class="nt">Properties</code><code class="p">:</code>&#13;
            <code class="nt">Path</code><code class="p">:</code> <code class="l-Scalar-Plain">/{proxy+}</code>&#13;
            <code class="nt">Method</code><code class="p">:</code> <code class="l-Scalar-Plain">any</code></pre>&#13;
&#13;
<p>Using API Gateway in this way means that all requests, no matter the path, are sent to one Lambda function, and routing behavior needs to be implemented in the Lambda function.&#13;
While Spring Boot can do that, (a) API Gateway will give you that functionality for free, and (b) it clutters up your Java code to keep it in the Lambda function.</p>&#13;
&#13;
<p>Earlier in the book we mentioned that on the whole we’re wary of using too many API Gateway features; for example, see the discussion of request and response mapping in <a data-type="xref" href="ch05.html#api-gateway-proxy-events">“API Gateway Proxy Events”</a>.&#13;
However, we feel that removing routing is typically a step too far down the line of abstracting out the use of API <span class="keep-together">Gateway</span>.</p>&#13;
&#13;
<p>As we discussed earlier on in the section on cold starts, application frameworks typically slow down function initialization.&#13;
While some people may argue that this is a good case to use Provisioned Concurrency, we would counter that this is a Band-Aid and not a solution.</p>&#13;
&#13;
<p>Finally, container and framework-based apps tend to have large distributable artifacts—partly because of the number of libraries depended upon, and partly, again, because such apps usually implement a number of functions.&#13;
Throughout this book we’ve been attempting to reduce the size of artifacts by minimizing dependencies, and dividing up applications into multiple distributable elements, all in the name of keeping our Lambda functions clean and lean.&#13;
Using an application framework runs counter to this way of thinking.</p>&#13;
&#13;
<p>In summary, building Java Lambda applications in this way is really a “square peg and round hole problem.”&#13;
Yes, you can make it work, but it’s inefficient, and you won’t get all the benefits of Lambda if you work in this way.&#13;
There’s a real danger of hitting a “local maximum” of value from Lambda, and assuming that there are no further upsides.</p>&#13;
&#13;
<p>So if we don’t recommend using these frameworks, how do we suggest you use your hard-earned knowledge and skills?</p>&#13;
&#13;
<p>Typically we find that programmers switching to “pure” Lambda development don’t take too long to shake off the frameworks they’ve been used to.&#13;
There’s a certain “lightness” that comes with just writing a handler function.&#13;
Also, there’s nothing wrong with bringing along old Java code to the party, as long as it’s not too ingrained in an application framework.&#13;
If you can extract your domain logic into something that just expresses your business needs, then you’re on the right path.</p>&#13;
&#13;
<p>Also, it’s still fine to use an ethos of “dependency injection” (DI), which the frameworks often provide.&#13;
You may choose to “hand roll” such DI (our preference), as you’ve seen in some of the examples (see <a data-type="xref" href="ch06.html#add-constructors">“Add Constructors”</a>). Alternatively, you can try to use a framework to provide just dependency injection, without the other features they often come with.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Virtual Private Clouds" data-type="sect1"><div class="sect1" id="idm46222412179000">&#13;
<h1>Virtual Private Clouds</h1>&#13;
&#13;
<p>In all of our examples so far any external resources called by a Lambda function have been secured via HTTPS/"layer 7” authentication.&#13;
For example, when we called DynamoDB in the serverless API example in <a data-type="xref" href="ch05.html#EX5-3">Example 5-3</a>, that connection was secured solely by credentials that were passed to DynamoDB from our Lambda <span class="keep-together">function</span>.</p>&#13;
&#13;
<p>In other words, DynamoDB is not a “firewalled” service—it sits open to the internet, and any machine anywhere else on the internet can connect to it.</p>&#13;
&#13;
<p>While this brave new world of “firewall-less” computing is gathering pace, there are still many situations where a Lambda function is going to need to connect to a resource that is shielded behind some kind of IP-address limited protection.&#13;
A common way of doing that with AWS is to use a VPC.</p>&#13;
&#13;
<p>VPCs are a lower-level piece of infrastructure than anything else we’ve discussed so far in the book.&#13;
They require understanding things like IP addresses, elastic network interfaces (ENIs), CIDR blocks, and security groups, and also expose the fact to us that AWS regions are made up of multiple AZs.&#13;
In other words, “Here be dragons!”</p>&#13;
&#13;
<p>Lambda functions can be configured to be able to access a VPC. Three typical reasons a Lambda function would need this are:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>To be able to access an RDS SQL database (see <a data-type="xref" href="#lambda-with-vpc">Figure 8-2</a>)</p>&#13;
</li>&#13;
<li>&#13;
<p>To be able to access ElastiCache</p>&#13;
</li>&#13;
<li>&#13;
<p>To be able to call an internal microservice running on a container cluster using IP/VPC-based security</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<figure><div class="figure" id="lambda-with-vpc">&#13;
<img alt="images/ch08_image02.png" src="assets/awsl_0802.png"/>&#13;
<h6><span class="label">Figure 8-2. </span>Lambda attached to VPC to access RDS database</h6>&#13;
</div></figure>&#13;
&#13;
<p>You should configure Lambda to use a VPC only if it actually needs it.&#13;
Adding a VPC is not “free”—it impacts other systems, it changes the behavior of how Lambda <span class="keep-together">interacts</span> with other services, and it adds complexity to your configuration and <span class="keep-together">architecture</span>.</p>&#13;
&#13;
<p>Further, we recommend you configure Lambda to use a VPC only if either (a) you understand VPCs and the implications of doing so or (b) you’ve discussed this requirement with another team in your organization that understands this.</p>&#13;
&#13;
<p>In the rest of this section, we assume that you understand, broadly, VPCs in general, but not necessarily any specifics with Lambda and VPCs.&#13;
As such, there are certain VPC terms, like ENIs and security groups, which we’ll mention but not explain.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section class="pagebreak-before less_space" data-pdf-bookmark="Architectural Concerns of Using Lambda with a VPCs" data-type="sect2"><div class="sect2" id="idm46222412122648">&#13;
<h2>Architectural Concerns of Using Lambda with a VPCs</h2>&#13;
&#13;
<p>Before you even enable Lambda to use a VPC, there are a few things to be aware of that might change your mind!</p>&#13;
&#13;
<p>First, each <em>subnet</em> you specify in your VPC configuration is specific to an AZ.&#13;
One of the nice things about Lambda is that we’ve completely ignored AZs until this point.&#13;
If you’re using Lambda + VPC, you need to make sure you configure enough subnets, across enough AZs, to allow you to continue to have the level of high availability (HA) you need.</p>&#13;
&#13;
<p>Second, when a Lambda function is configured to use a VPC, then <em>all</em> network traffic from that Lambda will be routed through the VPC.&#13;
That means if your Lambda function is using non-VPC AWS resources (like S3) or is using resources <em>external</em> to AWS, then you’ll need to consider network routing for those resources, just like you would any other service within the VPC.&#13;
For instance, for S3 you’ll likely want to set up a VPC endpoint, and for external services you’ll need to make sure your NAT Gateway is correctly configured.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Configuring Lambda to Use a VPC" data-type="sect2"><div class="sect2" id="idm46222412117688">&#13;
<h2>Configuring Lambda to Use a VPC</h2>&#13;
&#13;
<p>You’ve read all the warnings, and you’ve figured out which subnets and security groups to use.&#13;
How do you now actually configure your Lambda to use a VPC?</p>&#13;
&#13;
<p>Fortunately, SAM comes to the rescue, and makes it fairly simple.&#13;
By examining the <a href="https://oreil.ly/388NC">example provided by AWS</a> (slightly trimmed), we can see the additions that you need to make to each Lambda function:</p>&#13;
&#13;
<pre data-code-language="yaml" data-type="programlisting"><code class="nt">AWSTemplateFormatVersion </code><code class="p">:</code> <code class="s">'2010-09-09'</code>&#13;
<code class="nt">Transform</code><code class="p">:</code> <code class="l-Scalar-Plain">AWS::Serverless-2016-10-31</code>&#13;
&#13;
<code class="nt">Parameters</code><code class="p">:</code>&#13;
  <code class="nt">SecurityGroupIds</code><code class="p">:</code>&#13;
    <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">List&lt;AWS::EC2::SecurityGroup::Id&gt;</code>&#13;
    <code class="nt">Description</code><code class="p">:</code> <code class="l-Scalar-Plain">Security Group IDs that Lambda will use</code>&#13;
  <code class="nt">VpcSubnetIds</code><code class="p">:</code>&#13;
    <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">List&lt;AWS::EC2::Subnet::Id&gt;</code>&#13;
    <code class="nt">Description</code><code class="p">:</code> <code class="l-Scalar-Plain">VPC Subnet IDs that Lambda will use (min 2 for HA)</code>&#13;
&#13;
<code class="nt">Resources</code><code class="p">:</code>&#13;
  <code class="nt">HelloWorldFunction</code><code class="p">:</code>&#13;
    <code class="nt">Type</code><code class="p">:</code> <code class="l-Scalar-Plain">AWS::Serverless::Function</code>&#13;
    <code class="nt">Properties</code><code class="p">:</code>&#13;
      <code class="nt">Policies</code><code class="p">:</code>&#13;
        <code class="nt">— VPCAccessPolicy</code><code class="p">:</code> <code class="p-Indicator">{}</code>&#13;
      <code class="nt">VpcConfig</code><code class="p">:</code>&#13;
        <code class="nt">SecurityGroupIds</code><code class="p">:</code> <code class="kt">!Ref</code> <code class="l-Scalar-Plain">SecurityGroupIds</code>&#13;
        <code class="nt">SubnetIds</code><code class="p">:</code> <code class="kt">!Ref</code> <code class="l-Scalar-Plain">VpcSubnetIds</code></pre>&#13;
&#13;
<p>In summary, you need to:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Add privileges for the Lambda function to attach to the VPC (e.g., by using <code>VPC AccessPolicy</code>)</p>&#13;
</li>&#13;
<li>&#13;
<p>Add VPC configuration, with a list of security group IDs, and subnet IDs</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>And that’s it! This particular example assumes that you’ll use <a href="https://oreil.ly/0xs3v">CloudFormation parameters</a> to pass in the actual security group and subnet IDs at deployment time, but you should feel free to hardcode them in your template too.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Alternatives" data-type="sect2"><div class="sect2" id="idm46222412025400">&#13;
<h2>Alternatives</h2>&#13;
&#13;
<p>Say that all of our dire warnings were enough to put you off of using VPCs with Lambda.&#13;
What should you do instead?&#13;
Here are a few approaches.</p>&#13;
&#13;
<p>The first is to use roughly equivalent services that don’t require a VPC.&#13;
For example, if you were going to use a VPC to access an RDS database, consider using DynamoDB instead (although we do acknowledge that DynamoDB is not a relational database!).&#13;
Or think about using Aurora serverless, and its <a href="https://oreil.ly/uf2KE">Data API</a>.</p>&#13;
&#13;
<p>Next is to re-architect your solution. For example, instead of calling a downstream resource directly, would it be possible to use a message bus as an intermediary?</p>&#13;
&#13;
<p>Third—if what you needed to connect to was an internal service, then consider giving that internal service a “layer 7” authentication boundary.&#13;
One way to do this is to add an API Gateway to your internal service (or update an existing API Gateway if it already has one), and then use API Gateway’s <a href="https://oreil.ly/RJVSO">IAM/Sigv4 authentication scheme</a>.</p>&#13;
&#13;
<p>Finally, if you can’t modify your service, you could do something similar to the previous idea, but in this case use <a href="https://oreil.ly/OKiid">API Gateway as a proxy</a> to your downstream service.</p>&#13;
&#13;
<p>Of course, there is one more option—wait and see what AWS introduces next!&#13;
For example, the Data API for serverless Aurora that we mentioned is fairly new, and signals that there may be more functionality coming that will help Lambda developers avoid the perils of VPCs!</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Layers and Runtimes" data-type="sect1"><div class="sect1" id="idm46222412137720">&#13;
<h1>Layers and Runtimes</h1>&#13;
&#13;
<p>If you take a look at one of your Lambda functions in the AWS Web Console, you’ll now know what almost everything on there is for.&#13;
Roles, environment variables, memory, VPCs, DLQs, reserved concurrency, and more.&#13;
However, for the observant among you, you’ll see that there’s something towards the top of the page that is an omission so far: <em>layers</em>.&#13;
To close out this chapter, we’ll explain what layers are, why you (as a Java developer) probably won’t care about them too much, and how they relate to another capability known as <em>custom runtimes</em>.</p>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="What Are Layers?" data-type="sect2"><div class="sect2" id="idm46222412015336">&#13;
<h2>What Are Layers?</h2>&#13;
&#13;
<p>As you know by now, typically when you deploy a new version of a Lambda function, you package up the code and all of its dependencies into a ZIP file, and upload that file to the Lambda service.&#13;
As your dependencies get bigger, however, this artifact gets bigger, and deployment slows down.&#13;
Wouldn’t it be nice to be able to speed this up?</p>&#13;
&#13;
<p>This is where Lambda layers come in.&#13;
A layer is part of the deployed resources of your Lambda function, which is deployed separately from the function itself.&#13;
If your layer stays constant, then when you deploy your Lambda function, you only need to deploy the changes to your code that aren’t within the layer.</p>&#13;
&#13;
<p>Here’s an example.&#13;
Say that you are implementing the photo processing example from way back in <a data-type="xref" href="ch01.html#ch01">Chapter 1</a> (<a data-type="xref" href="ch01.html#file-processing-example">“File processing”</a>), and say that the actual part of your Lambda function that performs the image manipulation uses a third-party tool like <a href="https://imagemagick.org/index.php">ImageMagick</a>.</p>&#13;
&#13;
<p>Now, ImageMagick is probably a dependency that changes rarely.&#13;
With Lambda layers you can define a layer (which is just a ZIP artifact containing any content that you want) that contains the ImageMagick tool, and then refer to that layer with your code in the photo processing Lambda.&#13;
Now when you update your Lambda function, you’ll only need to upload your own code, not your code <em>and</em> ImageMagick.</p>&#13;
<div data-type="tip"><h6>Tip</h6>&#13;
<p>ImageMagick is often used by calling an external process from your application, rather than via a library API call. It’s perfectly OK to call an external process like this from within a Lambda function—the Lambda runtime is a full Linux environment.</p>&#13;
</div>&#13;
&#13;
<p>Another useful aspect to layers is that you can share layers across Lambda functions, and other AWS accounts—layers can in fact be shared publicly.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="When to Use, and Not Use, Layers" data-type="sect2"><div class="sect2" id="idm46222412006200">&#13;
<h2>When to Use, and Not Use, Layers</h2>&#13;
&#13;
<p>When layers were announced, certain parts of the Lambda-using world were very excited, since they saw layers as a universal dependency system for Lambda functions.&#13;
This was especially true for people using the Python language, since Python’s dependency management tools can be a little tricky for some people (e.g., your authors!) to wrap their heads around.&#13;
The Java ecosystem however, for all its faults, has a very strong story to tell around dependency management.</p>&#13;
&#13;
<p>We feel that there are some specific times when layers are useful. However, there are also a number of concerns that we have about embracing them wholeheartedly, for example:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>Since layers are combined with your Lambda function after you’ve uploaded the function, it’s not necessarily true that the version of a dependency you’ve used at test time (before deployment) is the same as that which is used with the deployed version.&#13;
This, to us, is a (typically) unnecessary headache of coordination that needs to be managed.</p>&#13;
</li>&#13;
<li>&#13;
<p>Lambda functions are limited to the number of layers that can be used (five), and so if you have more than five dependencies, you’re going to need to use a local deployment tool anyway, so why add the extra complexity of layers?</p>&#13;
</li>&#13;
<li>&#13;
<p>Layers don’t particularly provide any functional benefit—they are a deployment optimization tool (we’ll talk about cross-cutting behavior as a caveat for this).</p>&#13;
</li>&#13;
<li>&#13;
<p>Particularly for developing Lambda in Java—Java does a pretty good job of defining its “own world.”&#13;
For example, it’s usual to only depend on third-party code in Java that itself runs in the JVM, as opposed to calling out to system libraries or executables.&#13;
Given this, and the ubiquity of Maven dependencies, it’s easy to have one consolidated dependency management system with a Java application that doesn’t include the use of Lambda layers.</p>&#13;
</li>&#13;
<li>&#13;
<p>Some people like the fact that a layer can be manually updated for a function without having to deploy a new version of the function itself.&#13;
We personally believe strongly that apart from extenuating circumstances, the best way to deploy any changes to production is through an automated continuous delivery process, and therefore the difference between changing an application library dependency versus a configured template layer dependency should almost always be moot.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>We’d be remiss if we didn’t also point out the places that layers can be useful.</p>&#13;
&#13;
<p>First, if part of what a Lambda function executes is unrelated to the application, but more related to an organization’s cross-cutting technical platform, then using layers as an alternative deployment path can be useful.&#13;
For example, say that there is a security process that needs to be run, but as far as application developers are concerned, it’s just a “fire-and-forget” call.&#13;
In this case, publishing that code in a layer, and being able to query all the Lambda function configurations across an organization and making sure they’re using the correct version of the layer, aids in organizational <span class="keep-together">governance</span>.</p>&#13;
&#13;
<p>Another place where layers are useful is where a dependency is a large, system binary that rarely changes.&#13;
In this case, the extra complexity of using layers may be worth the value of improved deployment speed, especially if the number of deployments of functions using that layer is on the order of hundreds per day or more.</p>&#13;
&#13;
<p>A helpful example of this second case is where a Lambda function is using a custom runtime, which we’ll explore now.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Custom Runtimes" data-type="sect2"><div class="sect2" id="custom-runtimes">&#13;
<h2>Custom Runtimes</h2>&#13;
&#13;
<p>Throughout this book we have been using the Java Lambda runtime, apart from our very first example, which used the Node 10 runtime.&#13;
AWS offers <a href="https://oreil.ly/uLMNz">a number of <span class="keep-together">runtimes</span></a> associated with different programming languages, and this list is frequently updated.</p>&#13;
&#13;
<p>However, what happens if you want to use a language or runtime that AWS don’t support?&#13;
For example, what if you have some Cobol code you want to run in a Lambda function?&#13;
Or, perhaps more likely, what if you want to run a highly customized JVM, rather than the one AWS provides?</p>&#13;
&#13;
<p>The answer here is to use a <em>custom runtime</em>.&#13;
A custom runtime is a Linux process that runs in a Lambda execution environment, and that can process Lambda events.&#13;
There is a <a href="https://oreil.ly/onv6J">specific execution model</a> that a custom runtime needs to fulfill, but the basic idea is that when the runtime instance is started by the Lambda platform, it is configured with an instance-specific URL that it can query for the next event to process. In other words, custom runtimes use a polling architecture.</p>&#13;
&#13;
<p>As a Java developer, it will typically be rare that you want or need to use a custom runtime for production usages. Two reasons for this are as follows:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The custom runtime code itself needs to be part of your function’s deployed assets.&#13;
While you can package the runtime in a Lambda layer to avoid uploading it on every deployment, it will still be using up some of your <a href="https://oreil.ly/02nUm">250MB total unpacked deployment package size limit</a>.&#13;
Most JVMs are going to use a considerable part of that, if you want to ship a custom JVM, and so this will cut into the space available for your application code.</p>&#13;
</li>&#13;
<li>&#13;
<p>You will need to reimplement in your custom runtime a lot of what AWS has already implemented in its standard runtimes, such as deserialization/serialization of events and responses, error handling, and more.</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>That being said, for organizations of a certain size, building a custom runtime that handles various organizational-platform-related tasks might make actual Lambda development even more effective, but we would suggest a through analysis before jumping in!</p>&#13;
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="other-jvm-languages">&#13;
<h5>Other JVM Languages and Lambda</h5>&#13;
<p>This book has been focussed on the Java language, running on the Java runtime.&#13;
However, there are many other languages that can run on the JVM—Scala, Clojure, Kotlin, and more.&#13;
Since Lambda only specifies the Java <em>runtime</em>, it’s perfectly reasonable to use alternative languages.&#13;
In fact, we know of people using Scala and Lambda for significant load systems (thousands of concurrent executions).</p>&#13;
&#13;
<p>From the Java Lambda runtime’s point of view, all it cares is that you configure it with a valid handler method, so the way that most people use alternative JVM languages with Lambda is to use the Java runtime, and then an “interop” hook into their handler.&#13;
Here’s an example using Kotlin and Groovy on <a href="https://oreil.ly/4qUvM">the AWS blog</a>.&#13;
Depending on your specific language, the POJO serialization provided by the Java runtime may or may not play nicely, but alternatively you can use the <code>InputStream</code>/<code>OutputStream</code> handler signature to get the raw bytes of a JSON event.</p>&#13;
&#13;
<p>One drawback to using an alternative JVM language, especially one like Scala, is that it adds to your cold start time because the JVM has to JIT compile the language classes, as well as your application classes.&#13;
But the general rules about cold starts that we discussed earlier in this chapter still hold, especially if your functions are high throughput.</p>&#13;
&#13;
<p>An alternative to using the AWS Java runtime is to use a custom runtime to support your JVM language, but typically that isn’t necessary, assuming the standard Java runtime can support your alternative JVM language.</p>&#13;
</div></aside>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="idm46222411975848">&#13;
<h1>Summary</h1>&#13;
&#13;
<p>In this chapter, we took a deep dive into some advanced aspects of Lambda.&#13;
Some of these behaviors and configurations will be crucial as you deploy your serverless applications to production.</p>&#13;
&#13;
<p>You learned about the following:</p>&#13;
&#13;
<ul>&#13;
<li>&#13;
<p>The various different error handling strategies of Lambda and how you may choose to configure and program your functions to process errors</p>&#13;
</li>&#13;
<li>&#13;
<p>The liberating way that Lambda scales without any effort on your part, how you can control that scaling, and what this behavior means in the context of multi-threaded programming</p>&#13;
</li>&#13;
<li>&#13;
<p>What Lambda versions and aliases are, and how to use them with a “traffic shifting” approach for releasing new features</p>&#13;
</li>&#13;
<li>&#13;
<p>What cold starts are, when they occur, whether you should be concerned about them, and how to mitigate them if you need to reduce their impact in your <span class="keep-together">applications</span></p>&#13;
</li>&#13;
<li>&#13;
<p>How to consider persistent and cache state in Lambda development</p>&#13;
</li>&#13;
<li>&#13;
<p>How to use Lambda with AWS VPCs</p>&#13;
</li>&#13;
<li>&#13;
<p>What Lambda layers and custom runtimes are, and when to think about using them</p>&#13;
</li>&#13;
</ul>&#13;
&#13;
<p>In the next chapter, we carry on rounding out our discussion of the more advanced aspects of Lambda, but this time in the context of how Lambda interacts with other services.</p>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
<section data-pdf-bookmark="Exercises" data-type="sect1"><div class="sect1" id="idm46222411937976">&#13;
<h1>Exercises</h1>&#13;
<ol>&#13;
<li>&#13;
<p>Update <code>WeatherQueryLambda</code> in <a data-type="xref" href="ch05.html#serverless-api-example">“Example: Building a Serverless API”</a> to throw an exception. What behavior do you see when you try to call the API?</p>&#13;
</li>&#13;
<li>&#13;
<p>If you implemented the exercise from <a data-type="xref" href="ch05.html#ch05">Chapter 5</a> to use an SQS queue, then update the Lambda function that reads from SQS to throw an exception.&#13;
Does Lambda’s retry behavior do what you’d expect?</p>&#13;
</li>&#13;
<li>&#13;
<p>Investigate what happens with background threads and Lambda—start with the “Hello World” example from <a data-type="xref" href="ch02.html#ch02">Chapter 2</a> (see <a data-type="xref" href="ch02.html#java-hello-world">“Lambda Hello World (the Proper Way)”</a>) and within the handler use a <a href="https://oreil.ly/6cz67"><code>ScheduledExecutorService</code></a> and its <code>scheduleAtFixedRate</code> method to repeatedly log the event that you received.&#13;
What happens? Try using some <code>Thread.sleep</code> statements too.</p>&#13;
</li>&#13;
<li>&#13;
<p>Update <a data-type="xref" href="ch05.html#serverless-api-example">“Example: Building a Serverless API”</a> to use traffic shifting, starting with the <code>Linear10PercentEvery10Minutes</code> deployment preference.</p>&#13;
</li>&#13;
<li>&#13;
<p><em>Extended task</em>: If you program on the JVM with a different language—perhaps Clojure, Kotlin, or Scala—try building a Lambda function in one of those <span class="keep-together">languages</span>.</p>&#13;
</li>&#13;
&#13;
</ol>&#13;
</div></section>&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
&#13;
</div></section></body></html>