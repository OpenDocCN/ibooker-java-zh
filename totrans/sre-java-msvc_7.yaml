- en: Chapter 7\. Traffic Management
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 交通管理
- en: Cloud native applications expect failure and low availability from the other
    services and resources they interact with. In this chapter, we introduce important
    mitigation strategies involving load balancing (platform, gateway, and client-side)
    and call resilience patterns (retrying, rate limiters, bulkheads, and circuit
    breakers) that work together to ensure your microservices continue to perform.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 云原生应用程序期望其他服务和资源的故障和低可用性。在本章中，我们介绍了涉及负载均衡（平台、网关和客户端）和调用弹性模式（重试、速率限制器、舱壁和断路器）的重要缓解策略，这些策略共同确保您的微服务继续运行。
- en: These patterns won’t be applicable for every organization. Often introducing
    more complex traffic management trades off operational complexity for more predictable
    user experience or a lower overall failure rate. In other words, it’s easy to
    make a REST call to a downstream service with your HTTP client of choice; it’s
    a little more complicated to wrap that call in a retry. And a little more complicated
    still to provide a circuit breaker and fallback. But with greater complexity comes
    greater reliability.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模式并不适用于每个组织。通常，引入更复杂的交通管理会在操作复杂性和更可预测的用户体验或更低的整体故障率之间进行权衡。换句话说，使用您选择的HTTP客户端轻松地向下游服务发出REST调用；将该调用包装成重试则稍微复杂些。而提供断路器和回退则更为复杂。但随着复杂性的增加，可靠性也更高。
- en: Organizations should evaluate their need here based on the types of applications
    they have (for example, where circuit breaking is applicable) and which application
    frameworks microservices are primarily written in. Java has first-class library
    support for these patterns and integration into popular frameworks like Spring,
    but the lack of support in some other languages would make it preferable to use
    sidecars or service meshes, even if there is some loss of flexibility as a result.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 组织应根据其拥有的应用程序类型（例如，断路器适用的地方）以及微服务主要编写在哪种应用框架中来评估其需求。Java具有这些模式的一流库支持，并集成到像Spring这样的流行框架中，但某些其他语言的支持不足会使得使用边车或服务网格更可取，即使因此会损失一些灵活性。
- en: Microservices Offer More Potential Failure Points
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微服务提供更多潜在故障点。
- en: As the number of microservices involved in a user interaction grows, the likelihood
    of encountering a service instance (in any given user interaction) that is in
    a low availability state increases. A service can put load on a downstream service
    that it cannot sustain and cause it to fail. Call resiliency patterns protect
    a service from failures in the downstream services, as well as negatively impact
    downstream services. They alter the call sequence with a goal of providing a reduced
    service to end users, but a service nevertheless. For example, a personalized
    list of Netflix movie recommendations can be replaced with generic movie recommendations
    if the personalization service is suffering from low availability.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 随着参与用户交互的微服务数量增加，遇到处于低可用状态的服务实例的可能性增加。一个服务可以给下游服务施加负载，而后者可能无法承受并因此失败。调用弹性模式保护服务免受下游服务的失败影响，同时也会对下游服务产生负面影响。它们改变调用顺序的目标是为最终用户提供减少服务，但仍然是服务。例如，如果个性化服务遭遇低可用性，Netflix电影推荐的个性化列表可以替换为通用的电影推荐。
- en: 'Microservices are usually deployed in a horizontally scaled way across different
    availability zones to increase resiliency of the distributed system. Microservices
    are not static. At any given time several of them can be released (new versions
    are being deployed or canaried), scaled, moved, or failed over. Some instances
    may experience failures, but not all. They may be temporarily down or experiencing
    reduced performance. This dynamic, frequently changed system requires adopting
    a set of practices for dynamically routing traffic: from discovering where the
    services are in the first place to picking which instance to send the traffic
    to. This is covered by different load-balancing approaches.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务通常以水平扩展的方式部署在不同的可用区，以增加分布式系统的弹性。微服务并非静态。在任何给定时间，其中几个可以发布（部署新版本或进行金丝雀发布），进行扩展、迁移或故障切换。某些实例可能会经历故障，但不是全部。它们可能会暂时停机或者性能降低。这种动态且频繁变化的系统需要采用一套实践来动态路由流量：从首先发现服务所在的位置到选择哪个实例发送流量。这些都涉及到不同的负载均衡方法。
- en: 'Two approaches exist for implementing these patterns: the application framework
    (code) and the supporting infrastructure (platform or gateway load balancers,
    service mesh). A combination can also be used. Generally, implementing these in
    application frameworks allows more flexibility and customization that’s specialized
    to the business domain. For example, replacing personalized movie recommendations
    with generic ones is acceptable, but there is no obvious fallback response to
    a request to a payment or billing service—an understanding of the business domain
    matters.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以实现这些模式：应用程序框架（代码）和支持基础设施（平台或网关负载均衡器、服务网格）。也可以结合使用。一般来说，在应用程序框架中实现这些功能允许更多灵活性和专门用于业务领域的定制。例如，用通用推荐替换个性化电影推荐是可以接受的，但对支付或账单服务的请求却没有明显的回退响应——业务领域的理解至关重要。
- en: Concurrency of Systems
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统的并发性
- en: By “concurrency” I am referring to the number of requests a microservice can
    service at once. There is a natural bound to concurrency in any system, usually
    driven by a resource like CPU or memory or the performance of a downstream service
    when requests are satisfied in a blocking manner. Any attempted requests exceeding
    this bound cannot be satisfied immediately and must be queued or rejected. In
    the case of a typical Java microservice running on Tomcat, the number of threads
    in Tomcat’s thread pool represents an upper bound on its concurrency limit (though
    system resources may very well be exhausted by a number of concurrent requests
    less than the Tomcat thread pool). The accept queue maintained by the operating
    system effectively queues up requests in excess of that concurrency limit.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: “并发性”指的是一个微服务可以同时处理的请求数量。任何系统中都有一个自然的并发性界限，通常由像CPU或内存这样的资源，或者在请求以阻塞方式满足时的下游服务性能驱动。超过此界限的任何尝试请求无法立即满足，必须排队或拒绝。在典型的运行在Tomcat上的Java微服务中，Tomcat线程池中的线程数表示其并发性限制的上限（尽管系统资源很可能会被Tomcat线程池中较少数量的并发请求耗尽）。操作系统维护的接受队列有效地排队超出该并发性限制的请求。
- en: Services fail when during prolonged periods of time the request rate exceeds
    the response rate. As the queue grows, so will the latency (since requests don’t
    even begin getting serviced until they are removed from the queue). Eventually
    queued requests will start timing out.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当请求速率超过响应速率的时间延长时，服务将失败。随着队列增长，延迟也会增加（因为请求直到从队列中移除才开始被处理）。最终，排队的请求将开始超时。
- en: In this chapter, we will cover strategies to prevent a cascading failure from
    occurring because a concurrency limit has been reached. The discussion on load
    balancing, viewed from this perspective, is really a proactive approach to directing
    traffic in such a way as to prevent load-related failure in the first place.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖防止由于并发限制已达到而发生级联故障的策略。从这个角度看，负载平衡的讨论实际上是一种积极的方法，以便在第一时间就防止与负载相关的故障。
- en: Platform Load Balancing
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平台负载平衡
- en: Every modern runtime platform (e.g., IaaS offerings like AWS/GCP/Azure, a CaaS
    offering such as any Kubernetes distribution, or a PaaS offering like Cloud Foundry)
    has at least some basic cluster load balancer. These load balancers serve to distribute
    traffic across the instances in a cluster one way or another (often round-robin),
    but also have a wide range of other responsibilities. For example, AWS Elastic
    Load Balancers also serve the interests of TLS termination, content-based routing,
    sticky sessions, etc.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 每个现代运行时平台（例如AWS/GCP/Azure等的IaaS提供、任何Kubernetes分发的CaaS提供或Cloud Foundry等的PaaS提供）至少有一些基本的集群负载均衡器。这些负载均衡器用于以某种方式（通常是轮询）在集群的实例之间分发流量，但也具有各种其他责任。例如，AWS
    Elastic Load Balancers还服务于TLS终止、基于内容的路由、粘性会话等。
- en: In on-premises environments, even simpler configurations are still prevalent
    with IIS, Nginx, Apache, etc., serving as statically configured load balancers
    in front of a fixed set of named virtual machines or physical machines.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地环境中，即使是更简单的配置，IIS、Nginx、Apache等仍然是固定一组命名虚拟机或物理机器前的静态配置负载均衡器。
- en: Before discussing more complex options, it’s worth noting that there is nothing
    wrong with this setup for a particular level of scale. One regional casualty/property
    insurer primarily serves a web application for its captive agents, so capacity
    requirements for this user pool is incredibly stable. While such an organization
    can benefit from an active-active deployment for greater resiliency to failure
    in an individual datacenter, its traffic pattern doesn’t warrant the more complex
    load balancing at a gateway or on the client side.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论更复杂的选项之前，值得注意的是，对于特定规模的设置并没有什么问题。一个区域性的财产/事故保险公司主要为其专有代理人提供Web应用程序服务，因此对于此用户群体的容量需求非常稳定。虽然这样的组织可以从主动-主动部署中受益，以增强单个数据中心的故障弹性，但其流量模式并不需要在网关或客户端负载均衡上进行更复杂的处理。
- en: Gateway Load Balancing
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 网关负载均衡
- en: Software-based gateways are readily available in open source. [Spring Cloud
    Gateway](https://oreil.ly/yTMx-) is a reasonably modern incarnation of such a
    gateway, influenced by experience learned from working with [Zuul](https://oreil.ly/fNqHm).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 开源中有许多软件化的网关可用。[Spring Cloud Gateway](https://oreil.ly/yTMx-) 是这样一个网关的现代化体现，受到与[Zuul](https://oreil.ly/fNqHm)的工作经验影响。
- en: The ability of a runtime platform to load balance traffic to optimize availability
    is limited. For some availability signals like latency, the caller is the best
    source of information. The load balancer and calling application are similarly
    positioned to observe and react to latency as an availability signal. But for
    other signals, especially those involving utilization, the server itself is the
    best (and often only) source of this information. Combining these two sources
    of availability signals yields the most effective load-balancing strategy.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时平台将流量负载均衡以优化可用性的能力是有限的。对于诸如延迟之类的可用性信号，调用方是最佳信息来源。负载均衡器和调用应用程序同样能观察和响应延迟作为可用性信号。但对于其他信号，尤其是涉及利用率的信号，服务器本身是这些信息的最佳（并且通常是唯一）来源。结合这两个可用性信号源可以产生最有效的负载均衡策略。
- en: From the perspective of reliability, the goal of load balancing is to direct
    traffic away from servers that have high error rates. The goal should *not* be
    to optimize for the fastest response time. Optimizing for response time tends
    to result in strategies that can *herd* traffic to a healthy instance or group
    of instances, causing them to become overloaded and unavailable. Avoiding instances
    with high error rates still allows traffic to be distributed to instances that
    are not optimally performant, but available enough. If all instances in a cluster
    are overloaded, choosing one instance over another offers no benefit no matter
    how smart the load-balancing strategy. However, in many cases a subset of instances
    are overloaded because of temporary conditions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从可靠性的角度来看，负载均衡的目标是将流量从错误率高的服务器上移开。目标*不*应该是优化最快的响应时间。优化响应时间往往会导致策略将流量*赶*向一个健康的实例或实例组，导致它们过载和不可用。避免具有高错误率的实例仍然允许流量分布到性能不是最佳但足够可用的实例。如果集群中的所有实例都超载，选择一个实例而不是另一个不管负载平衡策略多么聪明都没有好处。然而，在许多情况下，由于临时条件，一些实例的负载过重。
- en: A temporarily overloaded subset is found wherever there is a process whose execution
    is likely to be staggered across the cluster. For example, not all instances are
    likely to undergo GC or VM pauses, data updates, or cache swapping at the same
    time. This staggering tends to be present whenever there is no cluster-wide coordination
    of these processes. If all instances perform some sort of data update based on
    a synchronized clock, cluster coordination exists. For an example of a lack of
    coordination, consider what causes a GC pause to occur. Allocations incurred satisfying
    any given request eventually lead to a GC event. Since traffic will almost certainly
    be distributed nonuniformly across the cluster regardless of the load-balancing
    strategy, allocations will be staggered, leading to staggered GC events.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 无论何时集群中的某个过程可能在不同实例间交错执行时，都会出现临时超载的子集。例如，并非所有实例都可能同时进行GC或VM暂停、数据更新或缓存交换。只要这些过程没有集群范围的协调，交错就会存在。如果所有实例根据同步时钟执行某种数据更新，则存在集群协调。例如缺乏协调的示例，考虑导致GC暂停发生的原因。满足任何给定请求所产生的分配最终会导致GC事件。由于无论负载平衡策略如何，流量几乎肯定会在集群中非均匀分布，因此分配将会交错，从而导致交错的GC事件。
- en: Another example of a subset of low-availability instances is the set of cold
    instances post-startup, such as instances brought into service by an autoscaling
    event or a zero-downtime deployment. With the rising popularity of serverless
    technologies, focus has been directed at application start time up to the point
    where health checks pass (effectively when the application instance is placed
    in service). But it’s important to note a second phase of cold start ill performance
    that begins on the first request, as in [Figure 7-1](part0012_split_004.html#first_five_minutes_latency),
    and ends when runtime optimizations have taken effect (i.e., the JVM’s JIT optimization,
    or application-specific behaviors like memory mapping a working set of data into
    memory). It’s this second phase that is so important to mitigate.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 低可用性实例的子集另一个例子是冷实例，比如由自动扩展事件或零停机部署引入服务的实例。随着无服务器技术的普及，焦点已经转向了应用启动时间，直到健康检查通过（实际上是应用实例投入服务时）。但重要的是要注意到冷启动第二阶段的性能问题，即从第一个请求开始，如[图 7-1](part0012_split_004.html#first_five_minutes_latency)所示，直到运行时优化生效（例如JVM的JIT优化，或应用特定行为如将工作数据集内存映射）。正是这第二阶段的问题需要得到有效缓解。
- en: '![The max will be much higher in the first few requests](../images/00070.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![在最初的几个请求中，最大值将会更高](../images/00070.png)'
- en: Figure 7-1\. The max is more than an order of magnitude worse than P99 in the
    first few requests
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-1\. 在最初的几个请求中，最大值比P99糟糕一个数量级以上
- en: The chart plots the two Prometheus queries shown in [Example 7-1](part0012_split_004.html#first_five_minutes_latency_prometheus).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图表绘制了例 7-1 中所示的两个Prometheus查询，展示了REST端点的最大和P99延迟。
- en: Example 7-1\. Prometheus queries plotting max and P99 latency for a REST endpoint/persons
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例 7-1\. Prometheus查询绘制REST端点/人员的最大和P99延迟
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Some instances will run slower than others more or less permanently because
    of either bad underlying hardware or, increasingly, a noisy neighbor.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一些实例会因为糟糕的底层硬件或者越来越多的“吵闹邻居”，永久地运行得比其他实例慢。
- en: Clearly, round-robin load balancing can be improved upon. Architecturally, the
    logic for this load balancer resides in the edge gateway, as shown in [Figure 7-2](part0012_split_004.html#gateway_load_balancer).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，轮询负载均衡可以得到改进。从架构上看，这个负载均衡器的逻辑驻留在边缘网关中，如图 7-2（part0012_split_004.html#gateway_load_balancer）所示。
- en: '![srej 0702](../images/00064.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0702](../images/00064.png)'
- en: Figure 7-2\. Using an API gateway as a smarter load balancer
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-2\. 使用API网关作为更智能的负载均衡器
- en: User-facing traffic comes in through a platform load balancer, which distributes
    the traffic in a round-robin fashion to a cluster of gateway instances. In this
    case, we’ve shown one gateway serving requests to multiple microservices behind
    the edge. Gateway instances communicate directly with service instances by fetching
    an instance list from a discovery service like Netflix Eureka or HashiCorp Consul.
    There is no need for a platform load balancer in front of individual microservices
    that are load balanced by the gateway.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 用户面向的流量通过平台负载均衡器进入，该负载均衡器将流量以轮询方式分发到一组网关实例。在这种情况下，我们展示了一个网关向边缘后面的多个微服务提供请求服务的情况。网关实例通过从诸如Netflix
    Eureka或HashiCorp Consul等发现服务获取的实例列表直接与服务实例通信。不需要在个别微服务前加上平台负载均衡器，因为这些微服务由网关进行负载均衡。
- en: With this general setup in mind, we can progressively come up with a load-balancing
    strategy that takes into account application instances’ notion of their own availability.
    Then we’ll consider its unintended side effects. The goal is for you to be exposed
    to techniques that can be used in combination with domain-specific knowledge to
    craft a load-balancing strategy that works well for you while learning to think
    through and anticipate side effects.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑这种一般性设置的同时，我们可以逐步提出一种负载均衡策略，考虑应用实例对其自身可用性的概念。然后我们将考虑其意外的副作用。目标是让您接触到可以与领域特定知识结合使用的技术，以制定对您有效的负载均衡策略，并学会思考和预见副作用。
- en: Join the Shortest Queue
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入最短队列
- en: Perhaps the simplest “adaptive” load balancer that goes beyond simple round-robin
    is “join the shortest queue.”
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 或许最简单的“自适应”负载均衡器，超越简单的轮询法的是“加入最短队列”。
- en: Join the shortest queue is implemented by comparing some instance availability
    signal visible to the load balancer. A good example of this is in-flight requests
    to each instance that the load balancer is aware of. Suppose the load balancer
    is directing traffic to three application instances, two of which have an in-flight
    request. When the load balancer receives a new request, it will direct the request
    to the one instance that has no in-flight requests, as shown in [Figure 7-3](part0012_split_005.html#jsq_load_balancer).
    This is computationally cheap (just minimizing/maximizing some statistic) and
    easy to implement.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '"加入最短队列"的实现是通过比较负载均衡器可见的某些实例可用信号来完成的。一个很好的例子是对每个负载均衡器可见的每个实例的在飞请求。假设负载均衡器正在将流量引导到三个应用实例，其中两个有在飞请求。当负载均衡器收到一个新请求时，它将该请求定向到没有在飞请求的实例，如[图 7-3](part0012_split_005.html#jsq_load_balancer)所示。这是计算上廉价的（只需最小化/最大化某些统计数据）且易于实现。'
- en: '![The load balancer knows which instance to direct traffic to](../images/00058.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![负载均衡器知道将流量定向到哪个实例](../images/00058.png)'
- en: Figure 7-3\. Join the shortest queue with one load balancer node
  id: totrans-37
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-3\. 使用一个负载均衡器节点的加入最短队列
- en: It starts to break down when there is more than one load balancer instance.
    To this point, the algorithm described makes decisions based on in-flight requests
    known to any one load balancer instance (those requests that passed through it).
    In other words, in a pool of load balancers, each load balancer is making its
    own independent decision, unaware of in-flight requests occurring on the others.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当存在多个负载均衡器实例时，它开始出现问题。到目前为止，所描述的算法基于任何一个负载均衡器实例已知的在飞请求（通过它传递的请求）。换句话说，在负载均衡器池中，每个负载均衡器都在做出自己独立的决策，不知道其他负载均衡器上正在进行的在飞请求。
- en: '[Figure 7-4](part0012_split_005.html#jsq_many_load_balancers) shows how Load
    Balancer 1 will make a bad decision to send an incoming request to Server 3 on
    the basis of incomplete information it has about in-flight requests managed by
    other load balancer nodes. The arrows indicate in-flight requests. So before a
    new request comes in, Load Balancer 1 has an in-flight request to Service 1 and
    2\. Load Balancer 2 has an in-flight request to Service 2 and 3\. Load Balancer
    3 has three in-flight requests to Service 3\. As a new request comes in to Load
    Balancer 1, since it only knows about its own in-flight requests, it will decide
    to send the new request to Service 3, even though it is the busiest service instance,
    with four in-flight requests coming from the other load balancers in the cluster.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-4](part0012_split_005.html#jsq_many_load_balancers) 显示了负载均衡器 1 将基于它所知道的其他负载均衡器节点管理的不完整信息，错误地将一个新请求发送到服务器
    3 的情况。箭头表示在飞请求。因此，在新请求到来之前，负载均衡器 1 已经有一个到服务 1 和 2 的在飞请求。负载均衡器 2 有一个到服务 2 和 3 的在飞请求。负载均衡器
    3 有三个到服务 3 的在飞请求。当负载均衡器 1 收到一个新请求时，由于它只知道自己的在飞请求，它会决定将新请求发送到服务 3，即使这是最繁忙的服务实例，来自集群中其他负载均衡器的四个在飞请求。'
- en: '![The load balancer will make a bad decision based on incomplete information](../images/00024.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![负载均衡器会基于不完整信息做出错误决策](../images/00024.png)'
- en: Figure 7-4\. Join the shortest queue with several load balancer nodes
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-4\. 使用多个负载均衡器节点的加入最短队列
- en: Join the shortest queue is an example of load balancing based on only the load
    balancer’s view of the situation. One consequence of this for low-throughput applications
    is that a load balancer is managing only a few in-flight requests to a subset
    of the instances in a cluster. The choice of which instance in the cluster is
    least utilized can lead to a choice between two instances with zero in-flight
    requests, a random choice since no other information is available.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '"加入最短队列"是基于负载均衡器对情况的视图进行负载平衡的一个例子。对于低吞吐量应用程序的一个后果是，负载均衡器仅管理少量在飞请求到集群中的一些实例。在集群中最少使用的实例的选择可以导致在没有在飞请求的两个实例之间进行随机选择，因为没有其他可用信息。'
- en: Avoid the Temptation to Coordinate!
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 避免协调的诱惑！
- en: It may be tempting to consider sharing the state of each load balancer’s in-flight
    requests with other load balancers, but distributed coordination like this is
    difficult and should be avoided whenever possible. You wind up facing an engineering
    choice between rigging a peer-based distributed state system or choosing a shared
    datastore with the typical consistency, availability, and partitionability trade-offs.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 可能会诱人地考虑与其他负载平衡器共享每个负载平衡器的正在进行中请求的状态，但像这样的分布式协调是困难的，并且应尽可能避免。 在典型的一致性、可用性和分区性权衡中，您最终面临一个工程选择，要么是操纵基于对等的分布式状态系统，要么是选择具有这些权衡的共享数据存储。
- en: The next pattern uses information from the instances being load balanced.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个模式使用来自负载平衡实例的信息。
- en: Instance-Reported Availability and Utilization
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实例报告的可用性和利用率
- en: 'If instead we can inform each load balancer of the instance’s perspective on
    its own availability and utilization, then two load balancers using the same instance
    have the same information regarding its availability. There are two available
    solutions:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以向每个负载平衡器通知实例对其自身可用性和利用率的观点，那么使用相同实例的两个负载平衡器将具有关于其可用性的相同信息。 有两种可用的解决方案：
- en: Poll
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 轮询
- en: Poll each instance’s utilization, sampling data from health check endpoint detail.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 轮询每个实例的利用率，从健康检查端点详细数据中采样数据。
- en: Passively track
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 被动地跟踪
- en: Passively track a header on the responses coming from the server annotated with
    current utilization data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在来自带有当前利用率数据的服务器响应中 passively track 一个标头。
- en: Both approaches are equally simple to implement, and each has trade-offs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法的实现同样简单，并且各自都有权衡。
- en: Micrometer has a `MeterRegistry` implementation called `HealthMeterRegistry`
    (available in the `io.micrometer:micrometer-registry-health` module) specifically
    to convert metrics data into availability signals that can be mapped to health
    indicators watched by load balancers.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Micrometer 提供了一个名为`HealthMeterRegistry`的`MeterRegistry`实现（在`io.micrometer:micrometer-registry-health`模块中可用），专门用于将度量数据转换为可由负载均衡器监视的健康指标。
- en: A `HealthMeterRegistry` is configured with a set of service level objectives
    that are then mapped to framework health indicators and sampled each time the
    load balancer queries the health check endpoint.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`HealthMeterRegistry`配置了一组服务级别目标，然后将它们映射到框架健康指标，并在负载均衡器查询健康检查端点时进行采样。'
- en: Micrometer provides out-of-the-box service level objectives that are known to
    be applicable to a broad range of Java applications. These can be manually configured,
    as in [Example 7-2](part0012_split_007.html#oob_slos). Spring Boot Actuator also
    autoconfigures these objectives when `micrometer-registry-health` is present.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Micrometer 提供了一些开箱即用的服务级别目标，已知适用于广泛的 Java 应用程序。 这些可以像 [Example 7-2](part0012_split_007.html#oob_slos)
    中那样手动配置。 当`micrometer-registry-health`存在时，Spring Boot Actuator 也会自动配置这些目标。
- en: Example 7-2\. Creating a HealthMeterRegistry with recommended service level
    objectives
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-2\. 使用推荐的服务级别目标创建 `HealthMeterRegistry`。
- en: '[PRE1]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When this is bound to framework-level health indicators, these objectives are
    incorporated into the overall determination of an application’s health. Spring
    Boot Actuator’s health endpoint is shown configured with this default set of SLOs
    in [Figure 7-5](part0012_split_007.html#slo_actuator_health).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些目标绑定到框架级健康指标时，这些目标被纳入应用程序健康的整体确定中。 Spring Boot Actuator 的健康端点配置显示了这组默认的 SLOs，在
    [Figure 7-5](part0012_split_007.html#slo_actuator_health) 中。
- en: '![srej 0705](../images/00085.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0705](../images/00085.png)'
- en: Figure 7-5\. Spring Boot Actuator health endpoint with service level objectives
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-5\. Spring Boot Actuator 健康端点与服务级别目标。
- en: You can define your own service level objectives as well, and in [Example 7-3](part0012_split_007.html#poll_instance_utilization_server)
    we define an `api.utilization` service level objective to support sampling utilization
    data from health check endpoint detail on the server. Spring Boot Actuator adds
    this objective to the `HealthMeterRegistry` that it will automatically create;
    or if you are wiring your own `HealthMeterRegistry`, you can add it directly at
    construction time.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以定义自己的服务级别目标，并在 [Example 7-3](part0012_split_007.html#poll_instance_utilization_server)
    中，我们定义了一个`api.utilization`服务级别目标，以支持从服务器健康检查端点详细信息中采样利用率数据。 Spring Boot Actuator
    将此目标添加到它将自动创建的 `HealthMeterRegistry` 中；或者如果您正在进行自己的 `HealthMeterRegistry` 的连接，您可以在构建时直接添加它。
- en: Example 7-3\. A custom ServiceLevelObjective to report server utilization
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-3\. 自定义的服务级别目标，用于报告服务器利用率。
- en: '[PRE2]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[![1](../images/00112.png)](part0012_split_007.html#co_traffic_management_CO1-1)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](../images/00112.png)](part0012_split_007.html#co_traffic_management_CO1-1)'
- en: A name for the service level objective. This can be naming convention normalized
    just like a meter name when exposing it as the name of a health indicator component.
    Spring Boot would show this as a health component named `apiUtilization` (camel-cased)
    based on its convention.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 服务水平目标的名称。当将其作为健康指标组件的名称公开时，可以将其命名规范化，就像公制表名称一样。Spring Boot将其显示为`apiUtilization`（驼峰命名法），基于其约定。
- en: '[![2](../images/00059.png)](part0012_split_007.html#co_traffic_management_CO1-2)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](../images/00059.png)](part0012_split_007.html#co_traffic_management_CO1-2)'
- en: The unit of measure of utilization, which makes the output a little more human
    readable.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 利用率的测量单位，使输出更容易理解一些。
- en: '[![3](../images/00067.png)](part0012_split_007.html#co_traffic_management_CO1-3)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](../images/00067.png)](part0012_split_007.html#co_traffic_management_CO1-3)'
- en: What it means for this objective to not be met, in plain language.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于未达到此目标意味着什么，用通俗的语言说。
- en: '[![4](../images/00016.png)](part0012_split_007.html#co_traffic_management_CO1-4)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](../images/00016.png)](part0012_split_007.html#co_traffic_management_CO1-4)'
- en: We are retrieving a measure of throughput (`count`) here. Also available are
    `value` to retrieve a gauge value, `total` to retrieve timer total time, distribution
    summary total amount, long task timer active tasks, and `percentile`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里检索吞吐量（`count`）的度量。也可以使用`value`检索计量值，`total`检索定时器总时间，分布摘要总量，长任务定时器活动任务和`percentile`。
- en: '[![5](../images/00100.png)](part0012_split_007.html#co_traffic_management_CO1-5)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](../images/00100.png)](part0012_split_007.html#co_traffic_management_CO1-5)'
- en: A threshold that we are testing the measure against. When this service is receiving
    more than 10,000 requests/second, it reports itself as out of service to anything
    monitoring its health endpoint.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在对度量值进行测试的阈值。当该服务每秒接收超过10,000个请求时，它会将自身报告为服务终端点监控的任何内容的服务中断。
- en: When this health indicator is being consumed by a gateway that can contain custom
    code to respond to different conditions, it’s best to always report `UP` as the
    status for this health indicator. We could hardcode some fixed threshold in the
    application and report a different status like `OVERLOADED` when the utilization
    exceeds the threshold. Better would be to fetch the threshold from a dynamic configuration
    server such that the value can be changed on running instances in one stroke by
    changing the config server. Best is to leave the determination of what utilization
    is too much to the load balancer, which could be folding this decision into more
    complex criteria.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当此健康指标被消费者网关使用，并且可以包含自定义代码以响应不同条件时，最好始终报告此健康指标的状态为`UP`。我们可以在应用程序中硬编码一些固定的阈值，并在利用率超过阈值时报告不同的状态，例如`OVERLOADED`。更好的方法是从动态配置服务器中获取阈值，以便可以在运行实例中通过更改配置服务器的值来一次性更改该值。最好是将决定何时利用过多的任务留给负载均衡器，该负载均衡器可以将此决策折叠到更复杂的标准中。
- en: Health Checks
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 健康检查
- en: Sometimes, though, our “gateway” is something like a platform load balancer
    (by platform load balancer I mean something like an AWS Application Load Balancer)
    that can respond to coarser measures of availability for its decisions. For example,
    many platform load balancers offer a means to configure a health check path and
    port. This could easily be configured to `/actuator/health`, but the platform
    load balancer will be responding only to whether the HTTP status of the response
    is successful. There isn’t enough configurability to peek at the utilization detail
    and make a decision relative to a threshold. In this case, it really is up to
    the application code to set a threshold and return `Health.up()` or `Health.outOfService()`.
    While there is nothing really inherently wrong with leaving this decision up to
    the app, it does require some a priori knowledge of performance at the time the
    app is being written, and of course is less flexible in the deployed environment.
    As an example of a platform load balancer that looks at health checks, DigitalOcean
    provides a “health check” [configuration](https://oreil.ly/yYBzh) for Kubernetes
    load balancers, as shown in [Example 7-4](part0012_split_008.html#k8s_health_check_lb).
    Health check configurations are also available for AWS Auto Scaling Groups and
    Google Cloud load balancers. Azure load balancers offer a similar configuration
    that is called a “health probe.”
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 但有时我们的“网关”是某种平台负载均衡器（通过平台负载均衡器，我指的是类似于 AWS 应用负载均衡器的东西），它可以响应其决策的较粗略的可用性指标。例如，许多平台负载均衡器提供了配置健康检查路径和端口的手段。这可以很容易地配置为
    `/actuator/health`，但平台负载均衡器只会响应响应的 HTTP 状态是否成功。配置灵活度不足以查看利用率详细信息并根据阈值做出决策。在这种情况下，真正需要应用程序代码来设置阈值并返回
    `Health.up()` 或 `Health.outOfService()`。虽然将此决策留给应用程序并没有真正的本质问题，但这确实需要在编写应用程序时对性能有一些先验知识，并且在部署环境中灵活性较低。作为查看健康检查的平台负载均衡器的示例，DigitalOcean
    为 Kubernetes 负载均衡器提供了“健康检查” [配置](https://oreil.ly/yYBzh)，如示例 7-4 所示。AWS Auto Scaling
    Groups 和 Google Cloud 负载均衡器也提供了健康检查配置。Azure 负载均衡器提供了类似的称为“健康探针”的配置。
- en: Example 7-4\. A Kubernetes load balancer configured to look at instance-reported
    utilization
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-4\. 配置为查看实例报告的利用率的 Kubernetes 负载均衡器
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: When setting up a health indicator like this, the task is to find some key performance
    indicator that best summarizes the application’s availability. This performance
    indicator should monitor whatever the weak spot is in the application where an
    overabundance of traffic will eventually cause trouble. We are choosing to consider
    the `/persons` API endpoint the key performance indicator of utilization availability
    in this example. We could have selected more than one endpoint, multiple HTTP
    response outcomes, or any other combination of factors for HTTP endpoint throughput.
    Also, there are other measures of utilization that we could have used. If this
    was an event-driven application, then the rate of messages consumed from a message
    queue or Kafka topic would be reasonable. If multiple execution paths in the application
    all led to an interaction with some utilization-constrained resources like a datasource
    or the file system, as in [Figure 7-6](part0012_split_008.html#utilization_of_datasource),
    then measuring the utilization on that resource would also seem reasonable.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置类似这样的健康指标时，任务是找到一些最能概括应用可用性的关键性能指标。这个性能指标应该监控应用程序中的弱点，即过多的流量最终会导致问题。在本例中，我们选择将
    `/persons` API 端点视为此示例中利用率可用性的关键性能指标。我们可以选择多个端点、多个 HTTP 响应结果或任何其他 HTTP 端点吞吐量的组合。此外，还有其他可以使用的利用率测量。如果这是一个事件驱动的应用程序，那么从消息队列或
    Kafka 主题中消费消息的速率也是合理的。如果应用程序中的多个执行路径都导致与某些利用率受限资源（如数据源或文件系统）的交互，就像图 7-6 中描述的那样，那么测量该资源上的利用率也似乎是合理的。
- en: '![srej 0706](../images/00052.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0706](../images/00052.png)'
- en: Figure 7-6\. Measure throughput on a datasource when multiple execution paths
    lead it to be a bottleneck
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-6\. 当多个执行路径导致数据源成为瓶颈时，测量数据源的吞吐量
- en: This health indicator can be added to a new Spring Boot application generated
    from [start.spring.io](https://start.spring.io) that includes a runtime dependency
    on `io.micrometer:micrometer-core` and the configuration found in [Example 7-5](part0012_split_008.html#poll_instance_utilization_server_config).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此健康指标可以添加到从[start.spring.io](https://start.spring.io)生成的新的Spring Boot应用程序，该应用程序包括对`io.micrometer:micrometer-core`的运行时依赖项以及[示例 7-5](part0012_split_008.html#poll_instance_utilization_server_config)中找到的配置。
- en: Example 7-5\. Required application.yml configuration
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-5\. 必需的 application.yml 配置
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The response from `http://APP_HOST/actuator/health` includes the instance’s
    view of its own utilization. Whether or not this utilization represents near full
    capacity is not important from the perspective of the “choice of two” algorithm.
    It is the algorithm’s choice to weight higher the lower of two such figures. It
    is only when the gateway/load balancer needs to prefilter the list of instances
    presented to the “choice of two” algorithm that it needs some domain-specific
    knowledge of a reasonable cutoff threshold to use, being at that point imbued
    with some understanding of whether a particular utilization level represents *too
    much* utilization or not.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从`http://APP_HOST/actuator/health`收到的响应包括实例对其自身利用率的视图。从网关/负载均衡器需要对呈现给“选择两个”算法的实例列表进行预过滤时，是否此利用率表示接近满负载并不重要。算法选择权重更高的两个数字中较低的数字。仅当网关/负载均衡器需要预过滤呈现给“选择两个”算法的实例列表时，它才需要一些特定领域知识的合理截止阈值的某种理解，该阈值是超过某一利用率水平是否表示*过多*利用率的重要视角。
- en: By actively polling each instance, we are adding additional load on each instance
    proportional to the number of load balancer nodes. But for a service with low
    throughput, specifically when the utilization polling rate *exceeds* the request
    rate through a particular load balancer, polling provides a more accurate picture
    of utilization.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 通过主动轮询每个实例，我们在每个实例上增加了与负载均衡器节点数量成比例的额外负载。但对于吞吐量低的服务，特别是当利用率轮询速率*超过*特定负载均衡器的请求率时，轮询提供了更准确的利用率图片。
- en: The passive strategy provides as up-to-date a view of utilization as the last
    request to arrive at a particular instance. The higher the throughput to an instance,
    the more accurate the utilization measure is.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 被动策略提供了到达特定实例的最后请求的最新利用率视图。对实例的吞吐量越高，利用率测量就越准确。
- en: We can use instance-reported utilization or health as an input to a more randomizing
    heuristic, as described next.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用实例报告的利用率或健康作为更加随机化的启发式算法的输入，接下来将描述。
- en: Choice of Two
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择两个
- en: “Choice of two” selects two servers randomly and selects one based on the maximization
    of some criteria.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: “Choice of two” 会随机选择两个服务器，并基于某些标准最大化选择其中一个。
- en: Defining the criteria with multiple factors limits bias that could unintentionally
    lead to herding. For example, suppose one server is failing on every request and
    that (as is often the case) the failure mode is such that failed responses have
    a lower response time than a successful response. The instance’s utilization will
    appear lower. If utilization was the only factor used, then load balancers would
    start sending *more* requests to the unhealthy instance!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多因素定义标准限制了可能无意中导致羊群效应的偏见。例如，假设一个服务器在每个请求上都失败，并且（通常情况下）故障模式使得失败响应的响应时间低于成功响应。实例的利用率将显得更低。如果仅使用利用率作为唯一因素，那么负载均衡器将开始向不健康的实例发送*更多*请求！
- en: 'Compute an aggregate of these three factors and maximize on the aggregate for
    the choice of two selection:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 计算这三个因素的聚合，并在选择两个选择上进行最大化：
- en: Client health
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端健康
- en: A measure of connection-related errors for that instance
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 该实例的连接相关错误度量
- en: Server utilization
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器利用率
- en: The most recent utilization measure provided by the instance
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由实例提供的最新利用率测量
- en: Client utilization
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端利用率
- en: Count of in-flight requests to the instance from this load balancer
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从此负载均衡器到实例的正在进行的请求计数
- en: To make this even more robust, consider prefiltering the list of servers from
    which the two are chosen and compared. Make sure to bound the filtering in some
    way to avoid high CPU cost in searching for relatively healthy instances in a
    cluster with a large pool of instances that are unhealthy (e.g., by only attempting
    so many times to select a relatively healthy instance). By filtering, we can present
    the choice of two algorithm with a choice between relatively healthy instances
    even when some portion of the cluster is persistently unavailable.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这更加健壮，考虑预先过滤从中选择和比较的服务器列表。确保以某种方式限制过滤，以避免在具有大量不健康实例的集群中搜索相对健康实例时造成高CPU成本（例如，仅尝试多次选择相对健康实例）。通过过滤，我们可以在一部分集群持续不可用时，仍然向两个算法的选择提供相对健康实例之间的选择。
- en: We can add one last tweak to our selection algorithm to accommodate cold starts.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以向我们的选择算法中添加最后一个调整，以适应冷启动。
- en: Instance Probation
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实例试用期
- en: To avoid overloading new instances while they are still undergoing their second-phase
    warmup, we can simply place a static limit on the number of requests that are
    allowed to go to that new instance. The probationary period ends when the load
    balancer receives one or more utilization responses from the new instance.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在新实例仍在进行第二阶段预热时过载，我们可以简单地对允许发送到该新实例的请求数量设置一个静态限制。当负载均衡器从新实例接收到一个或多个利用率响应时，试用期结束。
- en: The concept of statically rate limiting new instances can be extended to include
    a gradually ramping-up rate limit based on the instance’s age. Micrometer includes
    a `process.uptime` metric out of the box that can be used to calculate instance
    age.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 静态限制新实例的速率限制概念可以扩展到基于实例年龄逐渐增加的速率限制。Micrometer开箱即用地包含了一个`process.uptime`度量标准，可用于计算实例的年龄。
- en: Now that we have a toolbox of load-balancing strategies, let’s think about some
    of the unintended side effects they can have.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们拥有了一系列负载均衡策略的工具箱，让我们思考一下它们可能产生的一些意外副作用。
- en: Knock-On Effects of Smarter Load Balancing
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更智能负载均衡的连锁效应
- en: 'The goal of developing the choice of two load balancers was to divert traffic
    away from instances suffering from availability problems. This has some interesting
    effects:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将流量从出现可用性问题的实例转移开来，开发了两个负载均衡器的选择目标。这带来了一些有趣的效果：
- en: When load balancing across two clusters in a blue/green deployment (or rolling
    blue/green), if one of the clusters has relatively worse performance, then it
    will receive less than an equal share of traffic.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在蓝/绿部署（或滚动蓝/绿）中负载均衡两个集群时，如果其中一个集群的性能相对较差，则它将接收到比相等份额更少的流量。
- en: In an automated canary analysis setup, the baseline and canary may receive different
    proportions of traffic for the same reason.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在自动金丝雀分析设置中，由于同样的原因，基准和金丝雀可能会接收到不同比例的流量。
- en: Anomaly detection may not pick up on outliers as quickly, as early signals of
    low reliability mean that fewer attempts are made against that instance.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常检测可能不会像往常那样迅速发现异常值，因为低可靠性的早期信号意味着对该实例的尝试较少。
- en: The request distribution will not be as uniform as a round-robin load balancer.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求分布不会像轮询负载均衡器那样均匀。
- en: Availability signals are always decayed over time. In the instance-reported
    utilization examples ([Example 7-3](part0012_split_007.html#poll_instance_utilization_server)),
    this is why we used a rate-per-interval measure of utilization. As soon as the
    interval rolls over, a period of instability is no longer reflected in utilization
    data. The end result is if an instance recovers from a period of low availability,
    it can win a choice-of-two comparison again and receive traffic from the load
    balancer.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 可用性信号始终随时间衰减。在实例报告的利用率示例中（[示例 7-3](part0012_split_007.html#poll_instance_utilization_server)），这就是为什么我们使用了利用率的每个时间间隔的速率测量。一旦时间间隔结束，不稳定期不再反映在利用率数据中。结果是，如果一个实例从低可用性期恢复过来，则它可以再次在选择两个实例时获胜，并从负载均衡器接收流量。
- en: Not every microservice architecture is designed such that inter-microservice
    requests always pass through a gateway (nor should they be).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 并非每种微服务架构都设计成使得微服务间请求总是通过网关传递（也不应该如此）。
- en: Client-Side Load Balancing
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户端负载均衡
- en: A third option is to implement a client-side load balancer. This leaves load-balancing
    decisions to the caller. Historically, client-side load balancing has been used
    for novel load-balancing strategies like cloud platform zone avoidance or zone
    affinity, preference for lowest weighted response times, etc. When these strategies
    work well in general, they tend to reemerge as features of platform load balancers.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种选择是实现客户端负载均衡器。这将负载均衡决策留给调用方。历史上，客户端负载均衡已被用于新颖的负载均衡策略，比如云平台区避免或区亲和性，对最低加权响应时间的偏好等。当这些策略在一般情况下运行良好时，它们往往会重新出现作为平台负载均衡器的特性。
- en: '[Figure 7-7](part0012_split_012.html#client_side_load_balancer) shows an interaction
    between Service A and Service B where Service A is using a client-side load balancer
    to distribute traffic to Service B. The client-side load balancer is part of Service
    A’s application code. Typically, an instance list will be fetched from a discovery
    service like Eureka or Consul, and because the client-side load balancer directs
    traffic to Service B instances picked from the instance list fetched from discovery,
    there is no need for a platform load balancer in front of Service B.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 7-7](part0012_split_012.html#client_side_load_balancer) 展示了服务 A 和服务 B 之间的交互，其中服务
    A 使用客户端负载均衡器将流量分发到服务 B。客户端负载均衡器是服务 A 应用代码的一部分。通常情况下，实例列表将从诸如 Eureka 或 Consul 的发现服务中获取，并且由于客户端负载均衡器将流量引导到从发现服务获取的服务
    B 实例，所以在服务 B 前面不需要平台负载均衡器。'
- en: '![srej 0707](../images/00087.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0707](../images/00087.png)'
- en: Figure 7-7\. Client-side load balancing
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-7\. 客户端负载均衡
- en: Client-side load balancing can be used for different purposes. One of the original
    purposes was to dynamically source a list of server IPs or host names from a central
    service discovery mechanism like Eureka or Consul.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端负载均衡可以用于不同的目的。最初的一个目的是从像 Eureka 或 Consul 这样的中央服务发现机制动态获取服务器 IP 或主机名列表。
- en: Why Service Discovery Instead of a Cloud Load Balancer?
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择服务发现而不是云负载均衡器？
- en: When Netflix first developed Eureka, AWS VPC did not yet exist, and Elastic
    Load Balancers always had public, internet-facing host names. Not desiring to
    expose internal microservices to the public internet, Netflix built Eureka to
    achieve centrally what a private Application Load Balancer (ALBs being a replacement
    for what is now considered a legacy ELB construct in AWS) can achieve on a per-microservice
    basis. Perhaps if VPC was around when Netflix first migrated to AWS, Eureka would
    never have come about. Nevertheless, its use has extended beyond just load balancing
    to available instances in a cluster. [Table 5-1](part0010_split_011.html#eureka_api_availability)
    showed how it is also used in blue/green deployments of event-driven microservices
    to take the instances in a disabled cluster out of service. Not every enterprise
    will take advantage of this kind of tooling, and if not, private cloud load balancers
    are probably simpler to manage.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Netflix 首次开发 Eureka 时，AWS VPC 还不存在，Elastic Load Balancers 总是具有公共的、面向互联网的主机名。不希望将内部微服务暴露给公共互联网，Netflix
    构建了 Eureka，以实现中心化管理，类似于私有应用负载均衡器（ALBs 现在被认为是 AWS 的旧 ELB 构造的替代品）可以在每个微服务基础上实现的功能。也许如果
    VPC 在 Netflix 首次迁移到 AWS 时就存在，Eureka 就永远不会出现。尽管如此，它的使用已经扩展到了集群中可用实例的范围之外。[表 5-1](part0010_split_011.html#eureka_api_availability)
    展示了它在事件驱动微服务的蓝/绿部署中的使用，以使禁用集群中的实例失效。并非每家企业都会利用这种工具，如果不使用的话，私有云负载均衡器可能更简单管理。
- en: Spring Cloud Commons has a client-side load-balancing abstraction that makes
    the configuration of these typical concerns fairly straightforward, as in [Example 7-6](part0012_split_013.html#load_balancing_zone_health_checks).
    Any use of the `WebClient` generated from such a configuration will cache the
    service listing for a period of time, preferring instances in the same zone, and
    using the configured `DiscoveryClient` to fetch the list of available names.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Cloud Commons 提供了一个客户端负载均衡抽象，使得配置这些典型问题变得相当简单，就像 [例子 7-6](part0012_split_013.html#load_balancing_zone_health_checks)
    中那样。从这样的配置生成的 `WebClient` 将缓存一段时间的服务列表，优先选择同一区域内的实例，并使用配置的 `DiscoveryClient` 获取可用名称列表。
- en: Example 7-6\. Load balancing health checks
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例子 7-6\. 负载均衡健康检查
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Not all load balancing is about server selection, however. There is one particular
    client-side load-balancing strategy used to cut off tail latencies above the 99th
    percentile.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并非所有的负载均衡都是关于服务器选择。有一种特定的客户端负载均衡策略用于切断高于第 99 百分位数的尾延迟。
- en: Hedge Requests
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 避免请求过载
- en: '[Chapter 2](part0006_split_000.html#5N3C4-2d714b853a094e9a910510217e0e3d73)
    showed that for *N* requests, the chance that at least one of these requests is
    in the top 1% of the latency distribution is <math alttext="left-parenthesis 1
    minus 0.99 Superscript upper N Baseline right-parenthesis asterisk 100 percent-sign"><mrow><mo>(</mo>
    <mn>1</mn> <mo>-</mo> <mn>0</mn> <mo>.</mo> <msup><mn>99</mn> <mi>N</mi></msup>
    <mo>)</mo> <mo>*</mo> <mn>100</mn> <mo>%</mo></mrow></math> . Furthermore, we
    saw how latency distributions are almost always multimodal, with the top 1% generally
    one to two orders of magnitude worse than the 99th percentile. For even 100 individual
    resource interactions, the chance of encountering one of these top 1% latencies
    is <math alttext="left-parenthesis 1 minus 0.99 Superscript 100 Baseline right-parenthesis
    asterisk 100 percent-sign equals 63.3 percent-sign"><mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mn>0</mn> <mo>.</mo> <msup><mn>99</mn> <mn>100</mn></msup> <mo>)</mo>
    <mo>*</mo> <mn>100</mn> <mo>%</mo> <mo>=</mo> <mn>63</mn> <mo>.</mo> <mn>3</mn>
    <mo>%</mo></mrow></math> .'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[第2章](part0006_split_000.html#5N3C4-2d714b853a094e9a910510217e0e3d73)显示，对于*N*个请求，至少有一个请求在延迟分布的前1%的几率是
    <math alttext="left-parenthesis 1 minus 0.99 Superscript upper N Baseline right-parenthesis
    asterisk 100 percent-sign"><mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mn>0</mn> <mo>.</mo>
    <msup><mn>99</mn> <mi>N</mi></msup> <mo>)</mo> <mo>*</mo> <mn>100</mn> <mo>%</mo></mrow></math>
    。此外，我们看到延迟分布几乎总是多模的，前1%通常比第99百分位数差一到两个数量级。即使是100个独立资源交互，遇到其中一个前1%延迟的几率是 <math
    alttext="left-parenthesis 1 minus 0.99 Superscript 100 Baseline right-parenthesis
    asterisk 100 percent-sign equals 63.3 percent-sign"><mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mn>0</mn> <mo>.</mo> <msup><mn>99</mn> <mn>100</mn></msup> <mo>)</mo>
    <mo>*</mo> <mn>100</mn> <mo>%</mo> <mo>=</mo> <mn>63</mn> <mo>.</mo> <mn>3</mn>
    <mo>%</mo></mrow></math> 。'
- en: One well-tested strategy to mitigate the effects of the top 1% latency when
    calling a downstream service or resource is to simply ship multiple requests downstream
    and accept whichever response comes back first, discarding the others.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 一种经过良好测试的策略，用于减轻调用下游服务或资源时顶部1%延迟的影响，是简单地向下游发送多个请求，并接受首先返回的响应，丢弃其他响应。
- en: This approach may seem surprising because obviously it increases the load on
    the downstream linearly according to the additional number of requests you include
    in your hedge (and potentially this fans out more than linearly beyond the direct
    downstream, as it in turn makes requests to *its* downstreams and so on). In many
    enterprises, services experience a throughput that doesn’t come close to their
    total capacity, and the most resilient services are scaled horizontally in some
    sort of active-active capacity to limit the impact of an outage in any one region.
    This has the effect of increasing capacity (generally unused) to improve resiliency.
    At its best, hedge requesting can serve to simply use this excess capacity while
    improving end-user response times by significantly reducing the frequency of the
    worst latencies.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可能会令人惊讶，因为显然它会根据您在您的对冲中包含的额外请求数量线性增加下游的负载（而且潜在地，这可能会超出直接下游的线性扇出，因为它反过来会向*它的*下游发出请求，依此类推）。在许多企业中，服务的吞吐量未能接近其总容量，而最具韧性的服务则以某种主动-主动的方式进行水平扩展，以限制在任何一个区域发生故障的影响。这样做的效果是增加容量（通常是未使用的），以提高韧性。在最好的情况下，对冲请求可以简单地利用这些多余的容量，同时通过显著减少最糟糕延迟的频率来改善终端用户响应时间。
- en: Obviously, the decision to employ hedge requesting requires some domain-specific
    knowledge about the downstream service being called. It wouldn’t be appropriate
    to ship three requests to a third-party payment system to charge a customer’s
    credit card three times! For this reason, hedge requesting is typically performed
    in application code.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，决定采用对冲请求需要一些关于所调用下游服务的领域特定知识。将三个请求发送到第三方支付系统以收取客户的信用卡费用是不合适的！因此，对冲请求通常在应用程序代码中执行。
- en: Hedge Requests Can’t Be Implemented by Service Mesh Client-Side Load Balancers
    (!!)
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对冲请求无法由服务网格客户端负载均衡器实施（！！）
- en: Since domain-specific knowledge essentially requires that the load-balancing
    decision is made in the calling application, notice that shifting the responsibility
    for client-side load balancing to a service mesh is unworkable for hedge requesting.
    Given that hedge requesting is one of the simplest and most effective means of
    compensating for long-tail latencies above the 99th percentile, the inability
    of service mesh to replace application code for this purpose should be a trigger
    to consider whether the service mesh pattern is really appropriate more generally.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于领域特定的知识基本上要求负载平衡决策是由调用应用程序进行的，因此要注意将客户端负载平衡的责任转移到服务网格对于对冲请求来说是行不通的。鉴于对冲请求是补偿长尾延迟超过99百分位数的最简单有效的方法之一，因此服务网格无法替代应用程序代码来实现这一目的应该是考虑服务网格模式是否真正适用于更广泛的情况的触发器。
- en: We now turn the discussion to patterns that compensate for failure in downstream
    microservices or lessen the likelihood of such services being overwhelmed in the
    first place.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论补偿下游微服务失败或减少这类服务首次被淹没的模式。
- en: Call Resiliency Patterns
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调用弹性模式
- en: Regardless of how well a load balancer makes a predictive decision about which
    instance *should* handle traffic best, any prediction is based on a projection
    of past performance. Past performance is never a guarantee of future results,
    so there still is a need for another level of resiliency to handle failure. Additionally,
    even a microservice cluster behind a load balancer that perfectly allocates traffic
    to the most available instances at any given time has a limit for what it can
    handle. The layers of a microservice architecture need to be guarded against overloading
    that could lead to complete failure.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 无论负载均衡器对哪个实例能够最好地处理流量做出多么准确的预测决策，任何预测都是基于过去性能的投影。过去的表现永远不是未来结果的保证，因此仍然需要另一层面的弹性来处理故障。此外，即使是一个完美地将流量分配给任何给定时间最可用的实例的负载均衡器后面的微服务集群，也有一个可以处理的极限。微服务架构的各层需要防止过载，这可能导致完全失败。
- en: These mechanisms together form different basic “backpressure” schemes.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这些机制一起形成了不同的基本“背压”方案。
- en: Backpressure is the signaling of failure from a serving system to the requesting
    system and how the requesting system handles those failures to prevent overloading
    itself and the serving system. Designing for backpressure means bounding resource
    use during times of overload and times of system failure. This is one of the basic
    building blocks of creating a robust distributed system. Implementations of backpressure
    usually involve either dropping new messages on the floor, or shipping errors
    back to users (and incrementing a metric in both cases) when a resource becomes
    limited or failures occur. Timeouts and exponential backoffs on connections and
    requests to other systems are also essential. Without backpressure mechanisms
    in place, cascading failure or unintentional message loss become likely. When
    a system is not able to handle the failures of another, it tends to emit failures
    to another system that depends on it.
  id: totrans-136
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 背压是从服务系统向请求系统发出失败信号以及请求系统如何处理这些失败，以防止过载自身和服务系统。设计背压意味着在过载时和系统故障时限制资源使用。这是创建强大的分布式系统的基本构建块之一。背压的实现通常涉及要么将新消息丢弃，要么在资源受限或发生故障时将错误返回给用户（并在这两种情况下递增度量）。对其他系统的连接和请求进行超时和指数级回退也是必不可少的。如果没有背压机制，级联故障或意外消息丢失变得可能。当系统无法处理另一个系统的故障时，它往往会向依赖于它的另一个系统发出故障。
- en: ''
  id: totrans-137
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jeff Hodges
  id: totrans-138
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Jeff Hodges
- en: 'The caller can combine four patterns to improve resiliency:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 调用者可以结合四种模式来提高弹性：
- en: Retries
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重试
- en: Rate limiters
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速率限制器
- en: Bulkheads
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 防波堤
- en: Circuit breakers
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 断路器
- en: Retries are an obvious first step to overcoming intermittent failure, but we
    need to be cautious of creating “retry storms,” i.e., overwhelming parts of the
    system that are already under duress with retries when the original requests begin
    to fail. The other patterns will help compensate. Still, let’s start with retries.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 重试是克服间歇性故障的明显第一步，但我们需要谨慎创建“重试风暴”，即在原始请求开始失败时，不要用重试淹没已经处于困境中的系统的部分。其他模式将有助于补偿。但是，让我们从重试开始。
- en: Retries
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重试
- en: 'Expect transient failure in downstream services, caused by temporarily full
    thread pools, slow network connections resulting in timeouts, or other temporary
    conditions that lead to unavailability. This class of faults typically self-correct
    after a short period of time. Callers should be prepared to handle transient failure
    by wrapping calls to downstreams in retry logic. Consider three factors when adding
    retries:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 预期下游服务的短暂失败，由于临时满线程池、慢网络连接导致超时或其他导致不可用的临时条件。这类故障通常在短时间后会自行纠正。调用者应准备通过在对下游的调用中包装重试逻辑来处理短暂的失败。在添加重试时考虑三个因素：
- en: Whether retries are appropriate. This often requires domain-specific knowledge
    of the called service. For example, should we retry a payment attempt on a downstream
    service that returned a timeout? Will the timed-out operation eventually be processed,
    making a retry a potential double charge?
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否适合重试。这通常需要对被调用服务的领域特定知识。例如，我们是否应该在下游服务返回超时时重试支付尝试？超时操作最终会被处理，重试是否可能导致重复收费？
- en: The maximum number of retry attempts, and the duration (including backoffs,
    as shown in [Example 7-7](part0012_split_017.html#retry_resilience4j)) to use
    between attempts.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大重试次数和重试尝试之间使用的持续时间（包括退避，如 [示例 7-7](part0012_split_017.html#retry_resilience4j)
    所示）。
- en: Which responses (and exception types) warrant a retry. For example, if a downstream
    returns a 400 because the inputs are malformed for some reason, we cannot expect
    a different result by retrying the same inputs.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些响应（和异常类型）需要重试。例如，如果下游因为某些原因返回 400 错误，表明输入格式错误，我们不能期望通过重试相同的输入获得不同的结果。
- en: Example 7-7\. Setting up an exponential backoff retry with Resilience4J
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-7\. 使用 Resilience4J 设置指数退避重试
- en: '[PRE6]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Resilience4J has a built-in metric for retry logic, enabled by binding your
    retry to a Micrometer meter registry, as in [Example 7-8](part0012_split_017.html#retry_metrics_bind).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Resilience4J具有用于重试逻辑的内置指标，通过将重试绑定到Micrometer的仪表注册表来启用，如 [示例 7-8](part0012_split_017.html#retry_metrics_bind)
    所示。
- en: Example 7-8\. Publishing metrics about retries via Micrometer
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-8\. 通过 Micrometer 发布关于重试的指标
- en: '[PRE7]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This exports a single gauge `resilience4j.retry.calls` with a `kind` tag segregating
    successful (with and without retry) and failed (with and without retry) calls.
    If you were to set an alert, it would be on a fixed threshold of calls where `kind`
    equals `failed.with.retry`. In many cases, the code where the call is being made
    is itself going to be timed. For example, a REST endpoint that, when invoked,
    does some work, including making downstream service calls with retry logic, is
    itself going to be timed with `http.server.requests`, and you should already be
    alerting on failures of that endpoint.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这导出了一个单一的仪表 `resilience4j.retry.calls`，带有一个 `kind` 标签，区分成功（带重试和无重试）和失败（带重试和无重试）的调用。如果设置了警报，它将基于
    `kind` 等于 `failed.with.retry` 的调用数阈值。在许多情况下，进行调用的代码本身将会计时。例如，一个 REST 端点在调用时会执行一些工作，包括使用重试逻辑进行下游服务调用，该端点本身将使用
    `http.server.requests` 进行计时，您应该已经在该端点的失败上设置了警报。
- en: Nevertheless, if your application contains a common component guarding access
    to a resource or downstream service with retry logic, then alerting on a high
    failure rate to that resource can be a good signal that several pieces of your
    application will be failing.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您的应用程序包含一个常见的组件，通过重试逻辑保护对资源或下游服务的访问，那么对该资源的高失败率报警可能是一个很好的信号，表明应用程序的几个部分将会失败。
- en: Rate Limiters
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 速率限制器
- en: The load on a microservice naturally varies over time based on user activity
    patterns, scheduled batch processes, etc. An atypical event could result in sudden
    and overwhelming bursts in activity. If increased load, maybe even for a particular
    business function served by a microservice, causes a strain on resources that
    could result in availability levels falling below an established SLO, rate limiting
    (also known as throttling) can keep the service up and serving requests, albeit
    at a defined rate of throughput.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务的负载根据用户活动模式、定期批处理等而自然变化。突发事件可能导致活动突然激增，给某个微服务提供的特定业务功能造成资源紧张，可能导致可用性低于已建立的
    SLO，速率限制（也称为节流）可以保持服务运行并响应请求，尽管在定义的吞吐率下。
- en: Resilience4J implements the rate limiter pattern with several options. In [Example 7-9](part0012_split_018.html#BE7EN-2d714b853a094e9a910510217e0e3d73),
    a rate limiter is used in a microservice that needs to make calls against downstream
    billing history and payment services. A diagram of the service interaction is
    shown in [Figure 7-8](part0012_split_018.html#call_resiliency_sample_services).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Resilience4J 使用多种选项实现速率限制器模式。在 [例子 7-9](part0012_split_018.html#BE7EN-2d714b853a094e9a910510217e0e3d73)
    中，一个微服务需要调用下游的账单历史和支付服务时使用了速率限制器。服务交互的图示如 [图 7-8](part0012_split_018.html#call_resiliency_sample_services)
    所示。
- en: '![srej 0708](../images/00107.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0708](../images/00107.png)'
- en: Figure 7-8\. Call resiliency example service interaction
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-8\. 调用恢复性示例服务交互
- en: Example 7-9\. Implementing a rate limiter with Resilience4J
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例子 7-9\. 使用 Resilience4J 实现速率限制器
- en: '[PRE8]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](../images/00112.png)](part0012_split_018.html#co_traffic_management_CO2-1)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](../images/00112.png)](part0012_split_018.html#co_traffic_management_CO2-1)'
- en: Concurrency limit allowed by the rate limiter.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 速率限制器允许的并发限制。
- en: '[![2](../images/00059.png)](part0012_split_018.html#co_traffic_management_CO2-2)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](../images/00059.png)](part0012_split_018.html#co_traffic_management_CO2-2)'
- en: Timeout for a blocked thread attempting to enter a saturated rate limiter.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 阻塞线程尝试进入饱和速率限制器的超时时间。
- en: '[![3](../images/00067.png)](part0012_split_018.html#co_traffic_management_CO2-3)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](../images/00067.png)](part0012_split_018.html#co_traffic_management_CO2-3)'
- en: A rate limiter for some service can be created with a configuration different
    from the global one (e.g., because this service is capable of a higher concurrency
    level than others).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用与全局配置不同的配置创建某个服务的速率限制器（例如，因为此服务能够承受比其他服务更高的并发级别）。
- en: Resilience4J has built-in metrics for rate limiters. Bind your rate limiter
    registry to a Micrometer meter registry, as in [Example 7-10](part0012_split_018.html#rate_limiter_metrics_bind).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Resilience4J 针对速率限制器内置了指标。将您的速率限制器注册到 Micrometer 米表注册中，例如 [例子 7-10](part0012_split_018.html#rate_limiter_metrics_bind)。
- en: Example 7-10\. Publishing metrics about rate limiting via Micrometer
  id: totrans-171
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例子 7-10\. 通过 Micrometer 发布有关速率限制的指标
- en: '[PRE9]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The metrics in [Table 7-1](part0012_split_018.html#rate_limiter_metrics) are
    then published.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后发布 [表格 7-1](part0012_split_018.html#rate_limiter_metrics) 中的指标。
- en: The two metrics have different benefits. Available permissions is an interesting
    *predictive* indicator. If permissions are reaching zero or near zero (where maybe
    they previously did not), but waiting threads is low, then end-user experience
    has not yet been degraded. A high number of waiting threads is a more *reactive*
    measure that the downstream service may need to be scaled up because end-user
    experience is being degraded (if response time is important and there are often
    waiting threads).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个指标具有不同的优势。可用权限是一个有趣的 *预测性* 指标。如果权限接近零（或之前没有），但等待线程数较低，则尚未降低终端用户体验。高等待线程数是一个更
    *反应性* 的度量，表示下游服务可能需要扩展，因为终端用户体验正在降低（如果响应时间很重要且经常有等待线程）。
- en: Table 7-1\. Rate limiter metrics exposed by Resilience4J
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 7-1\. Resilience4J 提供的速率限制器指标
- en: '| Metric name | Type | Description |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 指标名称 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| resilience4j.ratelimiter.available.permissions | Gauge | The number of available
    permissions, or unused concurrency capacity |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| resilience4j.ratelimiter.available.permissions | Gauge | 可用权限数或未使用的并发容量 |'
- en: '| resilience4j.ratelimiter.waiting.threads | Gauge | The number of waiting
    threads |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| resilience4j.ratelimiter.waiting.threads | Gauge | 等待线程数 |'
- en: In Atlas, the alert condition for waiting threads tests against a fixed threshold,
    as shown in [Example 7-11](part0012_split_018.html#atlas_ratelimit).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Atlas 中，等待线程的警报条件根据固定阈值进行测试，如 [例子 7-11](part0012_split_018.html#atlas_ratelimit)
    所示。
- en: Example 7-11\. Atlas rate limiter alert threshold
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例子 7-11\. Atlas 速率限制器警戒阈值
- en: '[PRE10]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: In Prometheus, the idea is similar, as shown in [Example 7-12](part0012_split_018.html#prometheus_ratelimit).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Prometheus 中，其思想类似，如 [例子 7-12](part0012_split_018.html#prometheus_ratelimit)
    所示。
- en: Example 7-12\. Prometheus rate limiter alert threshold
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 例子 7-12\. Prometheus 速率限制器警戒阈值
- en: '[PRE11]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Bulkheads
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 防护栅
- en: Microservices commonly execute requests on multiple downstream services. When
    a service suffers from low availability, it can cause dependent services to become
    unresponsive as well. This is particularly true when the dependent service is
    blocking and using a thread pool to make requests. For a microservice *A* with
    multiple downstream services, it could be that only a small portion of the traffic
    (requests of a certain type) cause a request to microservice *B*. If *A* is using
    a common thread pool to execute requests not only against *B* but against all
    of its other downstream services as well, then unavailability in *B* can gradually
    block requests, saturating threads in the common thread pool to the point where
    little or no work can happen.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: The bulkhead pattern isolates downstream services from one another, specifying
    different concurrency limits for each downstream service. In this way, only requests
    that require a service call to *B* become unresponsive, and the rest of the *A*
    service continues to be responsive.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Resilience4J implements the bulkhead pattern with several options, shown in
    [Example 7-13](part0012_split_019.html#BE7NH-2d714b853a094e9a910510217e0e3d73).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-13\. Implementing the bulkhead pattern with Resilience4J
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](../images/00112.png)](part0012_split_019.html#co_traffic_management_CO3-1)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency limit allowed by the bulkhead.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](../images/00059.png)](part0012_split_019.html#co_traffic_management_CO3-2)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Timeout for a blocked thread attempting to enter a saturated bulkhead.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](../images/00067.png)](part0012_split_019.html#co_traffic_management_CO3-3)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: A bulkhead for some service can be created with a configuration different from
    the global one (e.g., because this service is capable of a higher concurrency
    level than others).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Resilience4J has built-in metrics for bulkheads. Bind the bulkhead registry
    to a Micrometer meter registry, as in [Example 7-14](part0012_split_019.html#bulkhead_metrics_bind).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-14\. Publishing metrics about bulkheads via Micrometer
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Table 7-2](part0012_split_019.html#bulkhead_metrics) shows the bulkhead metrics
    shipped by Resilience4J. Alert when available concurrent calls frequently reach
    zero or near zero.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Table 7-2\. Bulkhead metrics exposed by Resilience4J
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric name | Type | Description |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
- en: '| resilience4j.bulkhead.available.concurrent.calls | Gauge | The number of
    available permissions, or unused capacity |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| resilience4j.bulkhead.max.allowed.concurrent.calls | Gauge | The maximum
    number of available permissions |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: In Atlas, the alert condition for waiting threads tests against a fixed threshold,
    as shown in [Example 7-15](part0012_split_019.html#bulkhead_atlas). This might
    also be a good place to use a `:roll-count` to limit alert chattiness.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-15\. Atlas bulkhead alert criteria
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In Prometheus, the idea is similar, as shown in [Example 7-16](part0012_split_019.html#bulkhead_prometheus).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Example 7-16\. Prometheus bulkhead alert criteria
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Circuit Breakers
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 断路器
- en: Circuit breakers are a further extension of bulkheading with a twist. A circuit
    breaker maintains a finite state machine, as shown in [Figure 7-9](part0012_split_020.html#circuit_breaker_states),
    for an execution block that it guards with states of closed, half-open, and open.
    In the closed and half-open states, executions are allowed. In the open state,
    a fallback defined by the application is executed instead.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 断路器是与一种扭曲的批处理延伸的进一步扩展。断路器维护一个有限状态机，如[图 7-9](part0012_split_020.html#circuit_breaker_states)所示，用于它保护的执行块，其状态为关闭、半开和打开。在关闭和半开状态下，允许执行。在打开状态下，执行应用程序定义的回退。
- en: '![Closed, half-open, and open](../images/00099.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![关闭、半开和打开](../images/00099.png)'
- en: Figure 7-9\. The states of a circuit breaker
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9. 断路器的状态
- en: One classic example of a circuit breaker is Netflix’s list of movie recommendations.
    This is generally personalized to the subscriber based on past viewing history,
    etc. A circuit breaker guarding a call to the personalization service might respond
    with a generic list of content as a fallback when the circuit breaker is in an
    open state.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 断路器的一个经典例子是Netflix的电影推荐列表。这通常是根据过去的观看历史等个性化订阅者的。一个守卫对个性化服务的调用的断路器可能会在断路器处于打开状态时以一份通用内容列表作为回退而响应。
- en: For some classes of business problems, a fallback that doesn’t ultimately present
    the user with a failure is impossible. There is no reasonable fallback to accepting
    a payment from a user (assuming it couldn’t be stored somewhere for later processing).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些类别的业务问题，一个最终不会向用户呈现失败的回退是不可能的。假设无法接受用户的付款（假设不能将其存储在某个地方以供以后处理）。
- en: Successful and unsuccessful executions are maintained in a ring buffer. When
    the ring buffer initially fills, the failure ratio is tested against a preconfigured
    threshold. The state of the circuit breaker changes from closed to open when the
    failure rate is above a configurable threshold. When the circuit breaker is tripped
    and opens, it will stop allowing executions for a defined period of time, after
    which the circuit half-opens, permits a small amount of traffic through, and tests
    the failure ratio of that small amount of traffic against the threshold. If the
    failure ratio falls below the threshold, the circuit is closed again.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 成功和失败的执行被维护在一个环形缓冲区中。当环形缓冲区初始填满时，失败率被测试是否超过预配置的阈值。当失败率高于可配置阈值时，断路器的状态从关闭变为打开。当断路器被触发并打开时，它将在定义的时间段内停止允许执行，之后断路器半开，允许少量流量通过，并测试该少量流量的失败率是否超过阈值。如果失败率低于阈值，则再次关闭电路。
- en: '[Netflix Hystrix](https://oreil.ly/By0-M) was the first major open source circuit
    breaker library, and while still well known, it has now been deprecated. Resilience4J
    implements the circuit breaker pattern with improvements to library hygiene and
    support for more threading models. An example is shown in [Example 7-17](part0012_split_020.html#BE7VQ-2d714b853a094e9a910510217e0e3d73).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[Netflix Hystrix](https://oreil.ly/By0-M) 是第一个主要的开源断路器库，尽管它仍然很有名，但现在已经被弃用。Resilience4J通过改进库的卫生性和支持更多线程模型来实现断路器模式。示例见[Example 7-17](part0012_split_020.html#BE7VQ-2d714b853a094e9a910510217e0e3d73)。'
- en: Example 7-17\. Implementing the circuit breaker pattern with Resilience4J
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 7-17. 使用Resilience4J实现断路器模式
- en: '[PRE16]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](../images/00112.png)](part0012_split_020.html#co_traffic_management_CO4-1)'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](../images/00112.png)](part0012_split_020.html#co_traffic_management_CO4-1)'
- en: A circuit breaker for some service can be created with a configuration different
    from the global one.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 某个服务的断路器可以使用与全局不同的配置创建。
- en: Resilience4J contains built-in metrics instrumentation for circuit breakers
    that you should monitor for open circuits. Enable it by binding your circuit breaker
    registry to a Micrometer meter registry, as in [Example 7-18](part0012_split_020.html#circuit_breaker_metrics_bind).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Resilience4J包含用于断路器的内置指标仪表，您应该监视打开的断路器。通过将断路器注册表绑定到Micrometer计量器注册表来启用它，如[Example 7-18](part0012_split_020.html#circuit_breaker_metrics_bind)所示。
- en: Example 7-18\. Bind circuit breaker metrics
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Example 7-18. 绑定断路器指标
- en: '[PRE17]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Table 7-3](part0012_split_020.html#circuit_breaker_metrics) shows the two
    metrics exposed by Resilience4J for *each* circuit breaker.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[Table 7-3](part0012_split_020.html#circuit_breaker_metrics) 显示了Resilience4J为*每个*断路器公开的两个指标。'
- en: Table 7-3\. Circuit breaker metrics exposed by Resilience4J
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-3. Resilience4J公开的断路器指标
- en: '| Metric name | Type | Description |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 指标名称 | 类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| resilience4j.circuitbreaker.calls | Timer | Total number of successful and
    failed calls |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| resilience4j.circuitbreaker.calls | 计时器 | 成功和失败调用的总数 |'
- en: '| resilience4j.circuitbreaker.state | Gauge | Set to 0 or 1 depending on whether
    the state described by the state tag is active (open, closed, etc.) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| resilience4j.circuitbreaker.state | 计量 | 根据状态标签描述的状态（开、闭等），设置为0或1。 |'
- en: '| resilience4j.circuitbreaker.failure.rate | Gauge | The failure rate of the
    circuit breaker |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| resilience4j.circuitbreaker.failure.rate | 计量 | 断路器的失败率 |'
- en: Since these are gauges, you can alert on whether *any* circuit breaker is currently
    open by performing a `max` aggregation. At this point, end users are already experiencing
    a degraded experience by receiving a fallback response or having the failure propagate
    to them directly.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些是计量器，您可以通过执行 `max` 聚合来警告任何断路器当前是否打开。此时，最终用户已经通过接收到回退响应或直接传播失败而经历了降级的体验。
- en: In Atlas, the alert condition checks for the open state, as shown in [Example 7-19](part0012_split_020.html#atlas_circuitbreaker).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Atlas 中，警报条件检查是否处于打开状态，如示例 7-19 中所示。
- en: Example 7-19\. Atlas circuit breaker alert threshold
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-19\. Atlas 断路器警报阈值
- en: '[PRE18]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](../images/00112.png)](part0012_split_020.html#co_traffic_management_CO5-1)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](../images/00112.png)](part0012_split_020.html#co_traffic_management_CO5-1)'
- en: If any circuit breaker is open, this alert will match on it.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何断路器打开，此警报将匹配它。
- en: In Prometheus, the idea is similar, as shown in [Example 7-20](part0012_split_020.html#prometheus_circuitbreaker).
    We can use `sum(..) > 0` or `max(..) == 1` with the same effect.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Prometheus 中，这个思路类似，如示例 7-20 中所示。我们可以使用 `sum(..) > 0` 或 `max(..) == 1` 来达到相同的效果。
- en: Example 7-20\. Prometheus circuit breaker alert threshold
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-20\. Prometheus 断路器警报阈值
- en: '[PRE19]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: It’s probably OK if circuits briefly open and then close again though, so to
    limit alert chattiness, it may be better to instead set an error ratio indicator
    on `resilience4j.circuitbreaker.calls`, allowing for a certain number of failed
    requests going to the fallback before alerting.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管电路可能会暂时打开然后再次关闭，但为了限制警报频繁性，可能最好是在 `resilience4j.circuitbreaker.calls` 上设置一个错误率指示器，允许在警报之前有一定数量的失败请求转向回退。
- en: Next we’ll discuss how we can improve the flexibility of the alert thresholds
    themselves by responding to changing conditions in the code and environment.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将讨论如何通过响应代码和环境中的变化来提高警报阈值本身的灵活性。
- en: Adaptive Concurrency Limits
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自适应并发限制
- en: Each of the call resiliency patterns presented so far (rate limiters, bulkheads,
    and circuit breakers) effectively serve to guard, either proactively or reactively,
    against load-related problems. They each have their own way of limiting concurrency.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，每个呼叫恢复模式（限速器、舱壁和断路器）都有效地用于防范与负载相关的问题，无论是预防性还是响应性的。
- en: In each case, the pattern was configured with a threshold value determined in
    advance of the microservice actually running in production. Rate limits are configured
    to constrain the number of requests that can be executed inside an interval, bulkheads
    limit instantaneous concurrency, and circuit breakers shed load away from instances
    that are experiencing failure (including load-related failure). These thresholds
    can be determined through careful performance testing, but their values tend to
    diverge from the true limit over time as code changes, the size of downstream
    clusters and their availability changes, etc.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，模式都配置有一个在微服务实际运行之前预先确定的阈值。速率限制器被配置为限制在间隔内可以执行的请求数量，舱壁限制瞬时并发，而断路器则将负载从遇到故障的实例（包括与负载相关的故障）中卸载出去。这些阈值可以通过仔细的性能测试确定，但随着代码更改、下游集群的规模及其可用性的变化等，它们的值往往会与真实限制偏离。
- en: A common theme throughout this book is to replace fixed thresholds or manual
    judgments with adaptive judgments. We saw this in [Chapter 2](part0006_split_000.html#5N3C4-2d714b853a094e9a910510217e0e3d73)
    with thresholds set with forecasting algorithms, and in [Chapter 5](part0010_split_000.html#9H5K4-2d714b853a094e9a910510217e0e3d73)
    with automated canary analysis. It is possible to adopt a similar adaptive approach
    to concurrency limits.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 本书始终以用自适应判断替代固定阈值或手动判断为主题。我们在第 2 章中看到了使用预测算法设置的阈值，在第 5 章中看到了自动金丝雀分析。类似的自适应方法也可以应用于并发限制。
- en: Choosing the Right Call Resiliency Pattern
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择正确的调用恢复模式
- en: In code, the patterns for bulkheads, rate limiters, and circuit breakers look
    remarkably similar. In fact, the three patterns have overlapping responsibilities,
    as shown in [Figure 7-10](part0012_split_022.html#call_resiliency_pattern_relationship).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，用于舱壁、速率限制器和断路器的模式看起来非常相似。实际上，这三种模式具有重叠的责任，如 [图 7-10](part0012_split_022.html#call_resiliency_pattern_relationship)
    所示。
- en: '![srej 0710](../images/00081.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![srej 0710](../images/00081.png)'
- en: Figure 7-10\. Overlapping responsibilities of three call resiliency patterns
  id: totrans-253
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 7-10\. 三种调用恢复模式的重叠责任
- en: All three patterns accomplish rate limiting, but with varying mechanisms, summarized
    in [Table 7-4](part0012_split_022.html#call_resiliency_pattern_rate_limiting_mechanisms).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三种模式都实现了速率限制，但采用了不同的机制，总结在 [表 7-4](part0012_split_022.html#call_resiliency_pattern_rate_limiting_mechanisms)
    中。
- en: Table 7-4\. The rate-limiting mechanisms of different call resiliency patterns
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-4\. 不同调用恢复模式的速率限制机制
- en: '| Pattern | Limiting mechanism | Notes |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 模式 | 限制机制 | 注意事项 |'
- en: '| --- | --- | --- |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Rate limiter | Limits rate per interval | This does not limit instantaneous
    concurrency (e.g., a spike of traffic inside of the interval) unless that instantaneous
    concurrency exceeds the limit for the whole interval to be reached. |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 速率限制器 | 限制间隔内的速率 | 这不限制瞬时并发性（例如，间隔内的流量突增）除非瞬时并发性超过了整个间隔的限制。 |'
- en: '| Bulkhead | Limits instantaneous concurrency level | Limits the concurrency
    level at the time that a new request is attempted. This does not limit the number
    of requests to the downstream inside an interval except indirectly since the downstream
    service responds in a nonzero amount of time. |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 舱壁 | 限制瞬时并发级别 | 限制在尝试新请求时的并发级别。这不直接限制间隔内的向下游请求数量，除非向下游服务以非零时间响应。 |'
- en: '| Circuit breaker | Responds to errors (some of which occur due to the natural
    limit in the downstream’s concurrency) | Either the RPC request guarded by the
    circuit breaker will time out or the downstream service (or its load balancer)
    will respond with a failure, such as an HTTP 502 (unavailable). This does not
    limit either the instantaneous or per-interval rate except indirectly when the
    downstream begins to be saturated. |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 断路器 | 响应错误（其中一些是由于下游并发的自然限制所导致） | 由断路器保护的 RPC 请求要么超时，要么下游服务（或其负载均衡器）会以失败响应，例如
    HTTP 502（不可用）。这不直接限制瞬时或每个间隔的速率，除非向下游开始饱和。 |'
- en: For this reason, it isn’t common to see a single block of code guarded by more
    than one of the patterns of rate limiter, bulkhead, or circuit breaker.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，很少见到一块代码同时由速率限制器、舱壁或断路器中的多个模式保护。
- en: The implementations of the call resiliency patterns shown to this point have
    all employed Resilience4J, making them an application development concern. Let’s
    compare keeping this as an application concern with externalizing the responsibility
    in service mesh.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止所展示的调用恢复模式的实现都使用了 Resilience4J，使其成为应用开发的关注点。让我们将这作为一个应用关注点与将责任外部化到服务网格中进行比较。
- en: Implementation in Service Mesh
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在服务网格中的实现
- en: Ultimately, the decision of whether to try to pry traffic management away from
    application concern has to be made in each organization based on the weight it
    places on several criteria shown in [Table 7-5](part0012_split_023.html#traffic_management_decision_matrix).
    “Application responsibility” here means that the functionality is achieved through
    application code or autoconfigured via a binary dependency on a shared library.
    The weights assigned to each criterion across the header row are just an example
    and will vary from organization to organization. For example, an organization
    that has a large number of programming languages in use may assign a much higher
    weight to language support. This should drive your decision.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，是否尝试将流量管理从应用关注点中剥离的决定必须根据每个组织的情况做出，基于其对 [表 7-5](part0012_split_023.html#traffic_management_decision_matrix)
    中所示多个标准的重视程度。这里的“应用责任”意味着通过应用代码实现功能或通过对共享库的二进制依赖自动配置。跨标题行分配给每个标准的权重仅是一个示例，并且会因组织而异。例如，对于使用大量编程语言的组织，可能会更高地分配给语言支持的权重。这应该是您决策的驱动因素。
- en: Table 7-5\. The service mesh versus application responsibility decision matrix
    for traffic management (higher score = higher cost)
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7-5\. 交通管理的服务网格与应用责任决策矩阵（得分越高 = 成本越高）
- en: '|  | Service mesh | Application responsibility |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '|  | 服务网格 | 应用程序责任 |'
- en: '| --- | --- | --- |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Language support = 5 | Low: only a thin client needed to connect to mesh
    (1 x 5 = 5) | High: distinct implementation required for each language (5 x 5
    = 25) |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 语言支持 = 5 | 低：只需要一个轻客户端来连接到网格（1 x 5 = 5） | 高：每种语言都需要不同的实现（5 x 5 = 25） |'
- en: '| Runtime support = 5 | High: for example, Istio is a Kubernetes CRD, so bound
    to a specific runtime (5 x 5 = 25) | Low: only has an impact if the library wants
    to take advantage of some specific feature of the runtime (1 x 5 = 5) |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 运行支持 = 5 | 高：例如，Istio 是 Kubernetes 的 CRD，因此绑定到特定运行时（5 x 5 = 25） | 低：仅在库希望利用运行时的特定功能时才会有影响（1
    x 5 = 5） |'
- en: '| Deployment complexity = 4 | Medium: requires changes to deployment practices
    (3 x 4 = 12) | Very low: doesn’t alter deployment at all (0 x 4 = 0) |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 部署复杂性 = 4 | 中：需要更改部署实践（3 x 4 = 12） | 非常低：根本不改变部署（0 x 4 = 0） |'
- en: '| Anti-flexibility = 3 | Medium: as patterns become known, they are generalized
    in the mesh, but not immediately (3 x 3 = 9) | Medium: introducing new patterns
    requires dependency updates across the stack (4 x 3 = 12) |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 反灵活性 = 3 | 中：随着模式变得更为熟知，它们在网格中被概括，但不会立即实现（3 x 3 = 9） | 中：引入新模式需要跨堆栈进行依赖更新（4
    x 3 = 12） |'
- en: '| Operational cost = 2 | High: often much higher resource consumption (5 x
    2 = 10) and operational experience upgrading the mesh independent of application
    footprint | Low: no additional processes or containers allocated per application
    (1 x 2 = 2) |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 操作成本 = 2 | 高：通常会消耗更多资源（5 x 2 = 10），并且在独立于应用程序的足迹的操作经验中进行升级 | 低：每个应用程序不会分配额外的进程或容器（1
    x 2 = 2） |'
- en: '| Total cost | 5 + 25 + 12 + 9 + 10 = 61 | 25 + 5 + 0 + 12 + 2 = 44 (best option
    with these weights) |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 总成本 | 5 + 25 + 12 + 9 + 10 = 61 | 25 + 5 + 0 + 12 + 2 = 44（在这些权重下的最佳选项） |'
- en: The choice-of-two load balancer described earlier is an example of a sophisticated
    load balancer that requires coordination with application code, so it could never
    be fully encapsulated by a service mesh technology.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 前文提到的双选负载均衡器是一个复杂的负载均衡器的示例，需要与应用程序代码协调，因此无法完全由服务网格技术封装。
- en: There is one other complication of this lack of coordination with application
    code. Consider a task like [request timeouts](https://oreil.ly/ZvOIn), handled
    by a sidecar proxy. While the proxy may hang up on a caller after the configured
    timeout, the application instance is still at work handling the request all the
    way to completion. If the application is using a conventional blocking thread-pool
    model like Tomcat, a thread continues to be consumed after the timeout.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这种与应用程序代码协调不足的另一个复杂性是另一项任务的复杂性，例如 [请求超时](https://oreil.ly/ZvOIn)，由边车代理处理。尽管在配置的超时后，代理可能会挂断调用者，但应用实例仍在工作中处理请求，直到完成。如果应用程序使用像
    Tomcat 这样的传统阻塞线程池模型，超时后仍会继续消耗线程。
- en: Currently, Istio only supports a “circuit breaker” that more closely resembles
    a bulkhead as we’ve defined it, since it supports limiting instantaneous concurrency
    to a service by controlling maximum connections or requests. Rather than this
    being an application concern, with Istio the bulkhead would be applied with YAML
    configuration from the Istio Kubernetes custom resource definition, as in [Example 7-21](part0012_split_023.html#istio_circuit_breaker).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Istio 只支持一个更接近我们定义的隔离舱的“断路器”，因为它支持通过控制最大连接或请求来限制对服务的瞬时并发。与这成为应用程序关注的情况不同，使用
    Istio 时，隔离舱将通过来自 Istio Kubernetes 自定义资源定义的 YAML 配置应用，如 [Example 7-21](part0012_split_023.html#istio_circuit_breaker)
    中所示。
- en: Example 7-21\. Istio circuit breaker
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 7-21\. Istio 断路器
- en: '[PRE20]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This also demonstrates the anti-flexibility of service mesh. The sophistication
    of your traffic management policy will be limited to what can be expressed in
    YAML. It’s important to consider how this limited expressiveness is a *necessary*
    condition of not being application code.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 这也显示了服务网格的反灵活性。您的流量管理策略的复杂性将受到 YAML 可表达性的限制。考虑到这种有限表达能力是不成为应用程序代码的 *必要* 条件的重要性。
- en: If, for example, Istio CRD YAML follows the typical evolution of markup that
    stretches to meet more diverse expectations, we would expect to see the imposition
    of boolean logic (appearing already in [multi-match](https://oreil.ly/vA0ng))
    for assembling more complex rules together, etc. It feels like looping is inevitable.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 Istio CRD YAML 遵循了标记语言的典型演进，以满足更多不同的期望，我们预计会看到布尔逻辑的引入（如在 [multi-match](https://oreil.ly/vA0ng)
    中已出现）用于组装更复杂的规则等。感觉循环是不可避免的。
- en: Again, trends in software engineering are often cyclic. This desire to simplify
    application development through static configuration or markup has happened before
    with interesting consequences. Remember, way back in [“Configuration as Code”](part0005_split_013.html#configuration_as_code),
    the example of how XSLT wandered gradually into becoming a Turing complete language.
    This is significant, because the moment this happens, it becomes impossible to
    verify all sorts of characteristics of the configuration (now full-blown software)
    with static analysis. At that point, we’re far from the original stated goal of
    keeping functionality out of code.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，软件工程中的趋势往往是循环的。通过静态配置或标记简化应用程序开发的愿望以前曾发生过，并带来了有趣的后果。请记住，远在[“配置即代码”](part0005_split_013.html#configuration_as_code)时，XSLT慢慢演变成为图灵完备语言的例子。这一点很重要，因为一旦发生这种情况，使用静态分析验证配置（现在是完整的软件）的各种特性就变得不可能。此时，我们已远离最初的目标，即将功能性保持在代码之外。
- en: Implementation in RSocket
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在RSocket中的实现
- en: 'Reactive Streams provides a standard for asynchronous stream processing with
    nonblocking backpressure. [RSocket](https://rsocket.io) is a persistent bidirectional
    remote procedure call protocol implementing Reactive Streams semantics. The goal
    of backpressure was described in 2014 in the [Reactive Manifesto](https://oreil.ly/YLrAY):'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 响应式流提供了一种标准的异步流处理方式，支持非阻塞回压。[RSocket](https://rsocket.io) 是一种持久的双向远程过程调用协议，实现了响应式流的语义。回压的目标在2014年的[响应式宣言](https://oreil.ly/YLrAY)中有所描述：
- en: When one component is struggling to keep up, the system as a whole needs to
    respond in a sensible way. It is unacceptable for the component under stress to
    fail catastrophically or to drop messages in an uncontrolled fashion. Since it
    can’t cope and it can’t fail, it should communicate the fact that it is under
    stress to upstream components and so get them to reduce the load. This backpressure
    is an important feedback mechanism that allows systems to gracefully respond to
    load rather than collapse under it. The backpressure may cascade all the way up
    to the user, at which point responsiveness may degrade, but this mechanism will
    ensure that the system is resilient under load, and will provide information that
    may allow the system itself to apply other resources to help distribute the load…
  id: totrans-284
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 当一个组件难以跟上时，整个系统需要以明智的方式作出响应。组件在压力下不能灾难性地失败或以不受控制的方式丢弃消息是不可接受的。由于它无法应对并且不能失败，它应该向上游组件传达其处于压力之下的事实，以便让它们减少负载。这种回压是一个重要的反馈机制，允许系统在负载增加时优雅地响应而不是崩溃。回压可能一直级联到用户这一层，此时响应性可能会下降，但此机制将确保系统在负载下具有弹性，并提供信息，这些信息可以让系统本身应用其他资源来帮助分配负载...
- en: ''
  id: totrans-285
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Reactive Manifesto
  id: totrans-286
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 响应式宣言
- en: The concept of backpressure across the network layer may very well eliminate
    the need for rate limiters, bulkheads, and circuit breakers in application code
    (or in a sidecar process). As an application instance observes its own decline
    in availability, it places backpressure on callers. Effectively, callers cannot
    make a call to an unavailable application instance.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 网络层上的回压概念很可能消除应用程序代码（或旁路进程）中速率限制器、防护舱和断路器的需要。当应用实例观察到自身可用性下降时，它会对调用者施加回压。有效地，调用者无法对不可用的应用实例发起调用。
- en: Expect to see the further evolution of infrastructure, extending backpressure
    up and down the application stack. [R2DBC](https://r2dbc.io) has extended backpressure
    down even to database interactions. [Netifi](https://www.netifi.com) has built
    an entire control plane around this concept, a sort of alternative to service
    mesh without many of the disadvantages.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 预计将看到基础设施的进一步演进，扩展回压到应用程序堆栈的上下游。[R2DBC](https://r2dbc.io) 已将回压扩展至数据库交互。[Netifi](https://www.netifi.com)
    围绕此概念构建了整个控制平面，一种没有许多缺点的服务网格替代品。
- en: Summary
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Failure and degradation of performance should be expected and planned for in
    any production microservice architecture. In this chapter, we introduced a number
    of strategies for dealing with these conditions, from load balancing to call resilience
    patterns.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何生产微服务架构中，都应预期并计划故障和性能下降。在本章中，我们介绍了一些处理这些条件的策略，从负载平衡到调用恢复模式。
- en: Your organizational commitment to these patterns is almost entirely in application
    code. As with other crosscutting concerns that impact application code like metrics
    instrumentation and distributed tracing, there is an opportunity for an effective
    platform engineering team to step in and provide some of this cross-organizationally
    by shipping good default opinions in core libraries and configuration that are
    consumed by all of the organization’s microservices.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 你的组织在这些模式中的承诺几乎完全体现在应用代码中。与影响应用代码的其他横切关注点（如度量仪表和分布式跟踪）一样，一个有效的平台工程团队有机会通过在核心库和所有组织微服务都使用的配置中提供一些这些跨组织的良好默认观点来介入。
- en: This book has described a journey toward more reliable systems. Go as far as
    you can on this journey, recognizing that at each step your business is better
    off. It starts with simply measuring the existing state of the system, building
    a greater degree of awareness about what your end users are experiencing day to
    day. Continue by adding debuggability signals that allow you to ask questions
    about why failure is occurring as you become aware of it. Improve your software
    delivery pipeline to limit the chances that you introduce more failure into the
    system as you continue to build out your software. Build the capability to observe
    the state of the deployed assets themselves so that you can begin to reason about
    how to make change cross-organizationally when needed. These compensations for
    expected failures are the last step in the journey toward building more reliable
    distributed systems.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 本书描述了通往更可靠系统的旅程。在这个旅程中，你可以尽可能远地前行，意识到每一步都让你的业务受益。它从简单地测量系统现状开始，增加对终端用户日常体验的更高程度认知。继续添加调试信号，使你能够在意识到故障原因时提出问题。优化软件交付流水线，减少在软件构建过程中引入更多故障的可能性。建立能够观察部署资产状态的能力，以便在需要时开始跨组织地推理如何进行变更。这些针对预期故障的补偿措施是通向构建更可靠分布式系统旅程的最后一步。
- en: At each step, build guardrails instead of gates!
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步都要设置护栏而不是大门！
