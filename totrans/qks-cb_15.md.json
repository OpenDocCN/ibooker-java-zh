["```java\n@GET\n@PATH(\"/reactive\")\n@Produces(MediaType.TEXT_PLAIN) ![1](Images/1.png)\npublic CompletionStage<String> reactiveHello() { ![2](Images/2.png)\n    return ReactiveStreams.of(\"h\", \"e\", \"l\", \"l\", \"o\")\n        .map(String::toUpperCase)\n        .toList()\n        .run()\n        .thenApply(list -> list.toString());\n}\n```", "```java\n@GET\n@PATH(\"/reactive\")\n@Produces(MediaType.TEXT_PLAIN)\npublic Multi<String> helloMutiny() {\n    return Multi.createFrom().items(\"h\", \"e\", \"l\", \"l\", \"o\");\n}\n```", "```java\n@GET\n@Path(\"/integers\")\n@Produces(MediaType.SERVER_SENT_EVENTS) ![1](Images/1.png)\npublic Publisher<Long> longPublisher() { ![2](Images/2.png)\n    return Multi.createFrom()\n            .ticks().every(Duration.ofMillis(500));\n}\n```", "```java\npackage com.acme.vertx;\n\nimport javax.enterprise.context.ApplicationScoped;\n\nimport io.quarkus.vertx.ConsumeEvent;\n\n@ApplicationScoped\npublic class GreetingService {\n    @ConsumeEvent   ![1](Images/1.png)\n    public String consumeNormal(String name) { ![2](Images/2.png)\n        return name.toUpperCase();\n    }\n}\n```", "```java\nbus.send(\"address\", \"hello\"); ![1](Images/1.png)\nbus.publish(\"address\", \"hello\"); ![2](Images/2.png)\nbus.send(\"address\", \"hello, how are you?\") ![3](Images/3.png)\n    .thenAccept(msg -> {\n    // do something with the message });\n```", "```java\n    @ConsumeEvent(value = \"greeting\")\n    public String consumeNamed(String name) {\n        return name.toUpperCase();\n    }\n```", "```java\npackage org.acme.kafka;\n\nimport java.util.concurrent.CompletableFuture;\nimport java.util.concurrent.CompletionStage;\n\nimport javax.enterprise.context.ApplicationScoped;\n\nimport org.eclipse.microprofile.reactive.messaging.Incoming;\nimport org.eclipse.microprofile.reactive.messaging.Message;\n\n@ApplicationScoped\npublic class CharacterReceiver {\n  @Incoming(\"ascii-char\")\n  public CompletionStage<Void> processKafkaChar(Message<String> character) {\n    return CompletableFuture.runAsync(() -> {\n      System.out.println(\"Received a message from Kafka \"\n          + \"using CompletableFuture: '\" + character.getPayload() + \"'\");\n    });\n  }\n\n  @Incoming(\"ascii-char\")\n  public void processCharacter(String character) {\n    System.out.println(\"Received a String from kafka: '\" + character + \"'\");\n  }\n}\n```", "```java\nmp.messaging.incoming.ascii-char.connector=smallrye-kafka\nmp.messaging.incoming.ascii-char.value.deserializer=org.apache.kafka.common\\\n .serialization\\\n .StringDeserializer\nmp.messaging.incoming.ascii-char.broadcast=true\n```", "```java\nmp.messaging.[outgoing|incoming].{channel-name}.property=value\n```", "```java\nversion: '2'\n\nservices:\n\n  zookeeper:\n    image: strimzi/kafka:0.11.3-kafka-2.1.0\n    command: [\n      \"sh\", \"-c\",\n      \"bin/zookeeper-server-start.sh config/zookeeper.properties\"\n    ]\n    ports:\n      - \"2181:2181\"\n    environment:\n      LOG_DIR: /tmp/logs\n\n  kafka:\n    image: strimzi/kafka:0.11.3-kafka-2.1.0\n    command: [\n      \"sh\", \"-c\",\n      \"bin/kafka-server-start.sh config/server.properties\n      --override listeners=$${KAFKA_LISTENERS}\n      --override advertised.listeners=$${KAFKA_ADVERTISED_LISTENERS}\n      --override zookeeper.connect=$${KAFKA_ZOOKEEPER_CONNECT}\"\n    ]\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"\n    environment:\n      LOG_DIR: \"/tmp/logs\"\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n```", "```java\npackage org.acme.kafka;\n\nimport java.time.Duration;\nimport java.util.concurrent.ThreadLocalRandom;\n\nimport io.smallrye.mutiny.Multi;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\nimport org.reactivestreams.Publisher;\n\npublic class CharacterGenerator {\n    @Outgoing(\"letter-out\")\n    public Publisher<String> generate() {\n        return Multi.createFrom()\n                .ticks().every(Duration.ofSeconds(1))\n                .map(tick -> {\n                    final int i = ThreadLocalRandom.current().nextInt(95);\n                    return String.valueOf((char) (i + 32));\n                });\n    }\n}\n```", "```java\nmp.messaging.outgoing.letter-out.connector=smallrye-kafka\nmp.messaging.outgoing.letter-out.topic=ascii-char\nmp.messaging.outgoing.letter-out.value.serializer=org.apache.kafka.common\\\n .serialization\\\n .StringSerializer\n```", "```java\n@Inject @Channel(\"price-create\") Emitter<Double> priceEmitter;\n\n@POST\n@Consumes(MediaType.TEXT_PLAIN)\npublic void addPrice(Double price) {\n    priceEmitter.send(price);\n}\n```", "```java\npackage org.acme.kafka;\n\npublic class Book {\n    public String title;\n    public String author;\n    public Long isbn;\n\n    public Book() {\n    }\n\n    public Book(String title, String author, Long isbn) {\n        this.title = title;\n        this.author = author;\n        this.isbn = isbn;\n    }\n}\n```", "```java\npackage org.acme.kafka;\n\nimport io.quarkus.kafka.client.serialization.JsonbDeserializer;\n\npublic class BookDeserializer extends JsonbDeserializer<Book> {\n    public BookDeserializer() {\n        super(Book.class);\n    }\n}\n```", "```java\npackage com.acme.kafka;\n\nimport io.quarkus.kafka.client.serialization.ObjectMapperDeserializer;\n\npublic class BookDeserializer extends ObjectMapperDeserializer<Book> {\n    public BookDeserializer() {\n        super(Book.class);\n    }\n}\n```", "```java\n# Configure the Kafka source (we read from it)\nmp.messaging.incoming.book-in.connector=smallrye-kafka\nmp.messaging.incoming.book-in.topic=book-in\nmp.messaging.incoming.book-in.value.deserializer=com.acme\\\n .kafka.BookDeserializer\n\n# Configure the Kafka sink (we write to it)\nmp.messaging.outgoing.book-out.connector=smallrye-kafka\nmp.messaging.outgoing.book-out.topic=book-out\nmp.messaging.outgoing.book-out.value.serializer=io.quarkus.kafka\\\n .client.serialization\\\n .JsonbSerializer\n```", "```java\n# Configure the Kafka source (we read from it)\nmp.messaging.incoming.book-in.connector=smallrye-kafka\nmp.messaging.incoming.book-in.topic=book-in\nmp.messaging.incoming.book-in.value.deserializer=com.acme\\\n .kafka.BookDeserializer\n\n# Configure the Kafka sink (we write to it)\nmp.messaging.outgoing.book-out.connector=smallrye-kafka\nmp.messaging.outgoing.book-out.topic=book-out\nmp.messaging.outgoing.book-out.value.serializer=io.quarkus.kafka.client\\\n .serialization\\\n .ObjectMapperSerializer\n```", "```java\nversion: '3.5'\n\nservices:\n  zookeeper:\n    image: strimzi/kafka:0.11.3-kafka-2.1.0\n    command: [\n      \"sh\", \"-c\",\n      \"bin/zookeeper-server-start.sh config/zookeeper.properties\"\n    ]\n    ports:\n      - \"2181:2181\"\n    environment:\n      LOG_DIR: /tmp/logs\n    networks:\n      - kafkastreams-network\n  kafka:\n    image: strimzi/kafka:0.11.3-kafka-2.1.0\n    command: [\n      \"sh\", \"-c\",\n      \"bin/kafka-server-start.sh config/server.properties\n      --override listeners=$${KAFKA_LISTENERS}\n      --override advertised.listeners=$${KAFKA_ADVERTISED_LISTENERS}\n      --override zookeeper.connect=$${KAFKA_ZOOKEEPER_CONNECT}\n      --override num.partitions=$${KAFKA_NUM_PARTITIONS}\"\n    ]\n    depends_on:\n      - zookeeper\n    ports:\n      - \"9092:9092\"\n    environment:\n      LOG_DIR: \"/tmp/logs\"\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092\n      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n      KAFKA_NUM_PARTITIONS: 3\n    networks:\n      - kafkastreams-network\n```", "```java\npackage org.acme.kafka.jukebox;\n\nimport java.time.Duration;\nimport java.time.Instant;\nimport java.util.Arrays;\nimport java.util.Collections;\nimport java.util.List;\nimport java.util.concurrent.ThreadLocalRandom;\n\nimport javax.enterprise.context.ApplicationScoped;\n\nimport io.smallrye.mutiny.Multi;\nimport io.smallrye.reactive.messaging.kafka.KafkaRecord;\nimport org.eclipse.microprofile.reactive.messaging.Outgoing;\nimport org.jboss.logging.Logger;\n\n@ApplicationScoped\npublic class Jukebox {\n    private static final Logger LOG = Logger.getLogger(Jukebox.class);\n\n    private ThreadLocalRandom random = ThreadLocalRandom.current();\n\n    private List<Song> songs = Collections.unmodifiableList(\n            Arrays.asList(\n                new Song(1, \"Confessions\", \"Usher\"),\n                new Song(2, \"How Do I Live\", \"LeAnn Rimes\"),\n                new Song(3, \"Physical\", \"Olivia Newton-John\"),\n                new Song(4, \"You Light Up My Life\", \"Debby Boone\"),\n                new Song(5, \"The Twist\", \"Chubby Checker\"),\n                new Song(6, \"Mack the Knife\", \"Bobby Darin\"),\n                new Song(7, \"Night Fever\", \"Bee Gees\"),\n                new Song(8, \"Bette Davis Eyes\", \"Kim Carnes\"),\n                new Song(9, \"Macarena (Bayside Boys Mix)\", \"Los Del Rio\"),\n                new Song(10, \"Yeah!\", \"Usher\")\n            )\n    );\n\n    @Outgoing(\"song-values\")\n    public Multi<KafkaRecord<Integer, String>> generate() {\n        return Multi.createFrom().ticks().every(Duration.ofMillis(500))\n                .onOverflow().drop()\n                .map(tick -> {\n                   Song s = songs.get(random.nextInt(songs.size()));\n                   int timesPlayed = random.nextInt(1, 100);\n\n                   LOG.infov(\"song {0}, times played: {1,number}\",\n                           s.title, timesPlayed);\n                   return KafkaRecord.of(s.id, Instant.now()\n                                               + \";\" + timesPlayed);\n                });\n    }\n\n    @Outgoing(\"songs\")\n    public Multi<KafkaRecord<Integer, String>> songs() {\n        return Multi.createFrom().iterable(songs)\n                .map(s -> KafkaRecord.of(s.id,\n                        \"{\\n\" +\n                        \"\\t\\\"id\\\":\\\"\"+ s.id + \"\\\",\\n\" +\n                        \"\\t\\\"title\\\":\\\"\" + s.title + \"\\\",\\n\" +\n                        \"\\t\\\"artist\\\":\\\"\" + s.artist + \"\\\"\\n\" +\n                        \"}\"\n                        ));\n    }\n\n    private static class Song {\n        int id;\n        String title;\n        String artist;\n\n        public Song(int id, String title, String artist) {\n            this.id = id;\n            this.title = title;\n            this.artist = artist;\n        }\n    }\n}\n```", "```java\npackage org.acme.kafka.jukebox;\n\npublic class Song {\n    public int id;\n    public String title;\n    public String artist;\n}\n```", "```java\npackage org.acme.kafka.jukebox;\n\nimport java.time.Instant;\n\npublic class PlayedCount {\n    public int count;\n    public String title;\n    public String artist;\n    public int id;\n    public Instant timestamp;\n\n    public PlayedCount(int id, String title, String artist,\n                       int count, Instant timestamp) {\n        this.count = count;\n        this.title = title;\n        this.artist = artist;\n        this.id = id;\n        this.timestamp = timestamp;\n    }\n}\n```", "```java\npackage org.acme.kafka.jukebox;\n\nimport java.math.BigDecimal;\nimport java.math.RoundingMode;\n\npublic class Aggregation {\n    public int songId;\n    public String songTitle;\n    public String songArtist;\n    public int count;\n    public int sum;\n    public int min;\n    public int max;\n    public double avg;\n\n    public Aggregation updateFrom(PlayedCount playedCount) {\n        songId = playedCount.id;\n        songTitle = playedCount.title;\n        songArtist = playedCount.artist;\n\n        count++;\n        sum += playedCount.count;\n        avg = BigDecimal.valueOf(sum / count)\n                .setScale(1, RoundingMode.HALF_UP).doubleValue();\n        min = Math.min(min, playedCount.count);\n        max = Math.max(max, playedCount.count);\n\n        return this;\n    }\n}\n```", "```java\npackage org.acme.kafka.jukebox;\n\nimport java.time.Instant;\n\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.enterprise.inject.Produces;\n\nimport io.quarkus.kafka.client.serialization.JsonbSerde;\nimport org.apache.kafka.common.serialization.Serdes;\nimport org.apache.kafka.streams.StreamsBuilder;\nimport org.apache.kafka.streams.Topology;\nimport org.apache.kafka.streams.kstream.Consumed;\nimport org.apache.kafka.streams.kstream.GlobalKTable;\nimport org.apache.kafka.streams.kstream.Materialized;\nimport org.apache.kafka.streams.kstream.Produced;\nimport org.apache.kafka.streams.state.KeyValueBytesStoreSupplier;\nimport org.apache.kafka.streams.state.Stores;\n\n@ApplicationScoped\npublic class TopologyProducer {\n    static final String SONG_STORE = \"song-store\";\n\n    private static final String SONG_TOPIC = \"songs\";\n    private static final String SONG_VALUES_TOPIC = \"song-values\";\n    private static final String SONG_AGG_TOPIC = \"song-aggregated\";\n\n    @Produces\n    public Topology buildTopology() {\n        StreamsBuilder builder = new StreamsBuilder();\n\n        JsonbSerde<Song> songSerde = new JsonbSerde<>(Song.class);\n        JsonbSerde<Aggregation> aggregationSerde =\n                new JsonbSerde<>(Aggregation.class);\n\n        KeyValueBytesStoreSupplier storeSupplier =\n                Stores.persistentKeyValueStore(SONG_STORE);\n\n        GlobalKTable<Integer, Song> songs = builder.globalTable(SONG_TOPIC,\n                Consumed.with(Serdes.Integer(), songSerde));\n\n        builder.stream(SONG_VALUES_TOPIC, Consumed.with(Serdes.Integer(),\n                Serdes.String()))\n                .join(\n                        songs,\n                        (songId, timestampAndValue) -> songId,\n                        (timestampAndValue, song) -> {\n                            String[] parts = timestampAndValue.split(\";\");\n                            return new PlayedCount(song.id, song.title,\n                                    song.artist,\n                                    Integer.parseInt(parts[1]),\n                                    Instant.parse(parts[0]));\n                        }\n                )\n                .groupByKey()\n                .aggregate(\n                        Aggregation::new,\n                        (songId, value, aggregation) ->\n                                aggregation.updateFrom(value),\n                        Materialized.<Integer, Aggregation> as(storeSupplier)\n                            .withKeySerde(Serdes.Integer())\n                            .withValueSerde(aggregationSerde)\n                )\n                .toStream()\n                .to(\n                        SONG_AGG_TOPIC,\n                        Produced.with(Serdes.Integer(), aggregationSerde)\n                );\n        return builder.build();\n    }\n}\n```", "```java\nmp.messaging.[outgoing|incoming].[channel-name].property=value\n```", "```java\namqp-username=[my-username]\namqp-password=[my-secret-password]\n```", "```java\nmp.messaging.[outgoing|incoming].[channel-name].property=value\n```", "```java\nquarkus.datasource.db-kind=postgresql\nquarkus.datasource.username=quarkus_test\nquarkus.datasource.password=quarkus_test\nquarkus.datasource.reactive.url=postgresql://localhost:5432/quarkus_test\n```", "```java\npackage org.acme.pg;\n\nimport io.smallrye.mutiny.Multi;\nimport io.smallrye.mutiny.Uni;\nimport io.vertx.mutiny.pgclient.PgPool;\nimport io.vertx.mutiny.sqlclient.Row;\nimport io.vertx.mutiny.sqlclient.Tuple;\n\npublic class Book {\n    public Long id;\n    public String title;\n    public String isbn;\n\n    public Book() {\n    }\n\n    public Book(String title, String isbn) {\n        this.title = title;\n        this.isbn = isbn;\n    }\n\n    public Book(Long id, String title, String isbn) {\n        this.id = id;\n        this.title = title;\n        this.isbn = isbn;\n    }\n\n    public static Book from(Row row) {\n        return new Book(row.getLong(\"id\"),\n                        row.getString(\"title\"),\n                        row.getString(\"isbn\"));\n    }\n\n    public static Multi<Book> findAll(PgPool client) {\n        return client.query(\"SELECT id, title, isbn \" +\n                            \"FROM books ORDER BY title ASC\") ![1](Images/1.png)\n                .onItem().produceMulti(Multi.createFrom()::iterable) ![2](Images/2.png)\n                .map(Book::from); ![3](Images/3.png)\n    }\n}\n```", "```java\npackage org.acme.pg;\n\nimport javax.annotation.PostConstruct;\nimport javax.inject.Inject;\nimport javax.ws.rs.Consumes;\nimport javax.ws.rs.GET;\nimport javax.ws.rs.Path;\nimport javax.ws.rs.Produces;\nimport javax.ws.rs.core.MediaType;\nimport javax.ws.rs.core.Response;\n\nimport io.smallrye.mutiny.Uni;\nimport io.vertx.mutiny.pgclient.PgPool;\nimport org.eclipse.microprofile.config.inject.ConfigProperty;\n\n@Path(\"/books\")\n@Produces(MediaType.APPLICATION_JSON)\n@Consumes(MediaType.APPLICATION_JSON)\npublic class BookResource {\n\n    @Inject\n    PgPool client;\n    @GET\n    public Uni<Response> get() {\n        return Book.findAll(client)\n                .collectItems().asList()\n                .map(Response::ok)\n                .map(Response.ResponseBuilder::build);\n    }\n}\n```", "```java\n    public static Uni<Boolean> delete(PgPool client, Long id) {\n        return client.preparedQuery(\"DELETE FROM books \" +\n                                    \"WHERE id = $1\", Tuple.of(id))\n                .map(rowSet -> rowSet.rowCount() == 1); ![1](Images/1.png)\n    }\n```", "```java\nquarkus.datasource.db-kind=mysql\nquarkus.datasource.username=quarkus_test\nquarkus.datasource.password=quarkus_test\nquarkus.datasource.reactive.url=mysql://localhost:3306/quarkus_test\n```", "```java\n    public Uni<Long> save(MySQLPool client) {\n        String query = \"INSERT INTO books (title,isbn) VALUES (?,?)\";\n        return client.preparedQuery(query, Tuple.of(title, isbn))\n                .map(rowSet -> rowSet\n                        .property(MySQLClient.LAST_INSERTED_ID)); ![1](Images/1.png)\n    }\n```", "```java\npackage org.acme.mongodb;\n\nimport java.util.List;\nimport java.util.Objects;\n\nimport javax.enterprise.context.ApplicationScoped;\nimport javax.inject.Inject;\n\nimport com.mongodb.client.model.Filters;\nimport io.quarkus.mongodb.reactive.ReactiveMongoClient;\nimport io.quarkus.mongodb.reactive.ReactiveMongoCollection;\nimport io.smallrye.mutiny.Uni;\nimport org.bson.Document;\n\n@ApplicationScoped\npublic class ReactiveBookService {\n    @Inject\n    ReactiveMongoClient mongoClient;\n\n    public Uni<List<Book>> list() {\n        return getCollection().find()\n                .map(Book::from).collectItems().asList();\n    }\n\n    public Uni<Void> add(Book b) {\n        Document doc = new Document()\n                .append(\"isbn\", b.isbn)\n                .append(\"title\", b.title)\n                .append(\"authors\", b.authors);\n\n        return getCollection().insertOne(doc);\n    }\n\n    public Uni<Book> findSingle(String isbn) {\n        return Objects.requireNonNull(getCollection()\n                .find(Filters.eq(\"isbn\", isbn))\n                .map(Book::from))\n                .toUni();\n    }\n\n    private ReactiveMongoCollection<Document> getCollection() {\n        return mongoClient.getDatabase(\"book\")\n                .getCollection(\"book\");\n    }\n}\n```", "```java\npackage org.acme.mongodb;\n\nimport java.util.List;\n\nimport javax.inject.Inject;\nimport javax.ws.rs.Consumes;\nimport javax.ws.rs.GET;\nimport javax.ws.rs.POST;\nimport javax.ws.rs.Path;\nimport javax.ws.rs.PathParam;\nimport javax.ws.rs.Produces;\nimport javax.ws.rs.core.MediaType;\nimport javax.ws.rs.core.Response;\n\nimport io.smallrye.mutiny.Uni;\n\n@Path(\"/reactive_books\")\n@Produces(MediaType.APPLICATION_JSON)\n@Consumes(MediaType.APPLICATION_JSON)\npublic class ReactiveBookResource {\n    @Inject\n    ReactiveBookService service;\n\n    @GET\n    public Uni<List<Book>> getAll() {\n        return service.list();\n    }\n\n    @GET\n    @Path(\"{isbn}\")\n    public Uni<Book> getSingle(@PathParam(\"isbn\") String isbn) {\n        return service.findSingle(isbn);\n    }\n\n    @POST\n    public Uni<Response> add(Book b) {\n        return service.add(b).onItem().ignore()\n                .andSwitchTo(this::getAll)\n                .map(books -> Response.status(Response.Status.CREATED)\n                                      .entity(books).build());\n    }\n}\n```", "```java\npackage org.acme.neo4j;\n\nimport java.util.stream.Collectors;\n\nimport javax.inject.Inject;\nimport javax.ws.rs.Consumes;\nimport javax.ws.rs.GET;\nimport javax.ws.rs.POST;\nimport javax.ws.rs.Path;\nimport javax.ws.rs.Produces;\nimport javax.ws.rs.core.MediaType;\nimport javax.ws.rs.core.Response;\n\nimport io.smallrye.mutiny.Multi;\nimport io.smallrye.mutiny.Uni;\nimport org.neo4j.driver.Driver;\nimport org.neo4j.driver.Record;\nimport org.neo4j.driver.Value;\nimport org.neo4j.driver.Values;\nimport org.neo4j.driver.reactive.RxResult;\nimport org.reactivestreams.Publisher;\n\n@Path(\"/reactivebooks\")\n@Produces(MediaType.APPLICATION_JSON)\n@Consumes(MediaType.APPLICATION_JSON)\npublic class ReactiveBookResource {\n    @Inject\n    Driver driver;\n\n    @GET\n    @Produces(MediaType.SERVER_SENT_EVENTS) ![3](Images/3.png)\n    public Publisher<Response> getAll() {\n        return Multi.createFrom().resource( ![2](Images/2.png)\n                driver::rxSession,\n                rxSession -> rxSession.readTransaction(tx -> { ![1](Images/1.png)\n                    RxResult result = tx.run(\"MATCH (b:Book) RETURN \" +\n                                             \"b ORDER BY b.title\");\n                    return Multi.createFrom().publisher(result.records())\n                            .map(Record::values)\n                            .map(values -> values.stream().map(Value::asNode)\n                                                          .map(Book::from)\n                                                          .map(Book::toJson))\n                            .map(bookStream ->\n                                    Response.ok(bookStream\n                                            .collect(Collectors.toList()))\n                                    .build());\n                }))\n                .withFinalizer(rxSession -> { ![4](Images/4.png)\n                    return Uni.createFrom().publisher(rxSession.close());\n                });\n    }\n\n    @POST\n    public Publisher<Response> create(Book b) {\n        return Multi.createFrom().resource(\n                driver::rxSession,\n                rxSession -> rxSession.writeTransaction(tx -> {\n                    String query = \"CREATE \" +\n                                   \"(b:Book {title: $title, isbn: $isbn,\" +\n                                   \" authors: $authors}) \" +\n                                   \"RETURN b\";\n                    RxResult result = tx.run(query,\n                            Values.parameters(\"title\", b.title,\n                                    \"isbn\", b.isbn, \"authors\", b.authors));\n                    return Multi.createFrom().publisher(result.records())\n                            .map(record -> Response.ok(record\n                                    .asMap()).build());\n                })\n        ).withFinalizer(rxSession -> {\n            return Uni.createFrom().publisher(rxSession.close());\n        });\n    }\n}\n```"]