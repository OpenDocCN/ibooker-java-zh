- en: Chapter 3\. Debugging with Observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned at the beginning of [Chapter 2](part0006_split_000.html#5N3C4-2d714b853a094e9a910510217e0e3d73),
    observability signals can be roughly broken down into two categories, based on
    the value they bring: availability and debuggability. Aggregated application metrics
    provide the best availability signal. In this chapter, we will discuss the other
    two main signals, distributed tracing and logs.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll show one approach to correlating metrics and traces using only open source
    tooling. Some commercial vendors also work to provide this unified experience.
    Like in [Chapter 2](part0006_split_000.html#5N3C4-2d714b853a094e9a910510217e0e3d73),
    the purpose in showing a specific approach is to develop an expectation about
    the minimum level of sophistication you should be able to *expect* from your observability
    stack when it is fully assembled.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, distributed tracing instrumentation, given that it needs to propagate
    context across a microservice hierarchy, can be an efficient place to govern behavior
    deeper in a system. We’ll discuss a hypothetical failure injection testing feature
    as an example of the possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: The Three Pillars of Observability…or Is It Two?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed in [*Distributed Systems Observability*](https://learning.oreilly.com/library/view/distributed-systems-observability/9781492033431)
    by Cindy Sridharan (O’Reilly), three different types of telemetry form the “three
    pillars of observability”: logs, distributed traces, and metrics. This three pillars
    classification is common, to such an extent that it’s difficult to pinpoint its
    origin.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While logs, distributed traces, and metrics are three distinct forms of telemetry
    with unique characteristics, they roughly serve two purposes: proving availability
    and debugging for root cause identification.'
  prefs: []
  type: TYPE_NORMAL
- en: The operational cost of maintaining all this telemetry data would be high unless
    the volume of data is reduced in some way. Clearly we can only maintain telemetry
    data for a certain amount of time, so there is a need for other reduction strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregation
  prefs: []
  type: TYPE_NORMAL
- en: Precalculating statistics from every measurement. Data from timers (see [“Timers”](part0006_split_017.html#5N4J5-2d714b853a094e9a910510217e0e3d73)),
    for example, could be presented as a sum, a count, and some limited distribution
    statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling
  prefs: []
  type: TYPE_NORMAL
- en: Selecting only certain measurements for retention.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregation effectively compacts the representation at the expense of request-level
    granularity, where sampling retains request-level granularity at the expense of
    the holistic view of the system’s performance. Except in low-throughput systems,
    the cost of both full request-level granularity and a full representation of all
    requests is too expensive.
  prefs: []
  type: TYPE_NORMAL
- en: Retaining some information at full granularity is essential to *debugging* with
    observability tools, the focus of this chapter. Availability signals derived from
    metrics will point to a problem. Dimensional exploration of this data may in some
    cases be enough to identify the root cause of the issue. For example, breaking
    a particular signal into individual signals by instance may reveal a particular
    instance that is failing. There could be a whole region failing or an application
    version. In the rare cases where a pattern isn’t obvious, representative failures
    in distributed traces or logs will be the key to root cause identification.
  prefs: []
  type: TYPE_NORMAL
- en: Considering the characteristics of each of the “three pillars” shows that logging
    and tracing as event-level telemetry serve the purpose of debugging and metrics
    serve the purpose of proving availability.
  prefs: []
  type: TYPE_NORMAL
- en: Logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logs are ubiquitous in the software stack. Regardless of their structure and
    where they are ultimately stored, logs have some defining characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Logs grow proportionally to throughput through a system. The more times a log-instrumented
    code path is executed, the more log data is emitted. Even if log data is sampled,
    this proportional relationship in size still holds.
  prefs: []
  type: TYPE_NORMAL
- en: The context of a log is scoped to an event. Log data provides context into the
    execution behavior of a particular interaction. When data from several independent
    log events is aggregated together to reason about the overall performance of a
    system, the aggregate is effectively a metric.
  prefs: []
  type: TYPE_NORMAL
- en: Logs are obviously geared toward debugging. Sophisticated log analytics packages
    help to prove availability from logging data only through aggregation. There is
    a cost to performing this aggregation, to persisting the data subject to aggregation,
    and to allocating the payload that was persisted.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Tracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tracing telemetry, like logs, is recorded per instrumented execution (i.e.,
    it is event-driven) but links individual events across disparate parts of a system
    causally. A distributed tracing system can reason about a user interaction end
    to end across the whole system. So for a given request that was known to exhibit
    some degradation, this end-to-end view of the satisfaction of a user request shows
    which part of the distributed system was degraded.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing telemetry is even more commonly sampled than logs. Nevertheless, tracing
    data still grows proportionally to throughput through a system in much the same
    way that log data grows proportionally to throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing can be difficult to retrofit into an existing system, as each collaborator
    in an end-to-end process must be configured to propagate trace context forward.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed tracing shines especially for a particular type of performance problem
    where the entire system is slower than it should be but there is no obvious hotspot
    to quickly optimize. Sometimes you simply have to see the contribution of many
    subsystems to the performance of a system as whole to recognize that systemic
    “death by a thousand cuts” that needs to be visualized in order to build the organizational
    will to address, and thus the attention of time and resources.
  prefs: []
  type: TYPE_NORMAL
- en: “It’s slow” is the hardest problem you’ll ever debug. “It’s slow” might mean
    one or more of the number of systems involved in performing a user request is
    slow. It might mean one or more of the parts of a pipeline of transformations
    across many machines is slow. “It’s slow” is hard, in part, because the problem
    statement doesn’t provide many clues to the location of the flaw. Partial failures,
    ones that don’t show up on the graphs you usually look up, are lurking in a dark
    corner. And, until the degradation becomes very obvious, you won’t receive as
    many resources (time, money, and tooling) to solve it. Dapper and Zipkin were
    built for a reason.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jeff Hodges
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In organizations with a large set of microservices, distributed tracing helps
    to understand the service graph (the dependencies between services themselves)
    involved in the processing of a certain type of request. This assumes, of course,
    that each service in the graph has tracing instrumentation in one form or another.
    In the narrowest sense, the last tier of services can be uninstrumented and still
    appear in the service graph if named by a span wrapping a call on the client side.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed tracing, like logging, is inherently event-driven and so is best
    suited to act as a debugging signal, but one that carries important interservice
    relational context in addition to its tags.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logs and distributed traces are more similar to each other than they are to
    metrics, which were discussed in detail in [Chapter 2](part0006_split_000.html#5N3C4-2d714b853a094e9a910510217e0e3d73),
    since in some ways they are both sampled to control cost. Metrics are presented
    in aggregate and are used to understand some service level indicator (SLI) as
    a whole, rather than providing detail about the individual interactions that,
    taken together, are measured as an SLI.
  prefs: []
  type: TYPE_NORMAL
- en: Retrofitting an existing codebase with metrics is partially a manual effort,
    and partially comes out of the box with improvements in common frameworks and
    libraries which are increasingly shipping with instrumentation.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics SLIs are purposefully collected to be tested against a service level
    objective, so they are geared toward proving availability.
  prefs: []
  type: TYPE_NORMAL
- en: Which Telemetry Is Appropriate?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given this context about what each form of observability is intended for, consider
    how they overlap. Where they overlap, which form do we emphasize over another?
  prefs: []
  type: TYPE_NORMAL
- en: The idea that both tracing and logging are debugging signals implies that they
    can be redundant, though not equal. All things being equal about retrieving and
    searching for them, a trace with effective tags and metadata is superior to a
    log line when it also provides useful context about the chain of calls that led
    to it (and propagates this context further along as well).
  prefs: []
  type: TYPE_NORMAL
- en: Tracing instrumentation exists at all the same logical places where metrics
    timers do. Note that distributed traces *only* measure executions, though. Where
    execution timing is concerned, metrics and tracing instrumentation may both be
    appropriate because they complement one another. Metrics provide an aggregated
    view of all executions of the instrumented piece of code (and without caller context),
    and distributed tracing provides sampled examples of individual executions. In
    addition to timing executions though, metrics also count and gauge things. There
    are no tracing equivalents for these kinds of signals.
  prefs: []
  type: TYPE_NORMAL
- en: To make this more concrete, let’s take a look at excerpts from a typical application
    log in [Example 3-1](part0008_split_005.html#log_showing_telemetry_choices). Much
    of the beginning of this log excerpt contains one-time events about which components
    were configured and features were enabled. These pieces of information may be
    important in understanding why the application isn’t functioning as expected (for
    example, if a component was expected to be configured but wasn’t), but they don’t
    make sense as metrics because they aren’t recurring events that need to be aggregated
    over time to understand the overall performance of the system. They don’t make
    sense as distributed traces either, because these are events that are specific
    to the state of this service alone and have nothing to do with the coordinated
    satisfaction of an end-user request across multiple microservices.
  prefs: []
  type: TYPE_NORMAL
- en: There are other log lines that could be replaced with tracing or metrics, as
    noted in the callouts following the example.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-1\. A typical application log demonstrating telemetry choices
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](../images/00112.png)](part0008_split_005.html#co_debugging_with_observability_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Both metrics and logging, no tracing.* Mongo socket connection attempts could
    easily be timed with metrics, with a tag indicating success/failure and a tag
    with a summarized exception tag like `exception=ConnectException`. This summarized
    tag may be enough to understand the problem without viewing the whole stack trace.
    In other cases, where the summarized exception tag is something like `exception=NullPointerException`,
    logging the stack trace helps identify the specific problem once the monitoring
    system alerts us to a grouping of exceptions that have failed an established service
    level objective.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](../images/00059.png)](part0008_split_005.html#co_debugging_with_observability_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Both traces and metrics, no logging.* The log statement in the code could
    be removed entirely. Metrics and distributed traces capture all the interesting
    information about this payment in a way that allows us to reason about the retrieval
    of all payments holistically, as well as representative retrievals of individual
    payments. The metrics, for example, will show us that while most payments are
    retrieved in less than 40 ms, some will take an order of magnitude longer to retrieve.'
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](../images/00067.png)](part0008_split_005.html#co_debugging_with_observability_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Metrics, no traces or logs.* A near cache of frequently retrieved payments
    could be monitored strictly with a gauge metric. There is no equivalent to a gauge
    in tracing instrumentation, and logging this is redundant.'
  prefs: []
  type: TYPE_NORMAL
- en: Which Observability Tool Should You Choose?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tracing is preferable to logging whenever possible because it can contain the
    same information but with richer context. Where tracing and metrics overlap, start
    with metrics because the first task should be *knowing* when some system is unavailable.
    Adding additional telemetry to help remediate problems can come later. When you
    do add tracing, start at places where timed metrics instrumentation exists, because
    it is likely also worth tracing with a superset of the same tags.
  prefs: []
  type: TYPE_NORMAL
- en: Supposing then that you are ready to add distributed tracing, let’s next consider
    what makes up a trace and how it might be visualized.
  prefs: []
  type: TYPE_NORMAL
- en: Components of a Distributed Trace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A full distributed trace is a collection of individual *spans*, which contain
    information about the performance of each touchpoint in the satisfaction of an
    end-user request. These spans can be assembled into an “icicle” graph that shows
    relatively how much time was spent in each service, as shown in [Figure 3-1](part0008_split_007.html#zipkin_trace_view).
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0301](../images/00066.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-1\. Zipkin icicle graph
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Spans contain a name and set of key-value tag pairs, much like metrics instrumentation
    does. Many of the principles we covered in [“Naming Metrics”](part0006_split_007.html#5N3ND-2d714b853a094e9a910510217e0e3d73)
    apply equally to distributed traces. So if a trace span is named `http.server.requests`,
    then tags may identify region (in the public cloud sense), API endpoint, HTTP
    method, response status code, etc. Keeping metric and trace naming consistent
    is the key to allowing for correlation of telemetry (see [“Correlation of Telemetry”](part0008_split_023.html#telemetry_correlation)).
  prefs: []
  type: TYPE_NORMAL
- en: Unlike in metrics, the Zipkin span data model contains special fields for service
    name (used in the Zipkin Dependencies view, displaying a service graph). This
    is equivalent to tagging metrics with an application name, where most metrics
    backends don’t set aside a reserved tag name for this concept. Span name is also
    a defined field on the Zipkin data model. Both are indexed for lookup, so unbounded
    value set cardinality on span and service name should be avoided.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike metrics, it is not necessary to control tag cardinality of a trace in
    every circumstance. This has to do with the way traces are stored. [Table 2-1](part0006_split_002.html#storage_dimensional_metric)
    showed how metrics are stored logically in rows by unique ID (combination of name
    and key/value tags). Additional measurements are stored as samples in an existing
    row. The cost of metrics is then the product of the total number of IDs and the
    amount of samples maintained per ID. Distributed trace spans are stored individually
    without any regard to whether another span had the same name and tags. The cost
    of distributed traces is then the product of the throughput through the system
    and the sampling rate (viewed as a percentage).
  prefs: []
  type: TYPE_NORMAL
- en: While tag cardinality does not influence storage cost in a distributed tracing
    system, it does influence *lookup* cost. And in tracing systems, tags can be marked
    as indexable by the tracing backend (and autocompletable in the Zipkin UI). Clearly,
    these tag value sets should be bounded for index performance.
  prefs: []
  type: TYPE_NORMAL
- en: It’s best to overlap as many tags as possible between metrics and traces so
    that they can later be correlated. You should also tag distributed traces with
    additional high-cardinality tags that can be used to locate a request from a particular
    user or interaction, as in [Table 3-1](part0008_split_007.html#overlap_in_trace_metrics_tagging).
    Strive for the value to match wherever tag keys match.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3-1\. Overlap of distributed trace and metrics tagging
  prefs: []
  type: TYPE_NORMAL
- en: '| Metric tag key | Trace tag key | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| application | application | payments |'
  prefs: []
  type: TYPE_TB
- en: '| method | method | GET |'
  prefs: []
  type: TYPE_TB
- en: '| status | status | 200 |'
  prefs: []
  type: TYPE_TB
- en: '| uri | uri | /api/payment/{paymentId} |'
  prefs: []
  type: TYPE_TB
- en: '|  | detailedUri | /api/payment/abc123 |'
  prefs: []
  type: TYPE_TB
- en: '|  | user | user123456 |'
  prefs: []
  type: TYPE_TB
- en: It should be clear at this point that a trace is designed to give you insight
    into the end-to-end performance of a request. It shouldn’t be surprising then
    that the Zipkin UI is focused on searching for traces given a set of parameters,
    as shown in [Figure 3-2](part0008_split_007.html#zipkin_search). This kind of
    listing trades off an understanding of the overall distribution of end-to-end
    performance for a matching set of traces for a particular set of parameters. Building
    a correlation between the overall distribution and this view is the subject of
    [“Correlation of Telemetry”](part0008_split_023.html#telemetry_correlation).
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0302](../images/00031.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-2\. Searching for traces in the Zipkin Lens UI
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As with metrics, there are multiple ways of adding distributed tracing to your
    application. Let’s consider some of the advantages of each.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Distributed Tracing Instrumentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Everything discussed in [“Black Box Versus White Box Monitoring”](part0006_split_001.html#blackbox_whitebox)
    with respect to metrics also applies to distributed tracing instrumentation. Tracing
    instrumentation is available at various architectural levels (from infrastructure
    to individual components of an application).
  prefs: []
  type: TYPE_NORMAL
- en: Manual Tracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Libraries like Zipkin’s Brave or OpenTelemetry allow you to instrument your
    application explicitly in code. In an ideally traced distributed system, some
    level of manual tracing will certainly be present. Through it, key business-specific
    context can be added to traces that other forms of prepackaged instrumentation
    couldn’t possibly be aware of.
  prefs: []
  type: TYPE_NORMAL
- en: Agent Tracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like with metrics, agents (typically vendor-provided) can automatically add
    tracing instrumentation without making code changes. Attaching an agent is a change
    to your application delivery pipeline, and this complexity cost shouldn’t be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'This cost is real no matter which level of abstraction your platform operates
    at:'
  prefs: []
  type: TYPE_NORMAL
- en: For an infrastructure-as-a-service platform like Amazon EC2, you will have to
    add the agent and its configuration to your base Amazon Machine Image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a container-as-a-service (CaaS) platform, you will need another container
    level between a basic image like `openjdk:jre-alpine` and your application. This
    impact can leak into your build then. If you were using the Gradle `com.bmuschko.docker-spring-boot-application`
    plug-in to package Spring Boot applications for deployment to a CaaS, you now
    need to override the default container image with one including the agent. Also,
    any time the base image (which may very well be the default of `com.bmuschko.docker-spring-boot-application`)
    of the container image containing the agent is updated, you have to publish a
    new image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a platform as a service (PaaS) like Cloud Foundry or Heroku, you have to
    use a custom base unless integration with the agent is specifically supported
    by the PaaS vendor already.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Framework Tracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Frameworks can also include telemetry out of the box. Because frameworks are
    included in an application as a binary dependency, this form of telemetry is technically
    a black box solution. Framework-level instrumentation can have a white box feel
    to it when it allows for user-provided customizations to its automatically instrumented
    touchpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Frameworks are aware of their own implementation idiosyncracies, so they can
    provide rich contextual information as tags.
  prefs: []
  type: TYPE_NORMAL
- en: To give an example, framework instrumentation for an HTTP request handler can
    tag the span with a parameterized request URI (i.e., `/api/customers/(id)` versus
    `/api/customers/1`). Agent instrumentation would have to be aware of and switch
    over all supported frameworks to provide the same level of richness and keep up
    with changes to frameworks individually.
  prefs: []
  type: TYPE_NORMAL
- en: Another complication comes from increasingly prevalent asynchronous workflows
    in modern programming paradigms like reactive. A proper tracing implementation
    requires in-process propagation, which can be tricky in reactive contexts where
    you can’t just stuff context into a `ThreadLocal`. Also, dealing with [mapped
    diagnostic contexts](https://oreil.ly/h1p0-) to correlate logs and tracing can
    be tricky in those same contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Retrofitting an existing application with framework-level instrumentation can
    be relatively low touch. For example, Spring Cloud Sleuth adds tracing telemetry
    to an existing Spring Cloud—based application. You just need an additional dependency
    as in [Example 3-2](part0008_split_011.html#sleuth_dependency) and a bit of configuration
    as in [Example 3-3](part0008_split_011.html#sleuth_config), the latter of which
    can be done cross-organizationally if you are already using a centralized dynamic
    configuration server like Spring Cloud Config Server.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-2\. Sleuth runtime dependency in Gradle build
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](../images/00112.png)](part0008_split_011.html#co_debugging_with_observability_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `io.spring.dependency-management` plug-in is responsible for adding
    the version to this dependency specification.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-3\. Sleuth configuration in Spring Boot’s application.yml
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Service Mesh Tracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The service mesh is an infrastructure layer outside of application code that
    manages interaction between microservices. Many implementations accomplish this
    through sidecar proxies in some way associated with the application process.
  prefs: []
  type: TYPE_NORMAL
- en: In some ways, this form of instrumentation isn’t much different from how the
    framework might accomplish it, but don’t be fooled into thinking they are equal.
    They are similar in the point of instrumentation (decorating an RPC call). The
    framework will certainly have *more* information than the service mesh. For example,
    for REST endpoint tracing, the framework has access to exception details that
    are mapped in a lossy way to one of a small set of HTTP status codes. The service
    mesh would only have access to the status code. The framework has access to the
    unsubstituted path of the endpoint (e.g., `/api/person/{id}` instead of `/api/person/1`).
  prefs: []
  type: TYPE_NORMAL
- en: Agents also have more potential for richness than a sidecar because they can
    reach down into individual method invocations, a finer level of granularity than
    RPC calls.
  prefs: []
  type: TYPE_NORMAL
- en: Adding service mesh not only changes your delivery pipeline, it also comes at
    an additional resource and complexity cost in managing sidecars and their control
    plane as well.
  prefs: []
  type: TYPE_NORMAL
- en: Still, instrumenting at a service mesh layer means you don’t have to retrofit
    existing applications with framework instrumentation like Spring Cloud Sleuth,
    change your base images like with agent-based instrumentation, or perform manual
    instrumentation. Because of the lack of information available to the service mesh
    relative to frameworks, introducing a service mesh primarily to achieve telemetry
    instrumentation incurs the significant cost of maintaining the mesh for what ultimately
    will be less-rich telemetry. For example, a mesh will observe a request to `/api/customers/1`
    and not have the context that the framework does, that this is a request to `/api/customers/(id)`.
    As a result, telemetry streaming out of mesh-based instrumentation will be harder
    to group by parameterized URI. Adding the runtime dependency may very well be
    significantly easier in the end.
  prefs: []
  type: TYPE_NORMAL
- en: Blended Tracing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: White box (or framework telemetry that, because it is autoconfigured, *feels*
    like white box) and black box options aren’t mutually exclusive. They can in fact
    complement each other well. Consider the REST controller in [Example 3-4](part0008_split_013.html#7K4OG-2d714b853a094e9a910510217e0e3d73).
    Spring Cloud Sleuth is designed to automatically create a span around the request
    handler `findCustomerById`, tagging it with pertinent information. By injecting
    a `Tracer`, you can add a finer-grained span on just the database access. This
    breaks down the end-to-end user interaction into an even-finer-grained trace.
    Now we can identify where the database specifically is the cause of a service
    degradation in the satisfaction of the request in this particular microservice.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-4\. Blended black box and white box tracing instrumentation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](../images/00112.png)](part0008_split_013.html#co_debugging_with_observability_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Spring Cloud Sleuth will automatically instrument this endpoint, tagging the
    span with useful context like `http.uri`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](../images/00059.png)](part0008_split_013.html#co_debugging_with_observability_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Starting a new span adds another distinct element to the trace icicle graph.
    We can now reason about the cost of just this method `findCustomerById` in the
    context of an entire end-to-end user interaction.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](../images/00067.png)](part0008_split_013.html#co_debugging_with_observability_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Adding business-specific context that black box instrumentation would lack.
    Maybe your company just launched a service recently in a new country as part of
    a global expansion, and because of its recency, customers in that country lack
    a long activity history with your product. Seeing a surprising difference in lookup
    times between older and newer customers might suggest a change to how activity
    history is loaded in this situation.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](../images/00016.png)](part0008_split_013.html#co_debugging_with_observability_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The whole data access operation is manually instrumented with white box instrumentation.
  prefs: []
  type: TYPE_NORMAL
- en: Supposing database access was traced across the application stack (potentially
    by encapsulating the wrapping of database access with tracing instrumentation
    into a common library and sharing it across the organization), the database would
    be effectively traced without adding any form of white box or black box monitoring
    to the database layer itself. Instrumenting something like an IBM DB2 database
    running on a z/OS mainframe with Zipkin Brave seems like an impossible task initially,
    but it can be accomplished from the caller perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing All Calls to a Subsystem Effectively Traces the Subsystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By tracing all *calls* to a subsystem, the subsystem is covered with tracing
    in the same way as if it were instrumented itself. Many component frameworks (database,
    cache, message queue) offer some sort of event system for you to hook into. In
    many cases, the task of coating all callers with instrumentation can be reduced
    to ensuring all calling applications have a binary dependency on their runtimes
    that’s capable of automatically injecting an event handler into the component
    framework you want to instrument.
  prefs: []
  type: TYPE_NORMAL
- en: The other ramification of tracing from a caller is that it includes latency
    (e.g., network overhead) between the two systems. In a scenario where a series
    of callers is executing requests to a downstream service that is serving requests
    at a fixed maximum concurrency level (e.g., a thread pool), the downstream service
    may not even be aware of a request until it starts to be processed. Instrumenting
    from the caller side includes the time a request sat in a queue waiting to be
    processed by the downstream.
  prefs: []
  type: TYPE_NORMAL
- en: All this contextual information can get expensive. To control cost, at some
    point we have to sample tracing telemetry.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in [“The Three Pillars of Observability…or Is It Two?”](part0008_split_001.html#7K4GG-2d714b853a094e9a910510217e0e3d73),
    tracing data generally must be sampled to control cost, meaning some traces are
    published to the tracing backend and others are not.
  prefs: []
  type: TYPE_NORMAL
- en: No matter how smart the sampling strategy, it is important to remember that
    data is being *discarded*. Whatever collection of traces you get as a result are
    going to be skewed in some way. This is perfectly fine when you are pairing distributed
    tracing data with metrics data. Metrics should alert you to anomalous conditions
    and traces used to do in-depth debugging when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling strategies fall into a few basic categories, ranging from not sampling
    at all to propagating sampling decisions down from the edge.
  prefs: []
  type: TYPE_NORMAL
- en: No Sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is possible to retain every tracing sample. Some organizations even do this
    at a large scale, often at extraordinary cost. For Spring Cloud Sleuth, configure
    the `Sampler` via a bean definition, as shown in [Example 3-5](part0008_split_016.html#sleuth_no_sampling).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-5\. Configuring Spring Cloud Sleuth to always sample
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Rate-Limiting Samplers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By default, Spring Cloud Sleuth retains the first 10 samples per second (a configurable
    rate limit threshold) and downsamples probabilistically thereafter. Since rate-limiting
    sampling is the default in Sleuth, the rate limit can be set by a property, as
    in [Example 3-6](part0008_split_017.html#sleuth_rate_limit).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-6\. Configuring Spring Cloud Sleuth to retain the first 2,000 samples
    per second
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The logic behind this is that there is some rate of throughput for which it
    is reasonably cost-effective to not discard anything. This is largely going to
    be dictated by the nature of your business and the throughput to your applications.
    One regional property and casualty insurer receives 5,000 requests per minute
    through its flagship app, generated from the interactions of approximately 3,500
    insurance agents in the field. Since the pool of insurance agents is not going
    to suddenly grow by an order of magnitude overnight, a stable capacity plan for
    a tracing system that accepts 100% of traces for this system is determinable.
  prefs: []
  type: TYPE_NORMAL
- en: Even if your organization is like this insurer, it’s important to keep in mind
    where further investments in application observability are made, often in open
    source at tech companies with significant scale and at monitoring system vendors
    that can’t assume their customers all have such a stable capacity plan. Considering
    something like the high-percentile calculation of the latency of a service endpoint,
    it can still make sense to leverage high-percentile approximations from bucketed
    histograms over trying to calculate an exact percentile from tracing data, even
    though this is mathematically possible with 100% data.
  prefs: []
  type: TYPE_NORMAL
- en: The point is to avoid inventing new methods of calculating distribution statistics
    when similar methods are available from metrics telemetry designed to operate
    at scale.
  prefs: []
  type: TYPE_NORMAL
- en: One challenge of rate-based sampling is holes. When you have several microservices
    in a call chain, each independently making a decision about whether to retain
    a trace, holes are going to develop in the end-to-end picture of a given request.
    Put another way, rate-based samplers don’t make a consistent sampling decision
    given a trace ID. The moment any individual subsystem exceeds the rate threshold,
    a hole develops in traces involving this subsystem.
  prefs: []
  type: TYPE_NORMAL
- en: When making capacity planning decisions based off of rate-based samplers, be
    careful to recognize that these rates are on a *per-instance* basis. The rate
    of samples reaching the tracing system is the product of instances in the cluster
    times and the sampling rate in the worst case.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic Samplers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Probabilistic samplers count to see how many out of 100 traces should be retained.
    They guarantee that if you select a 10% probability, 10 out of 100 traces will
    be retained, but it likely won’t be the first 10 or the last 10.
  prefs: []
  type: TYPE_NORMAL
- en: In the presence of a probability property, Spring Cloud Sleuth configures a
    probabilistic sampler instead of a rate-limiting sampler, as in [Example 3-7](part0008_split_018.html#sleuth_probabilistic).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-7\. Configuring Spring Cloud Sleuth to retain 10% of traces
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Probabilistic samplers are rarely the right choice for a couple of reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Cost
  prefs: []
  type: TYPE_NORMAL
- en: No matter what probability you choose, your tracing cost grows linearly in proportion
    to traffic. Maybe you never expect an API endpoint to receive more than 100 requests
    per second and you’ve sampled to 10%. If there is a sudden increase in traffic
    to 10,000 requests per second, you’ll suddenly be shipping 1,000 traces per second
    rather than 10\. Rate-limiting samplers cap the cost in a way that places a fixed
    upper bound on cost, irrespective of throughput.
  prefs: []
  type: TYPE_NORMAL
- en: Holes
  prefs: []
  type: TYPE_NORMAL
- en: Like rate-based samplers, probabilistic samplers don’t look at trace IDs and
    headers to make their sampling decisions. Holes will develop in the end-to-end
    picture. In the case of relatively low throughput systems, a rate-based sampler
    may practically have no holes because no individual subsystem exceeded the rate
    threshold, but a probabilistic sampler has a uniform probability of holes per
    unit of throughput, so holes will likely exist even for low-throughput systems.
  prefs: []
  type: TYPE_NORMAL
- en: Boundary Sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Boundary samplers are a variant of probabilistic sampler that solves the problem
    of holes by making the sampling decision only once at the edge (the first interaction
    with your system) and propagating the sampling decision downstream to other services
    and components. The trace context in each component contains a sampling decision
    that is added as an HTTP header and extracted into trace context by the downstream
    component, as shown in [Figure 3-3](part0008_split_019.html#trace_boundary_sampling).
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0303](../images/00054.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-3\. B3 trace headers propagate the sampling decision to downstream
    components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Impact of Sampling on Anomaly Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s consider specifically the impact probabilistic sampling would have on
    anomaly detection. A similar effect would occur for any sampling strategy really,
    but we’ll use probabilistic sampling to make this concrete.
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection systems built on sampled traces are generally misguided unless
    your organization assumes the cost of 100% sampling. To show why, let’s consider
    a hypothetical sampling strategy that makes an up-front decision about whether
    to preserve a trace based on a weighted random number at the beginning of each
    request (as Google’s Dapper did originally). If we are sampling 1% of requests,
    then an outlier above the 99th percentile, like all other requests, has a 1% chance
    of surviving the sampling. There’s a 0.01% chance of seeing any of these individual
    outliers. Even at 1,000 requests/second, you could have 10 outliers happening
    per second but only see 1 every 5 minutes, as shown in [Figure 3-4](part0008_split_020.html#chance_of_outlier_in_trace_sampling)
    (a plot of <math alttext="left-parenthesis 1 minus 0.99 Superscript upper N Baseline
    right-parenthesis asterisk 100 percent-sign"><mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <mn>0</mn> <mo>.</mo> <msup><mn>99</mn> <mi>N</mi></msup> <mo>)</mo> <mo>*</mo>
    <mn>100</mn> <mo>%</mo></mrow></math> ).
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0304](../images/00026.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-4\. Chance of seeing an outlier in tracing data over time
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There can be a significant range of outliers above the 99th percentile, as shown
    in [Figure 4-20](part0009_split_010.html#avg_vs_p99_latency). You could have a
    huge business-critical outlier (above P99.9) happening once per second and only
    see it *once* in tracing data in any given hour! For the purposes of debuggability,
    having one or a small set of outliers survive over a given period is fine—we still
    get to examine in detail the nature of what’s happening in the outlier case.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Tracing and Monoliths
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Don’t be fooled by the name *distributed* tracing. It is absolutely reasonable
    to use this form of observability in a monolith as well. In the purest microservices
    architecture, tracing around RPC calls could be implemented in a black box fashion
    at either the framework level (like Spring) or in a sidecar such as those we find
    in service mesh technologies. Given the single-responsibility nature of a microservices
    architecture, tracing RPC could actually give you quite a bit of information about
    what’s going on; i.e., microservice boundaries are effectively business logic
    functional boundaries as well.
  prefs: []
  type: TYPE_NORMAL
- en: Inside a monolithic application that receives a single end-user request and
    performs many tasks to satisfy that request, framework-level instrumentation has
    a diminished value, of course, but you can still write tracing instrumentation
    at key functional boundaries inside the monolith in much the same way you write
    logging statements. In this way, you’ll be able to select specific tags that allow
    you to search for spans with business context that framework or service-mesh instrumentation
    will certainly lack.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, white box instrumentation with business-specific tagging winds up being
    essential even in a pure microservices architecture. In many cases, our key pieces
    of business functionality aren’t *completely* broken in production, but rather
    broken along specific (often unusual) business-specific fault lines. Maybe an
    insurance company’s policy administration system is failing to rate classic cars
    in a particular county in Kentucky. Having vehicle class, county, and state on
    both metrics and tracing telemetry allows an engineer to drill down dimensionally
    on a metric and find the problem area and then hop to debugging signals like tracing
    and logs to see example failures once the failing dimension is known.
  prefs: []
  type: TYPE_NORMAL
- en: Business Context Makes White Box Tracing as Important in Monoliths as in Distributed
    Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The density of white box distributed tracing instrumentation per bounded context
    of business functionality should roughly be the same in a microservices or monolithic
    architecture, because black box instrumentation will not tag spans with business-specific
    context that aids in later lookup.
  prefs: []
  type: TYPE_NORMAL
- en: So the only difference between microservices and monoliths is that you have
    more bounded contexts of business functionality packed into one process. And with
    each additional piece of business functionality comes all the trappings that support
    its existence. Observability is not an exception.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, even a microservice with a single responsibility can do a handful
    of things, including data access, in the satisfaction of a user request.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation of Telemetry
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since metrics data is a strong availability signal and tracing and logging data
    is useful for debugging, anything we can do to link them together makes the transition
    from an alert indicating a lack of availability to the debugging information that
    would best identify the underlying issue. In the case of latency, we will have
    a chart on a dashboard and an alert on a decaying max. Presenting a view of the
    latency distribution as a heatmap of the latency histogram is an interesting information-dense
    visualization but isn’t something we can plot an alert threshold on.
  prefs: []
  type: TYPE_NORMAL
- en: Metric to Trace Correlation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can plot example traces (exemplars), as shown in [Figure 3-5](part0008_split_024.html#trace_exemplars),
    on the heatmap and make the heatmap more interactive by making heatmap cells into
    links that take us directly to the tracing UI, where we can see a set of traces
    that match these criteria. So an engineer responsible for a system receives an
    alert on a latency condition, looks at the set of latency charts for this application,
    and can immediately click through to the distributed tracing system.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0305](../images/00041.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-5\. Zipkin trace data plotted on top of a Prometheus histogram represented
    as a heatmap in Grafana
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This sort of correlative plot makes metrics and tracing together more valuable.
    We’d normally lose through aggregation an understanding of what happened in particular
    cases when looking at metrics data. Traces, on the other hand, lack the understanding
    of the big picture that metrics provide.
  prefs: []
  type: TYPE_NORMAL
- en: Also, since it is certainly possible that trace sampling (again, to control
    cost) has thrown away all the traces that would match a particular latency bucket,
    we still get to understand what latencies end users experienced even if we aren’t
    able to drill into the trace detail.
  prefs: []
  type: TYPE_NORMAL
- en: This visualization is built through the combination of independent Prometheus
    and Zipkin queries, as shown in [Figure 3-6](part0008_split_024.html#prometheus_zipkin_queries_grafana).
    Notice that the tags don’t have to strictly line up between the metrics and tracing
    instrumentation. Micrometer `Timer` called `http.server.requests` (which yields
    a set of time series which are called `http_server_requests_second_bucket` in
    Prometheus when histograms—[“Histograms”](part0007_split_007.html#6LKA8-2d714b853a094e9a910510217e0e3d73)—are
    turned on) is collected with a tag called `uri`. Spring Cloud Sleuth instruments
    Spring in a similar way but tags traces with `http.uri`. These are of course logically
    equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0306](../images/00019.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-6\. Independent Prometheus and Zipkin queries form the combined trace
    exemplar heatmap
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It should be clear, however, that even though the tag keys (and even values)
    don’t have to be identical, if you want to filter the heatmap to a metrics tag
    that has no logical equivalent in the tracing data, then it won’t be possible
    to accurately find exemplars to match what is seen on the heatmap (there will
    be some false positives). For example, Spring Cloud Sleuth didn’t initially tag
    traces with HTTP status code or outcome, while Spring’s Micrometer instrumentation
    did. Often we want to limit a latency visualization to either successful or unsuccessful
    outcomes because their latency characteristics can be quite different (e.g., failures
    occur abnormally fast due to an external resource unavailability or abnormally
    slow due to a timeout).
  prefs: []
  type: TYPE_NORMAL
- en: So far, our look at distributed tracing has strictly been related to observability,
    but it can serve other purposes that influence or govern the way traffic is handled.
  prefs: []
  type: TYPE_NORMAL
- en: Using Trace Context for Failure Injection and Experimentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier, when discussing sampling methods for distributed tracing, we covered
    boundary sampling (see [“Boundary Sampling”](part0008_split_019.html#boundary_sampling)).
    In this method, a sampling decision is made up front (i.e., at the edge), and
    this decision is propagated downstream to the microservices that are involved
    in satisfying a request. There is an interesting opportunity to make other up-front
    decisions and leverage trace context to pass along other information unrelated
    to sampling decisions to downstream services as well.
  prefs: []
  type: TYPE_NORMAL
- en: A well-known example of this is *failure injection testing* (FIT), a specific
    form of chaos engineering. The overall discipline of chaos engineering is broad
    and covered in detail in [*Chaos Engineering*](http://shop.oreilly.com/product/0636920203957.do).
  prefs: []
  type: TYPE_NORMAL
- en: Failure injection decisions can be added by the API gateway up front in coordination
    with rules provided by a central FIT service and propagated downstream as a trace
    tag. Later, a microservice in the execution path can use this information about
    a failure test to unnaturally fail a request in some way. [Figure 3-7](part0008_split_025.html#failure_injection_testing)
    shows the whole process, end to end.
  prefs: []
  type: TYPE_NORMAL
- en: '![srej 0307](../images/00043.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3-7\. Failure injection testing process from user request to failure
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Attaching this kind of decision to telemetry has the added benefit that any
    sampled traces that also are part of a failure injection are tagged as such, so
    you can differentiate between real and intentional failures when looking at telemetry
    later. [Example 3-8](part0008_split_025.html#gateway_adding_fit) shows a simplified
    example of a Spring Cloud Gateway application (that also has the Spring Cloud
    Sleuth starter applied) looking up and adding a FIT decision as “baggage” to the
    trace context, which can automatically be converted to a trace tag by setting
    the property `spring.sleuth.baggage.tag-fields=failure.injection`.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-8\. Spring Cloud Gateway adding failure injection testing data to
    trace context
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Then, add an incoming request filter (in this case a WebFlux `WebFilter`) to
    all microservices that might participate in failure injection tests, as shown
    in [Example 3-9](part0008_split_025.html#fit_web_filter).
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-9\. WebFlux WebFilter for failure injection testing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can also add a failure injection test decision as a tag to HTTP client metrics,
    as shown in [Example 3-10](part0008_split_025.html#micrometer_webflux_metrics_fit).
    It may be useful to filter out failure injection tests from our notion of the
    error ratio of the HTTP client interaction with downstream services. Or perhaps
    they are left in to alert criteria to validate the engineering discipline of being
    alerted to and responding to unexpected failure, but the data will still be present
    so that the investigating engineer can dimensionally drill down to determine if
    the alert was caused by failure injection or by real issues.
  prefs: []
  type: TYPE_NORMAL
- en: Example 3-10\. Adding the failure injection testing decision as a Micrometer
    tag
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This is just a sketch, of course. It is up to you how you would define a failure
    injection service, and under what conditions to select requests for failure injection.
    For a simple set of rules, this service could even be an integral part of your
    gateway application.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to failure injection, trace baggage could be used to propagate decisions
    about whether a request is participating in A/B experiments as well.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve shown the difference between monitoring for availability
    and monitoring for debugging. The event-based nature of debugging signals means
    they tend to want to grow proportionally with increased throughput through a system,
    a cost-limiting measure is necessary. Different methods of sampling to control
    cost were discussed. The fact that debugging signals are typically sampled should
    give us pause about trying to build aggregations around them, since every form
    of sampling discards some part of the distribution and thus skews the aggregation
    in one form or another.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we showed how in addition to its chief function in publishing debugging
    information, we can piggyback on trace context propagation to propagate behaviors
    down a deep microservice call chain.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we return to metrics, showing which availability signals
    you should start with for basically every Java microservice.
  prefs: []
  type: TYPE_NORMAL
