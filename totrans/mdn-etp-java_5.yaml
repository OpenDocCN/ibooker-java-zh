- en: 'Chapter 5\. Beyond Lift and Shift: Working with Legacy'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Legacy is not what I did for myself. It’s what I’m doing for the next generation.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Vitor Belfort
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Many organizations are faced with the challenge of keeping their existing business
    operations running while also trying to innovate. There are typically increased
    expectations to deliver new functionality faster and to reduce cost, something
    that seems challenging when looking at the existing application landscape and
    prevalence of legacy systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We often use the term “legacy system” to describe an old methodology, or technology,
    or application that is not written according to the latest methods or uses an
    outdated technology stack. Admittedly, many of the systems we created early on
    in our career belong to this category. We do know that most of them are still
    in use. Some of them even paved the way for new approaches or even standards that
    followed them. We usually also imply that those systems would need a replacement,
    which ultimately contributes to the perceived negative connotation. Thankfully,
    this isn’t always true. Legacy also is a beautiful word to describe achievements
    and heritage. Calling something “legacy” doesn’t automatically make it outdated
    and unusable. There are plenty of reasons to keep the legacy systems in place,
    including:'
  prefs: []
  type: TYPE_NORMAL
- en: The system works as designed, and there is no need to change.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The business processes implemented are no longer known or documented, and replacing
    them is expensive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cost for replacing a system is higher than the benefit of keeping it unchanged.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The book [*Working Effectively with Legacy Code*](https://oreil.ly/iogGC) by
    Michael Feathers (O’Reilly) provides programmers with techniques to cost-effectively
    handle common legacy code problems without having to go through the hugely expensive
    task of rewriting all existing code.
  prefs: []
  type: TYPE_NORMAL
- en: Feathers said, “To me, legacy code is simply code without tests.” If we read
    the term “legacy” today, it primarily refers to monolithic applications. There
    are various approaches to handling legacy applications in a modern enterprise
    landscape, and picking the right one is the first and most crucial part of the
    modernization journey.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve only talked about individual systems so far. And developers usually only
    care about this specific system scope. Modernization plans should follow overarching
    company goals and should also take the company-wide IT strategy into account.
    A particularly exciting approach for cloud migration is presented in Gregor Hohpe’s
    book [*Cloud Strategy: A Decision-Based Approach to Successful Cloud Migration*](https://oreil.ly/EuW9J).
    It is a must-read if you want to know more about building the abstraction above
    individual migration efforts.'
  prefs: []
  type: TYPE_NORMAL
- en: Managing Legacy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every successful journey begins with a first step. The first step for an application
    migration journey is the assessment of the existing applications. We assume that
    you know the company-wide goals and directives. We can map them into assessment
    categories now. Another source for assessment categories is technical requirements—for
    example, existing blueprints or recommended master solutions or framework versions.
    Building and updating this list of assessment categories should be a recurring
    task that becomes part of your governance process. Ultimately, you can derive
    migration criteria from these assessment criteria and use them as decision-making
    cornerstones for your modernization journey.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing Applications for Migration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When assessing a migration or modernization effort, it is essential to consider
    the specific challenges that motivate or influence your organization. Some examples
    of challenges that organizations might face include:'
  prefs: []
  type: TYPE_NORMAL
- en: Limited budgets for development
  prefs: []
  type: TYPE_NORMAL
- en: Development teams need to become more efficient, and their velocity has to increase.
    Instead of working with complex specifications, they aim to switch to lightweight
    frameworks and prebuilt functionalities. Modernizations should be usually scheduled
    as part of an ongoing development or maintanance project.
  prefs: []
  type: TYPE_NORMAL
- en: Lack of in-house skills
  prefs: []
  type: TYPE_NORMAL
- en: The team skills for existing in-house technologies are decreasing. Examples
    of this are host programming or even earlier versions of Enterprise Java specifications
    that are no longer taught or state-of-the-art. Changing existing systems that
    use older technologies might mean needing to add specific skills for the development
    project.
  prefs: []
  type: TYPE_NORMAL
- en: Perceived risks
  prefs: []
  type: TYPE_NORMAL
- en: Following a famous proverb popularized around 1977, “If it ain’t broken, don’t
    fix it,” we do see a lot of perceived risks to changing well-established and running
    software. The reasons for this can be numerous and range from knowledge issues
    about the system to fear of stopped production in factories. These risks need
    to be addressed individually and mitigated through suitable actions in the migration
    plan.
  prefs: []
  type: TYPE_NORMAL
- en: No known predictable process
  prefs: []
  type: TYPE_NORMAL
- en: This book helps you with this particular point. Navigating the unknown can be
    a considerable challenge. Having a proven and repeatable process for modernization
    efforts in place that all parties respect and follow is critical for success.
  prefs: []
  type: TYPE_NORMAL
- en: Real effort estimation
  prefs: []
  type: TYPE_NORMAL
- en: Estimating migration efforts should not be magic. Unfortunately, many companies
    have a minimal idea about the genuine efforts to modernize Enterprise Java applications.
    Following a predictable and optimized approach will remove this challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Turning these challenges into actionable items for your assessment can look
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the level of effort and cost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling application migrations and handling conflicts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying all potential risks at a code, infrastructure, process, or knowledge
    level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting the return on investment to make the business case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying and mitigating risks to the business
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimizing disruption to existing business operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is sufficient to do this in a spreadsheet or document if you are only looking
    at a single application. However, every mid- to large-scale effort needs a better
    solution. Large-scale efforts need automated routines and rules to assess an install
    base and link applications to business services to plan the next steps reliably.
    An open source and straightforward way of gathering and managing all relevant
    information comes from the [Konveyor project](https://www.konveyor.io). It combines
    a set of tools that aim at helping with modernization and migration onto Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: The Konveyor subproject Forklift provides the ability to migrate virtual machines
    to KubeVirt with minimal downtime. The subproject Crane concentrates on migrating
    applications between Kubernetes clusters. Also part of the suite is Move2Kube
    to help accelerate the replatforming of Swarm and Cloud Foundry-based applications
    to Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: For application modernization in particular, Konveyor offers the [Tackle](https://oreil.ly/u99Gf)
    project. It assesses and analyzes applications for refactoring into containers
    and provides a standard inventory.
  prefs: []
  type: TYPE_NORMAL
- en: Tackle Application Inventory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This allows users to maintain their portfolio of applications, link them to
    the business services they support, and define their interdependencies. The Application
    Inventory uses an extensible tagging model to add metadata, which is a great way
    to link migration categories, as discussed earlier. The Application Inventory
    is used to select an application for an assessment by Pathfinder.
  prefs: []
  type: TYPE_NORMAL
- en: Tackle Pathfinder
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is an interactive, questionnaire-based tool that assesses the suitability
    of applications for modernization so they can be deployed in containers on an
    enterprise Kubernetes platform. [Pathfinder](https://oreil.ly/K4V4u) generates
    reports about an application’s suitability for Kubernetes, including the associated
    risk, and creates an adoption plan. Pathfinder does this based on the information
    present in the application inventory and additional assessment questions. If an
    application depends on a direct host system connection, it might disqualify this
    particular application for a migration to Kubernetes because it would overload
    the host parts. Some examples of assessment questions are:'
  prefs: []
  type: TYPE_NORMAL
- en: Are third-party vendor components supported in containers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the application under active development?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the application have any legal requirements (e.g., PCI, HIPAA)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the application provide metrics?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We strongly recommend looking at Pathfinder to manage large-scale modernization
    projects across complete landscapes. It will help you categorize and prioritize
    applications in your scope today and continuously track your migration assessment
    for future changes.
  prefs: []
  type: TYPE_NORMAL
- en: Tackle Controls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Controls are a collection of entities that add different values to the Application
    Inventory and the Pathfinder assessment. They comprise business services, stakeholders,
    stakeholder groups, job functions, tag types, and tags. In addition, you can capture
    company- or project-specific attributes by implementing your own entities. This
    will filter your Application Inventory, for example, all applications used by
    a certain “job function” to identify all applications used by the human resources
    department.
  prefs: []
  type: TYPE_NORMAL
- en: Tackle DiVA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, [DiVA](https://oreil.ly/UGzn2) is a data-centric application analysis
    tool. As a successor to the project [Windup](https://oreil.ly/sjiNq), it is the
    most exciting project to look at if you want to assess individual applications.
    It focuses on the traditional monolithic application and currently supports Servlets
    and Spring Boot applications. You can import a set of application source files
    (Java/XML), and DiVA then provides the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Service entry (exported API) inventory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database inventory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transaction inventory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code-to-Database dependencies (call graphs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Database-to-Database dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transaction-to-Transaction dependencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transaction refactoring recommendations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DiVA is currently under active development, and the incorporation of the original
    Windup project isn’t finished yet. However, it still gives you a solid foundation
    for your modernization efforts. Additionally, it presents an excellent opportunity
    to contribute your findings and become part of a larger community dedicated to
    automating migrations.
  prefs: []
  type: TYPE_NORMAL
- en: Migration Toolkit for Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While we wait for Windup to be fully integrated into DiVA, you can still use
    an automated migration assessment for Enterprise Java-based applications by using
    the [Migration Toolkit for Applications (MTA)](https://oreil.ly/SIOSR).
  prefs: []
  type: TYPE_NORMAL
- en: MTA assembles tools that support large-scale Enterprise Java application modernization
    and migration projects across many transformations and use cases. You can import
    your application binary or archives into it, and it automatically performs code
    analysis, including the application portfolio, application dependencies, migration
    challenges, and migration effort estimation in the form of story points. Initially
    it was designed to support Java EE server migrations (e.g., WebSphere or WebLogic
    to JBoss EAP). Still, it has a highly extensible rule set mechanism that allows
    developers to create their own set of rules or even adapt existing ones to their
    needs. Today it also covers Spring Boot to Quarkus migrations.
  prefs: []
  type: TYPE_NORMAL
- en: 'An excerpt from an example rule in Java looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This rule scans Java classes for `@WLServlet` annotations and adds an effort
    (story points) to this finding. You can learn more about rules and how to develop
    them in the [Windup documentation](https://oreil.ly/FbQKL).
  prefs: []
  type: TYPE_NORMAL
- en: Beyond that, it can also support nonmigration use cases as part of a build process
    (via a [Maven plug-in](https://oreil.ly/T8mom) or a [Command Line Interface](https://oreil.ly/U7Dsk)),
    either validating code regularly against organizational standards or ensuring
    application portability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the patterns MTA can detect include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Proprietary libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proprietary configurations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service locators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EJB descriptors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deprecated Java code
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transaction managers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Injection frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thread pooling mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Timer services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: WAR/EAR descriptors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Static IP addresses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MTA and DiVA are two potent tools that help us identify overall technical debt,
    resulting in a classification of migration needs and risks. However, they do not
    allow us to identify the functionality that should be migrated or modernized first.
    For this, we need to take a deeper look into the application design and functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Assessing Functionality for Migration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional monoliths come in various shapes, forms, and sizes. When someone
    uses the term “monolith,” they are usually referring to the deployment artifact
    itself. In Enterprise Java, this has traditionally been Enterprise Archives (EAR)
    or Web Archives (WAR). You can also look at them as single-process applications.
    They can be designed following modularity recommendations like OSGi (Open Services
    Gateway Initiative) or following more technical approaches like the three-tier
    design without significant business modules. The overall direction of your modernization
    efforts heavily depends on the type of monolith you are dealing with. As a rule
    of thumb, the more modular an existing application already is, the easier it is
    to modernize it. In a perfect world, modules directly translate into service boundaries.
    But this rarely happens.
  prefs: []
  type: TYPE_NORMAL
- en: If the monolith seems like a giant box, we have to apply a logical model to
    it. And we realize that inside this box are organized business and technical components,
    for example, order management, PDF rendering, client notifications, etc. While
    the code is probably not organized around these concepts, they exist in the codebase
    from a business-domain-model perspective. These business domain boundaries, often
    called “bounded contexts” in Domain-Driven-Design (DDD), become the new services.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'If you are interested in learning more, many consider Eric Evans’s book [*Domain-Driven
    Design: Tackling Complexity in the Heart of Software* (O’Reilly)](https://oreil.ly/kgLPl)
    the de facto standard introduction to DDD.'
  prefs: []
  type: TYPE_NORMAL
- en: Once you have identified the modules and functionality, you can start to think
    about your modernizing order. But first, make sure to look at the cost versus
    benefit tradeoffs for each module and start with the best candidate. [Figure 5-1](#fig5-1)
    gives a very high-level overview of how this could look for a sample application
    with six modules. Let’s assume we are talking about a fictitious online shop in
    this case. For modules that are heavily interdependent, for example, Order and
    Customer, it will be complex to extract them individually. If you also consider
    the necessity for scalability and with that the benefit of removing them from
    a monolith, it might not be very high. Those two modules reside on the lower left
    side of the graph. On the opposite side, we might find the Catalog service. It
    lists the available products and is a read-only service with very little interdependencies.
    During high demand on the website, this is the number-one requested module, and
    it benefits heavily from being extracted, as shown in Figure 5-1, indicated by
    the green module in the upper right of the graph. Do a similar exercise for all
    the modules in your application to assess cost versus benefit.
  prefs: []
  type: TYPE_NORMAL
- en: '![Cost vs. Benefit](Images/moej_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Cost versus benefit
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You’ve now reached the last checkpoint to validate your earlier strategic application
    assessment. Does the estimated modernization benefit outweigh the estimated modernization
    cost? Unfortunately, there is no generally applicable recommendation, as it heavily
    depends on the application itself, the business requirements, and the overarching
    company goals and challenges. Document your decisions and conclusions because
    now is the time to decide about the future direction of your modernization effort.
    Remember the 6 Rs from [Chapter 3](ch03.xhtml#travel_light_on_your_pathway)? Retain
    (change nothing), Retire (turn off), Repurchase (a new version), Rehost (put into
    containers), Replatform (some slight adjustments), or Refactor (build something
    new).
  prefs: []
  type: TYPE_NORMAL
- en: We’ve now assessed the application for migration, and we’ve evaluated the functionality
    for migration. We know which aspects of the application we’re ready to modernize.
    You’ve concluded that you do not want to build a new application but rather gently
    modernize the existing legacy. In the next section, we are going to take a deeper
    look at some approaches to migration.
  prefs: []
  type: TYPE_NORMAL
- en: Migration Approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The aforementioned tools and assessments will help you on your journey to identify
    the most suitable applications and services. Now it’s time to dig deeper into
    the strategies and challenges of a single application.
  prefs: []
  type: TYPE_NORMAL
- en: Protecting Legacy (Replatform)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With only one or two modules needing a business refresh or added functionality,
    the most straightforward way is to focus on the two modules and keep as much as
    possible of the existing application, making it runnable on modern infrastructure.
    Besides changes of the relevant modules, this also involves a reevaluation of
    the runtime, libraries, or even target infrastructure while touching as little
    code as possible.
  prefs: []
  type: TYPE_NORMAL
- en: This can be achieved by simply containerizing the application and databases
    and modifying relevant modules of a well-architected monolith or extracting certain
    functionality completely and reintegrating it to a partly distributed system,
    as [Figure 5-2](#fig5-2) shows.
  prefs: []
  type: TYPE_NORMAL
- en: '![Putting the pieces back together](Images/moej_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. Putting the pieces back together
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What is easily said isn’t quickly done. There are plenty of nonfunctional requirements
    that need to be reallocated from the application server platform to the [outer
    architecture](https://oreil.ly/rrcZG). We will focus on the more critical pieces
    in the next chapter. In this chapter, we want to focus on the migration of the
    application and database itself.
  prefs: []
  type: TYPE_NORMAL
- en: Service to application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once you’ve extracted certain functionality, the most pressing question is how
    to integrate the remaining monolith with the newly extracted service. Assuming
    that you switch to a container runtime, you should use an API Gateway to load
    balance and switch traffic on a URL basis. We’ll cover this in more detail in
    [Chapter 6](ch06.xhtml#microservices_architecture).
  prefs: []
  type: TYPE_NORMAL
- en: Another approach is to use an HTTP proxy. It is essential to have the proxy
    up in production before you even try to extract parts of the monolith. Ensure
    it does not break the existing monolith and take some time to push the new service
    into production regularly, even without it being used by end users. Gradually
    switch over by redirecting traffic if everything looks good.
  prefs: []
  type: TYPE_NORMAL
- en: For more simple service to monolith interactions, you can even think about implementing
    a simple JAX-RS direct communication. This approach is only suitable when you
    work with very few services, though. Make sure to treat the extracted service
    as an integration system from the perspective of the monolith.
  prefs: []
  type: TYPE_NORMAL
- en: All three approaches (API, gateway, HTTP proxy, and JAX-RS interface) are a
    pathway to your first successful microservice. They all implement the strangler
    pattern (refer to [Chapter 3](ch03.xhtml#travel_light_on_your_pathway)) and help
    to refactor the monolith into separate systems as a first step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interception is a potentially dangerous path: if you start building a custom
    protocol translation layer that is shared by multiple services, you risk adding
    too much intelligence to the shared proxy. This design approach leads away from
    independent microservices and becomes a more service-oriented architecture with
    too much intelligence in the routing layer. A better alternative is the so-called
    Sidecar pattern, which basically describes an additional container in the Pod.
    Rather than placing custom proxy logic in a shared layer, it becomes part of the
    new service. As a Kubernetes sidecar, it becomes a runtime binding and can serve
    legacy clients and new clients.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A *sidecar* is just a container that runs on the same Pod as the application
    container. It shares the same volume and network as the application container
    and can “help” or enhance application behavior with this. Typical examples are
    logging, or more generally agent functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Database to databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we have identified the functional boundary and the integration method,
    we need to decide how to approach database separation. While monolith applications
    typically rely on a single large database, each extracted service should operate
    on its own data. The correct way to solve this puzzle again depends on the existing
    data layout and transactions.
  prefs: []
  type: TYPE_NORMAL
- en: A relatively easy first step is to separate the tables necessary for the service
    into a read-only view and a write table and adjust the flow of the monolith to
    use an interface for both read and write operations. These interfaces can more
    easily be abstracted in a later step into a service access. This option requires
    changes to the monolith application only and should have minimal impact on the
    existing codebase. We can move the table into a separate database and adjust the
    dependent queries in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: All this solely happens in the old monolith as preparation. Evolving existing
    code into a more modularized structure as preparation can be risky. In particular,
    the risk increases as the data model complexity does. In the last step, we can
    separate the extracted tables into a new database and adjust the monolith to use
    the newly created service for interactions with the business object. This is relatively
    easy with pen and paper and quickly reaches the end of practicality if the data
    access requires many joins across tables. Simple candidates are master data objects,
    like “User.” More complex ones could be combined objects, like an “Order.” What
    was said about the modularization of the application code is even more true for
    the database. The better the design and modularization already are, the easier
    it will be to extract functionality and data into a separate service. There will
    be cases where you won’t find an excellent solution to extract objects from the
    data model. Or you may see different approaches not delivering suitable performance
    anymore. This is the time to revisit your chosen modernization path.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing on the happy path, you now have two separate databases and two very
    unequal “services” composing a system. Now it’s time to think about data synchronization
    strategies between your services. Most databases implement some functionality
    to execute behavior on data changes. Simple cases support trigger functionality
    on changed rows to add copies to other tables or even call higher-level features
    (e.g., WebServices) on change. It is often proprietary functionality and heavily
    depends on the database being used. This could be an option if you have a company-wide
    directive to use certain features or you’re confident enough in further altering
    the original legacy database.
  prefs: []
  type: TYPE_NORMAL
- en: If this isn’t possible, there’s the batch job-based synchronization. Changed
    timestamps, versions, or status columns indicate a needed replication. You can
    rely on this as a very mature and well-known version of data synchronization,
    which you can find in many legacy systems. The major drawback is that you’ll always
    end up with a discrepancy in data accuracy in the target system no matter the
    implementation. Higher replication intervals might also lead to additional costs
    for transactions or additional load on the source system. This approach is only
    suitable for infrequent updates that ideally have a non-time-sensitive process
    step in between. It is unsuitable for real- or near-time update requirements.
  prefs: []
  type: TYPE_NORMAL
- en: The modern approach to solving data synchronization challenges relies on log
    readers. As third-party libraries, they identify changes by scanning the database
    transaction log files. These log files exist for backup and recovery operations
    and provide a reliable way to capture all changes, including deletes. This concept
    is also known as change-data-capture. One of the most notable projects here is
    [Debezium](https://debezium.io). Using log readers is the least disruptive option
    for synchronizing changes between databases because they require no modification
    to the source database, and they don’t have a query load on the source systems.
    Change data events generate notifications for other systems with the help of the
    Outbox pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Build Something New (Refactor)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If, for whatever reasons, you’ve reached a fork in the road where you decide
    to re-implement and refactor your complete system into a new distributed architecture,
    you are most likely thinking about synergies and ways to keep effort small and
    predictable. Given the complexity of a full microservices stack, this isn’t an
    easy task. One critical factor with this approach is team knowledge. After many
    years of development on an Enterprise Java application server, a team should profit
    from continuous API and standards knowledge. There are various ways to implement
    services on the JVM that all help teams with reusing the most critical functionalities
    we all already know from Enterprise Java/Jakarta EE standards. Let’s discuss some
    of these methods for implementing services on the JVM.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Jakarta EE](https://jakarta.ee/about) is a set of specifications that enables
    Java developers to work on Java Enterprise applications. The specifications are
    developed by well-known industry leaders that instill confidence in technology
    developers and consumers. It is the open source version of the Java Enterprise
    Edition.'
  prefs: []
  type: TYPE_NORMAL
- en: MicroProfile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[MicroProfile](https://microprofile.io) was created in 2016 and quickly joined
    the Eclipse foundation. The primary purpose of MicroProfile is to create a Java
    Enterprise framework for implementing portable microservices in a vendor-neutral
    way. MicroProfile is composed of a vendor-agnostic programming model and configuration
    and services such as tracing, fault tolerance, health, and metrics. MicroProfile
    API components are built upon the model of Jakarta EE, making a transition to
    microservices more natural for Java developers. You can reuse the existing knowledge
    of Jakarta EE you’ve already accumulated in your career. MicroProfile defines
    12 specifications as shown in [Figure 5-3](#fig5-3), and the component model underneath
    uses a subset of the existing Jakarta EE standards. Compared to the full Jakarta
    EE specification, the more heavyweight specifications are missing. Most relevant
    for larger monolithic applications are Enterprise JavaBeans (EJB) and Jakarta
    XML Web Services.'
  prefs: []
  type: TYPE_NORMAL
- en: '![MicroProfile technologies overview](Images/moej_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. MicroProfile technologies overview
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are various implementations of the MicroProfile specifications available:
    Open Liberty, Thorntail, Paraya Server, TomEE, SmallRye, etc. As the MicroProfile
    relies on principles and components close to the Jakarta EE Web Profile, it is
    comparably easy to migrate existing applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Quarkus
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Quarkus](http://quarkus.io) is a relatively new member of the so-called microservices
    frameworks. It is a full stack, Kubernetes-native Java framework for JVMs and
    native compilation. It is optimized specifically for containers and constrained
    runtime environments. Its primary purpose is to be an ideal runtime for serverless,
    cloud, and Kubernetes environments.'
  prefs: []
  type: TYPE_NORMAL
- en: It works with popular Java standards, frameworks, and libraries like Eclipse
    MicroProfile, Spring Boot, Apache Kafka, RESTEasy (JAX-RS), Hibernate ORM (JPA),
    Infinispan, Camel, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: The dependency injection solution is based on CDI (Contexts and Dependency Injection)
    coming from Jakarta EE, making it compatible with established component models.
    An interesting part is the extension framework, which helps expand functionality
    to configure, boot, and integrate company-specific libraries into your application.
    It runs on JVMs and supports GraalVM (a general-purpose virtual machine for many
    languages).
  prefs: []
  type: TYPE_NORMAL
- en: Component models to services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most common questions among developers is how to migrate existing
    component models of Enterprise Java applications into microservices. Commonly,
    this question refers to Enterprise Java Beans or CDI Beans, especially the container-managed
    persistence beans (before EJB3), which need to be re-created on a Java Persistence
    API (JPA) basis. We strongly recommend checking if the underlying data/object
    mapping is still accurate and suitable for the new requirements and re-creating
    it entirely. This is not the most time- and cost-consuming part of modernization.
    Typically, the more challenging parts are the coded business requirements. While
    CDI Beans are technically part of MicroProfile-compatible implementations, the
    decision of whether a simple code migration is appropriate depends on the new
    business requirements. It is essential to look for existing code transaction boundaries
    to ensure no downstream resource needs to be involved. A general recommendation
    is to reuse as little source code as possible. The reason here is mainly the different
    approaches in system design between the two technologies. While we got away with
    a halfway modularized monolith, this isn’t possible with microservices anymore.
    Taking extra care to define the bounded contexts will pay off for the performance
    and design of the final solution.
  prefs: []
  type: TYPE_NORMAL
- en: Spring applications to services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can take a similar approach with applications following a different programming
    framework like Spring. While it will technically be easy to update and copy existing
    implementations, the drawbacks stay the same. In particular, it might be helpful
    for Spring-based development teams to use compatibility APIs in different frameworks
    like Quarkus.
  prefs: []
  type: TYPE_NORMAL
- en: Quarkus’s Spring API compatibility includes Spring DI, Spring Web, and Spring
    Data JPA. Additional Spring APIs are partially supported like Spring Security,
    Spring Cache, Spring Scheduled, and Spring Cloud Config. The Spring API compatibility
    in Quarkus is not intended to be a complete Spring platform to rehost existing
    Spring applications. The intent is to offer enough Spring API compatibility to
    develop new applications with Quarkus.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With assessment, planning, and care, you can decompose and modernize existing
    monolithic applications. It is not an automated process most of the time and will
    require a decent amount of work. There are some specific challenges to watch out
    for.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding Dual-Writes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you build a few microservices, you quickly realize that the most challenging
    part about them is data. As part of their business logic, microservices often
    have to update their local data store. At the same time, they also need to notify
    other services about the changes that happened. This challenge is not so evident
    in the world of monolithic applications, nor on legacy-distributed transactions
    operating on one data model. This situation isn’t easy to resolve. With a switch
    to distributed applications, you most likely lose consistency. This is described
    in the CAP theorem.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The [CAP theorem](https://oreil.ly/TVwYw), or the “two out of three” concept,
    states that we can only simultaneously provide two of the following three guarantees:
    consistency, availability, and partitition tolerance.'
  prefs: []
  type: TYPE_NORMAL
- en: Modern distributed applications use an event bus, like Apache Kafka, to transport
    data between services. Migrating your transactions from two-phase commit (2PC)
    in your monolith to a distributed world will significantly change the way your
    application behaves and reacts to failures. You need a way to control long-running
    and distributed transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Long-Running Transactions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Saga pattern offers a solution to dual writes and long-running transactions.
    While the Outbox pattern solves the more straightforward interservice communication
    problem, it is insufficient to solve the more complex, long-running, distributed
    business transactions use case. The latter requires executing multiple operations
    across multiple services with a consistent all-or-nothing semantic. Every multistep
    business process can be an example of this when split out across multiple services.
    The shopping cart application needs to generate confirmation emails and print
    a shipping label in the inventory. All actions must be carried out together or
    not at all. In the legacy world, or with a monolithic architecture, you might
    not be aware of this problem as the coordination between the modules is done in
    a single process and a single transactional context. The distributed world requires
    a different approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Saga pattern offers a solution to this problem by splitting up an overarching
    business transaction into multiple local database transactions, which are executed
    by the participating services. Generally, there are two ways to implement distributed
    sagas:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Choreography: In this approach, one participating service sends a message to
    the next one after it has executed its local transaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Orchestration: In this approach, one central coordinating service coordinates
    and invokes the participating services. Communication between the participating
    services might be either synchronous, via HTTP or gRPC, or asynchronous, via messaging
    such as Apache Kafka.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing Old Code Too Quickly
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As soon as we extract a service, we want to get rid of the old source code,
    maintenance costs, and duplicate development. But be careful. You can look at
    the old code as a reference and test changes in behavior against both code bases.
    It might also be helpful from time to time to check the timing of the newly created
    service. A recommendation is to run them side by side for a defined period and
    compare the results. After this, you can remove the old implementation. That is
    early enough.
  prefs: []
  type: TYPE_NORMAL
- en: Integration Aspects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Traditional monoliths have a solid relationship with complex integration logic.
    This is mainly proxied behind session facades or integrated with data synchronization
    logic. Every single system integrated into the overarching business process needs
    to be treated as a separate service. You can apply the same principles when extracting
    parts of the data from the existing data model and do this step by step. Another
    approach is to treat your integration logic as a service from the very beginning.
    A method that was primarily designed to support microservices is [Camel K](https://oreil.ly/JiOwc).
    It builds on the foundation of the well-known Apache Camel integration library
    and wraps integration routes into containers or better individual services. This
    way, you can separate the complete integration logic of your monolithic application
    and your services.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Modern enterprise Java systems are like generations of families: they evolve
    on top of legacy systems. Using proven patterns, standardized tools, and open
    source resources will help you create long-lasting systems that can grow and change
    with your needs. Fundamentally, your migration approach is directly related to
    what problems you’re trying to solve today and tomorrow. What are you trying to
    achieve that your current architecture doesn’t scale up to? Maybe microservices
    are the answer, or perhaps something else is. You must understand what you’re
    trying to achieve because it will be challenging to establish how to migrate the
    existing systems without that comprehension. Understanding your end goal will
    change how you decompose a system and how you prioritize that work.'
  prefs: []
  type: TYPE_NORMAL
