# 第四章。与 JIT 编译器一起工作

**即时编译器（JIT compiler）** 是 Java 虚拟机的核心；没有什么比 JIT 编译器更能控制应用程序的性能了。

本章深入讲解编译器。它从编译器的工作原理开始讲起，探讨了使用即时编译器（**JIT compiler**）的优缺点。直到 JDK 8 出现之前，你必须选择两个 Java 编译器之间。如今，这两个编译器仍然存在，但它们协同工作，虽然在少数情况下需要选择其中一个。最后，我们将研究一些编译器的中级和高级调优方法。如果一个应用程序运行缓慢且没有明显原因，这些部分可以帮助您确定是否是编译器的问题。

# 即时编译器：概述

我们将从一些介绍性材料开始；如果您理解即时编译的基础知识，可以直接跳过。

计算机——特别是 CPU——只能执行相对较少的特定指令，称为**机器码（machine code）**。因此，CPU 执行的所有程序都必须转换成这些指令。

类似 C++ 和 Fortran 的语言被称为**编译语言（compiled languages）**，因为它们的程序以二进制（编译后的）代码交付：程序编写完成后，静态编译器生成二进制代码。该二进制代码的汇编代码针对特定的 CPU。兼容的 CPU 可以执行相同的二进制代码：例如，AMD 和 Intel CPU 共享一组基本的汇编语言指令，并且后续版本的 CPU 几乎总是可以执行与前一版本相同的指令集。反之则不尽然；新版本的 CPU 通常会引入在旧版本 CPU 上无法运行的指令。

另一方面，像 PHP 和 Perl 这样的语言是解释执行的。只要机器有正确的解释器（即名为`php`或`perl`的程序），就可以在任何 CPU 上运行相同的程序源代码。解释器在执行每行程序时将该行翻译成二进制代码。

每个系统都有优缺点。用解释语言编写的程序是可移植的：你可以将相同的代码放到任何有适当解释器的机器上，它都能运行。但它可能运行得很慢。举个简单的例子，考虑循环中发生的情况：解释器在每次执行循环中的代码时都会重新翻译每一行代码。而编译代码则不需要重复进行这种翻译。

一个优秀的编译器在生成二进制代码时考虑了多个因素。一个简单的例子是二进制语句的顺序：并非所有的汇编语言指令执行时间都相同。例如，将两个寄存器中存储的值相加的语句可能在一个周期内执行，但从主存中检索所需的值可能需要多个周期。

因此，一个优秀的编译器将生成一个二进制文件，该文件在执行加载数据语句、执行其他指令，然后在数据可用时执行加法操作时进行操作。一次只看一行代码的解释器没有足够的信息来生成这种类型的代码；它会从内存中请求数据，等待数据可用，然后执行加法操作。顺便说一句，糟糕的编译器也会做同样的事情，即使是最好的编译器也不能完全防止偶尔需要等待指令执行完成的情况。

因此，解释性代码几乎总是比编译后的代码执行速度慢：编译器对程序有足够的信息，可以对二进制代码进行优化，而解释器无法执行这种优化。

解释性代码确实具有可移植性的优势。为 ARM CPU 编译的二进制文件显然不能在 Intel CPU 上运行。但是使用 Intel Sandy Bridge 处理器的最新 AVX 指令的二进制文件也不能在旧的 Intel 处理器上运行。因此，商业软件通常被编译成针对较旧版本处理器的二进制文件，不会利用可用的最新指令。有各种技巧可以解决这个问题，包括使用包含多个共享库的二进制文件，这些库执行对各种 CPU 版本敏感的性能关键代码。

Java 在这里试图找到一个折中点。Java 应用程序是被编译的，但不是编译成特定 CPU 的特定二进制文件，而是编译成中间低级语言。这种语言（称为 *Java 字节码*）然后由 `java` 二进制文件运行（就像解释性 PHP 脚本由 `php` 二进制文件运行一样）。这使得 Java 具有解释语言的平台独立性。因为它执行的是理想化的二进制代码，所以 `java` 程序能够在代码执行时将代码编译成平台二进制代码。这种编译发生在程序执行时：“即时” 发生。

这种编译仍然受平台依赖性的影响。例如，JDK 8 无法为 Intel Skylake 处理器的最新指令集生成代码，但 JDK 11 可以。我将在 “高级编译器标志” 中详细讨论这个问题。

Java 虚拟机在执行代码时编译的方式是本章的重点。

## 热点编译

如 第一章 中所讨论的那样，本书中讨论的 Java 实现是 Oracle 的 HotSpot JVM。这个名字（HotSpot）来自于它对编译代码的处理方式。在典型的程序中，只有很小一部分代码经常被执行，应用程序的性能主要取决于这些代码段的执行速度。这些关键部分被称为应用程序的热点；代码段被执行得越多，这部分就越热。

因此，当 JVM 执行代码时，并不立即开始编译代码。这有两个基本原因。首先，如果代码只会被执行一次，那么编译它基本上是一种浪费；解释 Java 字节码比编译它们并执行（只执行一次）编译代码更快。

但是，如果所讨论的代码是一个频繁调用的方法或运行多次迭代的循环，那么编译它是值得的：编译代码所需的周期将被多次执行更快的编译代码所节省。这种权衡是编译器首先执行解释代码的原因之一——编译器可以确定哪些方法被频繁调用以足以编译。

第二个原因是优化的一个方面：JVM 执行特定方法或循环的次数越多，它对该代码的了解越多。这使得 JVM 在编译代码时可以进行许多优化。

这些优化（以及影响它们的方法）将在本章后面讨论，但是作为一个简单的例子，考虑 `equals()` 方法。这个方法存在于每个 Java 对象中（因为它从 `Object` 类继承而来），并经常被重写。当解释器遇到语句 `b = obj1.equals(obj2)` 时，它必须查找 `obj1` 的类型（类）以知道要执行哪个 `equals()` 方法。这种动态查找可能需要一些时间。

随着时间的推移，JVM 注意到每次执行这个语句时，`obj1` 的类型是 `java.lang.String`。然后 JVM 可以生成直接调用 `String.equals()` 方法的编译代码。现在代码不仅因为被编译而更快，而且可以跳过调用哪个方法的查找。

事情并不像那么简单；下次执行代码时，`obj1`可能指向除了`String`以外的其他对象。JVM 将创建处理这种可能性的编译代码，其中涉及去优化和重新优化相关代码（你可以在“去优化”中看到一个例子）。尽管如此，总体上在这里生成的编译代码将会更快（至少在`obj1`继续指向`String`的情况下），因为它跳过了执行方法查找的步骤。这种优化只能在运行代码一段时间并观察其行为后才能进行：这是 JIT 编译器等待编译代码段的第二个原因。

# 快速总结

+   Java 旨在充分利用脚本语言的平台独立性和编译语言的本地性能。

+   Java 类文件被编译成一种中间语言（Java 字节码），然后由 JVM 进一步编译成汇编语言。

+   将字节码编译成汇编语言会执行优化，极大地提高了性能。

# 分层编译

曾几何时，JIT 编译器有两种版本，你需要根据你想要使用的编译器安装不同版本的 JDK。这些编译器被称为`客户端`和`服务器`编译器。在 1996 年，这是一个重要的区别；而在 2020 年，已经不再如此。如今，所有发布的 JVM 都包括这两种编译器（尽管在常见用法中，它们通常被称为`服务器`JVM）。

尽管称为服务器 JVM，但客户端和服务器编译器之间的区别仍然存在；JVM 同时可以使用这两种编译器，了解这种区别对理解编译器的工作原理很重要。

历史上，JVM 开发者（甚至一些工具）有时候会用`C1`（编译器 1，客户端编译器）和`C2`（编译器 2，服务器编译器）来称呼这些编译器。现在这些名称更加贴切，因为客户端和服务器计算机之间的区别早已不复存在，所以我们将在全文中沿用这些名称。

这两个编译器之间的主要区别在于它们在编译代码时的积极性。C1 编译器开始编译的时间比 C2 编译器早。这意味着在代码执行的初期阶段，C1 编译器会更快，因为它会编译比 C2 编译器对应更多的代码。

在这里的工程权衡在于 C2 编译器在等待过程中获得的知识：这种知识使得 C2 编译器能够在编译后的代码中做出更好的优化。最终，由 C2 编译器生成的代码将比 C1 编译器生成的代码更快。从用户的角度来看，这种权衡的好处取决于程序运行的时间长短以及程序启动时间的重要性。

当这些编译器是分开的时，显而易见的问题是为什么需要有选择的必要性：JVM 不能从 C1 编译器开始然后在代码变热时切换到 C2 编译器吗？这种技术被称为*分层编译*，现在所有的 JVM 都使用这种技术。可以通过 `-XX:-TieredCompilation` 标志显式禁用它（其默认值为 `true`）；在 “高级编译器标志” 中，我们将讨论这样做的后果。

# 常见的编译器标志

两个常用标志影响 JIT 编译器；我们将在本节中讨论它们。

## 调整代码缓存

当 JVM 编译代码时，它将汇编语言指令集保存在代码缓存中。代码缓存的大小是固定的，一旦填满，JVM 就无法再编译额外的代码了。

如果代码缓存太小，潜在问题显而易见。一些热方法将被编译，但其他方法不会：应用程序最终将运行大量（非常慢的）解释代码。

当代码缓存填满时，JVM 会输出如下警告：

```java
Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full.
         Compiler has been disabled.
Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the
         code cache size using -XX:ReservedCodeCacheSize=
```

有时候会错过这个消息；确定编译器是否停止编译代码的另一种方法是查看稍后本节讨论的编译日志输出。

实际上没有一个很好的机制来确定特定应用程序需要多少代码缓存。因此，当您需要增加代码缓存大小时，这有点是试错操作；一个典型的选项是简单地将默认值加倍或四倍。

代码缓存的最大大小通过 `-XX:ReservedCodeCacheSize=`*`N`* 标志设置（其中 `*N*` 是前述特定编译器的默认值）。代码缓存像 JVM 中的大多数内存一样进行管理：有一个初始大小（由 `-XX:InitialCodeCacheSize=`*`N`*` 指定）。代码缓存大小的分配从初始大小开始，并在缓存填满时增加。代码缓存的初始大小为 2,496 KB，而默认的最大大小为 240 MB。缓存的调整在后台进行，不会真正影响性能，因此通常只需要设置 `ReservedCodeCacheSize` 大小（即设置最大代码缓存大小）。

是否指定一个非常大的最大代码缓存大小，以便永远不会耗尽空间会有什么劣势？这取决于目标机器上的资源。如果指定了 1 GB 的代码缓存大小，JVM 将保留 1 GB 的本机内存。该内存在需要时才分配，但仍然被保留，这意味着您的机器上必须有足够的虚拟内存来满足保留。

此外，如果您仍然拥有一台带有 32 位 JVM 的旧 Windows 机器，则总进程大小不能超过 4 GB。这包括 Java 堆、JVM 本身的所有代码（包括其本机库和线程堆栈）、应用程序分配的任何本机内存（直接或通过新 I/O [NIO] 库）、当然还包括代码缓存。

这些是代码缓存不是无界的原因，有时需要调整大型应用程序的设置。在具有足够内存的 64 位机器上，将值设置得过高不太可能对应用程序产生实际效果：应用程序不会耗尽进程空间内存，并且额外的内存预留通常会被操作系统接受。

在 Java 11 中，代码缓存被分为三部分：

+   非方法代码

+   概要代码

+   非概要代码

默认情况下，代码缓存的大小相同（最高可达 240 MB），您仍然可以使用 `ReservedCodeCacheSize` 标志调整代码缓存的总大小。在这种情况下，非方法代码段根据编译器线程的数量分配空间（参见 “编译线程”）；在具有四个 CPU 的机器上，大约为 5.5 MB。然后，其他两个段等分剩余的总代码缓存——例如，在具有四个 CPU 的机器上，每个段约为 117.2 MB（总计 240 MB）。

您很少需要单独调整这些段，但如果需要，标志如下：

+   `-XX:NonNMethodCodeHeapSize=*N*`：用于非方法代码

+   `-XX:ProfiledCodeHapSize=*N*` 用于概要代码

+   `-XX:NonProfiledCodeHapSize=*N*` 用于非概要代码

代码缓存的大小（以及 JDK 11 段）可以通过使用 `jconsole` 实时监控，并在内存面板上选择内存池代码缓存图表来完成。您还可以按照 第八章 中描述的方式启用 Java 的本地内存跟踪功能。

# 快速摘要

+   代码缓存是一个具有定义的最大大小的资源，影响 JVM 可以运行的编译代码总量。

+   非常大的应用程序可以在其默认配置中使用完整的代码缓存；如果需要，监视代码缓存并增加其大小。

## 检查编译过程

第二个标志并非调优本身：它不会改善应用程序的性能。相反，`-XX:+PrintCompilation` 标志（默认为 `false`）使我们能够看到编译器的工作情况（尽管我们也会查看提供类似信息的工具）。

如果启用了 `PrintCompilation`，每次编译方法（或循环）时，JVM 都会打印一行信息，说明刚刚编译的内容。

编译日志的大多数行具有以下格式：

```java
timestamp compilation_id attributes (tiered_level) method_name size deopt
```

此处的时间戳是编译完成后的时间（相对于 JVM 启动时的时间 0）。

`compilation_id` 是内部任务 ID。通常，这个数字会单调递增，但有时你可能会看到一个无序的 `compilation_id`。这种情况最常见于多个编译线程同时运行，并且表明编译线程相对于彼此的运行速度快慢不一。不过，请不要断定某个特定的编译任务在某种程度上非常慢：这通常只是线程调度的一个功能。

`attributes` 字段是一个由五个字符组成的序列，表示正在编译的代码的状态。如果某个特定属性适用于给定的编译，则会打印下面列表中显示的字符；否则，该属性的空间会打印。因此，五字符属性字符串可以显示为由两个或多个以空格分隔的项。各种属性如下：

`%`

编译是 OSR。

`s`

该方法已同步。

`!`

该方法有一个异常处理程序。

`b`

编译以阻塞模式发生。

`n`

对一个本地方法的包装进行了编译。

这些属性中的第一个是 *on-stack replacement*（OSR）。JIT 编译是一个异步过程：当 JVM 决定某个方法应该被编译时，该方法会被放入一个队列中。而不是等待编译，JVM 接着会继续解释该方法，下次方法被调用时，JVM 将执行方法的编译版本（假设编译已经完成）。

但是考虑一个长时间运行的循环。JVM 将注意到应该编译循环本身，并将该代码排队进行编译。但这还不够：JVM 必须能够在循环仍在运行时开始执行已编译的循环版本——等待循环和封闭方法退出将是低效的（甚至可能根本不会发生）。因此，当循环的代码编译完成时，JVM 替换代码（在栈上），循环的下一次迭代将执行代码的更快编译版本。这就是 OSR。

下面两个属性应该是不言而喻的。在当前版本的 Java 中，默认情况下不会打印阻塞标志；它表示编译未在后台进行（有关更多详细信息，请参阅 “编译线程”）。最后，本地属性表示 JVM 生成了编译代码以便调用本地方法。

如果分层编译已禁用，则下一个字段（`tiered_level`）将为空白。否则，它将是一个表示已完成编译的层级的数字。

接下来是正在编译的方法的名称（或正在为 OSR 编译的循环的方法），打印为 `ClassName::method`。

接下来是代码的`size`（以字节为单位）。这是 Java 字节码的大小，而不是编译后代码的大小（因此，不幸的是，这不能用来预测如何调整代码缓存的大小）。

最后，在某些情况下，编译行末尾的消息将指示发生了某种去优化；这些通常是`made not entrant`或`made zombie`短语。详细信息请参阅“去优化”。

编译日志中可能还包括类似以下的一行：

```java
timestamp compile_id COMPILE SKIPPED: reason
```

此行（带有字面文本`COMPILE SKIPPED`）表示给定方法的编译出现了问题。在两种情况下，这是预期的，具体取决于指定的原因：

代码缓存已满

使用`ReservedCodeCache`标志需要增加代码缓存的大小。

并发类加载

当编译时，该类正在被修改。JVM 将稍后重新编译它；你应该期望在日志中稍后再次看到该方法被重新编译的信息。

在所有情况下（除了填充缓存），应重新尝试编译。如果没有重新尝试，则表示错误阻止了代码的编译。这通常是编译器中的一个错误，但在所有情况下的常规解决方法是将代码重构为编译器可以处理的更简单的东西。

这里是从启用`PrintCompilation`的股票 REST 应用程序中输出的几行：

```java
  28015  850       4     net.sdo.StockPrice::getClosingPrice (5 bytes)
  28179  905  s    3     net.sdo.StockPriceHistoryImpl::process (248 bytes)
  28226   25 %     3     net.sdo.StockPriceHistoryImpl::<init> @ 48 (156 bytes)
  28244  935       3     net.sdo.MockStockPriceEntityManagerFactory$\
                             MockStockPriceEntityManager::find (507 bytes)
  29929  939       3     net.sdo.StockPriceHistoryImpl::<init> (156 bytes)
 106805 1568   !   4     net.sdo.StockServlet::processRequest (197 bytes)
```

此输出仅包括一些与股票相关的方法（并不一定包括特定方法的所有行）。有几点有趣的地方需要注意：第一个这样的方法直到服务器启动后的 28 秒才被编译，而在此之前已编译了 849 个方法。在这种情况下，所有其他方法都是服务器或 JDK 的方法（已从此输出中过滤掉）。服务器启动大约需要 2 秒；在编译任何其他内容之前的剩余 26 秒基本上处于空闲状态，因为应用服务器在等待请求。

其余行用于指出有趣的特征。`process()`方法是同步的，因此属性包括一个`s`。内部类与任何其他类一样编译，并以通常的 Java 命名方式出现在输出中：`outer-classname$inner-classname`。`processRequest()`方法如预期般带有异常处理程序。

最后，回顾一下`StockPriceHistoryImpl`构造函数的实现，其中包含一个大循环：

```java
public StockPriceHistoryImpl(String s, Date startDate, Date endDate) {
    EntityManager em = emf.createEntityManager();
    Date curDate = new Date(startDate.getTime());
    symbol = s;
    while (!curDate.after(endDate)) {
         StockPrice sp = em.find(StockPrice.class, new StockPricePK(s, curDate));
         if (sp != null) {
            if (firstDate == null) {
                firstDate = (Date) curDate.clone();
            }
            prices.put((Date) curDate.clone(), sp);
            lastDate = (Date) curDate.clone();
        }
        curDate.setTime(curDate.getTime() + msPerDay);
    }
}
```

循环执行的频率比构造函数本身高，因此循环会进行 OSR 编译。请注意，该方法的编译需要一段时间；其编译 ID 为 25，但在编译 900 范围内的其他方法后才会出现。 （很容易像这个例子中的 OSR 行一样读成 25%，并想知道其他的 75%，但请记住，数字是编译 ID，%仅表示 OSR 编译。） 这是 OSR 编译的典型情况；栈替换设置更难，但与此同时可以进行其他编译。

## 分层编译级别

使用分层编译的程序的编译日志打印出每个方法编译的层级。在样本输出中，代码编译到级别 3 或 4，尽管到目前为止我们只讨论了两个编译器（加上解释器）。原来有五个编译级别，因为 C1 编译器有三个级别。所以编译的级别如下所示：

0

解释性代码

1

简单的 C1 编译代码

2

有限的 C1 编译代码

3

完全的 C1 编译代码

4

C2 编译代码

典型的编译日志显示，大多数方法首先在级别 3 处进行编译：完全的 C1 编译。（当然，所有方法都从级别 0 开始，但日志中不显示。） 如果方法运行频率足够高，则会在级别 4 进行编译（并且级别 3 代码将变为非输入）。 这是最常见的路径：C1 编译器等待进行编译直到它有关于代码如何使用的信息，可以利用这些信息进行优化。

如果 C2 编译器队列已满，方法将从 C2 队列中取出，并在级别 2 处进行编译，这是 C1 编译器使用调用和反向边计数器的级别（但不需要配置文件反馈）。这样可以更快地编译该方法；在 C1 编译器收集配置文件信息后，该方法稍后将在级别 3 处进行编译，并在 C2 编译器队列较少时最终编译到级别 4。

另一方面，如果 C1 编译器队列已满，计划在级别 3 进行编译的方法可能在等待在级别 3 处编译时就变得适合在级别 4 进行编译。在这种情况下，它会快速编译到级别 2，然后过渡到级别 4。

由于其微不足道的特性，简单的方法可能从级别 2 或 3 开始，但然后因为其微不足道而进入级别 1。如果由于某种原因 C2 编译器无法编译代码，它也会进入级别 1。当代码被取消优化时，它会回到级别 0。

标志控制某些行为，但期望在此级别进行调整时获得结果是乐观的。性能的最佳情况发生在方法按预期编译的情况下：tier 0 → tier 3 → tier 4。如果方法经常编译成 tier 2 并且有额外的 CPU 周期可用，考虑增加编译器线程的数量；这将减少 C2 编译器队列的大小。如果没有额外的 CPU 周期可用，则唯一能做的就是尝试减少应用程序的大小。

## 取消优化

输出`PrintCompilation`标志的讨论提到编译器取消优化代码的两种情况。*取消优化*意味着编译器必须“撤销”先前的编译。这会导致应用程序的性能降低，至少直到编译器重新编译相关代码为止。

取消优化发生在两种情况下：当代码成为非入口时和当代码变成僵尸时。

### 非入口代码

有两件事导致代码成为非入口。一个是由于类和接口的工作方式，另一个是分层编译的实现细节。

让我们看看第一个案例。回想一下股票应用程序有一个名为`StockPriceHistory`的接口。在示例代码中，这个接口有两个实现：一个基本实现（`StockPriceHistoryImpl`）和一个添加日志记录（`StockPriceHistoryLogger`）的实现。在 REST 代码中，所使用的实现基于 URL 的`log`参数：

```java
StockPriceHistory sph;
String log = request.getParameter("log");
if (log != null && log.equals("true")) {
    sph = new StockPriceHistoryLogger(...);
}
else {
    sph = new StockPriceHistoryImpl(...);
}
// Then the JSP makes calls to:
sph.getHighPrice();
sph.getStdDev();
// and so on
```

如果调用一堆`http://localhost:8080/StockServlet`（即没有`log`参数），编译器将会发现`sph`对象的实际类型是`StockPriceHistoryImpl`。然后会内联代码并根据此知识执行其他优化。

后来，假设调用了`http://localhost:8080/StockServlet?log=true`。现在编译器对`sph`对象类型的假设是错误的；先前的优化不再有效。这会生成一个取消优化陷阱，并且之前的优化将被丢弃。如果启用了大量带日志的附加调用，JVM 将快速重新编译该代码并进行新的优化。

针对该场景的编译日志将包含以下行：

```java
 841113   25 %           net.sdo.StockPriceHistoryImpl::<init> @ -2 (156 bytes)
                                 made not entrant
 841113  937  s          net.sdo.StockPriceHistoryImpl::process (248 bytes)
                                 made not entrant
1322722   25 %           net.sdo.StockPriceHistoryImpl::<init> @ -2 (156 bytes)
                                 made zombie
1322722  937  s          net.sdo.StockPriceHistoryImpl::process (248 bytes)
                                 made zombie
```

请注意，OSR 编译的构造函数和标准编译的方法都已被标记为非入口，并且在稍后的某个时候它们将变成僵尸。

取消优化听起来像是一件坏事，至少在性能方面是这样，但并不一定如此。表 4-1 显示了 REST 服务器在取消优化场景下达到的每秒操作数。

表 4-1\. 在取消优化的服务器吞吐量

| 场景 | OPS |
| --- | --- |
| 标准实现 | 24.4 |
| 取消优化后的标准实现 | 24.4 |
| 日志实现 | 24.1 |
| 混合实现 | 24.3 |

标准实现将给出 24.4 次/秒。假设在那个测试之后立即运行一个测试，触发了 `StockPriceHistoryLogger` 路径——这是产生刚刚列出的去优化示例的场景。`PrintCompilation` 的完整输出显示，当开始请求日志记录实现时，`StockPriceHistoryImpl` 类的所有方法都被去优化了。但是在去优化之后，如果重新运行使用 `StockPriceHistoryImpl` 实现的路径，那段代码将会被重新编译（带有稍微不同的假设），我们仍然会看到约 24.4 次/秒（在另一个预热期之后）。

当然，这是最好的情况。如果调用交错，以至于编译器永远无法真正假设代码将采取哪条路径呢？由于额外的日志记录，包含日志记录的路径通过服务器获得约 24.1 次/秒。如果操作混合，我们会得到约 24.3 次/秒：这几乎符合平均值的预期。因此，除了瞬间处理陷阱的时间点外，去优化没有对性能产生任何显著影响。

可能导致代码变为非入口的第二个因素是分层编译的工作方式。当代码由 C2 编译器编译时，JVM 必须替换已由 C1 编译器编译的代码。它通过将旧代码标记为非入口，并使用相同的去优化机制来替换新编译的（更有效的）代码来实现这一点。因此，当使用分层编译运行程序时，编译日志将显示大量被标记为非入口的方法。不要惊慌：实际上，这种“去优化”使得代码运行更快。

检测这一点的方法是注意编译日志中的层级等级：

```java
  40915   84 %     3       net.sdo.StockPriceHistoryImpl::<init> @ 48 (156 bytes)
  40923 3697       3       net.sdo.StockPriceHistoryImpl::<init> (156 bytes)
  41418   87 %     4       net.sdo.StockPriceHistoryImpl::<init> @ 48 (156 bytes)
  41434   84 %     3       net.sdo.StockPriceHistoryImpl::<init> @ -2 (156 bytes)
                                      made not entrant
  41458 3749       4       net.sdo.StockPriceHistoryImpl::<init> (156 bytes)
  41469 3697       3       net.sdo.StockPriceHistoryImpl::<init> (156 bytes)
                                      made not entrant
  42772 3697       3       net.sdo.StockPriceHistoryImpl::<init> (156 bytes)
                                      made zombie
  42861   84 %     3       net.sdo.StockPriceHistoryImpl::<init> @ -2 (156 bytes)
                                      made zombie
```

在这里，构造函数首先在第 3 级进行 OSR 编译，然后也在第 3 级进行完全编译。一秒钟后，OSR 代码有资格进行第 4 级编译，因此它在第 4 级进行编译，并且第 3 级的 OSR 代码被标记为非入口。然后，相同的过程发生在标准编译中，并且最终第 3 级代码变成了僵尸。

### 去优化僵尸代码

当编译日志报告它已经生成僵尸代码时，它是在说它已经回收了以前被标记为非入口的代码。在前面的示例中，在使用 `StockPriceHistoryLogger` 实现运行测试之后，`StockPriceHistoryImpl` 类的代码被标记为非入口。但 `StockPriceHistoryImpl` 类的对象仍然存在。最终，所有这些对象都被 GC 回收。当发生这种情况时，编译器注意到该类的方法现在有资格被标记为僵尸代码。

对于性能来说，这是一件好事。请记住，编译后的代码存储在固定大小的代码缓存中；当识别出僵尸方法时，相关代码可以从代码缓存中删除，为其他类编译腾出空间（或者限制 JVM 以后需要分配的内存量）。

可能的缺点是，如果类的代码变成僵尸，然后稍后重新加载并且再次大量使用，JVM 将需要重新编译和重新优化代码。不过，在先前的场景中确实发生了这种情况，在没有记录日志、有记录日志、再次没有记录日志的情况下运行测试；在这种情况下，性能并没有明显受到影响。一般来说，当僵尸代码重新编译时发生的小型重新编译不会对大多数应用程序产生可测量的影响。

# 快速总结

+   获得查看代码编译方式的最佳方法是启用 `PrintCompilation`。

+   通过启用 `PrintCompilation` 输出的信息可以用来确保编译按预期进行。

+   分层编译可以在两个编译器中的五个不同级别上操作。

+   去优化是 JVM 替换先前编译代码的过程。这通常发生在 C2 代码替换 C1 代码的情况下，但也可以因应用程序执行配置文件的变化而发生。

# 高级编译器标志

本节介绍了一些影响编译器的其他标志。主要是为了更好地理解编译器的工作原理；一般情况下，不应该使用这些标志。另一方面，它们包含在此处的另一个原因是它们曾经足够常见，以至于广泛使用，因此如果你遇到它们并想知道它们的作用，本节应该能解答这些问题。

## 编译阈值

这一章在定义触发代码编译的内容上有些模糊。主要因素是代码执行的频率；一旦代码执行达到一定次数，其编译阈值就会达到，编译器认为有足够的信息来编译代码。

调整会影响这些阈值。然而，本节的真正目的是为了让您更好地了解编译器的工作方式（并引入一些术语）；在当前的 JVM 中，调整阈值实际上从未有意义过。

编译基于 JVM 中的两个计数器：方法被调用的次数和方法中任何循环返回的次数。*循环返回* 实际上可以被视为循环完成执行的次数，无论是因为它到达了循环本身的结尾，还是因为它执行了像 `continue` 这样的分支语句。

当 JVM 执行 Java 方法时，它会检查这两个计数器的总和，并决定该方法是否有资格进行编译。如果有，该方法将被排队等待编译（详见“编译线程”了解更多排队详情）。这种编译没有官方名称，但通常被称为*标准* *编译*。

同样地，每当一个循环完成执行时，分支计数器都会递增和检查。如果分支计数器超过了其单独的阈值，该循环（而不是整个方法）就有资格进行编译。

调整会影响这些阈值。当停用分层编译时，标准编译由`-XX:CompileThreshold=`*`N`* 标志的值触发。*`N`* 的默认值为 10,000。改变`CompileThreshold`标志的值将导致编译器选择比通常更早（或更晚）编译代码。不过，请注意，虽然这里有一个标志，但阈值是通过添加回边循环计数器的总和加上方法入口计数器来计算的。

您经常会找到建议修改`CompileThreshold`标志的建议，并且一些 Java 基准测试的出版物在此标志之后使用它（例如，通常在 8,000 次迭代之后）。一些应用程序仍然默认设置了该标志。

但请记住，我说这个标志在停用分层编译时有效——这意味着当分层编译启用时（通常是这样），这个标志根本不起作用。实际上，使用这个标志只是从 JDK 7 及之前的日子保留下来的一个习惯。

这个标志曾经推荐使用有两个原因：首先，将其降低可以改善使用 C2 编译器的应用程序的启动时间，因为代码会更快地被编译（通常效果相同）。其次，它可能导致一些本来不会被编译的方法被编译。

最后一点是一个有趣的特点：如果一个程序永远运行下去，我们是否期望其所有代码最终都会被编译？事实并非如此，因为编译器使用的计数器会随着方法和循环的执行而增加，但它们也会随着时间的推移而减少。定期（具体来说，当 JVM 达到安全点时），每个计数器的值都会减少。

从实际角度来看，这意味着这些计数器是方法或循环*最近*热度的相对度量。一个副作用是，即使是经常执行的代码也可能永远不会被 C2 编译器编译，即使是那些永远运行的程序也是如此。这些方法有时被称为*温暖的*（与热的相对）。在分层编译之前，这是减少编译阈值有益的一个案例。

然而，即使是温和的方法现在也将被编译，尽管如果我们能让它们由 C2 编译器而不是 C1 编译器编译，可能会稍微改进一点点。实际上没有太多实际好处，但如果你真的感兴趣，可以尝试修改标志`-XX:Tier3InvocationThreshold=*N*`（默认 200）以更快地让 C1 编译方法，并且`-XX:Tier4InvocationThreshold=*N*`（默认 5000）以更快地让 C2 编译方法。类似的标志也适用于后向边界阈值。

# 快速总结

+   方法（或循环）编译的阈值通过可调参数设置。

+   没有分层编译时，调整这些阈值有时是有意义的，但是有了分层编译，不再建议进行此调整。

## 编译线程

“编译阈值”提到，当一个方法（或循环）符合编译条件时，它将被加入编译队列。这个队列由一个或多个后台线程处理。

这些队列不是严格的先进先出；调用计数更高的方法具有优先级。因此，即使程序开始执行并有大量代码需要编译，这种优先级排序也确保最重要的代码首先被编译。（这也是为什么`PrintCompilation`输出中的编译 ID 可能会出现顺序混乱的另一个原因。）

C1 和 C2 编译器有不同的队列，每个队列由（可能是多个）不同的线程处理。线程数基于复杂的对数公式，但表 4-2 列出了详细信息。

表 4-2。分层编译的默认 C1 和 C2 编译器线程数

| CPU | C1 线程 | C2 线程 |
| --- | --- | --- |
| 1 | 1 | 1 |
| 2 | 1 | 1 |
| 4 | 1 | 2 |
| 8 | 1 | 2 |
| 16 | 2 | 6 |
| 32 | 3 | 7 |
| 64 | 4 | 8 |
| 128 | 4 | 10 |

通过设置`-XX:CICompilerCount=`*`N`*标志可以调整编译器线程数。这是 JVM 用于处理队列的总线程数；对于分层编译，将使用三分之一（但至少一个）来处理 C1 编译器队列，其余的线程（但至少一个）将用于处理 C2 编译器队列。该标志的默认值是前述表格两列的总和。

如果禁用分层编译，只启动给定数量的 C2 编译器线程。

何时考虑调整此值？因为默认值基于 CPU 数量，这是一种情况，即在 Docker 容器内运行较旧版本的 JDK 8 可能会导致自动调整出现问题。在这种情况下，您需要根据 Docker 容器分配的 CPU 数量手动设置此标志到期望值（使用表 4-2 中的目标作为指导）。

同样，如果程序在单 CPU 虚拟机上运行，并且只有一个编译器线程可能会稍微有利：有限的 CPU 可用，并且较少的线程争夺该资源将在许多情况下有助于性能。然而，该优势仅限于初始热身期；之后，要编译的合格方法数量实际上不会导致对 CPU 的争用。当在单 CPU 机器上运行股票批处理应用程序并且编译器线程数限制为一个时，初始计算速度大约快 10%（因为它们不必经常竞争 CPU）。运行的迭代越多，该初始利益的整体效果越小，直到所有热方法都被编译，这种利益被消除。

另一方面，线程数量可能会轻易地压倒系统，特别是如果同时运行多个 JVM（每个 JVM 将启动许多编译线程）。在这种情况下减少线程数量可以帮助整体吞吐量（尽管可能会导致热身期持续时间更长的可能成本）。

如果有大量额外的 CPU 循环可用，那么在理论上，当编译线程的数量增加时，程序将受益——至少在其热身期间会受益。在现实生活中，这种好处几乎是难以获得的。此外，如果所有这些多余的 CPU 可用，你最好尝试利用整个应用程序执行过程中可用的 CPU 循环（而不仅仅是在开始时编译更快）。

应用于编译线程的另一个设置是 `-XX:+BackgroundCompilation` 标志的值，默认为 `true`。这意味着队列按照刚才描述的方式异步处理。但是该标志可以设置为 `false`，在这种情况下，当一个方法有资格进行编译时，希望执行它的代码将等待直到它实际编译为止（而不是继续在解释器中执行）。当指定 `-Xbatch` 时，后台编译也会被禁用。

# 快速总结

+   对于放置在编译队列上的方法，编译是异步进行的。

+   队列并不是严格有序的；热方法在编译日志中编译前于其他方法是另一个原因。

## 内联

编译器进行的最重要的优化之一是方法内联。遵循良好面向对象设计的代码通常包含通过 getter（可能还有 setter）访问的属性：

```java
public class Point {
    private int x, y;

    public void getX() { return x; }
    public void setX(int i)  { x = i; }
}
```

调用方法的开销非常高，特别是与方法内代码量相比。事实上，在 Java 早期，性能优化经常反对这种封装方式，因为所有这些方法调用对性能的影响很大。幸运的是，JVM 现在常规地对这类方法执行代码内联。因此，你可以这样编写这段代码：

```java
Point p = getPoint();
p.setX(p.getX() * 2);
```

编译后的代码本质上会执行以下操作：

```java
Point p = getPoint();
p.x = p.x * 2;
```

内联默认启用。可以使用`-XX:-Inline`标志禁用它，尽管这种内联是如此重要的性能提升，你实际上永远不会这样做（例如，禁用内联会使库存批处理测试的性能降低超过 50%）。但是，由于内联如此重要，也许是因为我们有许多其他可调整的参数，建议经常关于调整 JVM 内联行为。

不幸的是，没有基本的方式可以查看 JVM 如何内联代码。如果你从源代码编译 JVM，可以生成包含`-XX:+PrintInlining`标志的调试版本。该标志提供有关编译器做出的所有内联决策的各种信息。最好的办法是查看代码的概要，并且如果任何简单方法位于概要的顶部，并且看起来应该被内联，则尝试使用内联标志进行实验。

是否内联方法的基本决定取决于其热度和大小。JVM 确定方法是否热（即经常调用）基于内部计算；它不直接受任何可调参数的影响。如果一个方法因为频繁调用而有资格内联，它只有在其字节码大小小于 325 字节（或者指定为`-XX:MaxFreqInlineSize=`*`N`*标志）时才会被内联。否则，只有当其大小小于 35 字节（或者指定为`-XX:MaxInlineSize=`*`N`*标志）时才有资格内联。

有时你会看到建议增加`MaxInlineSize`标志的值，以便内联更多的方法。这种关系经常被忽视的一个方面是，如果将`MaxInlineSize`值设置得比 35 高，那么当方法首次被调用时可能会被内联。然而，如果该方法经常被调用——在这种情况下其性能更为重要——那么它最终会被内联（假设其大小小于 325 字节）。否则，调整`MaxInlineSize`标志的净效果可能会减少测试所需的预热时间，但长时间运行的应用程序不太可能产生重大影响。

# 快速总结

+   内联是编译器可以进行的最有益的优化，特别是对于良好封装属性的面向对象代码而言。

+   调整内联标志很少需要，建议这样做的往往没有考虑到正常内联和频繁内联之间的关系。在研究内联的影响时，请确保考虑这两种情况。

## 逃逸分析

如果启用了逃逸分析（`-XX:+DoEscapeAnalysis`，默认情况下为`true`），C2 编译器将进行激进的优化。例如，考虑以下与阶乘相关的类：

```java
public class Factorial {
    private BigInteger factorial;
    private int n;
    public Factorial(int n) {
        this.n = n;
    }
    public synchronized BigInteger getFactorial() {
        if (factorial == null)
            factorial = ...;
        return factorial;
    }
}
```

为了在数组中存储前 100 个阶乘值，使用以下代码：

```java
ArrayList<BigInteger> list = new ArrayList<BigInteger>();
for (int i = 0; i < 100; i++) {
    Factorial factorial = new Factorial(i);
    list.add(factorial.getFactorial());
}
```

`factorial`对象只在那个循环内部引用；其他代码永远无法访问该对象。因此，JVM 可以自由地对该对象进行优化：

+   在调用`getFactorial()`方法时，它不需要获取同步锁。

+   它不需要在内存中存储字段`n`；它可以将该值保存在寄存器中。同样，它可以将`factorial`对象引用保存在寄存器中。

+   事实上，它根本不需要分配实际的阶乘对象；它只需要跟踪对象的各个字段。

这种优化很复杂：在这个例子中它足够简单，但即使是更复杂的代码，这些优化也是可能的。根据代码的使用情况，并非所有优化都一定适用。但逃逸分析可以确定哪些优化是可能的，并对已编译的代码进行必要的更改。

逃逸分析默认启用。在极少数情况下，它会出错。这通常不太可能，在当前的 JVM 中确实很少见。尽管如此，由于曾经存在一些知名的 bug，你有时会看到建议禁用逃逸分析。不过，这些建议可能已经不再适用，尽管像所有激进的编译器优化一样，禁用此功能有可能导致代码更稳定。如果你发现确实如此，简化相关代码是最佳方案：更简单的代码编译效果更好。（这确实是一个 bug，应该报告。）

# 快速总结

+   逃逸分析是编译器能够执行的最复杂的优化。这种优化经常导致微基准测试出现问题。

## CPU 特定代码

我之前提到过，JIT 编译器的一个优点是它可以根据运行的位置为不同的处理器生成代码。当然，这假设 JVM 是基于新处理器的知识构建的。

这正是编译器为 Intel 芯片所做的。在 2011 年，Intel 为 Sandy Bridge（及之后的）芯片引入了高级向量扩展（AVX2）。这些指令的 JVM 支持很快跟进。然后在 2016 年，Intel 将其扩展到包括 AVX-512 指令；这些指令出现在 Knights Landing 和后续的芯片上。JDK 8 不支持这些指令，但 JDK 11 支持。

通常情况下，这个特性不是你需要担心的事情；JVM 将检测正在运行的 CPU，并选择适当的指令集。但是，像所有新特性一样，有时候会出现问题。

AVX-512 指令的支持首次出现在 JDK 9 中，尽管默认情况下未启用。在几次误启动之后，默认情况下启用了这些指令，然后又将其禁用。在 JDK 11 中，默认情况下启用了这些指令。然而，从 JDK 11.0.6 开始，默认情况下再次禁用了这些指令。因此，即使在 JDK 11 中，这仍然是一个正在进行中的工作。 （顺便说一句，这并不是 Java 才有的问题；许多程序都在努力正确支持 AVX-512 指令。）

因此，在某些较新的英特尔硬件上运行某些程序时，您可能会发现较早的指令集效果要好得多。那些从新指令集中受益的应用程序通常涉及比 Java 程序更多的科学计算。

这些指令集是通过 `-XX:UseAVX=`*`N`* 参数选择的，其中 *`N`* 如下所示：

0

不使用 AVX 指令。

1

使用 Intel AVX level 1 指令（适用于 Sandy Bridge 及更高版本处理器）。

2

使用 Intel AVX-512 指令（适用于 Knights Landing 及更高版本处理器）。

3

使用 Intel AVX level 2 指令（适用于 Haswell 及更高版本处理器）。

此标志的默认值取决于运行 JVM 的处理器；JVM 将检测 CPU 并选择支持的最高值。Java 8 不支持级别 3，因此在大多数处理器上您将看到使用的值为 2。在新的英特尔处理器上的 Java 11 中，默认情况下在 11.0.5 版本之前使用 3，在后续版本中使用 2。

这就是我在 第一章 中提到的其中一个原因，建议使用 Java 8 或 Java 11 的最新版本，因为这些最新版本中包含了重要的修复。如果必须在最新的英特尔处理器上使用较早的 Java 11 版本，请尝试设置 `-XX:UseAVX=2` 标志，这在许多情况下会提升性能。

谈到代码成熟度：为了完整起见，我将提到 `-XX:UseSSE=*N*` 标志支持 Intel 流式 SIMD 扩展（SSE）1 到 4。这些扩展适用于 Pentium 系列处理器。在 2010 年调整此标志有些合理，因为当时正在处理其所有的使用情况。今天，我们通常可以依赖该标志的稳健性。

# 分层编译的权衡

我已经多次提到当禁用分层编译时，JVM 的工作方式不同。考虑到它提供的性能优势，是否有理由关闭它呢？

其中一个原因可能是在内存受限的环境中运行。当然，您的 64 位机器可能有大量内存，但您可能在具有小内存限制的 Docker 容器中运行，或者在云虚拟机中运行，其内存不足。或者您可能在大型机器上运行数十个 JVM。在这些情况下，您可能希望减少应用程序的内存占用。

第 8 章 提供了关于此的一般建议，但在本节中我们将看看分层编译对代码缓存的影响。

表 4-3 显示了在我的系统上启动 NetBeans 时的结果，该系统有几十个项目将在启动时打开。

Table 4-3\. 分层编译对代码缓存的影响

| 编译器模式 | 编译的类 | 已分配的代码缓存 | 启动时间 |
| --- | --- | --- | --- |
| +TieredCompilation | 22,733 | 46.5 MB | 50.1 秒 |
| -TieredCompilation | 5,609 | 10.7 MB | 68.5 秒 |

C1 编译器编译的类约为四倍，并且根据预测需要大约四倍的代码缓存内存。在本例中节省 34 MB 可能不会产生很大的差异。在编译 200,000 个类的程序中节省 300 MB 在某些平台上可能会有不同的选择。

禁用分层编译会带来什么损失？正如表格所示，我们确实需要更多时间来启动应用程序并加载所有项目类。但在长时间运行的程序中，您期望所有热点都会被编译吗？

在这种情况下，给定足够长的热身时间后，当禁用分层编译时执行速度应该是一样的。表 4-4 展示了我们的库存 REST 服务器在热身时间为 0、60 和 300 秒后的性能。

Table 4-4\. 服务器应用程序的吞吐量（使用分层编译）

| 热身时间 | `-XX:-TieredCompilation` | `-XX:+TieredCompilation` |
| --- | --- | --- |
| 0 秒 | 23.72 | 24.23 |
| 60 秒 | 23.73 | 24.26 |
| 300 秒 | 24.42 | 24.43 |

测量期为 60 秒，因此即使没有预热，编译器也有机会获得足够的信息来编译热点；因此，即使没有预热期，差异也很小。（此外，在服务器启动期间编译了大量代码。）请注意，在最后，分层编译仍能够略微领先（尽管这种优势可能不会引人注目）。我们在讨论编译阈值时已经讨论了这一点：在使用分层编译时，总会有一小部分方法是由 C1 编译器编译而不是由 C2 编译器编译的。

# GraalVM

*GraalVM* 是一种新的虚拟机。它不仅可以运行 Java 代码，当然，还能运行许多其他语言的代码。这款通用虚拟机还能够运行 JavaScript、Python、Ruby、R 以及传统的 JVM 字节码（来自 Java 和其他编译为 JVM 字节码的语言，如 Scala、Kotlin 等）。Graal 有两个版本：完全开源的社区版（CE）和商业版（EE）。每个版本都有支持 Java 8 或 Java 11 的二进制文件。

GraalVM 对 JVM 性能有两个重要贡献。首先，一种附加技术使 GraalVM 能够生成完全的本地二进制文件；我们将在下一节中详细讨论这一点。

其次，GraalVM 可以以常规 JVM 的模式运行，但它包含了一个新的 C2 编译器的实现。这个编译器是用 Java 编写的（与传统的 C2 编译器不同，后者是用 C++ 编写的）。

传统的 JVM 包含 GraalVM JIT 的一个版本，具体取决于 JVM 构建的时间。这些 JIT 发布来自 GraalVM 的 CE 版本，比 EE 版本慢；与直接下载的 GraalVM 版本相比，它们通常也是过时的。

在 JVM 内部，使用 GraalVM 编译器被视为实验性质的，因此要启用它，您需要提供以下标志：`-XX:+UnlockExperimentalVMOptions`、`-XX:+EnableJVMCI` 和 `-XX:+UseJVMCICompiler`。所有这些标志的默认值都是 `false`。

表 4-5 显示了标准 Java 11 编译器、来自 EE 版本 19.2.1 的 Graal 编译器以及嵌入在 Java 11 和 13 中的 GraalVM 的性能。

表 4-5\. Graal 编译器性能

| JVM/compiler | OPS |
| --- | --- |
| JDK 11/Standard C2 | 20.558 |
| JDK 11/Graal JIT | 14.733 |
| Graal 1.0.0b16 | 16.3 |
| Graal 19.2.1 | 26.7 |
| JDK 13/Standard C2 | 21.9 |
| JDK 13/Graal JIT | 26.4 |

这次我们再次测试了我们的 REST 服务器的性能（尽管硬件略有不同，所以基准 OPS 只有 20.5 OPS，而不是 24.4）。

需要注意的是这里的进展情况：JDK 11 使用的是一个相当早期的 Graal 编译器版本，因此该编译器的性能落后于 C2 编译器。虽然 Graal 编译器通过其早期访问版本有所改进，但即使是其最新的早期访问版本（1.0），速度仍然不及标准 VM。然而，2019 年末的 Graal 版本（作为生产版本 19.2.1 发布）大幅提升了性能。JDK 13 的早期访问版本采用了这些较新的构建，即使 C2 编译器自 JDK 11 以来只有轻微改进，Graal 编译器的性能也接近于相同。

# 预编译

我们在本章开始时讨论了即时编译器背后的哲学。尽管它有其优点，但代码在执行之前仍然需要热身时间。如果在我们的环境中，传统的编译模型更好：一个没有额外内存的嵌入式系统，或者一个在有机会热身之前就完成的程序呢？

在这一节中，我们将介绍两个解决方案来应对这种情况的实验性功能。提前编译是标准 JDK 11 的实验性功能，而生成完全本地二进制的能力是 Graal VM 的功能。

## 提前编译

*提前（AOT）编译*首次在 JDK 9 中仅适用于 Linux，但在 JDK 11 中，它适用于所有平台。从性能的角度来看，它仍然是一个正在进行中的工作，但本节将为您提供一个预览。^(1)

AOT 编译允许您提前（或全部）编译应用程序的一部分，然后运行它。这个编译后的代码变成了 JVM 在启动应用程序时使用的共享库。理论上，这意味着 JIT 不必参与，至少在启动应用程序时：您的代码应该最初至少与 C1 编译的代码一样运行，而无需等待该代码被编译。

在实践中，情况有所不同：应用程序的启动时间受到共享库大小的影响（因此，将该共享库加载到 JVM 中所需的时间）。这意味着像“Hello, world”这样的简单应用程序在使用 AOT 编译时不会运行得更快（实际上，根据对共享库进行预编译的选择，可能会运行得更慢）。AOT 编译的目标是针对启动时间相对较长的 REST 服务器之类的应用程序。这样，加载共享库的时间被长启动时间抵消，并且 AOT 产生了好处。但请记住，AOT 编译是一个实验性功能，随着技术的发展，较小的程序可能会从中受益。

要使用 AOT 编译，您可以使用`jaotc`工具来生成包含所选编译类的共享库。然后，通过运行时参数将该共享库加载到 JVM 中。

`jaotc`工具有几个选项，但要生成最佳的库，最好的方式是这样的：

```java
$ jaotc --compile-commands=/tmp/methods.txt \
    --output JavaBaseFilteredMethods.so \
    --compile-for-tiered \
    --module java.base
```

这个命令将使用一组编译命令在给定的输出文件中生成*java.base*模块的编译版本。您可以选择对模块进行 AOT 编译，就像我们这里所做的那样，或者对一组类进行编译。

加载共享库的时间取决于其大小，这取决于库中方法的数量。您还可以加载多个共享库，预先编译不同的代码部分，这可能更容易管理，但性能相同，因此我们将专注于一个单独的库。

虽然您可能会尝试预编译所有内容，但如果您仅明智地预编译代码的子集，您将获得更好的性能。这就是为什么建议仅编译*java.base*模块的原因。

编译命令（在此示例中的*/tmp/methods.txt*文件中）还用于限制编译到共享库中的数据。该文件包含看起来像这样的行：

```java
compileOnly java.net.URI.getHost()Ljava/lang/String;
```

此行告诉`jaotc`在编译`java.net.URI`类时，应仅包括`getHost()`方法。我们可以有其他行引用该类的其他方法，以便包括它们的编译；最终，文件中列出的方法将作为共享库的一部分包括进去。

要创建编译命令列表，我们需要应用程序实际使用的每种方法的列表。为此，我们这样运行应用程序：

```java
$ java -XX:+UnlockDiagnosticVMOptions -XX:+LogTouchedMethods \
      -XX:+PrintTouchedMethodsAtExit <other arguments>
```

当程序退出时，它将以如下格式打印程序中使用的每个方法的行：

```java
java/net/URI.getHost:()Ljava/lang/String;
```

要生成*methods.txt*文件，请保存这些行，每行前面加上`compileOnly`指令，并删除方法参数之前的冒号。

被`jaotc`预编译的类将使用 C1 编译器的一种形式，因此在长时间运行的程序中，它们将无法进行最优化编译。因此，我们最终需要的选项是`--compile-for-tiered`。该选项安排共享库使其方法仍然有资格由 C2 编译器编译。

如果您正在为一个短期运行的程序使用 AOT 编译，可以不带这个参数，但请记住目标是一个服务器应用程序。如果我们不允许预编译方法有资格进行 C2 编译，服务器的热性能将比最终可能的性能慢。

或许并不奇怪，如果您使用启用分层编译的库运行应用程序，并使用`-XX:+PrintCompilation`标志，您将看到我们之前观察到的相同代码替换技术：AOT 编译将出现在输出中的另一层，并且您将看到 AOT 方法变为非入口并在 JIT 编译它们时替换。

一旦库创建完成，您可以像这样与应用程序一起使用它：

```java
$ java -XX:AOTLibrary=/path/to/JavaBaseFilteredMethods.so <other args>
```

如果您希望确保库正在使用，请在 JVM 参数中包含`-XX:+PrintAOT`标志；该标志默认为`false`。像`-XX:+PrintCompilation`标志一样，`-XX:+PrintAOT`标志将在 JVM 使用预编译方法时生成输出。典型的行如下所示：

```java
    373  105     aot[ 1]   java.util.HashSet.<init>(I)V
```

这里的第一列是自程序启动以来的毫秒数，所以直到`HashSet`类的构造函数从共享库加载并开始执行为止，花费了 373 毫秒。第二列是分配给该方法的 ID，第三列告诉我们该方法从哪个库加载的。索引（本例中为 1）也由此标志打印：

```java
18    1     loaded    /path/to/JavaBaseFilteredMethods.so  aot library
```

*JavaBaseFilteredMethods.so* 是此示例中加载的第一个（也是唯一的）库，因此其索引为 1（第二列），随后对具有该索引的`aot`的引用指的是此库。

## GraalVM 本地编译

AOT 编译对于相对较大的程序是有益的，但对于小型、快速运行的程序则没有帮助（甚至可能有害）。这是因为它仍然是一个实验性功能，并且因为其架构需要 JVM 加载共享库。

另一方面，GraalVM 可以生成无需 JVM 运行的完整本地可执行文件。这些可执行文件非常适合短期程序。如果你运行了示例，你可能会注意到某些事物（如被忽略的错误）中对 GraalVM 类的引用：AOT 编译使用 GraalVM 作为其基础。这是 GraalVM 的早期采用者功能；它可以在具有适当许可证的情况下用于生产，但不受保证。

GraalVM 生成的二进制文件启动速度相当快，特别是与在 JVM 中运行的程序相比。然而，在此模式下，GraalVM 不像 C2 编译器那样积极地优化代码，因此，对于足够长时间运行的应用程序，传统的 JVM 最终会胜出。与 AOT 编译不同，GraalVM 本地二进制文件不会在执行期间使用 C2 编译器编译类。

类似地，由 GraalVM 生成的本地程序的内存占用空间开始时比传统的 JVM 显着较小。然而，当程序运行并扩展堆时，这种内存优势会逐渐消失。

也存在一些限制，限制了可以在编译成本地代码的程序中使用的 Java 特性。这些限制包括以下内容：

+   动态类加载（例如，通过调用`Class.forName()`）。

+   终结器。

+   Java 安全管理器。

+   JMX 和 JVMTI（包括 JVMTI 分析）。

+   使用反射通常需要特殊的编码或配置。

+   使用动态代理通常需要特殊的配置。

+   使用 JNI 需要特殊的编码或配置。

通过使用 GraalVM 项目的演示程序，我们可以看到所有这些内容在实际中的应用，该程序递归地计算目录中的文件数。随着要计数的文件数量增加，由 GraalVM 生成的本地程序非常小且速度快，但随着更多工作的完成和 JIT 的启动，传统的 JVM 编译器会生成更好的代码优化，并且速度更快，正如我们在 表 4-6 中看到的。

表 4-6\. 使用本地和 JIT 编译代码计算文件所需的时间

| 文件数量 | Java 11.0.5 | 本地应用程序 |
| --- | --- | --- |
| 7 | 217 ms (36K) | 4 ms (3K) |
| 271 | 279 ms (37K) | 20 ms (6K) |
| 169,000 | 2.3 s (171K) | 2.1 s (249K) |
| 1.3 million | 19.2 s (212K) | 25.4 s (269K) |

这里的时间是计算文件数的时间；运行完成时的总占用空间（在括号中测量）列在括号中。

当然，GraalVM 本身正在快速发展，其本地代码中的优化也有望随着时间的推移而改善。

# 摘要

本章包含了关于编译器工作原理的大量背景知识。这样做是为了能够理解第一章中关于小方法和简单代码的一些总体建议，以及第二章中描述的编译器对微基准测试的影响。特别是：

+   不要害怕小方法——特别是 getter 和 setter，因为它们很容易被内联。如果您觉得方法开销可能很大，理论上您是正确的（我们展示了去除内联显著降低性能）。但在实践中并非如此，因为编译器解决了这个问题。

+   需要编译的代码位于编译队列中。队列中的代码越多，程序达到最佳性能所需的时间就越长。

+   虽然您可以（而且应该）调整代码缓存的大小，但它仍然是有限资源。

+   代码越简单，可以执行的优化就越多。性能分析反馈和逃逸分析可以产生更快的代码，但复杂的循环结构和大方法限制了它们的有效性。

最后，如果您分析您的代码并发现一些意外出现在性能分析榜首的方法——这些方法您认为不应该在那里——您可以使用这里的信息来查看编译器正在执行的操作，并确保它能够处理代码编写方式。

^(1) AOT 编译的一个好处是更快的启动速度，但应用程序类数据共享在启动性能方面至少目前更有利，并且是一个完全支持的特性；有关更多详情，请参阅“类数据共享”。
